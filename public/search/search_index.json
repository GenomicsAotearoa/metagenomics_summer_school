{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hi! \u00b6","title":"Hi!"},{"location":"#hi","text":"","title":"Hi!"},{"location":"day1/ex1_bash_scripting/","text":"Introduction to the command line \u00b6 Objectives \u00b6 Navigating your file system Copying, Moving, Renaming and Removing files Examining file contents Redirection, manipulation and extraction Text and file manipulation Loops Shell scripting Moving files between your laptop and NeSI Introduction to slurm Navigating your file system \u00b6 Log in to NeSI and find the current location by running the command pwd which stands for print working directory . At any given time, current working directory is current default directory. pwd # /home/UserName/ We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\". ls Navigating to the MGSS_Intro/ directory can be done with the cd command which stands for change directory . cd MGSS_Intro/ Run the ls command to list the contents of the current directory. Check whether there are two .fastq files. The mkdir command ( make directory ) is used to make a directory. Enter mkdir followed by a space, then the directory name you want to create mkdir backup/ Copying, Moving, Renaming and Removing files \u00b6 Make a second copy of SRR097977.fastq and rename it as Test_1_backup.fastq . Then move that file to backup/ directory. cp SRR097977.fastq Test_1_backup.fastq mv Test_1_backup.fastq backup Navigate to backup/ directory and use mv command to rename and move Test_1_backup.fastq as Test_1_copy.fastq to the directory immediately above. cd backup/ mv Test_1_backup.fastq ../Test_1_copy.fastq Return to the directory immediately above, check whether the Test_1_copy.fastq was moved and renamed as instructed and remove it by using the rm command. cd .. rm Test_1_copy.fastq See whether you can remove the backup/ directory by using the rm command as well. rm backup/ # rm : can not remove 'backup/': Is a directory By default, rm will not delete directories. This can be done by using -r (recursive) option. rm -r backup Examining file contents \u00b6 There are a number of ways to examine the content of a file. cat and less are two commonly used programs for a quick look. Check the content of SRR097977.fastq by using these commands. Take a note of the differences. cat SRR097977.fastq # less SRR097977.fastq A few useful shortcuts for navigating in less There are ways to take a look at parts of a file. For example, the head and tail commands will scan the beginning and end of a file, respectively. head SRR097977.fastq tail SRR097977.fastq Adding -n option to either of these commands will print the first or last n lines of a file. head -n 1 SRR097977.fastq # @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36 Redirection, manipulation and extraction \u00b6 Although using cat and less commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is grep . Let's use this command to search for the string NNNNNNNNNN in SRR098026.fastq file. grep NNNNNNNNNN SRR098026.fastq Retrieve and discuss the output you get when grep was executed with the -B1 and -A1 flags. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is > . Redirecting the string of bad reads that was searched using the grep command to a file named bad_reads.txt can be done with grep -B1 -A2 NNNNNNNNNN SRR098026.fastq > bad_reads.txt Use the wc command to count the number of words, lines and characters in the bad_reads.txt file. wc bad_reads.txt Add -l flag to wc command and compare the number with the above output wc -l bad_reads.txt In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of >> (append redirect) command which will append the new output to the end of the file, rather than overwriting it. grep -B1 -A2 NNNNNNNNNN SRR097977.fastq >> bad_reads.txt Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards , which are symbols or special characters that represent other characters. For an example. Using * wildcard, we can run the previous grep command on both files at the same time. grep -B1 -A2 NNNNNNNNNN *.fastq >> bad_reads.txt wc -l bad_reads.txt The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The | (pipe) command is a commonly used method to apply an operation for an ouput without creating intermediate files. It takes the output generated by one command and uses it as the input to another command. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l Text and file manipulation \u00b6 There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below. Cut \u00b6 The cut command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that cut uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the -d parameter. See what results you get from the file names.txt . cat names.txt cut -d \" \" -f 1 names.txt cut -d \" \" -f 1 -3 names.txt cut -d \" \" -f 1 ,3 names.txt basename \u00b6 basename is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use basename to remove the .fastq extension from the files that we've been working with. basename SRR097977.fastq .fastq sed \u00b6 sed is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the sed command in UNIX is for substitution or for find and replace. By using sed you can edit files even without opening them, which is extremely important when working with large files. View the contents of the animals.txt file using cat . cat animals.txt We will now use sed to make some replacements to the text in this file. Example - Replacing/substituting a string \u00b6 sed is mostly used to replace the text in a file. In the following example, sed replaces the word 'dogs' with 'cats' in the file. sed 's/dogs/cats/' animals.txt Here the s specifies the substitution operation. The / characters are delimiters. The dogs is the search pattern and the cats is the replacement string. By default, the sed command replaces the first occurrence of the pattern in each line and it won't replace additional occurrences in the line. Example - Replacing all occurrences \u00b6 The substitute flag g (global replacement) can be added to the command to replace all the occurrences of the string in the line. sed 's/dogs/cats/g' animals.txt Example - Deleting a line \u00b6 To delete a particular line, we can specify the line number followed by the d character. For example sed '1d' animals.txt # Delete the first line sed '2d' animals.txt # Delete the second line sed '$d' animals.txt # Delete the last line Loops \u00b6 Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in bash scripting, for - iterates over a list of items and performs the given set of commands for item in [LIST] do [COMMANDS] done while - Performs a given set of commands an unknown number of times as long as the given condition evaluates is true while [CONDITION] do [COMMANDS] done until - Execute a given set of commands as longs as the given condition evaluates to false For most of our uses, a for loop is sufficient for our needs, so that is what we will be focusing on for this exercise. Shell identifies the for command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the do and done keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the $ character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example i = \"DAVE WAS HERE\" echo i # i echo $i # DAVE WAS HERE echo ${ i } # DAVE WAS HERE This prevents the shell interpreter from treating i as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files: for filename in *.fastq do head -n 2 ${filename} done Another useful command to be used with for loops is basename which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed). basename SRR097977.fastq .fastq basename is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things for filename in *.fastq do name=$(basename ${filename} .fastq) echo ${name} done Scripts \u00b6 Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal. The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as nano , emacs , and vi provide the platforms to write scripts. For this workshop we will use nano to create a file named ForLoop.sh . nano ForLoop.sh Add the following for-loop to the script (note the header #!/bin/bash ). #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } done Because nano is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing Ctrl + O , then exit nano using Ctrl + X . If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word . Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. bash ForLoop.sh Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the ls -l ForLoop.sh command and evaluate the first part of the output ls -l ForLoop.sh # -rw-rw-r-- 1 user user 88 Dec 6 19:52 ForLoop.sh There are three file permission flags that a file we create on NeSI can possess. Two of these, the read ( r ) and write ( w ) are marked for the ForLoop.sh file .The third flag, executable ( x ) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using chmod command. Add the executable permissions ( +x ) to ForLoop.sh and run ls again to see what has changed. chmod +x ForLoop.sh ls -l ForLoop.sh # -rwxrwxr-x 1 user user 88 Dec 6 19:52 ForLoop.sh Re-open the file in nano and append the output to TwoLines.txt , save and exit #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } >> TwoLines.txt done Execute the file ForLoop.sh . We'll need to put ./ at the beginning so the computer knows to look here in this directory for the program. ./ForLoop.sh Moving Files between your laptop and remote cluster/machine \u00b6 There are multiple commands and tools to move files between your laptop and remote clusters/machines. scp and rsync are some of the commands, and Globus and Cyberduck are some of these tools. scp is commonly used, as this is a simple to use tool that makes use of the ssh configuration we have already established for connecting to NeSI. In order it use it error free, we need to pay attention to whether the file is moving FROM or TO the remote cluster/machine, absolute paths, relative paths, Local vs Remote, etc. The format of the scp command is: scp path/to/copy/from path/to/copy/to If you wish to copy whole directories, include the -r flag: scp -r path/to/copy/from path/to/copy/to Finally, include mahuika: or ga-vl01: at the start of the remote machine file path (in this case, NeSI) to identify that this path is located within the remote cluster/machine. NOTE: the following templates are written in a way where the commands are to be **executed from local* ; i.e. a terminal not logged in to NeSI. On some systems, it may be preferable to open one terminal, log in to NeSI in that terminal, and then open a new terminal (that is not logged into NeSI) to perform the scp command. This avoids having to provide your NeSI log in details each time you wish to copy something.* FROM local TO remote \u00b6 scp /path/from/local/ ga-vl01:/path/to/remote/ FROM remote TO local \u00b6 scp ga-vl01:/path/from/remote/ /path/to/local/ If the ~/.ssh/config is not set with aliases you will need to replace the shortcut ga-vl01 with the full address of the remote: scp -oProxyCommand=\"ssh -W %h:%p user@lander.nesi.org.nz\" /path/in/local/ user@ga-vl01.mahuika.nesi.org.nz:/path/to/remote/ NOTE: If you are already comfortable with using scp , you are welcome to use this. However, for the purposes of this workshop, the relevant example files are available for download from the workshop main page . Alternatively, if you are working within the NeSI Jupyter hub , you can download individual files by right clicking on the file in the navigation pane on the left and clicking download . Introduction to slurm \u00b6 Jobs running on NeSI are submitted in the form of a batch script containing the code you want to run and a header of information needed by a job scheduler. All NeSI systems use the slurm batch scheduler for the submission, control and management of user jobs. Slurm provides a rich set of features for organising your workload and an extensive array of tools for managing your resource usage. In most cases, you need to know the commands: Copy the contents of the BLAST/ folder to your current directory, using the following command cp -r /nesi/nobackup/nesi02659/SLURM/BLAST ./ We will then navigate into this directory with the cd command, and inspect the text of the file blast-test.sh using less or nano . cd BLAST/ less blast-test.sh Evaluate the contents of the blast-test.sh script. Take a note of the basic slurm variables, path variables, etc. We will revisit these in the afternoon, when you create your own slurm scripts. Submit the script to the job queue as below. sbatch blast-test.sh","title":"Introduction to the command line"},{"location":"day1/ex1_bash_scripting/#introduction-to-the-command-line","text":"","title":"Introduction to the command line"},{"location":"day1/ex1_bash_scripting/#objectives","text":"Navigating your file system Copying, Moving, Renaming and Removing files Examining file contents Redirection, manipulation and extraction Text and file manipulation Loops Shell scripting Moving files between your laptop and NeSI Introduction to slurm","title":"Objectives"},{"location":"day1/ex1_bash_scripting/#navigating-your-file-system","text":"Log in to NeSI and find the current location by running the command pwd which stands for print working directory . At any given time, current working directory is current default directory. pwd # /home/UserName/ We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\". ls Navigating to the MGSS_Intro/ directory can be done with the cd command which stands for change directory . cd MGSS_Intro/ Run the ls command to list the contents of the current directory. Check whether there are two .fastq files. The mkdir command ( make directory ) is used to make a directory. Enter mkdir followed by a space, then the directory name you want to create mkdir backup/","title":"Navigating your file system"},{"location":"day1/ex1_bash_scripting/#copying-moving-renaming-and-removing-files","text":"Make a second copy of SRR097977.fastq and rename it as Test_1_backup.fastq . Then move that file to backup/ directory. cp SRR097977.fastq Test_1_backup.fastq mv Test_1_backup.fastq backup Navigate to backup/ directory and use mv command to rename and move Test_1_backup.fastq as Test_1_copy.fastq to the directory immediately above. cd backup/ mv Test_1_backup.fastq ../Test_1_copy.fastq Return to the directory immediately above, check whether the Test_1_copy.fastq was moved and renamed as instructed and remove it by using the rm command. cd .. rm Test_1_copy.fastq See whether you can remove the backup/ directory by using the rm command as well. rm backup/ # rm : can not remove 'backup/': Is a directory By default, rm will not delete directories. This can be done by using -r (recursive) option. rm -r backup","title":"Copying, Moving, Renaming and Removing files"},{"location":"day1/ex1_bash_scripting/#examining-file-contents","text":"There are a number of ways to examine the content of a file. cat and less are two commonly used programs for a quick look. Check the content of SRR097977.fastq by using these commands. Take a note of the differences. cat SRR097977.fastq # less SRR097977.fastq A few useful shortcuts for navigating in less There are ways to take a look at parts of a file. For example, the head and tail commands will scan the beginning and end of a file, respectively. head SRR097977.fastq tail SRR097977.fastq Adding -n option to either of these commands will print the first or last n lines of a file. head -n 1 SRR097977.fastq # @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36","title":"Examining file contents"},{"location":"day1/ex1_bash_scripting/#redirection-manipulation-and-extraction","text":"Although using cat and less commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is grep . Let's use this command to search for the string NNNNNNNNNN in SRR098026.fastq file. grep NNNNNNNNNN SRR098026.fastq Retrieve and discuss the output you get when grep was executed with the -B1 and -A1 flags. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is > . Redirecting the string of bad reads that was searched using the grep command to a file named bad_reads.txt can be done with grep -B1 -A2 NNNNNNNNNN SRR098026.fastq > bad_reads.txt Use the wc command to count the number of words, lines and characters in the bad_reads.txt file. wc bad_reads.txt Add -l flag to wc command and compare the number with the above output wc -l bad_reads.txt In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of >> (append redirect) command which will append the new output to the end of the file, rather than overwriting it. grep -B1 -A2 NNNNNNNNNN SRR097977.fastq >> bad_reads.txt Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards , which are symbols or special characters that represent other characters. For an example. Using * wildcard, we can run the previous grep command on both files at the same time. grep -B1 -A2 NNNNNNNNNN *.fastq >> bad_reads.txt wc -l bad_reads.txt The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The | (pipe) command is a commonly used method to apply an operation for an ouput without creating intermediate files. It takes the output generated by one command and uses it as the input to another command. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l","title":"Redirection, manipulation and extraction"},{"location":"day1/ex1_bash_scripting/#text-and-file-manipulation","text":"There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below.","title":"Text and file manipulation"},{"location":"day1/ex1_bash_scripting/#cut","text":"The cut command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that cut uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the -d parameter. See what results you get from the file names.txt . cat names.txt cut -d \" \" -f 1 names.txt cut -d \" \" -f 1 -3 names.txt cut -d \" \" -f 1 ,3 names.txt","title":"Cut"},{"location":"day1/ex1_bash_scripting/#basename","text":"basename is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use basename to remove the .fastq extension from the files that we've been working with. basename SRR097977.fastq .fastq","title":"basename"},{"location":"day1/ex1_bash_scripting/#sed","text":"sed is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the sed command in UNIX is for substitution or for find and replace. By using sed you can edit files even without opening them, which is extremely important when working with large files. View the contents of the animals.txt file using cat . cat animals.txt We will now use sed to make some replacements to the text in this file.","title":"sed"},{"location":"day1/ex1_bash_scripting/#example-replacingsubstituting-a-string","text":"sed is mostly used to replace the text in a file. In the following example, sed replaces the word 'dogs' with 'cats' in the file. sed 's/dogs/cats/' animals.txt Here the s specifies the substitution operation. The / characters are delimiters. The dogs is the search pattern and the cats is the replacement string. By default, the sed command replaces the first occurrence of the pattern in each line and it won't replace additional occurrences in the line.","title":"Example - Replacing/substituting a string"},{"location":"day1/ex1_bash_scripting/#example-replacing-all-occurrences","text":"The substitute flag g (global replacement) can be added to the command to replace all the occurrences of the string in the line. sed 's/dogs/cats/g' animals.txt","title":"Example - Replacing all occurrences"},{"location":"day1/ex1_bash_scripting/#example-deleting-a-line","text":"To delete a particular line, we can specify the line number followed by the d character. For example sed '1d' animals.txt # Delete the first line sed '2d' animals.txt # Delete the second line sed '$d' animals.txt # Delete the last line","title":"Example - Deleting a line"},{"location":"day1/ex1_bash_scripting/#loops","text":"Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in bash scripting, for - iterates over a list of items and performs the given set of commands for item in [LIST] do [COMMANDS] done while - Performs a given set of commands an unknown number of times as long as the given condition evaluates is true while [CONDITION] do [COMMANDS] done until - Execute a given set of commands as longs as the given condition evaluates to false For most of our uses, a for loop is sufficient for our needs, so that is what we will be focusing on for this exercise. Shell identifies the for command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the do and done keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the $ character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example i = \"DAVE WAS HERE\" echo i # i echo $i # DAVE WAS HERE echo ${ i } # DAVE WAS HERE This prevents the shell interpreter from treating i as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files: for filename in *.fastq do head -n 2 ${filename} done Another useful command to be used with for loops is basename which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed). basename SRR097977.fastq .fastq basename is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things for filename in *.fastq do name=$(basename ${filename} .fastq) echo ${name} done","title":"Loops"},{"location":"day1/ex1_bash_scripting/#scripts","text":"Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal. The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as nano , emacs , and vi provide the platforms to write scripts. For this workshop we will use nano to create a file named ForLoop.sh . nano ForLoop.sh Add the following for-loop to the script (note the header #!/bin/bash ). #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } done Because nano is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing Ctrl + O , then exit nano using Ctrl + X . If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word . Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. bash ForLoop.sh Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the ls -l ForLoop.sh command and evaluate the first part of the output ls -l ForLoop.sh # -rw-rw-r-- 1 user user 88 Dec 6 19:52 ForLoop.sh There are three file permission flags that a file we create on NeSI can possess. Two of these, the read ( r ) and write ( w ) are marked for the ForLoop.sh file .The third flag, executable ( x ) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using chmod command. Add the executable permissions ( +x ) to ForLoop.sh and run ls again to see what has changed. chmod +x ForLoop.sh ls -l ForLoop.sh # -rwxrwxr-x 1 user user 88 Dec 6 19:52 ForLoop.sh Re-open the file in nano and append the output to TwoLines.txt , save and exit #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } >> TwoLines.txt done Execute the file ForLoop.sh . We'll need to put ./ at the beginning so the computer knows to look here in this directory for the program. ./ForLoop.sh","title":"Scripts"},{"location":"day1/ex1_bash_scripting/#moving-files-between-your-laptop-and-remote-clustermachine","text":"There are multiple commands and tools to move files between your laptop and remote clusters/machines. scp and rsync are some of the commands, and Globus and Cyberduck are some of these tools. scp is commonly used, as this is a simple to use tool that makes use of the ssh configuration we have already established for connecting to NeSI. In order it use it error free, we need to pay attention to whether the file is moving FROM or TO the remote cluster/machine, absolute paths, relative paths, Local vs Remote, etc. The format of the scp command is: scp path/to/copy/from path/to/copy/to If you wish to copy whole directories, include the -r flag: scp -r path/to/copy/from path/to/copy/to Finally, include mahuika: or ga-vl01: at the start of the remote machine file path (in this case, NeSI) to identify that this path is located within the remote cluster/machine. NOTE: the following templates are written in a way where the commands are to be **executed from local* ; i.e. a terminal not logged in to NeSI. On some systems, it may be preferable to open one terminal, log in to NeSI in that terminal, and then open a new terminal (that is not logged into NeSI) to perform the scp command. This avoids having to provide your NeSI log in details each time you wish to copy something.*","title":"Moving Files between your laptop and remote cluster/machine"},{"location":"day1/ex1_bash_scripting/#from-local-to-remote","text":"scp /path/from/local/ ga-vl01:/path/to/remote/","title":"FROM local TO remote"},{"location":"day1/ex1_bash_scripting/#from-remote-to-local","text":"scp ga-vl01:/path/from/remote/ /path/to/local/ If the ~/.ssh/config is not set with aliases you will need to replace the shortcut ga-vl01 with the full address of the remote: scp -oProxyCommand=\"ssh -W %h:%p user@lander.nesi.org.nz\" /path/in/local/ user@ga-vl01.mahuika.nesi.org.nz:/path/to/remote/ NOTE: If you are already comfortable with using scp , you are welcome to use this. However, for the purposes of this workshop, the relevant example files are available for download from the workshop main page . Alternatively, if you are working within the NeSI Jupyter hub , you can download individual files by right clicking on the file in the navigation pane on the left and clicking download .","title":"FROM remote TO local"},{"location":"day1/ex1_bash_scripting/#introduction-to-slurm","text":"Jobs running on NeSI are submitted in the form of a batch script containing the code you want to run and a header of information needed by a job scheduler. All NeSI systems use the slurm batch scheduler for the submission, control and management of user jobs. Slurm provides a rich set of features for organising your workload and an extensive array of tools for managing your resource usage. In most cases, you need to know the commands: Copy the contents of the BLAST/ folder to your current directory, using the following command cp -r /nesi/nobackup/nesi02659/SLURM/BLAST ./ We will then navigate into this directory with the cd command, and inspect the text of the file blast-test.sh using less or nano . cd BLAST/ less blast-test.sh Evaluate the contents of the blast-test.sh script. Take a note of the basic slurm variables, path variables, etc. We will revisit these in the afternoon, when you create your own slurm scripts. Submit the script to the job queue as below. sbatch blast-test.sh","title":"Introduction to slurm"},{"location":"day1/ex2_quality_filtering/","text":"Quality filtering raw reads \u00b6 Objectives \u00b6 Visualising raw reads with FastQC on NeSI Read trimming and adapter removal with trimmomatic Diagnosing poor libraries Understand common issues and best practices Optional : Filtering out host DNA with BBMap Visualising raw reads \u00b6 FastQC is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues: Adapter/barcode sequences Low quality regions of sequence Quality drop-off towards the end of read-pair sequence Loading FastQC \u00b6 These exercises will take place in the 2.fastqc/ folder. First, navigate to this folder. Copy the command below into your terminal (logged in to NeSI), replacing <YOUR FOLDER> , and then running the command. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ To activate FastQC on NeSI, you need to first load the module using the command module purge module load FastQC/0.11.7 Running FastQC \u00b6 We will run FastQC from the command line as follows: fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz Viewing the outputs from FastQC \u00b6 FastQC generates output reports in .html files that can be viewed in a standard web browser. Unfortunately, these can not be viewed from within the standard NeSI terminal environment. In day-to-day usage, it will be necessary to download the resulting files using scp and view them on your computer. Fortunately, if you're currently using the terminal within Jupyter hub for today's session, we can open the .html files directly from here: Click on the folder icon in the top left to open the folder navigator pane (if not already open). The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). Click through MGSS_U , into your folder, and then into the 2.fastqc/ folder. Double click on the output ...fastqc.html files to open them in the a new tab within the Jupyter hub . Examples of the output files are also available for download here . Note that FastQC does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. At a first glance, we can see the follow statistics: The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming. There are 100,000 reads in the file The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing. Have a quick look through the left hand columns. As you can see, the data has passed most of the basic parameters. Per-base sequence quality Per base sequence content Adapter Content Per sequence GC content The only aspect of the data that FastQC is flagging as potentially problematic is the GC% content of the data set. This is a common observation, as we are dealing with a mixed community and organisms and it is therefore unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of FastQC , but completely consistent with the biology of the system. FastQC outputs for libraries with errors \u00b6 Lets take a look at a library with significant errors. Process the sequence file mock_R1.adapter_decay.fastq with FastQC . fastqc mock_R1.adapter_decay.fastq.gz Compare the results with the mock_R1.good.fastq.gz file. Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not? Per-base sequence quality Read trimming and adapter removal with trimmomatic \u00b6 There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use trimmomatic , but this should in no way be interpreted as an endorsement of trimmomatic over equivalent tools like BBMap , sickle , cutadapt or any other. For a first run with trimmomatic , type the following commands into your console: module load Trimmomatic/0.39-Java-1.8.0_144 trimmomatic PE -threads 10 -phred33 \\ mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\ mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\ HEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80 There is a lot going on in this command, so here is a breakdown of the parameters in the command above Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 10 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used mock_R1.adapter_decay.fastq.gz / mock_R2.adapter_decay.fastq.gz positional The paired forward and reverse reads to trim mock_R1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner also passed mock_s1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) mock_R2.qc.fastq.gz / mock_s2.qc.fastq.gz positional The reverse-sequence equivalent of above HEADCROP:10 positional Adapter trimming command. Remove the first 10 positions in the sequence SLIDINGWINDOW:4:30 positional Quality filtering command. Analyses each sequence in a 4 base pair sliding window, truncating if the average quality drops below Q30 MINLEN:80 positional Length filtering command, discard sequences that are shorter than 80 base pairs after trimming Running the trimmed files back through FastQC , we can see that this signifcantly improves the output. Quality of reads \u00b6 Nucleotide distribution \u00b6 Considerations when working with trimmomatic \u00b6 Order of operations The basic format for a trimmomatic command is trimmomatic PE <keyword flags> <sequence input> <sequence output> <trimming parameters> The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users. For example, consider these two scenarios: trimmomatic PE <keyword flags> <sequence input> <sequence output> SLIDINGWINDOW:4:30 MINLEN:80 trimmomatic PE <keyword flags> <sequence input> <sequence output> MINLEN:80 SLIDINGWINDOW:4:30 In the first run we would not expect any sequence shorter than 80 base pairs to exist in the output files, but we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming the MINLEN filtering does not execute a second time. In the first instance, we are excluding performing trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the MINLEN run. A more subtle consequence of this behaviour is the interplay between adapter removal and quality filtering. In the command above, we try to identify adapters before quality trimming. Why do you think this is? Optional: Working with the ILLUMINACLIP command \u00b6 Trimmomatic also provides the ILLUMINACLIP command, which can be used to pass a fastA file of adapter and barcode sequences to be found and removed from your data. The format for the ILLUMINACLIP parameter can be quite confusing to work with and so we generally favour a HEADCROP where possitble. If you wish to work with the ILLUMINACLIP command, then you can see its use here in the trimmomatic manual . ILLUMINACLIP:iua.fna:1:25:7 | | | | | | | | | | | Simple clip threshold | | Palindrome clip threshold | Seed mismatches File of expected sequences There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. Where possible, we favour the use of simple positional trimming. Diagnosing poor libraries \u00b6 Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating FastQC : Does the sequencing length match what you ordered from the facility? If the sequences are shorter than expected, is adapter read-through a concern? What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation? Are adapters and/or barcodes removed? (look at the Per base sequence content to diagnose this) Is there unexpected sequence duplication? (this can occur when low-input library preparations are used) Are over-represented k -mers present? (this can be a sign of adapter and barcode contamination) Understand common issues and best practices \u00b6 Do I need to remove (rare) adapters? I don\u2019t know if adapters have been removed or not How do I identify and remove adapter read-through Identifying incomplete barcode/adapter removal Over aggressive trimming GC skew is outside of expected range Optional : Filtering out host DNA \u00b6 Metagenome data derived from microbial communities associated with a host should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species. There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. NOTE: This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via Kraken2 and then filtering out all reads that map to the pylum or kingdom of your host taxa). This exercise provides an example using BBMap to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping the quality-filtered reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that: are presumbed microbial contaminant in the reference; have high homology to microbial genes/genomes (e.g. ribosomes); or those that are of low complexity. This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed. NOTE: The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using bbmask. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread . Downloading the masked human reference genome \u00b6 The masked reference genome is available via a google drive link. We can use gdown to download this file from google drive via the command line. To install gdown , we can use pip . # Install gdown (for downloading from google drive) module purge module load Python/3.8.2-gimkl-2020a pip install --user gdown Nest, download the reference. It will also be necessary to first add your local bin location to the PATH variable via the export PATH=... command, as this is where gdown is located (modify <your_username> before running the code below). mkdir BBMask_human_reference/ cd BBMask_human_reference/ export PATH = \"/home/<your_username>/.local/bin: $PATH \" gdown https://drive.google.com/uc?id = 0B3llHR93L14wd0pSSnFULUlhcUk Indexing the reference genome and read mapping with BBMap \u00b6 We will cover more about read mapping in later exercises . For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a BBMap index, and then use BBMap to map the quality-filtered reads to that index, ultimately retaining only those reads that do not map to the index. Build index reference via BBMap . We will do this by submitting the job via slurm. NOTE: See Preparing an assembly job for slurm for more information about how to submit a job via slurm. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J host_filt_bbmap_index #SBATCH --res SummerSchool #SBATCH -J 2.qc_bbmap_ref #SBATCH --time 00:20:00 #SBATCH --mem 23GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH -e host_filt_bbmap_index.err #SBATCH -o host_filt_bbmap_index.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/BBMask_human_reference/ # Load BBMap module module purge module load BBMap/38.90-gimkl-2020a # Build indexed reference file via BBMap srun bbmap.sh ref = hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz -Xmx23g Finally, map the quality-filtered reads to the reference via BBMap . Here we will submit the job as a slurm array, with one array job per sample. Breaking down this command a little: We pass the path to the ref file (the reference we just built) to path=... . Provide quality-filtered reads as input (i.e. output of the trimmomatic process above). In this case, we will provide the fastq files located in ../3.assembly/ which have been processed via trimmomatic in the same manner as the exercise above. These are four sets of paired reads (representing metagenome data from four 'samples') that the remainder of the workshop exercises will be working with. The flags -Xmx27g and -t=20 set the max memory and threads allocations, and must match the --mem and --cpus_per_task allocations in the slurm headers at the top of the script. The rest of the settings in the BBMap call here are as per the recommendations found within this thread about processing data to remove host reads. Finally, the filtered output fastq files for downstream use are output to the host_filtered_reads/ folder (taken from the outputs outu1= and otu2= , which include only those reads that did not map to the host reference genome). NOTE: Slurm array jobs automatically create a variable SLURM_ARRAY_TASK_ID for that job, which contains the array task number (i.e. between 1 and 4 in the case below). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" ( sample${SLURM_ARRAY_TASK_ID} is read in as sample3 ). #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name host_filt_bbmap_map #SBATCH --res SummerSchool #SBATCH --time 01:00:00 #SBATCH --mem 27GB #SBATCH --ntasks 1 #SBATCH --array=1-4 #SBATCH --cpus-per-task 20 #SBATCH --error host_filt_bbmap_map_%a.err #SBATCH --output host_filt_bbmap_map_%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ mkdir -p host_filtered_reads/ # Load BBMap module module purge module load BBMap/38.95-gimkl-2020a # Run bbmap srun bbmap.sh -Xmx27g -t = 20 \\ minid = 0 .95 maxindel = 3 bwr = 0 .16 bw = 12 quickmatch fast minhits = 2 qtrim = rl trimq = 10 untrim \\ in1 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ in2 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ path = BBMask_human_reference/ \\ outu1 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R1_hostFilt.fastq \\ outu2 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R2_hostFilt.fastq The filtered reads are now available in host_filtered_reads/ for downstream use.","title":"Quality filtering raw reads"},{"location":"day1/ex2_quality_filtering/#quality-filtering-raw-reads","text":"","title":"Quality filtering raw reads"},{"location":"day1/ex2_quality_filtering/#objectives","text":"Visualising raw reads with FastQC on NeSI Read trimming and adapter removal with trimmomatic Diagnosing poor libraries Understand common issues and best practices Optional : Filtering out host DNA with BBMap","title":"Objectives"},{"location":"day1/ex2_quality_filtering/#visualising-raw-reads","text":"FastQC is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues: Adapter/barcode sequences Low quality regions of sequence Quality drop-off towards the end of read-pair sequence","title":"Visualising raw reads"},{"location":"day1/ex2_quality_filtering/#loading-fastqc","text":"These exercises will take place in the 2.fastqc/ folder. First, navigate to this folder. Copy the command below into your terminal (logged in to NeSI), replacing <YOUR FOLDER> , and then running the command. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ To activate FastQC on NeSI, you need to first load the module using the command module purge module load FastQC/0.11.7","title":"Loading FastQC"},{"location":"day1/ex2_quality_filtering/#running-fastqc","text":"We will run FastQC from the command line as follows: fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz","title":"Running FastQC"},{"location":"day1/ex2_quality_filtering/#viewing-the-outputs-from-fastqc","text":"FastQC generates output reports in .html files that can be viewed in a standard web browser. Unfortunately, these can not be viewed from within the standard NeSI terminal environment. In day-to-day usage, it will be necessary to download the resulting files using scp and view them on your computer. Fortunately, if you're currently using the terminal within Jupyter hub for today's session, we can open the .html files directly from here: Click on the folder icon in the top left to open the folder navigator pane (if not already open). The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). Click through MGSS_U , into your folder, and then into the 2.fastqc/ folder. Double click on the output ...fastqc.html files to open them in the a new tab within the Jupyter hub . Examples of the output files are also available for download here . Note that FastQC does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. At a first glance, we can see the follow statistics: The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming. There are 100,000 reads in the file The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing. Have a quick look through the left hand columns. As you can see, the data has passed most of the basic parameters. Per-base sequence quality Per base sequence content Adapter Content Per sequence GC content The only aspect of the data that FastQC is flagging as potentially problematic is the GC% content of the data set. This is a common observation, as we are dealing with a mixed community and organisms and it is therefore unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of FastQC , but completely consistent with the biology of the system.","title":"Viewing the outputs from FastQC"},{"location":"day1/ex2_quality_filtering/#fastqc-outputs-for-libraries-with-errors","text":"Lets take a look at a library with significant errors. Process the sequence file mock_R1.adapter_decay.fastq with FastQC . fastqc mock_R1.adapter_decay.fastq.gz Compare the results with the mock_R1.good.fastq.gz file. Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not? Per-base sequence quality","title":"FastQC outputs for libraries with errors"},{"location":"day1/ex2_quality_filtering/#read-trimming-and-adapter-removal-with-trimmomatic","text":"There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use trimmomatic , but this should in no way be interpreted as an endorsement of trimmomatic over equivalent tools like BBMap , sickle , cutadapt or any other. For a first run with trimmomatic , type the following commands into your console: module load Trimmomatic/0.39-Java-1.8.0_144 trimmomatic PE -threads 10 -phred33 \\ mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\ mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\ HEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80 There is a lot going on in this command, so here is a breakdown of the parameters in the command above Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 10 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used mock_R1.adapter_decay.fastq.gz / mock_R2.adapter_decay.fastq.gz positional The paired forward and reverse reads to trim mock_R1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner also passed mock_s1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) mock_R2.qc.fastq.gz / mock_s2.qc.fastq.gz positional The reverse-sequence equivalent of above HEADCROP:10 positional Adapter trimming command. Remove the first 10 positions in the sequence SLIDINGWINDOW:4:30 positional Quality filtering command. Analyses each sequence in a 4 base pair sliding window, truncating if the average quality drops below Q30 MINLEN:80 positional Length filtering command, discard sequences that are shorter than 80 base pairs after trimming Running the trimmed files back through FastQC , we can see that this signifcantly improves the output.","title":"Read trimming and adapter removal with trimmomatic"},{"location":"day1/ex2_quality_filtering/#quality-of-reads","text":"","title":"Quality of reads"},{"location":"day1/ex2_quality_filtering/#nucleotide-distribution","text":"","title":"Nucleotide distribution"},{"location":"day1/ex2_quality_filtering/#considerations-when-working-with-trimmomatic","text":"Order of operations The basic format for a trimmomatic command is trimmomatic PE <keyword flags> <sequence input> <sequence output> <trimming parameters> The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users. For example, consider these two scenarios: trimmomatic PE <keyword flags> <sequence input> <sequence output> SLIDINGWINDOW:4:30 MINLEN:80 trimmomatic PE <keyword flags> <sequence input> <sequence output> MINLEN:80 SLIDINGWINDOW:4:30 In the first run we would not expect any sequence shorter than 80 base pairs to exist in the output files, but we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming the MINLEN filtering does not execute a second time. In the first instance, we are excluding performing trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the MINLEN run. A more subtle consequence of this behaviour is the interplay between adapter removal and quality filtering. In the command above, we try to identify adapters before quality trimming. Why do you think this is?","title":"Considerations when working with trimmomatic"},{"location":"day1/ex2_quality_filtering/#optional-working-with-the-illuminaclip-command","text":"Trimmomatic also provides the ILLUMINACLIP command, which can be used to pass a fastA file of adapter and barcode sequences to be found and removed from your data. The format for the ILLUMINACLIP parameter can be quite confusing to work with and so we generally favour a HEADCROP where possitble. If you wish to work with the ILLUMINACLIP command, then you can see its use here in the trimmomatic manual . ILLUMINACLIP:iua.fna:1:25:7 | | | | | | | | | | | Simple clip threshold | | Palindrome clip threshold | Seed mismatches File of expected sequences There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. Where possible, we favour the use of simple positional trimming.","title":"Optional: Working with the ILLUMINACLIP command"},{"location":"day1/ex2_quality_filtering/#diagnosing-poor-libraries","text":"Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating FastQC : Does the sequencing length match what you ordered from the facility? If the sequences are shorter than expected, is adapter read-through a concern? What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation? Are adapters and/or barcodes removed? (look at the Per base sequence content to diagnose this) Is there unexpected sequence duplication? (this can occur when low-input library preparations are used) Are over-represented k -mers present? (this can be a sign of adapter and barcode contamination)","title":"Diagnosing poor libraries"},{"location":"day1/ex2_quality_filtering/#understand-common-issues-and-best-practices","text":"Do I need to remove (rare) adapters? I don\u2019t know if adapters have been removed or not How do I identify and remove adapter read-through Identifying incomplete barcode/adapter removal Over aggressive trimming GC skew is outside of expected range","title":"Understand common issues and best practices"},{"location":"day1/ex2_quality_filtering/#optional-filtering-out-host-dna","text":"Metagenome data derived from microbial communities associated with a host should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species. There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. NOTE: This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via Kraken2 and then filtering out all reads that map to the pylum or kingdom of your host taxa). This exercise provides an example using BBMap to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping the quality-filtered reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that: are presumbed microbial contaminant in the reference; have high homology to microbial genes/genomes (e.g. ribosomes); or those that are of low complexity. This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed. NOTE: The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using bbmask. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread .","title":"Optional: Filtering out host DNA"},{"location":"day1/ex2_quality_filtering/#downloading-the-masked-human-reference-genome","text":"The masked reference genome is available via a google drive link. We can use gdown to download this file from google drive via the command line. To install gdown , we can use pip . # Install gdown (for downloading from google drive) module purge module load Python/3.8.2-gimkl-2020a pip install --user gdown Nest, download the reference. It will also be necessary to first add your local bin location to the PATH variable via the export PATH=... command, as this is where gdown is located (modify <your_username> before running the code below). mkdir BBMask_human_reference/ cd BBMask_human_reference/ export PATH = \"/home/<your_username>/.local/bin: $PATH \" gdown https://drive.google.com/uc?id = 0B3llHR93L14wd0pSSnFULUlhcUk","title":"Downloading the masked human reference genome"},{"location":"day1/ex2_quality_filtering/#indexing-the-reference-genome-and-read-mapping-with-bbmap","text":"We will cover more about read mapping in later exercises . For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a BBMap index, and then use BBMap to map the quality-filtered reads to that index, ultimately retaining only those reads that do not map to the index. Build index reference via BBMap . We will do this by submitting the job via slurm. NOTE: See Preparing an assembly job for slurm for more information about how to submit a job via slurm. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J host_filt_bbmap_index #SBATCH --res SummerSchool #SBATCH -J 2.qc_bbmap_ref #SBATCH --time 00:20:00 #SBATCH --mem 23GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH -e host_filt_bbmap_index.err #SBATCH -o host_filt_bbmap_index.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/BBMask_human_reference/ # Load BBMap module module purge module load BBMap/38.90-gimkl-2020a # Build indexed reference file via BBMap srun bbmap.sh ref = hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz -Xmx23g Finally, map the quality-filtered reads to the reference via BBMap . Here we will submit the job as a slurm array, with one array job per sample. Breaking down this command a little: We pass the path to the ref file (the reference we just built) to path=... . Provide quality-filtered reads as input (i.e. output of the trimmomatic process above). In this case, we will provide the fastq files located in ../3.assembly/ which have been processed via trimmomatic in the same manner as the exercise above. These are four sets of paired reads (representing metagenome data from four 'samples') that the remainder of the workshop exercises will be working with. The flags -Xmx27g and -t=20 set the max memory and threads allocations, and must match the --mem and --cpus_per_task allocations in the slurm headers at the top of the script. The rest of the settings in the BBMap call here are as per the recommendations found within this thread about processing data to remove host reads. Finally, the filtered output fastq files for downstream use are output to the host_filtered_reads/ folder (taken from the outputs outu1= and otu2= , which include only those reads that did not map to the host reference genome). NOTE: Slurm array jobs automatically create a variable SLURM_ARRAY_TASK_ID for that job, which contains the array task number (i.e. between 1 and 4 in the case below). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" ( sample${SLURM_ARRAY_TASK_ID} is read in as sample3 ). #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name host_filt_bbmap_map #SBATCH --res SummerSchool #SBATCH --time 01:00:00 #SBATCH --mem 27GB #SBATCH --ntasks 1 #SBATCH --array=1-4 #SBATCH --cpus-per-task 20 #SBATCH --error host_filt_bbmap_map_%a.err #SBATCH --output host_filt_bbmap_map_%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ mkdir -p host_filtered_reads/ # Load BBMap module module purge module load BBMap/38.95-gimkl-2020a # Run bbmap srun bbmap.sh -Xmx27g -t = 20 \\ minid = 0 .95 maxindel = 3 bwr = 0 .16 bw = 12 quickmatch fast minhits = 2 qtrim = rl trimq = 10 untrim \\ in1 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ in2 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ path = BBMask_human_reference/ \\ outu1 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R1_hostFilt.fastq \\ outu2 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R2_hostFilt.fastq The filtered reads are now available in host_filtered_reads/ for downstream use.","title":"Indexing the reference genome and read mapping with BBMap"},{"location":"day1/ex3_assembly/","text":"Assembly \u00b6 Objectives \u00b6 Become familiar with the standard input files for SPAdes and IDBA-UD Understand the basic parameters that should be modified when using these assemblers Prepare an assembly job to run under slurm All work for this exercise will occur in the 3.assembly/ directory. The standard input files for SPAdes and IDBA-UD \u00b6 Although they both make use of the same types of data, both SPAdes and IDBA-UD have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by SPAdes : module load SPAdes/3.13.1-gimkl-2018b spades.py -h # ... #Input data: #-1 <filename> file with forward paired-end reads #-2 <filename> file with reverse paired-end reads #-s <filename> file with unpaired reads #--mp<#>-1 <filename> file with forward reads for mate-pair library number <#> (<#> = 1,2,..,9) #--mp<#>-2 <filename> file with reverse reads for mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-1 <filename> file with forward reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-2 <filename> file with reverse reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--nxmate<#>-1 <filename> file with forward reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--nxmate<#>-2 <filename> file with reverse reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--sanger <filename> file with Sanger reads #--pacbio <filename> file with PacBio reads #--nanopore <filename> file with Nanopore reads #--tslr <filename> file with TSLR-contigs #--trusted-contigs <filename> file with trusted contigs #--untrusted-contigs <filename> file with untrusted contigs At a glance, you could provide any of the following data types to SPAdes and have it perform an assembly: Illumina paired-end sequencing data, either as standard library or Mate Pairs Sanger sequences PacBio reads Oxford Nanopore reads Pre-assembled scaffolds for guiding the assembly Awkwardly, while SPAdes accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the -meta flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing. cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz > for_spades_R1.fq.gz cat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz > for_spades_R2.fq.gz Note that these fastQ files are compressed, yet we can concatenate them together with the cat command regardless. This is a nice feature of .gz files that is handy to remember. By contrast, what does IDBA-UD accept? module load IDBA/1.1.3-gimkl-2017a idba_ud # ... # -o, --out arg (=out) output directory # -r, --read arg fasta read file (<=128) # ... # -l, --long_read arg fasta long read file (>128) 'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using IDBA-UD we will need to pool the paired-end data into a single, interleaved fastA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example # Paired-end file, forward >read1_1 ... >read2_1 ... # Paired-end file, forward >read1_2 ... >read2_2 ... # Interleaved file >read1_1 ... >read1_2 ... >read2_1 ... >read2_2 ... Fortunately, the IDBA set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first. module load pigz/2.4-GCCcore-7.4.0 for i in sample1 sample2 sample3 sample4 ; do pigz --keep --decompress ${ i } _R1.fastq.gz ${ i } _R2.fastq.gz fq2fa --merge ${ i } _R1.fastq ${ i } _R2.fastq ${ i } .fna done cat sample1.fna sample2.fna sample3.fna sample4.fna > for_idba.fna Basic assembly parameters \u00b6 For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be. Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include: k -mer sizes to be assembled over, and step size if using a range Number of threads to use during assembly Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space Setting the k -mer size \u00b6 Depending on which assembler you are using, the commands for chosing the k -mer sizes for the assembly vary slightly, but they are recognisable between programs. In SPAdes , you can set the k -mer size using either spades.py -k 21 ,33,55,77,99,121 ... spades.py -k auto ... The first command lets us specify the k -mers ourselves, or we are letting SPAdes automatically pick the most appropriate size. For IDBA-UD , we can select the k -mer size using idba_ud --mink 21 --maxk 121 --step 22 Unlike SPAdes , we do not have fine-scale control over the k -mer sizes used in the assembly. We instead provide IDBA-UD with the first and last k -mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k -mer of uneven (odd) length in order to avoid the creation of palindromic k -mers. Specifying the number of threads \u00b6 This is simple in either assembler: spades.py -t 20 ... idba_ud --num_threads 20 ... The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, SPAdes will assemble with 16 threads. IDBA-UD will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm. Setting a memory limit \u00b6 By far, the worst feature of SPAdes is the high memory requirement for performing an assembly. In the absence of monitoring, SPAdes will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing SPAdes : spades.py -m 400GB ... No such parameter exists in IDBA-UD , but it requires far less RAM than SPAdes , so you are less likely to need it. Preparing an assembly job for slurm \u00b6 NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to ( lander02 ) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm . The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs. Submitting a SPAdes job to NeSI using slurm \u00b6 To begin, we need to open a text file using the nano text editor. cd 3 .assembly/ nano assembly_spades.sl Into this file, either write or copy/paste the following commands: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_assembly #SBATCH --res SummerSchool #SBATCH --time 00:50:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 12 #SBATCH --error spades_assembly.err #SBATCH --output spades_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load SPAdes/3.15.3-gimkl-2020a spades.py --meta -k 33 ,55,77,99,121 -t 12 -1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz -o spades_assembly/ To save your file, use Ctrl + O to save the file, then Ctrl + X to exit nano . Going through those lines one by one; Slurm parameter Function #!/bin/bash -e Header for the file, letting NeSI know how to interpret the following commands. The -e flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) --account ... The name of the project account to run the run under. You are provided with this when you create a project on NeSI --job-name ... The name of the job, to display when using the squeue command **--res ...* This is a parameter that we need due to a system reservation for this workshop. For your own work, ignore this flag --time ... Maximum run time for the job before it is killed --mem ... Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated --ntasks ... Number of tasks to distribute the job across. For all tools we use today, this will remain as 1 --cpus-per-task ... number of processing cores to assign to the job. This should match with the number used by your assembler --error ... File to log the standard error stream of the program. This is typically used to prove error reports, or to just inform the user of job progress --output ... File to log the standard output stream of the program. This is typically used to inform the user of job progress and supply messages The module load command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that There is no chance of having the job fail immediately because it cannot find the relevant files When looking back through your slurm logs, you know where the data is meant to be When executing the SPAdes command, there are a few parameters to note here: Parameter Function --meta Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions -k ... k -mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly -1 .. Forward reads, matched to their reverse partners -2 ... Reverse reads, matched to their forward partners -o ... Output directory for all files Note that we also prefix the command ( spades.py ) with the srun command. This is a command specific to slurm and allows NeSI to track the resource usage of the SPAdes job. We don't explicitly set memory or thread counts for this job, simply for the sake of keeping the command uncluttered. The default memory limit of SPAdes (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and SPAdes will terminate the assembly. We have also left the number of threads at the default value of 16, which matches the number specified in the slurm header. It is a good idea to match your number of threads request in the slurm script with what you intend to use with SPAdes because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e. request 10 in slurm, set thread count to 30 in SPAdes ) will result in reduced performance, as your SPAdes job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post . Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command sbatch assembly_spades.sl You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise. Monitoring job progress \u00b6 You can view the status of your current jobs using the command squeue -u <login name> JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly PD Priority 2019 -11-29T16:37:43 0 :00 00 :30:00 1 16 We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At the stage the START_TIME is an estimation of when the resources are expected to become available. When they do, the output will change to JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly R None 2019 -11-29T16:40:00 1 :20 00 :28:40 1 16 Which allows us to track how far into our run we are, and see the remaining time for the job. The START_TIME column now reports the time the job actually began. Submitting an IDBA-UD job to NeSI using slurm \u00b6 To run an equivalent assembly with IDBA-UD , create a new slurm script as follows nano idbaud_assembly.sl Paste or type in the following: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name idbaud_assembly #SBATCH --res SummerSchool #SBATCH --time 00:35:00 #SBATCH --mem 4GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 8 #SBATCH --error idbaud_assembly.err #SBATCH --output idbaud_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load IDBA/1.1.3-gimkl-2017a idba_ud --num_threads 8 --mink 33 --maxk 99 --step 22 -r for_idba.fna -o idbaud_assembly/ And submit the script as a slurm job: sbatch idbaud_assembly.sl Remember to record your job identification number.","title":"Assembly"},{"location":"day1/ex3_assembly/#assembly","text":"","title":"Assembly"},{"location":"day1/ex3_assembly/#objectives","text":"Become familiar with the standard input files for SPAdes and IDBA-UD Understand the basic parameters that should be modified when using these assemblers Prepare an assembly job to run under slurm All work for this exercise will occur in the 3.assembly/ directory.","title":"Objectives"},{"location":"day1/ex3_assembly/#the-standard-input-files-for-spades-and-idba-ud","text":"Although they both make use of the same types of data, both SPAdes and IDBA-UD have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by SPAdes : module load SPAdes/3.13.1-gimkl-2018b spades.py -h # ... #Input data: #-1 <filename> file with forward paired-end reads #-2 <filename> file with reverse paired-end reads #-s <filename> file with unpaired reads #--mp<#>-1 <filename> file with forward reads for mate-pair library number <#> (<#> = 1,2,..,9) #--mp<#>-2 <filename> file with reverse reads for mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-1 <filename> file with forward reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-2 <filename> file with reverse reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--nxmate<#>-1 <filename> file with forward reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--nxmate<#>-2 <filename> file with reverse reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--sanger <filename> file with Sanger reads #--pacbio <filename> file with PacBio reads #--nanopore <filename> file with Nanopore reads #--tslr <filename> file with TSLR-contigs #--trusted-contigs <filename> file with trusted contigs #--untrusted-contigs <filename> file with untrusted contigs At a glance, you could provide any of the following data types to SPAdes and have it perform an assembly: Illumina paired-end sequencing data, either as standard library or Mate Pairs Sanger sequences PacBio reads Oxford Nanopore reads Pre-assembled scaffolds for guiding the assembly Awkwardly, while SPAdes accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the -meta flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing. cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz > for_spades_R1.fq.gz cat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz > for_spades_R2.fq.gz Note that these fastQ files are compressed, yet we can concatenate them together with the cat command regardless. This is a nice feature of .gz files that is handy to remember. By contrast, what does IDBA-UD accept? module load IDBA/1.1.3-gimkl-2017a idba_ud # ... # -o, --out arg (=out) output directory # -r, --read arg fasta read file (<=128) # ... # -l, --long_read arg fasta long read file (>128) 'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using IDBA-UD we will need to pool the paired-end data into a single, interleaved fastA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example # Paired-end file, forward >read1_1 ... >read2_1 ... # Paired-end file, forward >read1_2 ... >read2_2 ... # Interleaved file >read1_1 ... >read1_2 ... >read2_1 ... >read2_2 ... Fortunately, the IDBA set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first. module load pigz/2.4-GCCcore-7.4.0 for i in sample1 sample2 sample3 sample4 ; do pigz --keep --decompress ${ i } _R1.fastq.gz ${ i } _R2.fastq.gz fq2fa --merge ${ i } _R1.fastq ${ i } _R2.fastq ${ i } .fna done cat sample1.fna sample2.fna sample3.fna sample4.fna > for_idba.fna","title":"The standard input files for SPAdes and IDBA-UD"},{"location":"day1/ex3_assembly/#basic-assembly-parameters","text":"For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be. Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include: k -mer sizes to be assembled over, and step size if using a range Number of threads to use during assembly Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space","title":"Basic assembly parameters"},{"location":"day1/ex3_assembly/#setting-the-k-mer-size","text":"Depending on which assembler you are using, the commands for chosing the k -mer sizes for the assembly vary slightly, but they are recognisable between programs. In SPAdes , you can set the k -mer size using either spades.py -k 21 ,33,55,77,99,121 ... spades.py -k auto ... The first command lets us specify the k -mers ourselves, or we are letting SPAdes automatically pick the most appropriate size. For IDBA-UD , we can select the k -mer size using idba_ud --mink 21 --maxk 121 --step 22 Unlike SPAdes , we do not have fine-scale control over the k -mer sizes used in the assembly. We instead provide IDBA-UD with the first and last k -mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k -mer of uneven (odd) length in order to avoid the creation of palindromic k -mers.","title":"Setting the k-mer size"},{"location":"day1/ex3_assembly/#specifying-the-number-of-threads","text":"This is simple in either assembler: spades.py -t 20 ... idba_ud --num_threads 20 ... The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, SPAdes will assemble with 16 threads. IDBA-UD will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm.","title":"Specifying the number of threads"},{"location":"day1/ex3_assembly/#setting-a-memory-limit","text":"By far, the worst feature of SPAdes is the high memory requirement for performing an assembly. In the absence of monitoring, SPAdes will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing SPAdes : spades.py -m 400GB ... No such parameter exists in IDBA-UD , but it requires far less RAM than SPAdes , so you are less likely to need it.","title":"Setting a memory limit"},{"location":"day1/ex3_assembly/#preparing-an-assembly-job-for-slurm","text":"NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to ( lander02 ) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm . The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs.","title":"Preparing an assembly job for slurm"},{"location":"day1/ex3_assembly/#submitting-a-spades-job-to-nesi-using-slurm","text":"To begin, we need to open a text file using the nano text editor. cd 3 .assembly/ nano assembly_spades.sl Into this file, either write or copy/paste the following commands: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_assembly #SBATCH --res SummerSchool #SBATCH --time 00:50:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 12 #SBATCH --error spades_assembly.err #SBATCH --output spades_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load SPAdes/3.15.3-gimkl-2020a spades.py --meta -k 33 ,55,77,99,121 -t 12 -1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz -o spades_assembly/ To save your file, use Ctrl + O to save the file, then Ctrl + X to exit nano . Going through those lines one by one; Slurm parameter Function #!/bin/bash -e Header for the file, letting NeSI know how to interpret the following commands. The -e flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) --account ... The name of the project account to run the run under. You are provided with this when you create a project on NeSI --job-name ... The name of the job, to display when using the squeue command **--res ...* This is a parameter that we need due to a system reservation for this workshop. For your own work, ignore this flag --time ... Maximum run time for the job before it is killed --mem ... Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated --ntasks ... Number of tasks to distribute the job across. For all tools we use today, this will remain as 1 --cpus-per-task ... number of processing cores to assign to the job. This should match with the number used by your assembler --error ... File to log the standard error stream of the program. This is typically used to prove error reports, or to just inform the user of job progress --output ... File to log the standard output stream of the program. This is typically used to inform the user of job progress and supply messages The module load command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that There is no chance of having the job fail immediately because it cannot find the relevant files When looking back through your slurm logs, you know where the data is meant to be When executing the SPAdes command, there are a few parameters to note here: Parameter Function --meta Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions -k ... k -mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly -1 .. Forward reads, matched to their reverse partners -2 ... Reverse reads, matched to their forward partners -o ... Output directory for all files Note that we also prefix the command ( spades.py ) with the srun command. This is a command specific to slurm and allows NeSI to track the resource usage of the SPAdes job. We don't explicitly set memory or thread counts for this job, simply for the sake of keeping the command uncluttered. The default memory limit of SPAdes (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and SPAdes will terminate the assembly. We have also left the number of threads at the default value of 16, which matches the number specified in the slurm header. It is a good idea to match your number of threads request in the slurm script with what you intend to use with SPAdes because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e. request 10 in slurm, set thread count to 30 in SPAdes ) will result in reduced performance, as your SPAdes job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post . Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command sbatch assembly_spades.sl You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise.","title":"Submitting a SPAdes job to NeSI using slurm"},{"location":"day1/ex3_assembly/#monitoring-job-progress","text":"You can view the status of your current jobs using the command squeue -u <login name> JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly PD Priority 2019 -11-29T16:37:43 0 :00 00 :30:00 1 16 We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At the stage the START_TIME is an estimation of when the resources are expected to become available. When they do, the output will change to JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly R None 2019 -11-29T16:40:00 1 :20 00 :28:40 1 16 Which allows us to track how far into our run we are, and see the remaining time for the job. The START_TIME column now reports the time the job actually began.","title":"Monitoring job progress"},{"location":"day1/ex3_assembly/#submitting-an-idba-ud-job-to-nesi-using-slurm","text":"To run an equivalent assembly with IDBA-UD , create a new slurm script as follows nano idbaud_assembly.sl Paste or type in the following: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name idbaud_assembly #SBATCH --res SummerSchool #SBATCH --time 00:35:00 #SBATCH --mem 4GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 8 #SBATCH --error idbaud_assembly.err #SBATCH --output idbaud_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load IDBA/1.1.3-gimkl-2017a idba_ud --num_threads 8 --mink 33 --maxk 99 --step 22 -r for_idba.fna -o idbaud_assembly/ And submit the script as a slurm job: sbatch idbaud_assembly.sl Remember to record your job identification number.","title":"Submitting an IDBA-UD job to NeSI using slurm"},{"location":"day1/ex4_assembly/","text":"Assembly (part 2) \u00b6 Objectives \u00b6 Examine the effect of changing parameters for assembly All work for this exercise will occur in the 3.assembly/ directory. Examine the effect of changing assembly parameters \u00b6 For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the asembly parameters. You will have a chance to compare the effects of these changes tomorrow. SPAdes parameters \u00b6 Make a few copies of your SPAdes slurm script like so; cp assembly_spades.sl assembly_spades_var1.sl Change a few of the parameters for run time. Some potential options include Change the k -mer sizes to either a different specification, or change to the auto option Disable error correction Assemble without the --meta flag Employ a coverage cutoff for assembling IDBA-UD parameters \u00b6 Make variants of your IDBA-UD assembly script and change some parameters. Some potential options include Change the minimum/maximum k -mer sizes, or the k -mer step size Change the alignment similarity parameter Adjust the prefix length for the k -mer sub-table Submit two or three jobs per variation.","title":"Assembly (part 2)"},{"location":"day1/ex4_assembly/#assembly-part-2","text":"","title":"Assembly (part 2)"},{"location":"day1/ex4_assembly/#objectives","text":"Examine the effect of changing parameters for assembly All work for this exercise will occur in the 3.assembly/ directory.","title":"Objectives"},{"location":"day1/ex4_assembly/#examine-the-effect-of-changing-assembly-parameters","text":"For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the asembly parameters. You will have a chance to compare the effects of these changes tomorrow.","title":"Examine the effect of changing assembly parameters"},{"location":"day1/ex4_assembly/#spades-parameters","text":"Make a few copies of your SPAdes slurm script like so; cp assembly_spades.sl assembly_spades_var1.sl Change a few of the parameters for run time. Some potential options include Change the k -mer sizes to either a different specification, or change to the auto option Disable error correction Assemble without the --meta flag Employ a coverage cutoff for assembling","title":"SPAdes parameters"},{"location":"day1/ex4_assembly/#idba-ud-parameters","text":"Make variants of your IDBA-UD assembly script and change some parameters. Some potential options include Change the minimum/maximum k -mer sizes, or the k -mer step size Change the alignment similarity parameter Adjust the prefix length for the k -mer sub-table Submit two or three jobs per variation.","title":"IDBA-UD parameters"},{"location":"day1/ex5_evaluating_assemblies/","text":"Evaluating the assemblies \u00b6 Objectives \u00b6 Evaluate the resource consumption of various assemblies Evaluate the assemblies Future considerations Evaluate the resource consumption of various assemblies \u00b6 Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the squeue command. squeue -u <user name> # JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS If there are no jobs listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the sacct command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying. sacct -S 2020 -11-16 # JobID JobName Elapsed TotalCPU Alloc MaxRSS State #-------------- --------------- ----------- ------------ ----- -------- ---------- #8744675 spades_assembly 00:15:16 01:41:43 10 COMPLETED #8744675.batch batch 00:15:16 00:00.547 10 3908K COMPLETED #8744675.extern extern 00:15:16 00:00:00 10 0 COMPLETED #8744675.0 spades.py 00:15:15 01:41:42 10 6260072K COMPLETED #8744677 idbaud_assembly 00:11:06 01:27:42 10 COMPLETED #8744677.batch batch 00:11:06 00:00.477 10 3760K COMPLETED #8744677.extern extern 00:11:06 00:00:00 10 0 COMPLETED #8744677.0 idba_ud 00:11:05 01:27:42 10 2541868K COMPLETED Each job has been broken up into several lines, but the main ones to keep an eye on are the base JobID values, and the values suffixed with .0 . The first of these references the complete job. The later (and any subsequent suffixes like .1 , .2 ) are the individual steps in the script that were called with the srun command. We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the seff command seff 8744675 #Job ID: 8744675 #Cluster: mahuika #User/Group: dwai012/dwai012 #State: COMPLETED (exit code 0) #Nodes: 1 #Cores per node: 10 #CPU Utilized: 01:41:44 #CPU Efficiency: 66.64% of 02:32:40 core-walltime #Job Wall-clock time: 00:15:16 #Memory Utilized: 5.97 GB #Memory Efficiency: 29.85% of 20.00 GB Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was not particularly efficient. (Note that for this particular job, 1 hr and 20 GB RAM were requested.) In hindsight I could have request a lot less time and RAM and still had the job run to completion. CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like bowtie and BBMap can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in SPAdes go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down. Evaluate the assemblies \u00b6 Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true. A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use seqmagick for performing the length filtering, and then just count sequence numbers using grep . These steps will take place in the 4.evaluation/ folder, which contains copies of our SPAdes and IDBA-UD assemblies. module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/4.evaluation/ seqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\ spades_assembly/spades_assembly.m1000.fna grep -c '>' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna # spades_assembly/spades_assembly.fna:1343 # spades_assembly/spades_assembly.m1000.fna:945 seqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\ idbaud_assembly/idbaud_assembly.m1000.fna grep -c '>' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna # idbaud_assembly/idbaud_assembly.fna:5057 # idbaud_assembly/idbaud_assembly.m1000.fna:1996 If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands. seqmagick convert --min-length 1000 ../3.assembly/spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna seqmagick convert --min-length 1000 ../3.assembly/idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna Note: The tool seqtk is also available on NeSI and performs many of the same functions as seqmagick . My choice of seqmagick is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either seqtk or seqmagick as both perform a lot of common *fastA and fastQ file manipulations.* As we can see here, the SPAdes assembly has completed with fewer contigs assembled than the IDBA-UD , both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has SPAdes managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 of the assembly with BBMap . module load BBMap/38.73-gimkl-2018b stats.sh in = spades_assembly/spades_assembly.m1000.fna This gives quite a verbose output: A C G T N IUPAC Other GC GC_stdev 0 .2537 0 .2465 0 .2462 0 .2536 0 .0018 0 .0000 0 .0000 0 .4928 0 .0956 Main genome scaffold total: 945 Main genome contig total: 2713 Main genome scaffold sequence total: 34 .296 MB Main genome contig sequence total: 34 .233 MB 0 .184% gap Main genome scaffold N/L50: 54 /160.826 KB Main genome contig N/L50: 106 /72.909 KB Main genome scaffold N/L90: 306 /15.236 KB Main genome contig N/L90: 817 /4.63 KB Max scaffold length: 1 .044 MB Max contig length: 1 .044 MB Number of scaffolds > 50 KB: 152 % main genome in scaffolds > 50 KB: 76 .69% Minimum Number Number Total Total Scaffold Scaffold of of Scaffold Contig Contig Length Scaffolds Contigs Length Length Coverage -------- -------------- -------------- -------------- -------------- -------- All 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 1 KB 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 2 .5 KB 750 2 ,453 33 ,961,961 33 ,903,372 99 .83% 5 KB 586 2 ,136 33 ,371,126 33 ,318,361 99 .84% 10 KB 396 1 ,590 32 ,000,574 31 ,962,900 99 .88% 25 KB 237 929 29 ,506,913 29 ,487,807 99 .94% 50 KB 152 587 26 ,301,390 26 ,289,386 99 .95% 100 KB 92 408 22 ,108,408 22 ,099,623 99 .96% 250 KB 30 138 12 ,250,722 12 ,247,681 99 .98% 500 KB 6 28 4 ,735,549 4 ,735,329 100 .00% 1 MB 1 1 1 ,043,932 1 ,043,932 100 .00% But what we can highlight here is that the statistics for the SPAdes assembly, with short contigs removed, yielded an N50 of 106 kbp at the contig level. We will now compute those same statistics from the other assembly options stats.sh in = spades_assembly/spades_assembly.fna stats.sh in = idbaud_assembly/idbaud_assembly.m1000.fna stats.sh in = idbaud_assembly/idbaud_assembly.fna Assembly N50 (contig) L50 (contig) SPAdes (filtered) 106 kbp 73 SPAdes (unfiltered) 107 kbp 72 IDBA-UD (filtered) 82 kbp 104 IDBA-UD (unfiltered) 88 kbp 97 Optional: Evaluating assemblies using MetaQUAST \u00b6 For more genome-informed evaluation of the assembly, we can use the MetaQUAST tool to view our assembled metagenome. This is something of an optional step because, like QUAST , MetaQUAST aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a MetaQUAST evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection: Kraken2 (DNA based, k -mer classification) CLARK (DNA based. k -mer classification) Kaiju (Protein based, BLAST classification) Centrifuge (DNA based, sequence alignment classification) MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification) MetaPhlAn2 (DNA based, clade-specific marker gene classification) A good summary and comparison of these tools (and more) was recently published by Ye et al. . However, since we do know the composition of the original communities used to build this mock metagenome, MetaQUAST will work very well for us today. In your 4.evaluation/ directory you will find a file called ref_genomes.txt . This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for MetaQUAST . module load QUAST/5.0.2-gimkl-2018b metaquast.py spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna \\ idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna \\ --references-list ref_genomes.txt --max-ref-number 21 -t 10 By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the --max-ref-number flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 names in the file ref_genomes.txt (10 prokaryote species and 11 viruses), MetaQUAST will download one of each. If we increase the number we will start to get multiple references per name provided which is usually desirable. We will now look at a few interesting assembly comparisons. When working from a standard terminal logged into NeSI, you can copy the entire folder 4.evaluation/quast_results/latest/ to your local environment using the scp -r mahuika:/nesi/path/to/quast_results/latest local/path/to/copy/to command to then open the report. Note that the browser requires JavaScript enabled to render the report: If you are working from the NeSI Jupyter hub environment today, the html viewer within the NeSI Jupyter hub does not currently support this (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the 4.evaluation/quast_results/latest/ folder, right-click report.html/ and select download. The downloaded file will then open within a new tab in the browser. ( NOTE: rendering the full report requires the other folders from within latest/ to also be downloaded and available in the same directory as report.html . Unfortunately, the Jupyter hub environment does not appear to currently support downloading entire folders using this method. ) An example of the MetaQUAST output files are also available for download here . Brief summary of assemblies \u00b6 Comparison of NGA50 between assemblies \u00b6 Comparison of aligned contigs \u00b6 Inspection of unaligned contigs \u00b6","title":"Evaluating the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#evaluating-the-assemblies","text":"","title":"Evaluating the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#objectives","text":"Evaluate the resource consumption of various assemblies Evaluate the assemblies Future considerations","title":"Objectives"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-resource-consumption-of-various-assemblies","text":"Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the squeue command. squeue -u <user name> # JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS If there are no jobs listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the sacct command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying. sacct -S 2020 -11-16 # JobID JobName Elapsed TotalCPU Alloc MaxRSS State #-------------- --------------- ----------- ------------ ----- -------- ---------- #8744675 spades_assembly 00:15:16 01:41:43 10 COMPLETED #8744675.batch batch 00:15:16 00:00.547 10 3908K COMPLETED #8744675.extern extern 00:15:16 00:00:00 10 0 COMPLETED #8744675.0 spades.py 00:15:15 01:41:42 10 6260072K COMPLETED #8744677 idbaud_assembly 00:11:06 01:27:42 10 COMPLETED #8744677.batch batch 00:11:06 00:00.477 10 3760K COMPLETED #8744677.extern extern 00:11:06 00:00:00 10 0 COMPLETED #8744677.0 idba_ud 00:11:05 01:27:42 10 2541868K COMPLETED Each job has been broken up into several lines, but the main ones to keep an eye on are the base JobID values, and the values suffixed with .0 . The first of these references the complete job. The later (and any subsequent suffixes like .1 , .2 ) are the individual steps in the script that were called with the srun command. We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the seff command seff 8744675 #Job ID: 8744675 #Cluster: mahuika #User/Group: dwai012/dwai012 #State: COMPLETED (exit code 0) #Nodes: 1 #Cores per node: 10 #CPU Utilized: 01:41:44 #CPU Efficiency: 66.64% of 02:32:40 core-walltime #Job Wall-clock time: 00:15:16 #Memory Utilized: 5.97 GB #Memory Efficiency: 29.85% of 20.00 GB Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was not particularly efficient. (Note that for this particular job, 1 hr and 20 GB RAM were requested.) In hindsight I could have request a lot less time and RAM and still had the job run to completion. CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like bowtie and BBMap can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in SPAdes go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down.","title":"Evaluate the resource consumption of various assemblies"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-assemblies","text":"Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true. A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use seqmagick for performing the length filtering, and then just count sequence numbers using grep . These steps will take place in the 4.evaluation/ folder, which contains copies of our SPAdes and IDBA-UD assemblies. module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/4.evaluation/ seqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\ spades_assembly/spades_assembly.m1000.fna grep -c '>' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna # spades_assembly/spades_assembly.fna:1343 # spades_assembly/spades_assembly.m1000.fna:945 seqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\ idbaud_assembly/idbaud_assembly.m1000.fna grep -c '>' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna # idbaud_assembly/idbaud_assembly.fna:5057 # idbaud_assembly/idbaud_assembly.m1000.fna:1996 If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands. seqmagick convert --min-length 1000 ../3.assembly/spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna seqmagick convert --min-length 1000 ../3.assembly/idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna Note: The tool seqtk is also available on NeSI and performs many of the same functions as seqmagick . My choice of seqmagick is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either seqtk or seqmagick as both perform a lot of common *fastA and fastQ file manipulations.* As we can see here, the SPAdes assembly has completed with fewer contigs assembled than the IDBA-UD , both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has SPAdes managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 of the assembly with BBMap . module load BBMap/38.73-gimkl-2018b stats.sh in = spades_assembly/spades_assembly.m1000.fna This gives quite a verbose output: A C G T N IUPAC Other GC GC_stdev 0 .2537 0 .2465 0 .2462 0 .2536 0 .0018 0 .0000 0 .0000 0 .4928 0 .0956 Main genome scaffold total: 945 Main genome contig total: 2713 Main genome scaffold sequence total: 34 .296 MB Main genome contig sequence total: 34 .233 MB 0 .184% gap Main genome scaffold N/L50: 54 /160.826 KB Main genome contig N/L50: 106 /72.909 KB Main genome scaffold N/L90: 306 /15.236 KB Main genome contig N/L90: 817 /4.63 KB Max scaffold length: 1 .044 MB Max contig length: 1 .044 MB Number of scaffolds > 50 KB: 152 % main genome in scaffolds > 50 KB: 76 .69% Minimum Number Number Total Total Scaffold Scaffold of of Scaffold Contig Contig Length Scaffolds Contigs Length Length Coverage -------- -------------- -------------- -------------- -------------- -------- All 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 1 KB 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 2 .5 KB 750 2 ,453 33 ,961,961 33 ,903,372 99 .83% 5 KB 586 2 ,136 33 ,371,126 33 ,318,361 99 .84% 10 KB 396 1 ,590 32 ,000,574 31 ,962,900 99 .88% 25 KB 237 929 29 ,506,913 29 ,487,807 99 .94% 50 KB 152 587 26 ,301,390 26 ,289,386 99 .95% 100 KB 92 408 22 ,108,408 22 ,099,623 99 .96% 250 KB 30 138 12 ,250,722 12 ,247,681 99 .98% 500 KB 6 28 4 ,735,549 4 ,735,329 100 .00% 1 MB 1 1 1 ,043,932 1 ,043,932 100 .00% But what we can highlight here is that the statistics for the SPAdes assembly, with short contigs removed, yielded an N50 of 106 kbp at the contig level. We will now compute those same statistics from the other assembly options stats.sh in = spades_assembly/spades_assembly.fna stats.sh in = idbaud_assembly/idbaud_assembly.m1000.fna stats.sh in = idbaud_assembly/idbaud_assembly.fna Assembly N50 (contig) L50 (contig) SPAdes (filtered) 106 kbp 73 SPAdes (unfiltered) 107 kbp 72 IDBA-UD (filtered) 82 kbp 104 IDBA-UD (unfiltered) 88 kbp 97","title":"Evaluate the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#optional-evaluating-assemblies-using-metaquast","text":"For more genome-informed evaluation of the assembly, we can use the MetaQUAST tool to view our assembled metagenome. This is something of an optional step because, like QUAST , MetaQUAST aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a MetaQUAST evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection: Kraken2 (DNA based, k -mer classification) CLARK (DNA based. k -mer classification) Kaiju (Protein based, BLAST classification) Centrifuge (DNA based, sequence alignment classification) MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification) MetaPhlAn2 (DNA based, clade-specific marker gene classification) A good summary and comparison of these tools (and more) was recently published by Ye et al. . However, since we do know the composition of the original communities used to build this mock metagenome, MetaQUAST will work very well for us today. In your 4.evaluation/ directory you will find a file called ref_genomes.txt . This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for MetaQUAST . module load QUAST/5.0.2-gimkl-2018b metaquast.py spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna \\ idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna \\ --references-list ref_genomes.txt --max-ref-number 21 -t 10 By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the --max-ref-number flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 names in the file ref_genomes.txt (10 prokaryote species and 11 viruses), MetaQUAST will download one of each. If we increase the number we will start to get multiple references per name provided which is usually desirable. We will now look at a few interesting assembly comparisons. When working from a standard terminal logged into NeSI, you can copy the entire folder 4.evaluation/quast_results/latest/ to your local environment using the scp -r mahuika:/nesi/path/to/quast_results/latest local/path/to/copy/to command to then open the report. Note that the browser requires JavaScript enabled to render the report: If you are working from the NeSI Jupyter hub environment today, the html viewer within the NeSI Jupyter hub does not currently support this (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the 4.evaluation/quast_results/latest/ folder, right-click report.html/ and select download. The downloaded file will then open within a new tab in the browser. ( NOTE: rendering the full report requires the other folders from within latest/ to also be downloaded and available in the same directory as report.html . Unfortunately, the Jupyter hub environment does not appear to currently support downloading entire folders using this method. ) An example of the MetaQUAST output files are also available for download here .","title":"Optional: Evaluating assemblies using MetaQUAST"},{"location":"day1/ex5_evaluating_assemblies/#brief-summary-of-assemblies","text":"","title":"Brief summary of assemblies"},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-nga50-between-assemblies","text":"","title":"Comparison of NGA50 between assemblies"},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-aligned-contigs","text":"","title":"Comparison of aligned contigs"},{"location":"day1/ex5_evaluating_assemblies/#inspection-of-unaligned-contigs","text":"","title":"Inspection of unaligned contigs"},{"location":"day2/ex6_initial_binning/","text":"Introduction to binning \u00b6 Objectives \u00b6 Remove short contigs from the data set Obtain coverage profiles for assembled contigs via read mapping Optional: Read mapping using a slurm array Remove short contigs from the data set \u00b6 Ideally, we do not want to be creating bins from all of the assembled contigs, as there is often a long tail of contigs which are only several k -mers long. These have little biological meaning, as they are too short for robust gene annotation, and they can introduce a significant degree of noise in the clustering algorithms used for binning. We therefore identify a suitable threshold for a minimum length of contigs to be considered for binning. We have already done this in the previous exercise so we could either use the existing filtering at 1,000 bp in length, or move to something stricter. Most binning tools have a default cut-off for minimum contig size - MetaBAT uses a default minimum of 2,500 bp, and recommends at least 1,500 bp. By contrast, MaxBin sets the minimum length at 1,000 bp. Obtain coverage profiles for assembled contigs via read mapping \u00b6 Binning is done using a combination of information encoded in the composition and coverage of the assembled contigs. Composition refers to k -mer (usually tetranucleotide) frequency profiles of the contigs, which are generally conserved within a genome. By contrast, coverage is a reflection of the abundance of the contigs in the assembly. Organisms which are more abundant will contribute more genomic material to the metagenome, and hence their DNA will be, on average, more abundant in the sample. When binning, we can look for pieces of DNA which are not assembled together, but have similar composition and occur at approximately equal abundances in the sample to identify contigs which likely originate in the same genome. The composition of the contigs is calculated by the binning tool at run time, but to obtain coverage information we must map our unassembled reads from each sample against the assembly to generate the differential abundance profiles for each contig. This is acheived using bowtie2 to map the reads against the assembly, then samtools to sort and compress the resulting file. Creating a mapping index \u00b6 Before we can map reads, we need to create a bowtie2 index file from the assembly, for use in read mapping. Navigate into the 5.binning/ folder to begin. module load Bowtie2/2.3.5-GCC-7.4.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ bowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades If you look inside the spades_assembly/ folder you will now see the following: ls spades_assembly/ # bw_spades.1.bt2 bw_spades.3.bt2 bw_spades.rev.1.bt2 spades_assembly.fna # bw_spades.2.bt2 bw_spades.4.bt2 bw_spades.rev.2.bt2 spades_assembly.m1000.fna These files ending in .bt2 are the index files for bowtie2 , and are specific to this tool. If you wish to map using an alternate tool (for example bowtie or BBMap ) you will need to create index/database files using these programs. Generally speaking, we don't need to know the names of the index files, as they are simply referred to be the output name we specified ( bw_spades ) when running bowtie2 . Mapping the reads \u00b6 We will create a slurm script to perform the mapping steps, as these benefit greatly from the multithreaded capacity of NeSI and we will use a for loop to iterate over each set of reads to simplify our script. The full script is provided here, and we will discuss it below. Open a new script using nano: nano spades_mapping.sl Paste in the following script. Remember to replace with your own folder. #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_mapping #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error spades_mapping.err #SBATCH --output spades_mapping.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x spades_assembly/bw_spades \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S ${ i } .sam # Step 3 samtools sort -@ 10 -o ${ i } .bam ${ i } .sam done Now run the script using sbatch sbatch spades_mapping.sl Step 1 - Loop through the sample files \u00b6 Since we just want to perform the same set of operations on each file, we can use a for loop to repeat each operation on a set of files. The structure of the loop, and use of variables was covered on the first day. For large sets of files, it can be beneficial to use a slurm array to send the jobs out to different nodes and distribute the process across many independent jobs. An example of how we could modify the above script is given at the bottom of this exercise, but is not necessary for the purposes of this workshop. Step 2 - Map the reads using bowtie2 \u00b6 This is performed using the following parameters Parameter Function --minins ... Minimum insert size, determines the minimum distance between the start of each read pair --maxins ... Maximum insert size, determines the maximum distance between the start of each read pair --threads ... Number of threads to use in read mapping --sensitive Specifies where we want to be positioned in the trade-off between speed and sensitivity. See the manual for more information -x ... The base name of our assembly index. Should be exactly the same as what was specified when running bowtie2-build -1 ... / -2 ... The forward and reverse read pairs to map to the assembly -S ... Name of the output file, to be written in sam format Step 3 - Sorting and compressing results \u00b6 The default output format for most maping tools is the Sequence Alignment/Map ( sam ) format. This is a compact text representation of where each short read sits in the contigs. You can view this file using any text viewer, although owing to the file size less is a good idea. Generally I wouldn't bother with this - there is a lot of information in here and unless you are looking to extract specific information from the alignment directly, this is just an intermediate file in our workflow. In order to save disk space, and prepare the file for downstream analysis we now perform two final steps: Sort the mapping information Compress the sam file into its binary equivalent, bam Which is achieved with the following parameters Parameter Function sort Subcommand for samtools to invoke the sort operation -@ ... Number of threads to use for sorting and compressing -o ... Output file name. When we specify the bam extension samtools automatically compresses the output Compressing the file to the bam format is an important step as when working with real data sam files can be massive and our storage capacity on NeSI is limited. It is also helpful to sort the mapping information so that reads mapped to a contig are listed in order of their start position. For example # Unsorted reads Ref: REFERENCECONTIG Map: --------ECONT-- Map: REFE----------- Map: --FERENCECO---- Map: -----------NTIG # Sorted reads Ref: REFERENCECONTIG Map: REFE----------- Map: --FERENCECO---- Map: --------ECONT-- Map: -----------NTIG Reads will initially be mapped in an unsorted order, as they are added to the sam file in more or less the same order as they are encountered in the original fastQ files. Sorting the mapping information is an important prerequisite for performing certain downstream processes. Not every tool we use requires reads to be sorted, but it can be frustrating having to debug the instances where read sorting matters, so we typically just get it done as soon as possible and then we don't have to worry about it again. In newer versions of samtools we can perform the sorting and compressing in a single operation (as shown in the script above). For older versions of samtools , you may need to use a command of the following form. samtools view -bS sample1.sam | samtools sort -o sample1.bam Optional: Read mapping using an array \u00b6 If you have a large number of files to process, it might be worth using a slurm array to distribute you individual mapping jobs across many separate nodes. An example script for how to perform this is given below, although it will not be covered in this workshop. Open a new script using nano: nano spades_mapping_array.sl #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_mapping_array #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 20GB #SBATCH --ntasks 1 #SBATCH --array 0-3 #SBATCH --cpus-per-task 10 #SBATCH --error spades_mapping_array.%j.err #SBATCH --output spades_mapping_array.%j.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ srun bowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades # Load the sample names into a bash array samples =( sample1 sample2 sample3 sample4 ) # Activate the srun command, using the SLURM_ARRAY_TASK_ID variable to # identify which position in the `samples` array to use srun bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive -x spades_assembly/bw_spades \\ -1 ../3.assembly/ ${ samples [ $SLURM_ARRAY_TASK_ID ] } _R1.fastq.gz \\ -2 ../3.assembly/ ${ samples [ $SLURM_ARRAY_TASK_ID ] } _R2.fastq.gz \\ -S ${ samples [ $SLURM_ARRAY_TASK_ID ] } .sam srun samtools sort -o ${ samples [ $SLURM_ARRAY_TASK_ID ] } .bam ${ samples [ $SLURM_ARRAY_TASK_ID ] } .sam submit the job to slurm sbatch spades_mapping_array.sl","title":"Introduction to binning"},{"location":"day2/ex6_initial_binning/#introduction-to-binning","text":"","title":"Introduction to binning"},{"location":"day2/ex6_initial_binning/#objectives","text":"Remove short contigs from the data set Obtain coverage profiles for assembled contigs via read mapping Optional: Read mapping using a slurm array","title":"Objectives"},{"location":"day2/ex6_initial_binning/#remove-short-contigs-from-the-data-set","text":"Ideally, we do not want to be creating bins from all of the assembled contigs, as there is often a long tail of contigs which are only several k -mers long. These have little biological meaning, as they are too short for robust gene annotation, and they can introduce a significant degree of noise in the clustering algorithms used for binning. We therefore identify a suitable threshold for a minimum length of contigs to be considered for binning. We have already done this in the previous exercise so we could either use the existing filtering at 1,000 bp in length, or move to something stricter. Most binning tools have a default cut-off for minimum contig size - MetaBAT uses a default minimum of 2,500 bp, and recommends at least 1,500 bp. By contrast, MaxBin sets the minimum length at 1,000 bp.","title":"Remove short contigs from the data set"},{"location":"day2/ex6_initial_binning/#obtain-coverage-profiles-for-assembled-contigs-via-read-mapping","text":"Binning is done using a combination of information encoded in the composition and coverage of the assembled contigs. Composition refers to k -mer (usually tetranucleotide) frequency profiles of the contigs, which are generally conserved within a genome. By contrast, coverage is a reflection of the abundance of the contigs in the assembly. Organisms which are more abundant will contribute more genomic material to the metagenome, and hence their DNA will be, on average, more abundant in the sample. When binning, we can look for pieces of DNA which are not assembled together, but have similar composition and occur at approximately equal abundances in the sample to identify contigs which likely originate in the same genome. The composition of the contigs is calculated by the binning tool at run time, but to obtain coverage information we must map our unassembled reads from each sample against the assembly to generate the differential abundance profiles for each contig. This is acheived using bowtie2 to map the reads against the assembly, then samtools to sort and compress the resulting file.","title":"Obtain coverage profiles for assembled contigs via read mapping"},{"location":"day2/ex6_initial_binning/#creating-a-mapping-index","text":"Before we can map reads, we need to create a bowtie2 index file from the assembly, for use in read mapping. Navigate into the 5.binning/ folder to begin. module load Bowtie2/2.3.5-GCC-7.4.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ bowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades If you look inside the spades_assembly/ folder you will now see the following: ls spades_assembly/ # bw_spades.1.bt2 bw_spades.3.bt2 bw_spades.rev.1.bt2 spades_assembly.fna # bw_spades.2.bt2 bw_spades.4.bt2 bw_spades.rev.2.bt2 spades_assembly.m1000.fna These files ending in .bt2 are the index files for bowtie2 , and are specific to this tool. If you wish to map using an alternate tool (for example bowtie or BBMap ) you will need to create index/database files using these programs. Generally speaking, we don't need to know the names of the index files, as they are simply referred to be the output name we specified ( bw_spades ) when running bowtie2 .","title":"Creating a mapping index"},{"location":"day2/ex6_initial_binning/#mapping-the-reads","text":"We will create a slurm script to perform the mapping steps, as these benefit greatly from the multithreaded capacity of NeSI and we will use a for loop to iterate over each set of reads to simplify our script. The full script is provided here, and we will discuss it below. Open a new script using nano: nano spades_mapping.sl Paste in the following script. Remember to replace with your own folder. #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_mapping #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error spades_mapping.err #SBATCH --output spades_mapping.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x spades_assembly/bw_spades \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S ${ i } .sam # Step 3 samtools sort -@ 10 -o ${ i } .bam ${ i } .sam done Now run the script using sbatch sbatch spades_mapping.sl","title":"Mapping the reads"},{"location":"day2/ex6_initial_binning/#step-1-loop-through-the-sample-files","text":"Since we just want to perform the same set of operations on each file, we can use a for loop to repeat each operation on a set of files. The structure of the loop, and use of variables was covered on the first day. For large sets of files, it can be beneficial to use a slurm array to send the jobs out to different nodes and distribute the process across many independent jobs. An example of how we could modify the above script is given at the bottom of this exercise, but is not necessary for the purposes of this workshop.","title":"Step 1 - Loop through the sample files"},{"location":"day2/ex6_initial_binning/#step-2-map-the-reads-using-bowtie2","text":"This is performed using the following parameters Parameter Function --minins ... Minimum insert size, determines the minimum distance between the start of each read pair --maxins ... Maximum insert size, determines the maximum distance between the start of each read pair --threads ... Number of threads to use in read mapping --sensitive Specifies where we want to be positioned in the trade-off between speed and sensitivity. See the manual for more information -x ... The base name of our assembly index. Should be exactly the same as what was specified when running bowtie2-build -1 ... / -2 ... The forward and reverse read pairs to map to the assembly -S ... Name of the output file, to be written in sam format","title":"Step 2 - Map the reads using bowtie2"},{"location":"day2/ex6_initial_binning/#step-3-sorting-and-compressing-results","text":"The default output format for most maping tools is the Sequence Alignment/Map ( sam ) format. This is a compact text representation of where each short read sits in the contigs. You can view this file using any text viewer, although owing to the file size less is a good idea. Generally I wouldn't bother with this - there is a lot of information in here and unless you are looking to extract specific information from the alignment directly, this is just an intermediate file in our workflow. In order to save disk space, and prepare the file for downstream analysis we now perform two final steps: Sort the mapping information Compress the sam file into its binary equivalent, bam Which is achieved with the following parameters Parameter Function sort Subcommand for samtools to invoke the sort operation -@ ... Number of threads to use for sorting and compressing -o ... Output file name. When we specify the bam extension samtools automatically compresses the output Compressing the file to the bam format is an important step as when working with real data sam files can be massive and our storage capacity on NeSI is limited. It is also helpful to sort the mapping information so that reads mapped to a contig are listed in order of their start position. For example # Unsorted reads Ref: REFERENCECONTIG Map: --------ECONT-- Map: REFE----------- Map: --FERENCECO---- Map: -----------NTIG # Sorted reads Ref: REFERENCECONTIG Map: REFE----------- Map: --FERENCECO---- Map: --------ECONT-- Map: -----------NTIG Reads will initially be mapped in an unsorted order, as they are added to the sam file in more or less the same order as they are encountered in the original fastQ files. Sorting the mapping information is an important prerequisite for performing certain downstream processes. Not every tool we use requires reads to be sorted, but it can be frustrating having to debug the instances where read sorting matters, so we typically just get it done as soon as possible and then we don't have to worry about it again. In newer versions of samtools we can perform the sorting and compressing in a single operation (as shown in the script above). For older versions of samtools , you may need to use a command of the following form. samtools view -bS sample1.sam | samtools sort -o sample1.bam","title":"Step 3 - Sorting and compressing results"},{"location":"day2/ex6_initial_binning/#optional-read-mapping-using-an-array","text":"If you have a large number of files to process, it might be worth using a slurm array to distribute you individual mapping jobs across many separate nodes. An example script for how to perform this is given below, although it will not be covered in this workshop. Open a new script using nano: nano spades_mapping_array.sl #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_mapping_array #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 20GB #SBATCH --ntasks 1 #SBATCH --array 0-3 #SBATCH --cpus-per-task 10 #SBATCH --error spades_mapping_array.%j.err #SBATCH --output spades_mapping_array.%j.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ srun bowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades # Load the sample names into a bash array samples =( sample1 sample2 sample3 sample4 ) # Activate the srun command, using the SLURM_ARRAY_TASK_ID variable to # identify which position in the `samples` array to use srun bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive -x spades_assembly/bw_spades \\ -1 ../3.assembly/ ${ samples [ $SLURM_ARRAY_TASK_ID ] } _R1.fastq.gz \\ -2 ../3.assembly/ ${ samples [ $SLURM_ARRAY_TASK_ID ] } _R2.fastq.gz \\ -S ${ samples [ $SLURM_ARRAY_TASK_ID ] } .sam srun samtools sort -o ${ samples [ $SLURM_ARRAY_TASK_ID ] } .bam ${ samples [ $SLURM_ARRAY_TASK_ID ] } .sam submit the job to slurm sbatch spades_mapping_array.sl","title":"Optional: Read mapping using an array"},{"location":"day2/ex7_initial_binning/","text":"Introduction to binning \u00b6 Objectives \u00b6 Overview Create initial bins using MetaBAT Create initial bins using MaxBin Overview \u00b6 With the mapping information computed in the last exercise, we can now perform binning. There are a multitude of good binning tools currently published, and each have their strengths and weaknesses. As there is no best tool for binning, the current strategy for binning is to use a number of different tools on your data, then use the tool DAS_Tool to evaluate all potential outcomes and define the best set of bins across all tools used. In our own workflow, we use the tools MetaBAT , MaxBin , and CONCOCT for binning, but there are many alternatives that are equally viable. In the interests of time, we are only going to demonstrate the first two tools. However, we recommend that you experiement with some of the following tools when conducting your own research. GroopM Tetra-ESOM VAMB MetaBAT \u00b6 MetaBAT binning occurs in two steps. First, the bam files from the last exercise are parsed into a tab-delimited table of the average coverage depth and variance per sample mapped. Binning is then performed using this table. The .bam files can be passed in via either a user-defined order, or using wildcards. module purge module load MetaBAT/2.13-GCC-7.4.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ # Manual specification of files jgi_summarize_bam_contig_depths --outputDepth metabat.txt sample1.bam sample2.bam sample3.bam sample4.bam # Wildcard jgi_summarize_bam_contig_depths --outputDepth metabat.txt sample*.bam Both give the same result, although the sample order may vary. We can then pass the table metabat.txt into the MetaBAT binning tool. Before we proceed, note that when you run MetaBAT on NeSI you will see the text vGIT-NOTFOUND appear in your command line. This has no impact on the performance of the tool. metabat2 -t 2 -m 1500 \\ -i spades_assembly/spades_assembly.m1000.fna \\ -a metabat.txt \\ -o metabat/metabat Note here that we are specifying a minimum contig size of 1,500 bp, which is the lower limit allowed by the authors of MetaBAT . This is larger than the minimum threshold of 1,000 bp when we filtered the assembly, which means there are some assembled contigs which cannot be binned. Consider the choice of this parameter and your initial contig filtering carefully when binning your own data. When specifying the output file, notice that we pass both a folder path ( metabat/ ) and file name ( metabat ). The reason I do this is that MetaBAT writes its output files using the pattern [USER VALUE].[BIN NUMBER].fa If we only provided the path, without a file name prefix, MetaBAT would create output like the following: metabat/.1.fa metabat/.2.fa metabat/.3.fa The problem with this is that on Linux systems, prefixing a file or folder name with a '.' character means the the file is hidden. This can lead to a lot of confusion when your binning job completes successfully but no files are visible! MaxBin \u00b6 Like MetaBAT , MaxBin requires a text representation of the coverage information for binning. Luckily, we can be sneaky here and just reformat the metabat.txt file into the format expected by MaxBin . We use cut to select only the columns of interest, which are the contigName and coverage columns, but not the contigLen , totalAvgDepth , or variance columns. We can inspect the metabat.txt file with head or less to identify the correct column indices for cut . less metabat.txt cut -f1,4,6,8,10 metabat.txt > maxbin.txt Generally speaking, this pattern of first, fourth, then [n + 2] th should work for any number of mapping files, although we always reocmmend that you check and confirm before you continue. This table is then passed to MaxBin . Unlike the case with MetaBAT , if we want to direct the output files into a folder, we must create that folder in advance. Create a new script to submit as a slurm job nano maxbin_clustering.sl Paste or type in the following. Remember to update <YOUR FOLDER> to your own folder. #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name maxbin_clustering #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error maxbin_clustering.err #SBATCH --output maxbin_clustering.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load MaxBin/2.2.6-gimkl-2018b-Perl-5.28.1 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ mkdir -p maxbin/ run_MaxBin.pl -thread 10 -min_contig_length 1500 \\ -contig spades_assembly/spades_assembly.m1000.fna \\ -abund maxbin.txt \\ -out maxbin/maxbin Submit the script as a slurm job: sbatch maxbin_clustering.sl This will take a bit longer to complete, as MaxBin uses gene prediction tools to identify the ideal contigs to use as the start of each bin.","title":"Introduction to binning"},{"location":"day2/ex7_initial_binning/#introduction-to-binning","text":"","title":"Introduction to binning"},{"location":"day2/ex7_initial_binning/#objectives","text":"Overview Create initial bins using MetaBAT Create initial bins using MaxBin","title":"Objectives"},{"location":"day2/ex7_initial_binning/#overview","text":"With the mapping information computed in the last exercise, we can now perform binning. There are a multitude of good binning tools currently published, and each have their strengths and weaknesses. As there is no best tool for binning, the current strategy for binning is to use a number of different tools on your data, then use the tool DAS_Tool to evaluate all potential outcomes and define the best set of bins across all tools used. In our own workflow, we use the tools MetaBAT , MaxBin , and CONCOCT for binning, but there are many alternatives that are equally viable. In the interests of time, we are only going to demonstrate the first two tools. However, we recommend that you experiement with some of the following tools when conducting your own research. GroopM Tetra-ESOM VAMB","title":"Overview"},{"location":"day2/ex7_initial_binning/#metabat","text":"MetaBAT binning occurs in two steps. First, the bam files from the last exercise are parsed into a tab-delimited table of the average coverage depth and variance per sample mapped. Binning is then performed using this table. The .bam files can be passed in via either a user-defined order, or using wildcards. module purge module load MetaBAT/2.13-GCC-7.4.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ # Manual specification of files jgi_summarize_bam_contig_depths --outputDepth metabat.txt sample1.bam sample2.bam sample3.bam sample4.bam # Wildcard jgi_summarize_bam_contig_depths --outputDepth metabat.txt sample*.bam Both give the same result, although the sample order may vary. We can then pass the table metabat.txt into the MetaBAT binning tool. Before we proceed, note that when you run MetaBAT on NeSI you will see the text vGIT-NOTFOUND appear in your command line. This has no impact on the performance of the tool. metabat2 -t 2 -m 1500 \\ -i spades_assembly/spades_assembly.m1000.fna \\ -a metabat.txt \\ -o metabat/metabat Note here that we are specifying a minimum contig size of 1,500 bp, which is the lower limit allowed by the authors of MetaBAT . This is larger than the minimum threshold of 1,000 bp when we filtered the assembly, which means there are some assembled contigs which cannot be binned. Consider the choice of this parameter and your initial contig filtering carefully when binning your own data. When specifying the output file, notice that we pass both a folder path ( metabat/ ) and file name ( metabat ). The reason I do this is that MetaBAT writes its output files using the pattern [USER VALUE].[BIN NUMBER].fa If we only provided the path, without a file name prefix, MetaBAT would create output like the following: metabat/.1.fa metabat/.2.fa metabat/.3.fa The problem with this is that on Linux systems, prefixing a file or folder name with a '.' character means the the file is hidden. This can lead to a lot of confusion when your binning job completes successfully but no files are visible!","title":"MetaBAT"},{"location":"day2/ex7_initial_binning/#maxbin","text":"Like MetaBAT , MaxBin requires a text representation of the coverage information for binning. Luckily, we can be sneaky here and just reformat the metabat.txt file into the format expected by MaxBin . We use cut to select only the columns of interest, which are the contigName and coverage columns, but not the contigLen , totalAvgDepth , or variance columns. We can inspect the metabat.txt file with head or less to identify the correct column indices for cut . less metabat.txt cut -f1,4,6,8,10 metabat.txt > maxbin.txt Generally speaking, this pattern of first, fourth, then [n + 2] th should work for any number of mapping files, although we always reocmmend that you check and confirm before you continue. This table is then passed to MaxBin . Unlike the case with MetaBAT , if we want to direct the output files into a folder, we must create that folder in advance. Create a new script to submit as a slurm job nano maxbin_clustering.sl Paste or type in the following. Remember to update <YOUR FOLDER> to your own folder. #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name maxbin_clustering #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error maxbin_clustering.err #SBATCH --output maxbin_clustering.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load MaxBin/2.2.6-gimkl-2018b-Perl-5.28.1 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ mkdir -p maxbin/ run_MaxBin.pl -thread 10 -min_contig_length 1500 \\ -contig spades_assembly/spades_assembly.m1000.fna \\ -abund maxbin.txt \\ -out maxbin/maxbin Submit the script as a slurm job: sbatch maxbin_clustering.sl This will take a bit longer to complete, as MaxBin uses gene prediction tools to identify the ideal contigs to use as the start of each bin.","title":"MaxBin"},{"location":"day2/ex8_bin_dereplication/","text":"Bin dereplication \u00b6 Objectives \u00b6 Bin dereplication using DAS_Tool - Creating input tables Bin dereplication using DAS_Tool - Running the tool Evaluating bins using CheckM Discussion: dereplication across multiple assemblies Bin dereplication using DAS_Tool - Creating input tables \u00b6 As we discussed in the previous exercise, we have now generated two sets of bins from the same single assembly. With this mock data set we can see that MetaBAT recovered 12 bins, while MaxBin recovered 10. Note that we are aiming to recover prokaryote genomes using these binning tools (we will use other tools to investigate viral genomes in later exercises), and 10 bacterial and archaeal genomes were used in the creation of this mock community. If our mock community only contained these 10 prokaryote genomes and omitted the viral genomes, we shouldn't expect to see more than 10 bins total. In our case here, these tools have likely recovered 10 bins of the same genomes. The additional two bins identified by MetaBAT may be the result of noise introduced into the binning process by the viral contigs included in the data. Furthermore, it is not clear which tool has done a better job of recruiting contigs to each bin - we very rarely expect to see the complete genome recovered from these kinds of data, so while it is probably the case that while an equivalent bin is present in the MetaBAT and MaxBin outputs, they will likely be of differing quality. DAS_Tool is a program designed to analyse the bins in each of our binning sets and determine where these equivalent pairs (or triplets if we use three binners) exist and return the 'best' one. DAS_Tool does not use the actual bins, but a set of text files that link contigs to their corresponding bins in each of the bin sets. We can produce these files using bash . For this exercise, we will continue working in the 5.binning/ directory. Creating contig/bin tables - MetaBAT \u00b6 For each of our binning tools, we need to extract the contigs assigned to each bin and create a single file that reports these as Contig [ tab ] Bin This can be done with a bit of bash scripting. There's quite a bit going on here, so we'll provide the full command, and then a step-by-step explanation of what's happening. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ for bin_path in metabat/*.fa ; do bin_name = $( basename ${ bin_path } .fa ) grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> metabat_associations.txt done You can check the contents of this file using less or head , and you should be something like: head -n5 metabat_associations.txt # NODE_14_length_365935_cov_2.017800 metabat.10 # NODE_17_length_326226_cov_2.043974 metabat.10 # NODE_22_length_261683_cov_2.059748 metabat.10 # NODE_24_length_257182_cov_2.023572 metabat.10 # NODE_25_length_256074_cov_2.059912 metabat.10 We will now walk through the content of the command above, breaking apart each individual step. for bin_path in metabat/*.fa ; do This is the initial loop, returning each file in the metabat/ folder that ends with the file extension .fa . bin_name = $( basename ${ bin_path } .fa ) As we have previously seen, the basename command removes the path information from the input variable bin_path (i.e. - metabat/metabat.1.fa becomes metabat.1.fa ) and assigns it to a new variable, bin_name . As an optional additional parameter, we can pass extra pieces of text to be removed from the variable, in this case the .fa extension. grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> metabat_associations.txt | | | | | | | Command 4 | | Command 3 | Command 2 Command 1 This next step uses piping between several commands to achieve the desired output. Command 1 \u00b6 The grep command searches the input file bin_path for lines containing the > character, which is the fastA demarcation for a sequence name. Command 2 \u00b6 The sed command replaces the > character with the empty character '' , as we do not need this character in the final file. Command 3 \u00b6 We now use sed again, this time to replace the $ character. In many command line tools and software environments (including R and python ) the characters ^ and $ are used as shortcuts for the beginning and ending of a line, respectively. By using the character in this way, we are telling sed to replace the end-of-line with the text \\t${bin_name} . sed will parse this text to mean the tab character followed by the content of the bin_name variable. The nature of sed is that it will automatically insert a new end-of-line. Command 4 \u00b6 This is similar to the stdout redirection we have previously used, but the double use of the > character means that we are appending our text to the end of the file metabat_associations.txt . Because we are looping through several files in this exercise, if we were to use the single > character then on each new fastA file read, the content of metabat_associations.txt would be replaced. It is important to note that because we are appending to the file, not replacing the contents, if you make a mistake in the command and need to re-run it, you will need to explicitly delete the metabat_associations.txt file using rm , otherwise your new (correct) output will be pasted to the end of your old (incorrect) output. Creating contig/bin tables - MaxBin \u00b6 The process for creating the MaxBin table is basically the same, we just need to change the file extension, as MaxBin writes outputs using the .fasta suffix rather than the .fa one. for bin_path in maxbin/*.fasta ; do bin_name = $( basename ${ bin_path } .fasta ) grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> maxbin_associations.txt done Warning for unbinned contigs \u00b6 Both MetaBAT and MaxBin have the option to output unbinned contigs after binning completes. We have not used that parameter here, but if you do choose to enable it you will end up with another fastA file in your output directory which you will need to avoid in the loops for creating DAS_Tool tables. Bin dereplication using DAS_Tool - Running the tool \u00b6 We are now ready to run DAS_Tool . This can be done from the command line, as it does not take a particularly long time to run for this data set. Start by loading DAS_Tool . module load DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1 Depending on whether or not your session has been continued from previous exercises, you may encounter an error performing this module load. This is because some of the tools we have used in previous exercises have dependencies which conflict with the dependencies in DAS_Tool . If this is the case for you, you can unload all previous module loads with the following: module purge module load DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1 module load DIAMOND/0.9.25-gimkl-2018b module load USEARCH/11.0.667-i86linux32 DAS_Tool should now load without issue. With 2 threads, DAS_Tool should take 10 - 15 minutes to complete. DAS_Tool -i metabat_associations.txt,maxbin_associations.txt -l MetaBAT,MaxBin \\ -t 2 --write_bins 1 --search_engine blast \\ -c spades_assembly/spades_assembly.m1000.fna \\ -o dastool_out/ As usual, we will break down the parameters: Parameter Function -i ... A comma-separated list of the contig/bin files we wish to process -l ... A comma-separated list of the binning tools used -t ... Number of threads to use --write_bins 1 A 0 or 1 value telling DAS_Tool whether or not to write out a new set of bins This is recommended, because DAS_Tool can create slices of old bins based on marker composition (see the paper for details) --search_engine blast Specify whether to use usearch , diamond , or BLAST as the alignment tool for comparing gene sequences (see note below) -c ... Path to the assembly used in binning -o .. Output directory for all files When DAS_Tool runs, you might see a dump of text looking like which: no diamond in ( ... ) /opt/nesi/CS400_centos7_bdw/DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1/DAS_Tool: line 237 : diamond: command not found which: no usearch in ( ... ) /opt/nesi/CS400_centos7_bdw/DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1/DAS_Tool: line 241 : usearch: command not found This is not a problem - DAS_Tool can use either BLAST , diamond , or usearch for performing its alignment operations. Regardless of which one you specify, it will search to see which ones are available. In this case, it is telling us that diamond and usearch cannot be found, which doesn't really matter because we have specified BLAST as our search engine. When DAS_Tool has completed, we will have a final set of bins located in the folder path dastool_out/_DASTool_bins . Have a look at the output and see which bins made it to the final selection. Did a single binning tool pick the best bins, or are the results a split between MetaBAT and MaxBin ? Evaluating bins using CheckM \u00b6 Now that we have our dereplicated set of bins, it is a good idea to determine estimates of their completeness (how much of the genome was recovered) and contamination (how many contigs we believe have been incorrectly assigned to the bin). For organisms that lack a reference genome there is not definitive way to do this, but the tool CheckM provides a robust estimate for these statistics by searching each of your bins for a number of highly conserved, single copy genes. The number of markers depends on whether or not you are working with bacterial (120 markers) or archaeal (122 markers) genomes, but CheckM is able to determine which set is more appropriate for each of your bins as it runs. There are several characteristics of the CheckM marker set worth noting: Highly conserved, single copy markers \u00b6 The marker sets used in CheckM were chosen because they are present in at least 95% of bacterial/archaeal genomes, and are single copy in \u226597% genomes tested. This means that if a gene is missing from a genome, it is likely due to incompleteness in either the original assembly or the binning approach. Similarly, if a marker is observed more than once in a bin it is likely the result of over-clustering of the data. Genes are considered as co-located clusters \u00b6 Rather than test the raw presence/absence of genes in the marker sets, the genes are organised into operon-like structures where genes known to be co-located are placed together. This is advantageous for two reasons These co-located groups are distributed around the prokaryotic genome, so estimates are not biased by lucky/unlucky recovery of a gene hotspot CheckM can account for how complete each individual gene cluster is, rather than just whether or not genes are present Lineage-specific duplications and losses can be identified \u00b6 As part of determining the correct marker set to use for each bin (bacterial or archaeal), CheckM uses a set of 43 conserved prokaryotic markers to insert each bin into a guide tree to estimate the phylogeny of the bin. There are several lineages which are known to have lost particular markers, or to have acquired a additional copies, and if CheckM places a bin into one of these lineages it can adjust its completeness/contamination estimates accordingly. This process isn't perfect, however, and we will discuss some times when you might need to create your own marker set in the next session. We will need to run CheckM under a slurm script. This is because the tree placement process requires a large amount of memory to perform, independently of the size of your data set. A basic script for submitting a CheckM job would be as follows: Create a new script nano bin_eval_checkm.sl Paste in the following script (replacing <YOUR FOLDER> with your own folder). #!/bin/bash #SBATCH --account nesi02659 #SBATCH --job-name bin_eval_checkm #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 50GB #SBATCH --cpus-per-task 10 #SBATCH --error bin_eval_checkm.err #SBATCH --output bin_eval_checkm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ checkm lineage_wf -t 10 --pplacer_threads 10 -x fa \\ --tab_table -f checkm.txt \\ dastool_out/_DASTool_bins/ checkm_out/ Submit the script as a slurm job sbatch bin_eval_checkm.sl The breakdown of parameters is as follows Parameter Function lineage_wf Specifies with mode of CheckM to run. This is the most common to use, but several others exist -t ... Number of threads to use for the initial marker gene detection and clustering --pplacer_threads ... Number of threads to use when inserting bins into the guide tree Note: Increasing this parameter results in a linear increase in memory requirement - seting it to 10 means that CheckM will need about 10 times more memory than with a single thread -x ... The fastA file extension to look for in the input folder. Default is .fna --tab_table If this parameter is present, a summary table will be written for the CheckM run -f ... The name of the file for the summary dastool/_DASTool_bins/ The location of the bins to test checkm_out/ The location to write intermediate and output files When your job completes, we will download the summary file and examine it. Discussion: dereplication across multiple assemblies \u00b6 In this workshop, we have generated a set of putative MAGs by binning scaffolds taken from a single co-assembly . Alternatively, we may have chosen to generate multiple assemblies (for example, mini-co-assemblies for each sample group, or individual assemblies for each sample). In this case, it would be necessary to work through the binning process for each assembly, and then conduct an additional dereplication step across the multiple assemblies to generate a single set of dereplicated bins for all assemblies. This is beyond the scope of this workshop (and unnecessary here, since we are working with a single co-assembly). For future reference for your own work, further information about how to dereplicate bins and viral contigs across multiple assemblies via dRep and dedupe has been provided as an appendix here .","title":"Bin dereplication"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication","text":"","title":"Bin dereplication"},{"location":"day2/ex8_bin_dereplication/#objectives","text":"Bin dereplication using DAS_Tool - Creating input tables Bin dereplication using DAS_Tool - Running the tool Evaluating bins using CheckM Discussion: dereplication across multiple assemblies","title":"Objectives"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool-creating-input-tables","text":"As we discussed in the previous exercise, we have now generated two sets of bins from the same single assembly. With this mock data set we can see that MetaBAT recovered 12 bins, while MaxBin recovered 10. Note that we are aiming to recover prokaryote genomes using these binning tools (we will use other tools to investigate viral genomes in later exercises), and 10 bacterial and archaeal genomes were used in the creation of this mock community. If our mock community only contained these 10 prokaryote genomes and omitted the viral genomes, we shouldn't expect to see more than 10 bins total. In our case here, these tools have likely recovered 10 bins of the same genomes. The additional two bins identified by MetaBAT may be the result of noise introduced into the binning process by the viral contigs included in the data. Furthermore, it is not clear which tool has done a better job of recruiting contigs to each bin - we very rarely expect to see the complete genome recovered from these kinds of data, so while it is probably the case that while an equivalent bin is present in the MetaBAT and MaxBin outputs, they will likely be of differing quality. DAS_Tool is a program designed to analyse the bins in each of our binning sets and determine where these equivalent pairs (or triplets if we use three binners) exist and return the 'best' one. DAS_Tool does not use the actual bins, but a set of text files that link contigs to their corresponding bins in each of the bin sets. We can produce these files using bash . For this exercise, we will continue working in the 5.binning/ directory.","title":"Bin dereplication using DAS_Tool - Creating input tables"},{"location":"day2/ex8_bin_dereplication/#creating-contigbin-tables-metabat","text":"For each of our binning tools, we need to extract the contigs assigned to each bin and create a single file that reports these as Contig [ tab ] Bin This can be done with a bit of bash scripting. There's quite a bit going on here, so we'll provide the full command, and then a step-by-step explanation of what's happening. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ for bin_path in metabat/*.fa ; do bin_name = $( basename ${ bin_path } .fa ) grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> metabat_associations.txt done You can check the contents of this file using less or head , and you should be something like: head -n5 metabat_associations.txt # NODE_14_length_365935_cov_2.017800 metabat.10 # NODE_17_length_326226_cov_2.043974 metabat.10 # NODE_22_length_261683_cov_2.059748 metabat.10 # NODE_24_length_257182_cov_2.023572 metabat.10 # NODE_25_length_256074_cov_2.059912 metabat.10 We will now walk through the content of the command above, breaking apart each individual step. for bin_path in metabat/*.fa ; do This is the initial loop, returning each file in the metabat/ folder that ends with the file extension .fa . bin_name = $( basename ${ bin_path } .fa ) As we have previously seen, the basename command removes the path information from the input variable bin_path (i.e. - metabat/metabat.1.fa becomes metabat.1.fa ) and assigns it to a new variable, bin_name . As an optional additional parameter, we can pass extra pieces of text to be removed from the variable, in this case the .fa extension. grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> metabat_associations.txt | | | | | | | Command 4 | | Command 3 | Command 2 Command 1 This next step uses piping between several commands to achieve the desired output.","title":"Creating contig/bin tables - MetaBAT"},{"location":"day2/ex8_bin_dereplication/#command-1","text":"The grep command searches the input file bin_path for lines containing the > character, which is the fastA demarcation for a sequence name.","title":"Command 1"},{"location":"day2/ex8_bin_dereplication/#command-2","text":"The sed command replaces the > character with the empty character '' , as we do not need this character in the final file.","title":"Command 2"},{"location":"day2/ex8_bin_dereplication/#command-3","text":"We now use sed again, this time to replace the $ character. In many command line tools and software environments (including R and python ) the characters ^ and $ are used as shortcuts for the beginning and ending of a line, respectively. By using the character in this way, we are telling sed to replace the end-of-line with the text \\t${bin_name} . sed will parse this text to mean the tab character followed by the content of the bin_name variable. The nature of sed is that it will automatically insert a new end-of-line.","title":"Command 3"},{"location":"day2/ex8_bin_dereplication/#command-4","text":"This is similar to the stdout redirection we have previously used, but the double use of the > character means that we are appending our text to the end of the file metabat_associations.txt . Because we are looping through several files in this exercise, if we were to use the single > character then on each new fastA file read, the content of metabat_associations.txt would be replaced. It is important to note that because we are appending to the file, not replacing the contents, if you make a mistake in the command and need to re-run it, you will need to explicitly delete the metabat_associations.txt file using rm , otherwise your new (correct) output will be pasted to the end of your old (incorrect) output.","title":"Command 4"},{"location":"day2/ex8_bin_dereplication/#creating-contigbin-tables-maxbin","text":"The process for creating the MaxBin table is basically the same, we just need to change the file extension, as MaxBin writes outputs using the .fasta suffix rather than the .fa one. for bin_path in maxbin/*.fasta ; do bin_name = $( basename ${ bin_path } .fasta ) grep \">\" ${ bin_path } | sed 's/>//g' | sed \"s/ $ /\\t ${ bin_name } /g\" >> maxbin_associations.txt done","title":"Creating contig/bin tables - MaxBin"},{"location":"day2/ex8_bin_dereplication/#warning-for-unbinned-contigs","text":"Both MetaBAT and MaxBin have the option to output unbinned contigs after binning completes. We have not used that parameter here, but if you do choose to enable it you will end up with another fastA file in your output directory which you will need to avoid in the loops for creating DAS_Tool tables.","title":"Warning for unbinned contigs"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool-running-the-tool","text":"We are now ready to run DAS_Tool . This can be done from the command line, as it does not take a particularly long time to run for this data set. Start by loading DAS_Tool . module load DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1 Depending on whether or not your session has been continued from previous exercises, you may encounter an error performing this module load. This is because some of the tools we have used in previous exercises have dependencies which conflict with the dependencies in DAS_Tool . If this is the case for you, you can unload all previous module loads with the following: module purge module load DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1 module load DIAMOND/0.9.25-gimkl-2018b module load USEARCH/11.0.667-i86linux32 DAS_Tool should now load without issue. With 2 threads, DAS_Tool should take 10 - 15 minutes to complete. DAS_Tool -i metabat_associations.txt,maxbin_associations.txt -l MetaBAT,MaxBin \\ -t 2 --write_bins 1 --search_engine blast \\ -c spades_assembly/spades_assembly.m1000.fna \\ -o dastool_out/ As usual, we will break down the parameters: Parameter Function -i ... A comma-separated list of the contig/bin files we wish to process -l ... A comma-separated list of the binning tools used -t ... Number of threads to use --write_bins 1 A 0 or 1 value telling DAS_Tool whether or not to write out a new set of bins This is recommended, because DAS_Tool can create slices of old bins based on marker composition (see the paper for details) --search_engine blast Specify whether to use usearch , diamond , or BLAST as the alignment tool for comparing gene sequences (see note below) -c ... Path to the assembly used in binning -o .. Output directory for all files When DAS_Tool runs, you might see a dump of text looking like which: no diamond in ( ... ) /opt/nesi/CS400_centos7_bdw/DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1/DAS_Tool: line 237 : diamond: command not found which: no usearch in ( ... ) /opt/nesi/CS400_centos7_bdw/DAS_Tool/1.1.1-gimkl-2018b-R-3.6.1/DAS_Tool: line 241 : usearch: command not found This is not a problem - DAS_Tool can use either BLAST , diamond , or usearch for performing its alignment operations. Regardless of which one you specify, it will search to see which ones are available. In this case, it is telling us that diamond and usearch cannot be found, which doesn't really matter because we have specified BLAST as our search engine. When DAS_Tool has completed, we will have a final set of bins located in the folder path dastool_out/_DASTool_bins . Have a look at the output and see which bins made it to the final selection. Did a single binning tool pick the best bins, or are the results a split between MetaBAT and MaxBin ?","title":"Bin dereplication using DAS_Tool - Running the tool"},{"location":"day2/ex8_bin_dereplication/#evaluating-bins-using-checkm","text":"Now that we have our dereplicated set of bins, it is a good idea to determine estimates of their completeness (how much of the genome was recovered) and contamination (how many contigs we believe have been incorrectly assigned to the bin). For organisms that lack a reference genome there is not definitive way to do this, but the tool CheckM provides a robust estimate for these statistics by searching each of your bins for a number of highly conserved, single copy genes. The number of markers depends on whether or not you are working with bacterial (120 markers) or archaeal (122 markers) genomes, but CheckM is able to determine which set is more appropriate for each of your bins as it runs. There are several characteristics of the CheckM marker set worth noting:","title":"Evaluating bins using CheckM"},{"location":"day2/ex8_bin_dereplication/#highly-conserved-single-copy-markers","text":"The marker sets used in CheckM were chosen because they are present in at least 95% of bacterial/archaeal genomes, and are single copy in \u226597% genomes tested. This means that if a gene is missing from a genome, it is likely due to incompleteness in either the original assembly or the binning approach. Similarly, if a marker is observed more than once in a bin it is likely the result of over-clustering of the data.","title":"Highly conserved, single copy markers"},{"location":"day2/ex8_bin_dereplication/#genes-are-considered-as-co-located-clusters","text":"Rather than test the raw presence/absence of genes in the marker sets, the genes are organised into operon-like structures where genes known to be co-located are placed together. This is advantageous for two reasons These co-located groups are distributed around the prokaryotic genome, so estimates are not biased by lucky/unlucky recovery of a gene hotspot CheckM can account for how complete each individual gene cluster is, rather than just whether or not genes are present","title":"Genes are considered as co-located clusters"},{"location":"day2/ex8_bin_dereplication/#lineage-specific-duplications-and-losses-can-be-identified","text":"As part of determining the correct marker set to use for each bin (bacterial or archaeal), CheckM uses a set of 43 conserved prokaryotic markers to insert each bin into a guide tree to estimate the phylogeny of the bin. There are several lineages which are known to have lost particular markers, or to have acquired a additional copies, and if CheckM places a bin into one of these lineages it can adjust its completeness/contamination estimates accordingly. This process isn't perfect, however, and we will discuss some times when you might need to create your own marker set in the next session. We will need to run CheckM under a slurm script. This is because the tree placement process requires a large amount of memory to perform, independently of the size of your data set. A basic script for submitting a CheckM job would be as follows: Create a new script nano bin_eval_checkm.sl Paste in the following script (replacing <YOUR FOLDER> with your own folder). #!/bin/bash #SBATCH --account nesi02659 #SBATCH --job-name bin_eval_checkm #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 50GB #SBATCH --cpus-per-task 10 #SBATCH --error bin_eval_checkm.err #SBATCH --output bin_eval_checkm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/5.binning/ checkm lineage_wf -t 10 --pplacer_threads 10 -x fa \\ --tab_table -f checkm.txt \\ dastool_out/_DASTool_bins/ checkm_out/ Submit the script as a slurm job sbatch bin_eval_checkm.sl The breakdown of parameters is as follows Parameter Function lineage_wf Specifies with mode of CheckM to run. This is the most common to use, but several others exist -t ... Number of threads to use for the initial marker gene detection and clustering --pplacer_threads ... Number of threads to use when inserting bins into the guide tree Note: Increasing this parameter results in a linear increase in memory requirement - seting it to 10 means that CheckM will need about 10 times more memory than with a single thread -x ... The fastA file extension to look for in the input folder. Default is .fna --tab_table If this parameter is present, a summary table will be written for the CheckM run -f ... The name of the file for the summary dastool/_DASTool_bins/ The location of the bins to test checkm_out/ The location to write intermediate and output files When your job completes, we will download the summary file and examine it.","title":"Lineage-specific duplications and losses can be identified"},{"location":"day2/ex8_bin_dereplication/#discussion-dereplication-across-multiple-assemblies","text":"In this workshop, we have generated a set of putative MAGs by binning scaffolds taken from a single co-assembly . Alternatively, we may have chosen to generate multiple assemblies (for example, mini-co-assemblies for each sample group, or individual assemblies for each sample). In this case, it would be necessary to work through the binning process for each assembly, and then conduct an additional dereplication step across the multiple assemblies to generate a single set of dereplicated bins for all assemblies. This is beyond the scope of this workshop (and unnecessary here, since we are working with a single co-assembly). For future reference for your own work, further information about how to dereplicate bins and viral contigs across multiple assemblies via dRep and dedupe has been provided as an appendix here .","title":"Discussion: dereplication across multiple assemblies"},{"location":"day2/ex9_refining_bins/","text":"Manually refining bins \u00b6 Objectives \u00b6 Prepare input files for VizBin Project a t-SNE using VizBin and examine bin clusters Refine bins by identifying incorrectly assigned contigs Optional: Refine and filter problematic contigs from bins Optional: Comparing pre- and post-filtered bins via CheckM Optional: Creating new VizBin profiles with different fragment lengths Optional: Scripts for processing data with ESOMana Prepare input files for VizBin \u00b6 VizBin is a handy, GUI-based tool for creating ordinations of our binning data using the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on YouTube that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS. On its own, VizBin takes a set of contigs and performs the t-SNE projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage VizBin only allows a single coverage value per contig, which is not ideal. This is because VizBin only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own t-SNE projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing VizBin with coverage values for sample1 only. The only required input file for VizBin is a single .fna file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement. NOTE: When running VizBin, it is often preferable to split long contigs into smaller pieces in order to increase the density of clustering in the **t-SNE* . The data we are working with today are based on our bins output by DAS_Tool in the last binning exercise, but have been further processed using the cut_up_fasta.py script that comes with the binning tool CONCOCT to cut long contigs into 20k fragments. When reviewing our VizBin plots and outputs, it is important to remember that here we are looking at the fragmented sub-contigs , rather than the full complete contigs (the importance of this will be clear when we are reviewing our vb_count_table.txt later in this exercise).* In the interests of time today, the input files have been generated and are provided in the 6.bin_refinement/ folder: all_bins.fna is a concatenation of the bins of fragmented sub-contigs (fragmented to 20k) all_bins.sample1.vizbin.ann is the annotation file containing per-subcontig coverage, label (bin ID), and length values. For future reference, and for working with your own data, a step-by-step process for generating these files from the curated bins generated by DAS_Tool has been provided as an Appendix . Let's first have a quick look at the annotation file. head -n5 all_bins.sample1.vizbin.ann # coverage,label,length # 17.6361,bin_0.chopped,20000 # 16.2822,bin_0.chopped,20000 # 17.7862,bin_0.chopped,20000 # 16.8073,bin_0.chopped,20000 This file is a comma-delimited table (csv file) that presents the information in the way that VizBin expects it. The order of rows in this file corresponds to the order of contigs in the concatentated fastA file of our fragmented bins, all_bins.fna . Create a few variations of the .ann file with various columns removed, in order to examine the different outputs they can generate. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Make a few different versions of the .ann file with various columns removed ## annotation with bin only (to colour by bin) cut -f2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.bin_only.ann ## no length, but coverage and bin included cut -f1,2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.no_length.ann Project a t-SNE and examine bin clusters \u00b6 We can now use these files in VizBin to curate the contigs in our bins. We will load and view the data in a few different steps. NOTE: Running VizBin remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like VizBin ) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing VizBin on your local machine and downloading the relevant input files (e.g. via scp ... ) to run locally. VizBin cannot be opened within the NeSI Jupyter hub . For today's exercise, open VizBin via either: A. Install and run VizBin locally Download VizBin-dist.jar from here You may also need to install Java locally (choose the relevant version for your machine). Download the required files from NeSI to your machine (via scp , or right-click > download, from within the Jupyter hub ), or download the example files from this link . Double-click VizBin-dist.jar to open VizBin , and then follow the steps below. OR B. Log in via a standard terminal. Log into NeSI via a standard terminal ( not the terminal within Jupyter hub ) and run the following command ( NOTE: this may return an error if X11 fowarding is not set up for your machine. In this case, proceed with the first option above ). cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ java -jar ../tools/vizbin.jar If this fails to open on your PC, or if it runs prohibitively slowly, team up with 2-3 others in the workshop to run through this exercise together on one machine. Loading the input files \u00b6 Once VizBin is open, to get started, simply click the 'Choose...' button then navigate to the fastA file. Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your 'bin only' .ann file into the 'Annotation file (optional)' field. Executing the t-SNE \u00b6 For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following: Contigs coloured by bin \u00b6 Contigs coloured by bin, sized by length, shaded by coverage \u00b6 Similar to any other projection technique, we interpret the closeness of points as a proxy for how similar they are, and because of our .ann file we can see which contigs belong to the same bin. Picking refined bins \u00b6 We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands: Left-click and drag: Highlight an area of the ordination to zoom into Right-click, 'Zoom Out', 'Both Axes': Rest of the view Left-click several points: Create a selection of contigs to extract from the data Right-click, 'Selection', 'Export': Save the selected contigs into a new file Right-click, 'Selection', 'Clear selection': Clear the current selection How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed. Today, we will run through an example of selecting potentially problematic (sub)contigs, and then deciding whether or not we want to filter these contigs out of our refined bins. We can use a combination of VizBin and seqmagick to remove contigs from bins where we do not trust the placement of the contig. We are aiming to reduce each bin to a trusted set of contigs. 1. Export VizBin clusters \u00b6 First, for each VizBin cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as cluster_1.fna . Try this for one or two clusters. In practice, we would do this for each VizBin cluster, saving each as a new cluster_n.fna file. Highlight a cluster to zoom into \u00b6 Select the cluster to export \u00b6 Left-click several points around the cluster Export the cluster \u00b6 Right-click, 'Selection', 'Export'. Save the output as cluster_1.fna . 2. Export potentially problematic contigs \u00b6 Select problematic contigs to examine \u00b6 Zoom in, make a selection of potentially problematic contigs, and export as above. Try this for one or two problematic contigs (or subsets of contigs). In practice, you could repeat this for all potentially problemtic contigs, saving each export as a new contigs_n.fna file. NOTE: for the subsequent step using vizbin_count_table.sh , all exported cluster files must share a common prefix (e.g. cluster...fna ), and all files of problematic contigs must also share a common prefix (e.g. contigs...fna ). Optional: Refine and filter problematic contigs from bins \u00b6 Create a count table of counts of our problematic contigs across each bin \u00b6 You'll recall that, prior running VizBin , the contigs in our bins were first cut into fragments to improve the density of the clusters in the t-SNE projection. As such, the problematic contigs we have exported from VizBin are sub-contig fragments, rather than full contigs from our bins. It is entirely possible that different fragments of the original contigs have been placed in different clusters during our VizBin analysis - including cases where most sub-contigs have clustered with the bin we expect, and a small number have been identified as \"problematic\" (i.e. clustered with other bins). Based on the information from these extracted problematic sub-contigs, we now have to carefully consider whether or not we want to remove the full contig from our bin data. To do this, we will generate a table containing each exported \"problematic\" sub-contig, and counts of how many of its sister sub-contigs (each of the other sub-contig fragments derived from the same original parent contig) fall into each VizBin cluster. For this exercise, a folder of the exported files from VizBin for all clusters ( cluster_[1-n].fna ) and problematic sub-contigs ( contigs_[1-n].fna ) has been provided at vizbin_example_exports/ We will input these files to the shell script vizbin_count_table.sh to generate a count table of the exprted subcontigs across each VizBin cluster ( vb_count_table.txt ), as well as a working list of contigs to potentially remove from our final bin data ( vb_omit_contigs_tmp.txt ). For future reference, a copy of this script is available for download here . ./vizbin_count_table.sh -i vizbin_example_exports/ The only required input to vizbin_count_table.sh is the path to the cluster and contigs files exported from VizBin . By default, the script looks for the prefix cluster... for the cluster file names, contig... for the files of problematic sub-contigs, and the file extension .fna for each. The arguments -s <contig_file_prefix> -c <cluster_file_prefix> -e <fasta_file_extension> can optionally be provided if your file name formats differ from the default. View the output count table: less vb_count_table.txt Example excerpt: Subcontig_ID Subcontig_vb_cluster cluster_10_count cluster_1_count cluster_2_count cluster_3_count cluster_4_count cluster_5_count cluster_6_count cluster_7_count cluster_8_count cluster_9_count Total_count >bin_5_NODE_826_length_1788_cov_0.154169 cluster_5 0 0 0 0 0 1 0 0 0 0 1 >bin_5_NODE_848_length_1686_cov_0.184026 cluster_5 0 0 0 0 0 1 0 0 0 0 1 >bin_9_NODE_4_length_793571_cov_0.517196.33 cluster_6 0 0 0 0 0 0 1 38 0 0 39 Note that in the case of the third contig from the excerpt above, the 'problematic' contig is only one of 39 sub-contigs, and all other 38 sub-contigs are in the expected cluster. In this case, we likely do not want to remove this contig from the bin. Generate a list of contigs to exclude from filtering \u00b6 Create a list of contigs identified from vb_count_table.txt that are not to be filtered out by seqmagick in the next step. For example, those contigs that have sub-contigs split across multiple vizbin clusters, and for which it's reasonable to actually keep the contig (such as when a flagged selected sub-contig exported from vizbin is in one unexpected cluster, but all other sub-contigs from that parent contig are in the expected cluster; in this case, you likely don't want to filter out the parent contig from the data set moving forward). Below is an example. Simply replace the contig IDs between the quotes for as many lines as necessary for your data. NOTES: The first line below must always have only one > character, while all subsequent lines must have two (i.e. >> ) to append correctly to the list. We want the original contig ID here, *not the sub-contig, so make sure to remove the .xx fragment number at the end if there is one.* echo \"bin_0_NODE_9_length_392609_cov_1.038712\" > vb_keep_contigs.txt echo \"bin_9_NODE_4_length_793571_cov_0.517196\" >> vb_keep_contigs.txt echo \"bin_1_NODE_182_length_42779_cov_1.585353\" >> vb_keep_contigs.txt Create final vb_omit_contigs_filtered.txt list of contigs to filter from bins \u00b6 Using grep , filter contigs we wish to keep (after assessing vb_count_table.txt ) out of the working vb_omit_contigs_tmp.txt list. This creates vb_omit_contigs_filtered.txt , which we will then pass to seqmagick to filter these contigs out of our actual bin fasta files. grep -v -f vb_keep_contigs.txt vb_omit_contigs_tmp.txt > vb_omit_contigs_filtered.txt Filter suspect contigs (based on VizBin analysis) from the bin data \u00b6 Use seqmagick --exclude-from-file ... to filter problematic contigs (those contigs listed in vb_omit_contigs_filtered.txt ) out of the initial unchopped bin fasta files, generating final bins for downstream processing. mkdir filtered_bins/ # Load seqmagick module purge module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 # filter problematic contigs out of original bin files for bin_file in example_data_unchopped/*.fna ; do bin_name = $( basename ${ bin_file } .fna ) seqmagick convert --exclude-from-file vb_omit_contigs_filtered.txt ${ bin_file } filtered_bins/ ${ bin_name } .filtered.fna done Our filtered bins for downstream use are now in filtered_bins/ Optional: Comparing pre- and post-filtered bins via CheckM \u00b6 The end goal of this process is the generation of a final set of refined bins. Following this, the CheckM procedure should be re-run, this time on the refined filtered_bins/ . This provides CheckM metrics for the final actual (filtered) bin set, and also an opportunity to compare between pre- and post-filtering to see if the VizBin bin refinement steps have, for example, improved the degree of contamination in the bins. For this exercise, a copy of the output from running CheckM on the filtered_bins/ is available at 6.bin_refinement/filtered_bins_checkm.txt . View the previous CheckM output and the filtered bins output to compare via cat . cat filtered_bins_checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity bin_0.filtered k__Bacteria (UID3060) 138 338 246 1 329 7 1 0 0 99.59 2.98 0.00 bin_1.filtered k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 bin_2.filtered k__Bacteria (UID2565) 2921 149 91 10 137 2 0 0 0 91.21 0.61 0.00 bin_3.filtered g__Staphylococcus (UID301) 45 940 178 14 924 2 0 0 0 98.58 0.11 0.00 bin_4.filtered c__Betaproteobacteria (UID3959) 235 414 211 1 410 3 0 0 0 99.97 0.26 0.00 bin_5.filtered o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 bin_6.filtered c__Deltaproteobacteria (UID3218) 61 284 169 17 267 0 0 0 0 91.72 0.00 0.00 bin_7.filtered f__Bradyrhizobiaceae (UID3695) 47 693 296 2 691 0 0 0 0 99.80 0.00 0.00 bin_8.filtered p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 bin_9.filtered g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 cat ../5.binning/checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity maxbin.001_sub.contigs k__Bacteria (UID3060) 138 338 246 1 328 8 1 0 0 99.59 3.39 0.00 maxbin.002_sub.contigs k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 maxbin.009_sub.contigs k__Bacteria (UID2565) 2921 149 91 10 137 2 0 0 0 91.21 0.61 0.00 metabat.1.contigs c__Deltaproteobacteria (UID3218) 61 284 169 17 267 0 0 0 0 91.72 0.00 0.00 metabat.10.contigs g__Staphylococcus (UID301) 45 940 178 14 924 2 0 0 0 98.58 0.11 0.00 metabat.11.contigs c__Betaproteobacteria (UID3959) 235 414 211 1 408 5 0 0 0 99.97 0.90 0.00 metabat.12.contigs o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 metabat.4.contigs f__Bradyrhizobiaceae (UID3695) 47 693 296 2 691 0 0 0 0 99.80 0.00 0.00 metabat.6.contigs p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 metabat.8.contigs g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 An example of an updated slurm script to run CheckM on the filtered_bins/ is as follows: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name bin_eval_checkm #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 50GB #SBATCH --cpus-per-task 10 #SBATCH --error bin_eval_checkm.err #SBATCH --output bin_eval_checkm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ checkm lineage_wf -t 10 --pplacer_threads 10 -x fna \\ --tab_table -f filtered_bins_checkm.txt \\ filtered_bins/ filtered_bins_checkm_out/ Optional: Creating new VizBin profiles with different fragment lengths \u00b6 The data you have been working with was created using the cut_up_fasta.py script that comes with the binning tool CONCOCT . It was run to cut contigs into 20k fragments, to better add density to the cluster. If you would like to visualise the data using different contig fragment sizes, you can create these using the following commands (replace YOUR_CONTIG_SIZE with the size of interest, e.g. 10000 ): module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16 mkdir custom_chop/ # Fragment contigs within each bin, outputting to custom_chop/ for bin_file in example_data_unchopped/* ; do bin_name = $( basename ${ bin_file } .fna ) cut_up_fasta.py -c YOUR_CONTIG_SIZE -o 0 --merge_last ${ bin_file } > custom_chop/ ${ bin_name } .chopped.fna done # Concatenate the chopped bins into single .fna cat custom_chop/*.fna > all_bins_custom_chop.fna You can open all_bins_custom_chop.fna in VizBin to view the clustering with this new fragmentation threshold. If you wish to also provide an annotation file to colour by bin, this can be generated with the following: # Set up annotation file headers echo \"label\" > custom_chop.vizbin.ann # loop through custom_chop .fna files for bin_file in custom_chop/*.fna ; do # extract bin ID binID = $( basename ${ bin_file } .fna ) # loop through each sequence header in bin_file, adding binID to custom_chop.vizbin.ann for each header present for header in ` grep \">\" ${ bin_file } ` ; do # Add binID to vizbin.ann for each header present echo \" ${ binID } \" >> custom_chop.vizbin.ann done done If you wish to generate the full annotation file, including coverage and length values, you will need to go through the process outlined in the Appendix for this exercise . Optional: Scripts for processing data with ESOMana \u00b6 A suite of tools for creating input files for ESOMana can be found on github here . The tool ESOMana can be downloaded from SourceForge .","title":"Manually refining bins"},{"location":"day2/ex9_refining_bins/#manually-refining-bins","text":"","title":"Manually refining bins"},{"location":"day2/ex9_refining_bins/#objectives","text":"Prepare input files for VizBin Project a t-SNE using VizBin and examine bin clusters Refine bins by identifying incorrectly assigned contigs Optional: Refine and filter problematic contigs from bins Optional: Comparing pre- and post-filtered bins via CheckM Optional: Creating new VizBin profiles with different fragment lengths Optional: Scripts for processing data with ESOMana","title":"Objectives"},{"location":"day2/ex9_refining_bins/#prepare-input-files-for-vizbin","text":"VizBin is a handy, GUI-based tool for creating ordinations of our binning data using the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on YouTube that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS. On its own, VizBin takes a set of contigs and performs the t-SNE projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage VizBin only allows a single coverage value per contig, which is not ideal. This is because VizBin only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own t-SNE projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing VizBin with coverage values for sample1 only. The only required input file for VizBin is a single .fna file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement. NOTE: When running VizBin, it is often preferable to split long contigs into smaller pieces in order to increase the density of clustering in the **t-SNE* . The data we are working with today are based on our bins output by DAS_Tool in the last binning exercise, but have been further processed using the cut_up_fasta.py script that comes with the binning tool CONCOCT to cut long contigs into 20k fragments. When reviewing our VizBin plots and outputs, it is important to remember that here we are looking at the fragmented sub-contigs , rather than the full complete contigs (the importance of this will be clear when we are reviewing our vb_count_table.txt later in this exercise).* In the interests of time today, the input files have been generated and are provided in the 6.bin_refinement/ folder: all_bins.fna is a concatenation of the bins of fragmented sub-contigs (fragmented to 20k) all_bins.sample1.vizbin.ann is the annotation file containing per-subcontig coverage, label (bin ID), and length values. For future reference, and for working with your own data, a step-by-step process for generating these files from the curated bins generated by DAS_Tool has been provided as an Appendix . Let's first have a quick look at the annotation file. head -n5 all_bins.sample1.vizbin.ann # coverage,label,length # 17.6361,bin_0.chopped,20000 # 16.2822,bin_0.chopped,20000 # 17.7862,bin_0.chopped,20000 # 16.8073,bin_0.chopped,20000 This file is a comma-delimited table (csv file) that presents the information in the way that VizBin expects it. The order of rows in this file corresponds to the order of contigs in the concatentated fastA file of our fragmented bins, all_bins.fna . Create a few variations of the .ann file with various columns removed, in order to examine the different outputs they can generate. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Make a few different versions of the .ann file with various columns removed ## annotation with bin only (to colour by bin) cut -f2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.bin_only.ann ## no length, but coverage and bin included cut -f1,2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.no_length.ann","title":"Prepare input files for VizBin"},{"location":"day2/ex9_refining_bins/#project-a-t-sne-and-examine-bin-clusters","text":"We can now use these files in VizBin to curate the contigs in our bins. We will load and view the data in a few different steps. NOTE: Running VizBin remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like VizBin ) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing VizBin on your local machine and downloading the relevant input files (e.g. via scp ... ) to run locally. VizBin cannot be opened within the NeSI Jupyter hub . For today's exercise, open VizBin via either: A. Install and run VizBin locally Download VizBin-dist.jar from here You may also need to install Java locally (choose the relevant version for your machine). Download the required files from NeSI to your machine (via scp , or right-click > download, from within the Jupyter hub ), or download the example files from this link . Double-click VizBin-dist.jar to open VizBin , and then follow the steps below. OR B. Log in via a standard terminal. Log into NeSI via a standard terminal ( not the terminal within Jupyter hub ) and run the following command ( NOTE: this may return an error if X11 fowarding is not set up for your machine. In this case, proceed with the first option above ). cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ java -jar ../tools/vizbin.jar If this fails to open on your PC, or if it runs prohibitively slowly, team up with 2-3 others in the workshop to run through this exercise together on one machine.","title":"Project a t-SNE and examine bin clusters"},{"location":"day2/ex9_refining_bins/#loading-the-input-files","text":"Once VizBin is open, to get started, simply click the 'Choose...' button then navigate to the fastA file. Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your 'bin only' .ann file into the 'Annotation file (optional)' field.","title":"Loading the input files"},{"location":"day2/ex9_refining_bins/#executing-the-t-sne","text":"For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following:","title":"Executing the t-SNE"},{"location":"day2/ex9_refining_bins/#contigs-coloured-by-bin","text":"","title":"Contigs coloured by bin"},{"location":"day2/ex9_refining_bins/#contigs-coloured-by-bin-sized-by-length-shaded-by-coverage","text":"Similar to any other projection technique, we interpret the closeness of points as a proxy for how similar they are, and because of our .ann file we can see which contigs belong to the same bin.","title":"Contigs coloured by bin, sized by length, shaded by coverage"},{"location":"day2/ex9_refining_bins/#picking-refined-bins","text":"We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands: Left-click and drag: Highlight an area of the ordination to zoom into Right-click, 'Zoom Out', 'Both Axes': Rest of the view Left-click several points: Create a selection of contigs to extract from the data Right-click, 'Selection', 'Export': Save the selected contigs into a new file Right-click, 'Selection', 'Clear selection': Clear the current selection How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed. Today, we will run through an example of selecting potentially problematic (sub)contigs, and then deciding whether or not we want to filter these contigs out of our refined bins. We can use a combination of VizBin and seqmagick to remove contigs from bins where we do not trust the placement of the contig. We are aiming to reduce each bin to a trusted set of contigs.","title":"Picking refined bins"},{"location":"day2/ex9_refining_bins/#1-export-vizbin-clusters","text":"First, for each VizBin cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as cluster_1.fna . Try this for one or two clusters. In practice, we would do this for each VizBin cluster, saving each as a new cluster_n.fna file.","title":"1. Export VizBin clusters"},{"location":"day2/ex9_refining_bins/#highlight-a-cluster-to-zoom-into","text":"","title":"Highlight a cluster to zoom into"},{"location":"day2/ex9_refining_bins/#select-the-cluster-to-export","text":"Left-click several points around the cluster","title":"Select the cluster to export"},{"location":"day2/ex9_refining_bins/#export-the-cluster","text":"Right-click, 'Selection', 'Export'. Save the output as cluster_1.fna .","title":"Export the cluster"},{"location":"day2/ex9_refining_bins/#2-export-potentially-problematic-contigs","text":"","title":"2. Export potentially problematic contigs"},{"location":"day2/ex9_refining_bins/#select-problematic-contigs-to-examine","text":"Zoom in, make a selection of potentially problematic contigs, and export as above. Try this for one or two problematic contigs (or subsets of contigs). In practice, you could repeat this for all potentially problemtic contigs, saving each export as a new contigs_n.fna file. NOTE: for the subsequent step using vizbin_count_table.sh , all exported cluster files must share a common prefix (e.g. cluster...fna ), and all files of problematic contigs must also share a common prefix (e.g. contigs...fna ).","title":"Select problematic contigs to examine"},{"location":"day2/ex9_refining_bins/#optional-refine-and-filter-problematic-contigs-from-bins","text":"","title":"Optional: Refine and filter problematic contigs from bins"},{"location":"day2/ex9_refining_bins/#create-a-count-table-of-counts-of-our-problematic-contigs-across-each-bin","text":"You'll recall that, prior running VizBin , the contigs in our bins were first cut into fragments to improve the density of the clusters in the t-SNE projection. As such, the problematic contigs we have exported from VizBin are sub-contig fragments, rather than full contigs from our bins. It is entirely possible that different fragments of the original contigs have been placed in different clusters during our VizBin analysis - including cases where most sub-contigs have clustered with the bin we expect, and a small number have been identified as \"problematic\" (i.e. clustered with other bins). Based on the information from these extracted problematic sub-contigs, we now have to carefully consider whether or not we want to remove the full contig from our bin data. To do this, we will generate a table containing each exported \"problematic\" sub-contig, and counts of how many of its sister sub-contigs (each of the other sub-contig fragments derived from the same original parent contig) fall into each VizBin cluster. For this exercise, a folder of the exported files from VizBin for all clusters ( cluster_[1-n].fna ) and problematic sub-contigs ( contigs_[1-n].fna ) has been provided at vizbin_example_exports/ We will input these files to the shell script vizbin_count_table.sh to generate a count table of the exprted subcontigs across each VizBin cluster ( vb_count_table.txt ), as well as a working list of contigs to potentially remove from our final bin data ( vb_omit_contigs_tmp.txt ). For future reference, a copy of this script is available for download here . ./vizbin_count_table.sh -i vizbin_example_exports/ The only required input to vizbin_count_table.sh is the path to the cluster and contigs files exported from VizBin . By default, the script looks for the prefix cluster... for the cluster file names, contig... for the files of problematic sub-contigs, and the file extension .fna for each. The arguments -s <contig_file_prefix> -c <cluster_file_prefix> -e <fasta_file_extension> can optionally be provided if your file name formats differ from the default. View the output count table: less vb_count_table.txt Example excerpt: Subcontig_ID Subcontig_vb_cluster cluster_10_count cluster_1_count cluster_2_count cluster_3_count cluster_4_count cluster_5_count cluster_6_count cluster_7_count cluster_8_count cluster_9_count Total_count >bin_5_NODE_826_length_1788_cov_0.154169 cluster_5 0 0 0 0 0 1 0 0 0 0 1 >bin_5_NODE_848_length_1686_cov_0.184026 cluster_5 0 0 0 0 0 1 0 0 0 0 1 >bin_9_NODE_4_length_793571_cov_0.517196.33 cluster_6 0 0 0 0 0 0 1 38 0 0 39 Note that in the case of the third contig from the excerpt above, the 'problematic' contig is only one of 39 sub-contigs, and all other 38 sub-contigs are in the expected cluster. In this case, we likely do not want to remove this contig from the bin.","title":"Create a count table of counts of our problematic contigs across each bin"},{"location":"day2/ex9_refining_bins/#generate-a-list-of-contigs-to-exclude-from-filtering","text":"Create a list of contigs identified from vb_count_table.txt that are not to be filtered out by seqmagick in the next step. For example, those contigs that have sub-contigs split across multiple vizbin clusters, and for which it's reasonable to actually keep the contig (such as when a flagged selected sub-contig exported from vizbin is in one unexpected cluster, but all other sub-contigs from that parent contig are in the expected cluster; in this case, you likely don't want to filter out the parent contig from the data set moving forward). Below is an example. Simply replace the contig IDs between the quotes for as many lines as necessary for your data. NOTES: The first line below must always have only one > character, while all subsequent lines must have two (i.e. >> ) to append correctly to the list. We want the original contig ID here, *not the sub-contig, so make sure to remove the .xx fragment number at the end if there is one.* echo \"bin_0_NODE_9_length_392609_cov_1.038712\" > vb_keep_contigs.txt echo \"bin_9_NODE_4_length_793571_cov_0.517196\" >> vb_keep_contigs.txt echo \"bin_1_NODE_182_length_42779_cov_1.585353\" >> vb_keep_contigs.txt","title":"Generate a list of contigs to exclude from filtering"},{"location":"day2/ex9_refining_bins/#create-final-vb_omit_contigs_filteredtxt-list-of-contigs-to-filter-from-bins","text":"Using grep , filter contigs we wish to keep (after assessing vb_count_table.txt ) out of the working vb_omit_contigs_tmp.txt list. This creates vb_omit_contigs_filtered.txt , which we will then pass to seqmagick to filter these contigs out of our actual bin fasta files. grep -v -f vb_keep_contigs.txt vb_omit_contigs_tmp.txt > vb_omit_contigs_filtered.txt","title":"Create final vb_omit_contigs_filtered.txt list of contigs to filter from bins"},{"location":"day2/ex9_refining_bins/#filter-suspect-contigs-based-on-vizbin-analysis-from-the-bin-data","text":"Use seqmagick --exclude-from-file ... to filter problematic contigs (those contigs listed in vb_omit_contigs_filtered.txt ) out of the initial unchopped bin fasta files, generating final bins for downstream processing. mkdir filtered_bins/ # Load seqmagick module purge module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 # filter problematic contigs out of original bin files for bin_file in example_data_unchopped/*.fna ; do bin_name = $( basename ${ bin_file } .fna ) seqmagick convert --exclude-from-file vb_omit_contigs_filtered.txt ${ bin_file } filtered_bins/ ${ bin_name } .filtered.fna done Our filtered bins for downstream use are now in filtered_bins/","title":"Filter suspect contigs (based on VizBin analysis) from the bin data"},{"location":"day2/ex9_refining_bins/#optional-comparing-pre-and-post-filtered-bins-via-checkm","text":"The end goal of this process is the generation of a final set of refined bins. Following this, the CheckM procedure should be re-run, this time on the refined filtered_bins/ . This provides CheckM metrics for the final actual (filtered) bin set, and also an opportunity to compare between pre- and post-filtering to see if the VizBin bin refinement steps have, for example, improved the degree of contamination in the bins. For this exercise, a copy of the output from running CheckM on the filtered_bins/ is available at 6.bin_refinement/filtered_bins_checkm.txt . View the previous CheckM output and the filtered bins output to compare via cat . cat filtered_bins_checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity bin_0.filtered k__Bacteria (UID3060) 138 338 246 1 329 7 1 0 0 99.59 2.98 0.00 bin_1.filtered k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 bin_2.filtered k__Bacteria (UID2565) 2921 149 91 10 137 2 0 0 0 91.21 0.61 0.00 bin_3.filtered g__Staphylococcus (UID301) 45 940 178 14 924 2 0 0 0 98.58 0.11 0.00 bin_4.filtered c__Betaproteobacteria (UID3959) 235 414 211 1 410 3 0 0 0 99.97 0.26 0.00 bin_5.filtered o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 bin_6.filtered c__Deltaproteobacteria (UID3218) 61 284 169 17 267 0 0 0 0 91.72 0.00 0.00 bin_7.filtered f__Bradyrhizobiaceae (UID3695) 47 693 296 2 691 0 0 0 0 99.80 0.00 0.00 bin_8.filtered p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 bin_9.filtered g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 cat ../5.binning/checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity maxbin.001_sub.contigs k__Bacteria (UID3060) 138 338 246 1 328 8 1 0 0 99.59 3.39 0.00 maxbin.002_sub.contigs k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 maxbin.009_sub.contigs k__Bacteria (UID2565) 2921 149 91 10 137 2 0 0 0 91.21 0.61 0.00 metabat.1.contigs c__Deltaproteobacteria (UID3218) 61 284 169 17 267 0 0 0 0 91.72 0.00 0.00 metabat.10.contigs g__Staphylococcus (UID301) 45 940 178 14 924 2 0 0 0 98.58 0.11 0.00 metabat.11.contigs c__Betaproteobacteria (UID3959) 235 414 211 1 408 5 0 0 0 99.97 0.90 0.00 metabat.12.contigs o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 metabat.4.contigs f__Bradyrhizobiaceae (UID3695) 47 693 296 2 691 0 0 0 0 99.80 0.00 0.00 metabat.6.contigs p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 metabat.8.contigs g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 An example of an updated slurm script to run CheckM on the filtered_bins/ is as follows: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name bin_eval_checkm #SBATCH --res SummerSchool #SBATCH --time 00:20:00 #SBATCH --mem 50GB #SBATCH --cpus-per-task 10 #SBATCH --error bin_eval_checkm.err #SBATCH --output bin_eval_checkm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ checkm lineage_wf -t 10 --pplacer_threads 10 -x fna \\ --tab_table -f filtered_bins_checkm.txt \\ filtered_bins/ filtered_bins_checkm_out/","title":"Optional: Comparing pre- and post-filtered bins via CheckM"},{"location":"day2/ex9_refining_bins/#optional-creating-new-vizbin-profiles-with-different-fragment-lengths","text":"The data you have been working with was created using the cut_up_fasta.py script that comes with the binning tool CONCOCT . It was run to cut contigs into 20k fragments, to better add density to the cluster. If you would like to visualise the data using different contig fragment sizes, you can create these using the following commands (replace YOUR_CONTIG_SIZE with the size of interest, e.g. 10000 ): module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16 mkdir custom_chop/ # Fragment contigs within each bin, outputting to custom_chop/ for bin_file in example_data_unchopped/* ; do bin_name = $( basename ${ bin_file } .fna ) cut_up_fasta.py -c YOUR_CONTIG_SIZE -o 0 --merge_last ${ bin_file } > custom_chop/ ${ bin_name } .chopped.fna done # Concatenate the chopped bins into single .fna cat custom_chop/*.fna > all_bins_custom_chop.fna You can open all_bins_custom_chop.fna in VizBin to view the clustering with this new fragmentation threshold. If you wish to also provide an annotation file to colour by bin, this can be generated with the following: # Set up annotation file headers echo \"label\" > custom_chop.vizbin.ann # loop through custom_chop .fna files for bin_file in custom_chop/*.fna ; do # extract bin ID binID = $( basename ${ bin_file } .fna ) # loop through each sequence header in bin_file, adding binID to custom_chop.vizbin.ann for each header present for header in ` grep \">\" ${ bin_file } ` ; do # Add binID to vizbin.ann for each header present echo \" ${ binID } \" >> custom_chop.vizbin.ann done done If you wish to generate the full annotation file, including coverage and length values, you will need to go through the process outlined in the Appendix for this exercise .","title":"Optional: Creating new VizBin profiles with different fragment lengths"},{"location":"day2/ex9_refining_bins/#optional-scripts-for-processing-data-with-esomana","text":"A suite of tools for creating input files for ESOMana can be found on github here . The tool ESOMana can be downloaded from SourceForge .","title":"Optional: Scripts for processing data with ESOMana"},{"location":"day3/ex10_viruses/","text":"Identifying viral contigs in metagenomic data \u00b6 Objectives \u00b6 Identifying viral contigs using VIBRANT Examine prophage identified by VIBRANT Examine viral metabolism and auxiliary metabolic genes (AMGs) outputs from VIBRANT Check quality and estimate completeness of the viral contigs via CheckV Identifying viral contigs \u00b6 Viral metagenomics is a rapidly progressing field, and new software are constantly being developed and released each year that aim to better identify and characterise viral genomic sequences from assembled metagenomic sequence reads. Currently, the most commonly used methods are VIBRANT , VirSorter , and VirFinder (or the machine learning implementation of this, DeepVirFinder ). A number of recent studies use either one of these tools, or a combination of several at once. VIBRANT uses a machine learning approach based on protein similarity (non-reference-based similarity searches with multiple HMM sets), and is in principle applicable to bacterial and archaeal DNA and RNA viruses, integrated proviruses (which are excised from contigs by VIBRANT ), and eukaryotic viruses. VirSorter uses a predicted protein homology reference database-based approach, together with searching for a number of pre-defined metrics based on known viral genomic features. The authors note that VirSorter is currently best applied to DNA viruses (including prophage, which are also excised from contigs by VirSorter ), but is likely poor with RNA viruses (from metatranscriptome data) and is also poor with eukaryotic viruses (as the database currently lacks eukaryotic viruses, and the genomic features incorporated were developed based on viruses of prokaryotes). Hot off the press: an early test version of VirSorter2 was recently released, which expands VirSorter 's target viruses to now include dsDNAphage, ssDNA and RNA viruses, and the viral groups Nucleocytoviricota and lavidaviridae. DeepVirFinder uses a machine learning based approach based on k -mer frequencies. Having developed a database of the differences in k -mer frequencies between prokaryote and viral genomes, VirFinder examines assembled contigs and identifies whether their k -mer frequencies are comparable to known viruses in the database, using this to predict viral genomic sequence. This method has some limitation based on the viruses that were included when building the database (bacterial DNA viruses, but very few archaeal viruses, and, at least in some versions of the software, no eukaryotic viruses). However, tools are also provided to build your own database should you wish to develop an expanded one. Due to its distinctive k -mer frequency-based approach, VirFinder may also have the capability of identifying some novel viruses overlooked by tools such as VIBRANT or VirSorter . Further documentation github.com/AnantharamanLab/VIBRANT github.com/simroux/VirSorter github.com/jessieren/DeepVirFinder Identifying viral contigs using VIBRANT \u00b6 For this exercise, we will use VIBRANT to identify viral contigs from our assembled contigs. As an added bonus, VIBRANT also provides outputs for gene and protein predictions, identified integrated prophage, and metabolism (including viral metabolism and auxiliary-metabolic genes (AMGs) - prokaryote host-derived genes that have been integrated in, and are encoded by, viral genomes). These exercises will take place in the 7.viruses/ folder. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/ Run Vibrant \u00b6 For this we will input the assembled contigs from the SPAdes assembly we performed earlier. These assembly files are available at 7.viruses/spades_assembly/ NOTE: The path to the required databases for the VIBRANT NeSI module have been linked to the variable $DB_PATH . This needs to be included in the command (using the -d flag) for VIBRANT to be able to locate them. Create a new script nano vibrant.sl Paste in the following (updating <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name vibrant #SBATCH --time 00:30:00 #SBATCH --mem=4GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=16 #SBATCH --error vibrant.err #SBATCH --output vibrant.out #SBATCH --profile=task #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load VIBRANT module module purge module load VIBRANT/1.2.1-gimkl-2020a # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/ mkdir -p vibrant # Run VIBRANT srun VIBRANT_run.py -t 16 \\ -i spades_assembly/spades_assembly.m1000.fna \\ -d $DB_PATH \\ -folder vibrant/ Submit the script as a slurm job sbatch vibrant.sl Examine outputs of VIBRANT \u00b6 Exercise: VIBRANT provides a number of different outputs. Explore through the various folders within the vibrant/ folder and identify some that might be of particular interest. Open some of these files to see if you can find the following information: How many viral contigs did VIBRANT identify? Of these, how many did VIBRANT identify as prophage? What are some annotations of interest within the output annotations file? NOTE: the VIBRANT annotations file includes multiple columns for both **prokaryote* and viral protein predictions. Be careful as to which column you are looking at (as well as its associated confidence score) when assessing viral annotations vs. AMGs*. Among these annotations, how many were flagged as AMGs? What broad metabolic categories did the AMGs fall into? NOTE: as well as providing this information in table format, VIBRANT also generates a number of summary figures. These figures are not viewable within NeSI, but can be downloaded and opened on your local machine (e.g. via scp ... ) if you wish to look more closely at these . Discussion point: How might we investigate whether identified putative AMGs are actually within the viral genomes, rather than residual contaminating host genomic sequence attached to the end of integrated prophage (but incompletely trimmed off in the excision process)? Check quality and estimate completeness of the viral contigs via CheckV \u00b6 CheckV was recently developed as an analogue to CheckM . CheckV first performs a 'contaminating sequence' trim, removing any retained (prokaryote) host sequence on the end of contigs with integrated prophage, and then assesses the quality and completeness of the assembled viral contigs. The quality of the contigs are also categoriesed based on the recently developed Minimum Information about an Unclutivated Virus Genome (MIUViG) standards for reporting sequences of unclutivated virus geneomes (such as those recovered from metagenomic sequencing data). The MIUViG were developed as an extension of the Minimum Information about any (x) Sequence ( MIxS ) standards, which include, among others, standards for Metagenome-Assembled Genomes (MIMAG). Installation and further instructions for CheckV can be found here . Run CheckV \u00b6 Run CheckV providing the fastA file of combined (virus and prophage) viral contigs output by VIBRANT as input ( spades_assembly.m1000.phages_combined.fna ). Create a new script nano checkv.sl Paste in the following (updating <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name checkv #SBATCH --time 00:10:00 #SBATCH --mem=3GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH --error checkv.err #SBATCH --output checkv.out #SBATCH --profile=task #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load the module module purge module load CheckV/0.7.0-gimkl-2020a-Python-3.8.2 # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/ mkdir -p checkv_out # Run main analyses checkv_in = \"vibrant/VIBRANT_spades_assembly.m1000/VIBRANT_phages_spades_assembly.m1000/spades_assembly.m1000.phages_combined.fna\" srun checkv end_to_end ${ checkv_in } checkv_out -t 10 --quiet Submit the script as a slurm job sbatch checkv.sl Examine outputs of CheckV \u00b6 CheckV provides summary outputs for contamination, completeness, repeats, and an overall quality summary. Have a brief look at some examples of the information you can draw from each of these CheckV outputs. Exercise: Examining checkv_out/quality_summary.tsv How many viral contigs meet the \"High-quality\" (MIUViG) standard? How many might we consider \"complete\" genomes based on CheckV 's completeness estimation? Were any terminal repeat regions identified for any of the contigs? How many prophage were identified by CheckV ? Why might this differ to VIBRANT 's identification of prophage above? Are there any suspicious contigs that you might want to flag for closer examination (and/or careful consideration in downstream analyses)?","title":"Identifying viral contigs in metagenomic data"},{"location":"day3/ex10_viruses/#identifying-viral-contigs-in-metagenomic-data","text":"","title":"Identifying viral contigs in metagenomic data"},{"location":"day3/ex10_viruses/#objectives","text":"Identifying viral contigs using VIBRANT Examine prophage identified by VIBRANT Examine viral metabolism and auxiliary metabolic genes (AMGs) outputs from VIBRANT Check quality and estimate completeness of the viral contigs via CheckV","title":"Objectives"},{"location":"day3/ex10_viruses/#identifying-viral-contigs","text":"Viral metagenomics is a rapidly progressing field, and new software are constantly being developed and released each year that aim to better identify and characterise viral genomic sequences from assembled metagenomic sequence reads. Currently, the most commonly used methods are VIBRANT , VirSorter , and VirFinder (or the machine learning implementation of this, DeepVirFinder ). A number of recent studies use either one of these tools, or a combination of several at once. VIBRANT uses a machine learning approach based on protein similarity (non-reference-based similarity searches with multiple HMM sets), and is in principle applicable to bacterial and archaeal DNA and RNA viruses, integrated proviruses (which are excised from contigs by VIBRANT ), and eukaryotic viruses. VirSorter uses a predicted protein homology reference database-based approach, together with searching for a number of pre-defined metrics based on known viral genomic features. The authors note that VirSorter is currently best applied to DNA viruses (including prophage, which are also excised from contigs by VirSorter ), but is likely poor with RNA viruses (from metatranscriptome data) and is also poor with eukaryotic viruses (as the database currently lacks eukaryotic viruses, and the genomic features incorporated were developed based on viruses of prokaryotes). Hot off the press: an early test version of VirSorter2 was recently released, which expands VirSorter 's target viruses to now include dsDNAphage, ssDNA and RNA viruses, and the viral groups Nucleocytoviricota and lavidaviridae. DeepVirFinder uses a machine learning based approach based on k -mer frequencies. Having developed a database of the differences in k -mer frequencies between prokaryote and viral genomes, VirFinder examines assembled contigs and identifies whether their k -mer frequencies are comparable to known viruses in the database, using this to predict viral genomic sequence. This method has some limitation based on the viruses that were included when building the database (bacterial DNA viruses, but very few archaeal viruses, and, at least in some versions of the software, no eukaryotic viruses). However, tools are also provided to build your own database should you wish to develop an expanded one. Due to its distinctive k -mer frequency-based approach, VirFinder may also have the capability of identifying some novel viruses overlooked by tools such as VIBRANT or VirSorter . Further documentation github.com/AnantharamanLab/VIBRANT github.com/simroux/VirSorter github.com/jessieren/DeepVirFinder","title":"Identifying viral contigs"},{"location":"day3/ex10_viruses/#identifying-viral-contigs-using-vibrant","text":"For this exercise, we will use VIBRANT to identify viral contigs from our assembled contigs. As an added bonus, VIBRANT also provides outputs for gene and protein predictions, identified integrated prophage, and metabolism (including viral metabolism and auxiliary-metabolic genes (AMGs) - prokaryote host-derived genes that have been integrated in, and are encoded by, viral genomes). These exercises will take place in the 7.viruses/ folder. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/","title":"Identifying viral contigs using VIBRANT"},{"location":"day3/ex10_viruses/#run-vibrant","text":"For this we will input the assembled contigs from the SPAdes assembly we performed earlier. These assembly files are available at 7.viruses/spades_assembly/ NOTE: The path to the required databases for the VIBRANT NeSI module have been linked to the variable $DB_PATH . This needs to be included in the command (using the -d flag) for VIBRANT to be able to locate them. Create a new script nano vibrant.sl Paste in the following (updating <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name vibrant #SBATCH --time 00:30:00 #SBATCH --mem=4GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=16 #SBATCH --error vibrant.err #SBATCH --output vibrant.out #SBATCH --profile=task #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load VIBRANT module module purge module load VIBRANT/1.2.1-gimkl-2020a # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/ mkdir -p vibrant # Run VIBRANT srun VIBRANT_run.py -t 16 \\ -i spades_assembly/spades_assembly.m1000.fna \\ -d $DB_PATH \\ -folder vibrant/ Submit the script as a slurm job sbatch vibrant.sl","title":"Run Vibrant"},{"location":"day3/ex10_viruses/#examine-outputs-of-vibrant","text":"Exercise: VIBRANT provides a number of different outputs. Explore through the various folders within the vibrant/ folder and identify some that might be of particular interest. Open some of these files to see if you can find the following information: How many viral contigs did VIBRANT identify? Of these, how many did VIBRANT identify as prophage? What are some annotations of interest within the output annotations file? NOTE: the VIBRANT annotations file includes multiple columns for both **prokaryote* and viral protein predictions. Be careful as to which column you are looking at (as well as its associated confidence score) when assessing viral annotations vs. AMGs*. Among these annotations, how many were flagged as AMGs? What broad metabolic categories did the AMGs fall into? NOTE: as well as providing this information in table format, VIBRANT also generates a number of summary figures. These figures are not viewable within NeSI, but can be downloaded and opened on your local machine (e.g. via scp ... ) if you wish to look more closely at these . Discussion point: How might we investigate whether identified putative AMGs are actually within the viral genomes, rather than residual contaminating host genomic sequence attached to the end of integrated prophage (but incompletely trimmed off in the excision process)?","title":"Examine outputs of VIBRANT"},{"location":"day3/ex10_viruses/#check-quality-and-estimate-completeness-of-the-viral-contigs-via-checkv","text":"CheckV was recently developed as an analogue to CheckM . CheckV first performs a 'contaminating sequence' trim, removing any retained (prokaryote) host sequence on the end of contigs with integrated prophage, and then assesses the quality and completeness of the assembled viral contigs. The quality of the contigs are also categoriesed based on the recently developed Minimum Information about an Unclutivated Virus Genome (MIUViG) standards for reporting sequences of unclutivated virus geneomes (such as those recovered from metagenomic sequencing data). The MIUViG were developed as an extension of the Minimum Information about any (x) Sequence ( MIxS ) standards, which include, among others, standards for Metagenome-Assembled Genomes (MIMAG). Installation and further instructions for CheckV can be found here .","title":"Check quality and estimate completeness of the viral contigs via CheckV"},{"location":"day3/ex10_viruses/#run-checkv","text":"Run CheckV providing the fastA file of combined (virus and prophage) viral contigs output by VIBRANT as input ( spades_assembly.m1000.phages_combined.fna ). Create a new script nano checkv.sl Paste in the following (updating <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name checkv #SBATCH --time 00:10:00 #SBATCH --mem=3GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH --error checkv.err #SBATCH --output checkv.out #SBATCH --profile=task #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load the module module purge module load CheckV/0.7.0-gimkl-2020a-Python-3.8.2 # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/7.viruses/ mkdir -p checkv_out # Run main analyses checkv_in = \"vibrant/VIBRANT_spades_assembly.m1000/VIBRANT_phages_spades_assembly.m1000/spades_assembly.m1000.phages_combined.fna\" srun checkv end_to_end ${ checkv_in } checkv_out -t 10 --quiet Submit the script as a slurm job sbatch checkv.sl","title":"Run CheckV"},{"location":"day3/ex10_viruses/#examine-outputs-of-checkv","text":"CheckV provides summary outputs for contamination, completeness, repeats, and an overall quality summary. Have a brief look at some examples of the information you can draw from each of these CheckV outputs. Exercise: Examining checkv_out/quality_summary.tsv How many viral contigs meet the \"High-quality\" (MIUViG) standard? How many might we consider \"complete\" genomes based on CheckV 's completeness estimation? Were any terminal repeat regions identified for any of the contigs? How many prophage were identified by CheckV ? Why might this differ to VIBRANT 's identification of prophage above? Are there any suspicious contigs that you might want to flag for closer examination (and/or careful consideration in downstream analyses)?","title":"Examine outputs of CheckV"},{"location":"day3/ex11_coverage_and_taxonomy/","text":"Per-sample coverage and assigning taxonomy \u00b6 Objectives \u00b6 Calculate per-sample coverage stats for the filtered prokaryote bins Calculate per-sample coverage stats for the viral contigs output by VIBRANT Assign taxonomy to the refined bins Introduction to vContact2 for predicting taxonomy of viral contigs Calculate per-sample coverage stats of the filtered prokaryote bins \u00b6 One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? With bins of metagenome-assembled genome (MAG) data, we can investigate this by mapping the quality-filtered unassembled reads back to the refined bins to then generate coverage profiles. Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. As per the preparation step at the start of the binning process, we can do this using read mapping tools such as Bowtie , Bowtie2 , and BBMap . Here we will follow the same steps as before using Bowtie2 , samtools , and MetaBAT 's jgi_summarize_bam_contig_depths , but this time inputting our refined filtered bins. These exercises will take place in the 8.coverage_and_taxonomy/ folder. Our final filtered refined bins from the previous bin refinement exercise have been copied to the 8.coverage_and_taxonomy/filtered_bins/ folder. First, concatenate the bin data into a single file to then use to generate an index for the read mapper. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ cat filtered_bins/*.fna > filtered_bins.fna Now build the index for Bowtie2 using the concatenated bin data. We will also make a new directory bin_coverage/ to store the index and read mapping output into. mkdir -p bin_coverage/ # Load Bowtie2 module purge module load Bowtie2/2.3.5-GCC-7.4.0 # Build Bowtie2 index bowtie2-build filtered_bins.fna bin_coverage/bw_bins Map the quality-filtered reads (from ../3.assembly/ ) to the index using Bowtie2 , and sort and convert to .bam format via samtools . Create a new script nano mapping_bins.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH --res SummerSchool #SBATCH -J mapping_filtered_bins #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH -e mapping_filtered_bins.err #SBATCH -o mapping_filtered_bins.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x bin_coverage/bw_bins \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S bin_coverage/ ${ i } .sam # Step 3 samtools sort -@ 10 -o bin_coverage/ ${ i } .bam bin_coverage/ ${ i } .sam done Submit the script sbatch mapping_bins.sl Finally, generate the per-sample coverage table for each contig in each bin via MetaBAT 's jgi_summarize_bam_contig_depths . # Load MetaBAT module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth bins_cov_table.txt bin_coverage/sample*.bam The coverage table will be generated as bins_cov_table.txt . As before, the key columns of interest are the contigName , and each sample[1-n].bam column. NOTE: Here we are generating a per-sample table of coverage values for **each contig* within each bin. To get per-sample coverage of each bin as a whole, we will need to generate average coverage values based on all contigs contained within each bin. We will do this in R during our data visualisation exercises on day 4 of the workshop, leveraging the fact that we added bin IDs to the sequence headers.* Normalising coverage values \u00b6 Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. For an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix . Calculate per-sample coverage stats of viral contigs \u00b6 Here we can follow the same steps as outlined above for the bin data, but with a concatenated fastA file of viral contigs. To quickly recap: In previous exercises, we first used VIBRANT to identify viral contigs from the assembled reads, generating a new fasta file of viral contigs: spades_assembly.m1000.phages_combined.fna We then processed this file using CheckV to generate quality information for each contig, and to further trim any retained (prokaryote) sequence on the ends of prophage contigs. The resultant fasta files generated by CheckV ( proviruses.fna and viruses.fna ) have been copied to to the 8.coverage_and_taxonomy/checkv folder for use in this exercise. NOTE: due to the rapid mutation rates of viruses, with full data sets it will likely be preferable to first further reduce viral contigs down based on a percentage-identity threshold using a tool such as BBMap 's dedupe.sh . This would be a necessary step in cases where you had opted for generating multiple individual assemblies or mini-co-assemblies (and would be comparable to the use of a tool like dRep for prokaryote data), but may still be useful even in the case of single co-assemblies incorporating all samples. We will first need to concatenate these files together. cat checkv/proviruses.fna checkv/viruses.fna > checkv_combined.fna Now build the index for Bowtie2 using the concatenated viral contig data. We will also make a new directory viruses_coverage/ to store the index and read mapping output into. mkdir -p viruses_coverage/ # Load Bowtie2 module load Bowtie2/2.3.5-GCC-7.4.0 # Build Bowtie2 index bowtie2-build checkv_combined.fna viruses_coverage/bw_viruses Map the quality-filtered reads (from ../3.assembly/ ) to the index using Bowtie2 , and sort and convert to .bam format via samtools . Create a new script nano mapping_viruses.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name mapping_filtered_viruses #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --cpus-per-task 10 #SBATCH --error mapping_filtered_viruses.err #SBATCH --output mapping_filtered_viruses.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x viruses_coverage/bw_viruses \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S viruses_coverage/ ${ i } .sam # Step 3 samtools sort -@ 10 -o viruses_coverage/ ${ i } .bam viruses_coverage/ ${ i } .sam done Run the script sbatch mapping_viruses.sl Finally, generate the per-sample coverage table for each viral contig via MetaBAT 's jgi_summarize_bam_contig_depths . # Load MetaBAT module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth viruses_cov_table.txt viruses_coverage/sample*.bam The coverage table will be generated as viruses_cov_table.txt . As before, the key columns of interest are the contigName , and each sample[1-n].bam column. NOTE: Unlike the prokaryote data, we have not used a binning process on the viral contigs (since many of the binning tools use hallmark characteristics of prokaryotes in the binning process). Here, viruses_cov_table.txt is the final coverage table. This can be combined with CheckV quality and completeness metrics to, for example, examine the coverage profiles of only those viral contigs considered to be \"High-quality\" or \"Complete\". Normalising coverage values \u00b6 Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. For an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix . Assign taxonomy to the refined bins \u00b6 It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us: Extract 16S rRNA gene sequences from the MAGs and classify them Annotate each gene in the MAG and take the consensus taxonomy Use a profiling tool like Kraken , which matches pieces of DNA to a reference database using k -mer searches Identify a core set of genes in the MAG, and use these to compute a species phylogeny For this exercise, we will use the last option in the list, making use of the GTDB-TK software (available on github ) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, GTDB-TK will perform the following steps on a set of bins. Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG. Based on the recovered numbers, identify which domain is a more likely assignment for each MAG Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions Filter the alignment down to approximately 5,000 informative sites Insert each MAG into a reference tree create from type material and published MAGs Scale the branch lengths of the resulting tree, as described in Parks et al. , to identify an appropriate rank to each branch event in the tree Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree Report the resulting taxonomic assignment, and gene alignment This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process. Create a new script nano gtdbtk_test.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash #SBATCH --account nesi02659 #SBATCH --job-name gtdbtk_test #SBATCH --res SummerSchool #SBATCH --time 00:30:00 #SBATCH --mem 140GB #SBATCH --cpus-per-task 10 #SBATCH --error gtdbtk_test.err #SBATCH --output gtdbtk_test.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load GTDB-Tk/1.5.0-gimkl-2020a-Python-3.8.2 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy gtdbtk classify_wf -x fna --cpus 10 --genome_dir filtered_bins/ --out_dir gtdbtk_out/ Submit the script sbatch gtdbtk_test.sl As usual, lets look at the parameters here Parameter Function classify_wf Specifies the sub-workflow from GTDB-TK that we wish to use -x ... Specify the file extension for MAGs within our input directory. Default is .fna , but it's always good practice to specify it anyway --cpus ... Number of CPUs to use when finding marker genes, and performing tree insertion operations --genome_dir ... Input directory containing MAGs as individual fastA files --out_dir ... Output directory to write the final set of files Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw DAS_Tool outputs in the ../6.bin_refinement/dastool_out/_DASTool_bins/ folder, the renamed set of bins in the ../6.bin_refinement/example_data_unchopped/ folder, the set of curated bins in the filtered_bins/ folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here. When the task completes, you will have a number of output files provided. The main ones to look for are gtdbtk.bac120.summary.tsv and gtdbtk.arch122.summary.tsv which report the taoxnomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the gtdbtk.arch122.summary.tsv file. If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the gtdbtk.bac120.msa.fasta and gtdbtk.arch122.msa.fasta files. Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop. Introduction to vContact2 for predicting taxonomy of viral contigs \u00b6 Even more so than prokaryote taxonomy, establishing a coherent system for viral taxonomy is complex and continues to evolve. Just in the last year, the International Committee on Taxonomy of Viruses ( ICTV ) overhauled the classification code into 15 hierarchical ranks . Furthermore, the knowledge gap in databases of known and taxonomically assigned viruses remains substantial, and so identifying the putative taxonomy of viral contigs from environmental metagenomics data remains challenging. There are a number of approaches that can be used to attempt to predict the taxonomy of the set of putative viral contigs output by programs such as VIBRANT , VirSorter , and VirFinder . vContact2 is one such method that uses 'guilt-by-contig-association' to predict the potential taxonomy of viral genomic sequence data based on relatedness to known viruses within a reference database (such as viral RefSeq). The principle is that, to the extent that the 'unknown' viral contigs cluster closely with known viral genomes, we can then expect that they are closely related enough to be able to predict a shared taxonomic rank. NOTE: Anecdotally, however, in my own experience with this processes I have unfortunately been unable to predict the taxonomy of the vast majority of the viral contigs ouput by VIBRANT , VirSorter , or VirFinder from an environmental metagenomic data set (due to not clustering closely enough with known viruses in the reference database). Running vContact2 can require a considerable amount of computational resources, and so we won't be running this in the workshop today. The required process is outlined for reference in an Appendix for this exercise , should you wish to experiment with this on your own data in the future. For today, we have provided the final two output files from this process when applied to our mock metagenome data. These can be viewed in the folder 8.coverage_and_taxonomy/vConTACT2_Results/ via head or less . less vConTACT2_Results/genome_by_genome_overview.csv less vConTACT2_Results/tax_predict_table.txt A few notes to consider: You will see that the genome_by_genome_overview.csv file contains entries for the full reference database used as well as the input viral contigs (contigs starting with NODE ). You can use a command such as grep \"NODE\" vConTACT2_Results/genome_by_genome_overview.csv | less to view only the lines for the input contigs of interest. Note also that these lines however will not contain taxonomy information. See the notes in the Appendix for further information about why this might be. As per the notes in the Appendix , the tax_predict_table.txt file contains predictions of potential taxonomy (and or taxonom*ies*) of the input viral contigs for order, family, and genus, based on whether they clustered with any viruses in the reference database. Bear in mind that these may be lists of multiple potential taxonomies, in the cases where viral contigs clustered with multiple reference viruses representing more than one taxonomy at the given rank. NOTE: The taxonomies are deliberately enclosed in square brackets ( [ ] ) to highlight the fact that these are **predictions* , rather than definitive taxonomy assignments .*","title":"Per-sample coverage and assigning taxonomy"},{"location":"day3/ex11_coverage_and_taxonomy/#per-sample-coverage-and-assigning-taxonomy","text":"","title":"Per-sample coverage and assigning taxonomy"},{"location":"day3/ex11_coverage_and_taxonomy/#objectives","text":"Calculate per-sample coverage stats for the filtered prokaryote bins Calculate per-sample coverage stats for the viral contigs output by VIBRANT Assign taxonomy to the refined bins Introduction to vContact2 for predicting taxonomy of viral contigs","title":"Objectives"},{"location":"day3/ex11_coverage_and_taxonomy/#calculate-per-sample-coverage-stats-of-the-filtered-prokaryote-bins","text":"One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? With bins of metagenome-assembled genome (MAG) data, we can investigate this by mapping the quality-filtered unassembled reads back to the refined bins to then generate coverage profiles. Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. As per the preparation step at the start of the binning process, we can do this using read mapping tools such as Bowtie , Bowtie2 , and BBMap . Here we will follow the same steps as before using Bowtie2 , samtools , and MetaBAT 's jgi_summarize_bam_contig_depths , but this time inputting our refined filtered bins. These exercises will take place in the 8.coverage_and_taxonomy/ folder. Our final filtered refined bins from the previous bin refinement exercise have been copied to the 8.coverage_and_taxonomy/filtered_bins/ folder. First, concatenate the bin data into a single file to then use to generate an index for the read mapper. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ cat filtered_bins/*.fna > filtered_bins.fna Now build the index for Bowtie2 using the concatenated bin data. We will also make a new directory bin_coverage/ to store the index and read mapping output into. mkdir -p bin_coverage/ # Load Bowtie2 module purge module load Bowtie2/2.3.5-GCC-7.4.0 # Build Bowtie2 index bowtie2-build filtered_bins.fna bin_coverage/bw_bins Map the quality-filtered reads (from ../3.assembly/ ) to the index using Bowtie2 , and sort and convert to .bam format via samtools . Create a new script nano mapping_bins.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH --res SummerSchool #SBATCH -J mapping_filtered_bins #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH -e mapping_filtered_bins.err #SBATCH -o mapping_filtered_bins.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x bin_coverage/bw_bins \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S bin_coverage/ ${ i } .sam # Step 3 samtools sort -@ 10 -o bin_coverage/ ${ i } .bam bin_coverage/ ${ i } .sam done Submit the script sbatch mapping_bins.sl Finally, generate the per-sample coverage table for each contig in each bin via MetaBAT 's jgi_summarize_bam_contig_depths . # Load MetaBAT module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth bins_cov_table.txt bin_coverage/sample*.bam The coverage table will be generated as bins_cov_table.txt . As before, the key columns of interest are the contigName , and each sample[1-n].bam column. NOTE: Here we are generating a per-sample table of coverage values for **each contig* within each bin. To get per-sample coverage of each bin as a whole, we will need to generate average coverage values based on all contigs contained within each bin. We will do this in R during our data visualisation exercises on day 4 of the workshop, leveraging the fact that we added bin IDs to the sequence headers.*","title":"Calculate per-sample coverage stats of the filtered prokaryote bins"},{"location":"day3/ex11_coverage_and_taxonomy/#normalising-coverage-values","text":"Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. For an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix .","title":"Normalising coverage values"},{"location":"day3/ex11_coverage_and_taxonomy/#calculate-per-sample-coverage-stats-of-viral-contigs","text":"Here we can follow the same steps as outlined above for the bin data, but with a concatenated fastA file of viral contigs. To quickly recap: In previous exercises, we first used VIBRANT to identify viral contigs from the assembled reads, generating a new fasta file of viral contigs: spades_assembly.m1000.phages_combined.fna We then processed this file using CheckV to generate quality information for each contig, and to further trim any retained (prokaryote) sequence on the ends of prophage contigs. The resultant fasta files generated by CheckV ( proviruses.fna and viruses.fna ) have been copied to to the 8.coverage_and_taxonomy/checkv folder for use in this exercise. NOTE: due to the rapid mutation rates of viruses, with full data sets it will likely be preferable to first further reduce viral contigs down based on a percentage-identity threshold using a tool such as BBMap 's dedupe.sh . This would be a necessary step in cases where you had opted for generating multiple individual assemblies or mini-co-assemblies (and would be comparable to the use of a tool like dRep for prokaryote data), but may still be useful even in the case of single co-assemblies incorporating all samples. We will first need to concatenate these files together. cat checkv/proviruses.fna checkv/viruses.fna > checkv_combined.fna Now build the index for Bowtie2 using the concatenated viral contig data. We will also make a new directory viruses_coverage/ to store the index and read mapping output into. mkdir -p viruses_coverage/ # Load Bowtie2 module load Bowtie2/2.3.5-GCC-7.4.0 # Build Bowtie2 index bowtie2-build checkv_combined.fna viruses_coverage/bw_viruses Map the quality-filtered reads (from ../3.assembly/ ) to the index using Bowtie2 , and sort and convert to .bam format via samtools . Create a new script nano mapping_viruses.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --res SummerSchool #SBATCH --job-name mapping_filtered_viruses #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --cpus-per-task 10 #SBATCH --error mapping_filtered_viruses.err #SBATCH --output mapping_filtered_viruses.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x viruses_coverage/bw_viruses \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S viruses_coverage/ ${ i } .sam # Step 3 samtools sort -@ 10 -o viruses_coverage/ ${ i } .bam viruses_coverage/ ${ i } .sam done Run the script sbatch mapping_viruses.sl Finally, generate the per-sample coverage table for each viral contig via MetaBAT 's jgi_summarize_bam_contig_depths . # Load MetaBAT module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth viruses_cov_table.txt viruses_coverage/sample*.bam The coverage table will be generated as viruses_cov_table.txt . As before, the key columns of interest are the contigName , and each sample[1-n].bam column. NOTE: Unlike the prokaryote data, we have not used a binning process on the viral contigs (since many of the binning tools use hallmark characteristics of prokaryotes in the binning process). Here, viruses_cov_table.txt is the final coverage table. This can be combined with CheckV quality and completeness metrics to, for example, examine the coverage profiles of only those viral contigs considered to be \"High-quality\" or \"Complete\".","title":"Calculate per-sample coverage stats of viral contigs"},{"location":"day3/ex11_coverage_and_taxonomy/#normalising-coverage-values_1","text":"Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. For an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix .","title":"Normalising coverage values"},{"location":"day3/ex11_coverage_and_taxonomy/#assign-taxonomy-to-the-refined-bins","text":"It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us: Extract 16S rRNA gene sequences from the MAGs and classify them Annotate each gene in the MAG and take the consensus taxonomy Use a profiling tool like Kraken , which matches pieces of DNA to a reference database using k -mer searches Identify a core set of genes in the MAG, and use these to compute a species phylogeny For this exercise, we will use the last option in the list, making use of the GTDB-TK software (available on github ) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, GTDB-TK will perform the following steps on a set of bins. Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG. Based on the recovered numbers, identify which domain is a more likely assignment for each MAG Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions Filter the alignment down to approximately 5,000 informative sites Insert each MAG into a reference tree create from type material and published MAGs Scale the branch lengths of the resulting tree, as described in Parks et al. , to identify an appropriate rank to each branch event in the tree Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree Report the resulting taxonomic assignment, and gene alignment This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process. Create a new script nano gtdbtk_test.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash #SBATCH --account nesi02659 #SBATCH --job-name gtdbtk_test #SBATCH --res SummerSchool #SBATCH --time 00:30:00 #SBATCH --mem 140GB #SBATCH --cpus-per-task 10 #SBATCH --error gtdbtk_test.err #SBATCH --output gtdbtk_test.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load GTDB-Tk/1.5.0-gimkl-2020a-Python-3.8.2 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy gtdbtk classify_wf -x fna --cpus 10 --genome_dir filtered_bins/ --out_dir gtdbtk_out/ Submit the script sbatch gtdbtk_test.sl As usual, lets look at the parameters here Parameter Function classify_wf Specifies the sub-workflow from GTDB-TK that we wish to use -x ... Specify the file extension for MAGs within our input directory. Default is .fna , but it's always good practice to specify it anyway --cpus ... Number of CPUs to use when finding marker genes, and performing tree insertion operations --genome_dir ... Input directory containing MAGs as individual fastA files --out_dir ... Output directory to write the final set of files Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw DAS_Tool outputs in the ../6.bin_refinement/dastool_out/_DASTool_bins/ folder, the renamed set of bins in the ../6.bin_refinement/example_data_unchopped/ folder, the set of curated bins in the filtered_bins/ folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here. When the task completes, you will have a number of output files provided. The main ones to look for are gtdbtk.bac120.summary.tsv and gtdbtk.arch122.summary.tsv which report the taoxnomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the gtdbtk.arch122.summary.tsv file. If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the gtdbtk.bac120.msa.fasta and gtdbtk.arch122.msa.fasta files. Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop.","title":"Assign taxonomy to the refined bins"},{"location":"day3/ex11_coverage_and_taxonomy/#introduction-to-vcontact2-for-predicting-taxonomy-of-viral-contigs","text":"Even more so than prokaryote taxonomy, establishing a coherent system for viral taxonomy is complex and continues to evolve. Just in the last year, the International Committee on Taxonomy of Viruses ( ICTV ) overhauled the classification code into 15 hierarchical ranks . Furthermore, the knowledge gap in databases of known and taxonomically assigned viruses remains substantial, and so identifying the putative taxonomy of viral contigs from environmental metagenomics data remains challenging. There are a number of approaches that can be used to attempt to predict the taxonomy of the set of putative viral contigs output by programs such as VIBRANT , VirSorter , and VirFinder . vContact2 is one such method that uses 'guilt-by-contig-association' to predict the potential taxonomy of viral genomic sequence data based on relatedness to known viruses within a reference database (such as viral RefSeq). The principle is that, to the extent that the 'unknown' viral contigs cluster closely with known viral genomes, we can then expect that they are closely related enough to be able to predict a shared taxonomic rank. NOTE: Anecdotally, however, in my own experience with this processes I have unfortunately been unable to predict the taxonomy of the vast majority of the viral contigs ouput by VIBRANT , VirSorter , or VirFinder from an environmental metagenomic data set (due to not clustering closely enough with known viruses in the reference database). Running vContact2 can require a considerable amount of computational resources, and so we won't be running this in the workshop today. The required process is outlined for reference in an Appendix for this exercise , should you wish to experiment with this on your own data in the future. For today, we have provided the final two output files from this process when applied to our mock metagenome data. These can be viewed in the folder 8.coverage_and_taxonomy/vConTACT2_Results/ via head or less . less vConTACT2_Results/genome_by_genome_overview.csv less vConTACT2_Results/tax_predict_table.txt A few notes to consider: You will see that the genome_by_genome_overview.csv file contains entries for the full reference database used as well as the input viral contigs (contigs starting with NODE ). You can use a command such as grep \"NODE\" vConTACT2_Results/genome_by_genome_overview.csv | less to view only the lines for the input contigs of interest. Note also that these lines however will not contain taxonomy information. See the notes in the Appendix for further information about why this might be. As per the notes in the Appendix , the tax_predict_table.txt file contains predictions of potential taxonomy (and or taxonom*ies*) of the input viral contigs for order, family, and genus, based on whether they clustered with any viruses in the reference database. Bear in mind that these may be lists of multiple potential taxonomies, in the cases where viral contigs clustered with multiple reference viruses representing more than one taxonomy at the given rank. NOTE: The taxonomies are deliberately enclosed in square brackets ( [ ] ) to highlight the fact that these are **predictions* , rather than definitive taxonomy assignments .*","title":"Introduction to vContact2 for predicting taxonomy of viral contigs"},{"location":"day3/ex12_gene_prediction/","text":"Gene prediction \u00b6 Objectives \u00b6 Overview/refresher of prodigal Predicting protein coding sequences in metagenome-assembled genomes Predicting RNA features and non-coding regions Overview/refresher of prodigal \u00b6 At this stage we have recovered a number of high quality genomes or population genomes. While there are interesting biological questions we can ask of the genomes at the DNA/organisational level, it is more likely that we are interested in the genes present in the organism. How we predict genes in the metagenomic data varies depending on what features we are trying to detect. Most often, we are interested in putatively protein coding regions and open reading frames. For features that are functional but not not translated, such as ribosomal RNA and tRNA sequences we need to use alternative tools. When considering protein coding sequences, we avoid the use of the term 'open reading frame' (ORF). The nature of a fragmented assembly is that you may encounter a partial gene on the start or end of a contig that is a function gene, but lacks the start or stop codon due to issues with assembly or sequencing depth. There are many software tools to predict gene sequences and in this workshop we will start with the tool prodigal (PROkaryotic\u202fDynamic Programming Genefinding\u202fALgorithm). prodigal has gone on to become one of the most popular microbial gene prediction algorithms as in incorporates modeling algorithms to profile the coding sequences within your genome and better identify the more cryptic (or partial) genes. prodigal is execellent for the following use cases: Predicting protein-coding genes in draft genomes and metagenomes Quick and unsupervised execution, with minimal resource requirements Ability to handle gaps, scaffolds, and partial genes Identification of translation initiation sites Multiple output formats, including either straight fastA files or the DNA sequence and protein translation for genes, as well as detailed summary statistics for each gene (e.g. contig length, gene length, GC content, GC skew, RBS motifs used, and start and stop codon usage) prodigal is not the best tool to use for the following cases: Predicting RNA genes Handling genes with introns Deal with frame shifts It is also not advised to use prodigal when making predictions through your unassembled reads. If you are working with unassembled data, FragGeneScan is a better tool, as it is more sensitive for partial genes and does not assume every piece of DNA in the input fastA file must be coding. Predicting protein coding sequences in MAGs \u00b6 To get started, move into the exercise directory. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/ Examining the prodigal parameters \u00b6 Before we start runnning prodigal , we will take quick look at the parameters. module purge module load Prodigal/2.6.3-GCC-9.2.0 prodigal -h # Usage: prodigal [-a trans_file] [-c] [-d nuc_file] [-f output_type] # [-g tr_table] [-h] [-i input_file] [-m] [-n] [-o output_file] # [-p mode] [-q] [-s start_file] [-t training_file] [-v] # -a: Write protein translations to the selected file. # -c: Closed ends. Do not allow genes to run off edges. # -d: Write nucleotide sequences of genes to the selected file. # -f: Select output format (gbk, gff, or sco). Default is gbk. # -g: Specify a translation table to use (default 11). # -h: Print help menu and exit. # -i: Specify FASTA/Genbank input file (default reads from stdin). # -m: Treat runs of N as masked sequence; don't build genes across them. # -n: Bypass Shine-Dalgarno trainer and force a full motif scan. # -o: Specify output file (default writes to stdout). # -p: Select procedure (single or meta). Default is single. # -q: Run quietly (suppress normal stderr output). # -s: Write all potential genes (with scores) to the selected file. # -t: Write a training file (if none exists); otherwise, read and use # the specified training file. # -v: Print version number and exit. There are a few parameters that are worth considering in advance. Output files \u00b6 When running prodigal the default behaviour is to create a gbk file from your genome and write it to the stdout of your interface. This can either be captured using a redirect, or the output can instead be placed into a file using the -o flag. You can also change the format of the file using the -f flag. Since we often want to go straight from gene prediction to annotation, prodigal also has the option to create fastA files of the gene prediction ( -d ) and protein translation ( -a ) at the same time. This is an extremely helpful feature, and it is worth running all three outputs at the same time. Generally speaking, you will probably find that the amino acid sequence for your genes is all you need for most practical purposes, but having the corresponding nucleotide sequence can sometimes be useful if we want to mine other data sets. Modes of gene prediction \u00b6 As mentioned in the introduction to this exercise, prodigal uses the profiles of genes it detects in your data set to better tune its prediction models and improve coding sequence recovery. It has three algorithms for how the training is performed which you must determine in advance: Parameter Mode Description Normal mode -p single Take the sequence(s) you provide and profiles the sequence(s) properties. Gene predictions are then made based upon those properties. Normal mode\u202fshould be used on finished genomes, reasonable quality draft genomes, and big viruses. Anonymous mode -p meta Apply pre-calculated training files to the provided input sequences. Anonymous mode\u202fshould be used on metagenomes, low quality draft genomes, small viruses, and small plasmids. Training mode -p train Works like normal mode, but prodigal saves a training file for future use. Anecdotally, when applied to a MAG or genome, anonymous mode ( -p meta ) will identify slightly fewer genes than normal mode ( -p single ). However, single mode can miss laterally transfered elements. There is not necesarily a best choice for which version to use and this is at the users discretion. Executing prodigal \u00b6 We will now run prodigal over the 10 bins in anonymous mode. As usual, we can use a loop to get the predictions for all of our bins. Create a new script nano prodigal.sl Paste in the script (modifying <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name prodigal #SBATCH --res SummerSchool #SBATCH --time 00:10:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH --error prodigal.err #SBATCH --output prodigal.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Prodigal/2.6.3-GCC-9.2.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/ mkdir -p predictions/ for bin_file in filtered_bins/*.fna ; do pred_file = $( basename ${ bin_file } .fna ) prodigal -i ${ bin_file } -p meta \\ -d predictions/ ${ pred_file } .genes.fna \\ -a predictions/ ${ pred_file } .genes.faa \\ -o predictions/ ${ pred_file } .genes.gbk done Submit the script sbatch prodigal.sl Once prodigal has completed, let's check one of the output files: head -n8 predictions/bin_0.filtered.genes.faa # >bin_0_NODE_5_length_607162_cov_1.000802_1 # 1 # 1503 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.322 # RYLIMKKITEEDIIESIADACQYISFYHPEDFVKGMVEAYNVEKGEAAKNAIGQILINSK # MCAMGHRPLCQDTGSVNIFVKVGLNAPLDIKKELVDLLNEGVAKGYTDPDNTLRYSVVAD # PAGKRVNTKNNTPAVIHVSVDNSDEIDITVAAKGGGSENKSKFAVLNPSDSIYDWVMENV # RNMGAGWCPPGILGIGIGGNPEKSMLLAKESLMSHVDIHELKARGPKNALEELRLKLYED # INKVGIGAQGLGGITTVLDVKILDYPCHAASLPVAMIPNCAATRHIHFKLKGDGPAVFNK # PDLDLWPDIELPMDTIKRVNIEDLTKENLSQFKSGDTLLLSGKILTARDAAHKKIVEYKN # AGKDLPNGVKLEDRFIYYVGPVDPVRDEAVGPAGPTTSTRMDKFTKDMMEIGIMGMIGKA There are a few thing to unpack here. First lets look at the first line of the fastA file: >bin_0_NODE_5_length_607162_cov_1.000802_1 # 1 # 1503 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.322 | | | | | | | | | | | | | Are the gene boundaries complete or not | | | | | Unique gene ID | | | | Orientation | | | Stop position | | Start position | Gene suffix Contig name Here are the first few pieces of information in the fastA header identified by what they mean. prodigal names genes using the contig name followed by an underscore then the number of the gene along the contig. The next two pieces of information are the start and stop coordinates of the gene. Next, a 1 is report for a gene that is in the forward orientation (relative to the start of the contig) and a -1 for genes that are in reverse orientation. There is also a unique gene ID provided although this may not be necessary. As long as your contig names are unique, then all gene names generated from them will also be unique. The last option that is important to check is the partial parameter. This is reported as two digits which correspond to the start and end of the gene and report whether or not the gene has the expected amino acids for the start (M) and end of a gene (* in the protein file). A 0 indicates a complete gene edge, and 1 means partial. In this case, we have '10' which indicates the gene runs off the left edge of the contig. Alternate outcomes for this field are '00', '01', or '11'. Stripping metadata \u00b6 While this header information can be very informative, its presence in the fastA files can lead to some downstream issues. The fastA file format specifies that the sequence name for each entry runs from the '>' character to the first space, and everything after the space is metadata. Some bioinformatic programs are aware of this convention and will strip the metadata when producing their outputs, but some tools do not do this. It's really easy to end up in situtations where your gene names are failing to match between analyses because of this inconsistency, so we recommend creating new fastA files with the metadata removed to preempt this problem. for pred_file in predictions/*.fna ; do file_base = $( basename ${ pred_file } .fna ) cut -f1 -d ' ' predictions/ ${ file_base } .fna > predictions/ ${ file_base } .no_metadata.fna cut -f1 -d ' ' predictions/ ${ file_base } .faa > predictions/ ${ file_base } .no_metadata.faa done head -n8 predictions/bin_0.filtered.genes.no_metadata.faa # >bin_0_NODE_5_length_607162_cov_1.000802_1 # RYLIMKKITEEDIIESIADACQYISFYHPEDFVKGMVEAYNVEKGEAAKNAIGQILINSK # MCAMGHRPLCQDTGSVNIFVKVGLNAPLDIKKELVDLLNEGVAKGYTDPDNTLRYSVVAD # PAGKRVNTKNNTPAVIHVSVDNSDEIDITVAAKGGGSENKSKFAVLNPSDSIYDWVMENV # RNMGAGWCPPGILGIGIGGNPEKSMLLAKESLMSHVDIHELKARGPKNALEELRLKLYED # INKVGIGAQGLGGITTVLDVKILDYPCHAASLPVAMIPNCAATRHIHFKLKGDGPAVFNK # PDLDLWPDIELPMDTIKRVNIEDLTKENLSQFKSGDTLLLSGKILTARDAAHKKIVEYKN # AGKDLPNGVKLEDRFIYYVGPVDPVRDEAVGPAGPTTSTRMDKFTKDMMEIGIMGMIGKA Predicting RNA features and non-coding regions \u00b6 Predicting rRNA sequences \u00b6 While they will not be covered in great detail here, there are a few other prediction tools that are useful when working with metagenomic data. The first of these is MeTaxa2 , which can be used to predict ribosomal RNA sequences in a genome. Detection of these is a handy way to link your MAGs to the scientific literature and taxonomy, although recovery of ribosomal sequences like the 16S rRNA subunit is often not successful. To attempt to find the small (16S, SSU) and large (28S, LSU) ribosomal subunits in our data, use the following commands. Create a new script nano metaxa2.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name metaxa2 #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error metaxa2.err #SBATCH --output metaxa2.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Metaxa2/2.1.3-gimkl-2017a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/ mkdir -p ribosomes/ for bin_file in filtered_bins/*.fna ; do pred_file = $( basename ${ bin_file } .fna ) metaxa2 --cpu 10 -g ssu -i ${ bin_file } -o ribosomes/ ${ pred_file } .ssu metaxa2 --cpu 10 -g lsu -i ${ bin_file } -o ribosomes/ ${ pred_file } .lsu done Submit the script sbatch metaxa2.sl The parameters here are fairly self-explanatory, so we won't discuss them in detail. Briefly, --cpu tells the program how many CPUs to use in sequence prediction, and the -g flag determines whether we are using the training data set for SSU or LSU regions. the -i and -o flags denote the input file and output prefix. The only other parameter that can be helpful is the -A flag. By default, MeTaxa2 will search your genome for the following ribosomal signatures: Bacteria Archaea Chloroplast Mitochondira Eukaryote It is usually worth letting it search for all options as detecting multiple rRNAs from different lineages can be a good sign of binning contamination. However, if you want to restrict the search or provide a custom training set this can be set with the -A flag. Predicting tRNA and tmRNA sequences \u00b6 The MIMAG standard specifies that in order to reach particular quality criteria, a MAG must contain a certain number or tRNA sequences. We can search a MAG or genome for these using Aragorn ( link here ).","title":"Gene prediction"},{"location":"day3/ex12_gene_prediction/#gene-prediction","text":"","title":"Gene prediction"},{"location":"day3/ex12_gene_prediction/#objectives","text":"Overview/refresher of prodigal Predicting protein coding sequences in metagenome-assembled genomes Predicting RNA features and non-coding regions","title":"Objectives"},{"location":"day3/ex12_gene_prediction/#overviewrefresher-of-prodigal","text":"At this stage we have recovered a number of high quality genomes or population genomes. While there are interesting biological questions we can ask of the genomes at the DNA/organisational level, it is more likely that we are interested in the genes present in the organism. How we predict genes in the metagenomic data varies depending on what features we are trying to detect. Most often, we are interested in putatively protein coding regions and open reading frames. For features that are functional but not not translated, such as ribosomal RNA and tRNA sequences we need to use alternative tools. When considering protein coding sequences, we avoid the use of the term 'open reading frame' (ORF). The nature of a fragmented assembly is that you may encounter a partial gene on the start or end of a contig that is a function gene, but lacks the start or stop codon due to issues with assembly or sequencing depth. There are many software tools to predict gene sequences and in this workshop we will start with the tool prodigal (PROkaryotic\u202fDynamic Programming Genefinding\u202fALgorithm). prodigal has gone on to become one of the most popular microbial gene prediction algorithms as in incorporates modeling algorithms to profile the coding sequences within your genome and better identify the more cryptic (or partial) genes. prodigal is execellent for the following use cases: Predicting protein-coding genes in draft genomes and metagenomes Quick and unsupervised execution, with minimal resource requirements Ability to handle gaps, scaffolds, and partial genes Identification of translation initiation sites Multiple output formats, including either straight fastA files or the DNA sequence and protein translation for genes, as well as detailed summary statistics for each gene (e.g. contig length, gene length, GC content, GC skew, RBS motifs used, and start and stop codon usage) prodigal is not the best tool to use for the following cases: Predicting RNA genes Handling genes with introns Deal with frame shifts It is also not advised to use prodigal when making predictions through your unassembled reads. If you are working with unassembled data, FragGeneScan is a better tool, as it is more sensitive for partial genes and does not assume every piece of DNA in the input fastA file must be coding.","title":"Overview/refresher of prodigal"},{"location":"day3/ex12_gene_prediction/#predicting-protein-coding-sequences-in-mags","text":"To get started, move into the exercise directory. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/","title":"Predicting protein coding sequences in MAGs"},{"location":"day3/ex12_gene_prediction/#examining-the-prodigal-parameters","text":"Before we start runnning prodigal , we will take quick look at the parameters. module purge module load Prodigal/2.6.3-GCC-9.2.0 prodigal -h # Usage: prodigal [-a trans_file] [-c] [-d nuc_file] [-f output_type] # [-g tr_table] [-h] [-i input_file] [-m] [-n] [-o output_file] # [-p mode] [-q] [-s start_file] [-t training_file] [-v] # -a: Write protein translations to the selected file. # -c: Closed ends. Do not allow genes to run off edges. # -d: Write nucleotide sequences of genes to the selected file. # -f: Select output format (gbk, gff, or sco). Default is gbk. # -g: Specify a translation table to use (default 11). # -h: Print help menu and exit. # -i: Specify FASTA/Genbank input file (default reads from stdin). # -m: Treat runs of N as masked sequence; don't build genes across them. # -n: Bypass Shine-Dalgarno trainer and force a full motif scan. # -o: Specify output file (default writes to stdout). # -p: Select procedure (single or meta). Default is single. # -q: Run quietly (suppress normal stderr output). # -s: Write all potential genes (with scores) to the selected file. # -t: Write a training file (if none exists); otherwise, read and use # the specified training file. # -v: Print version number and exit. There are a few parameters that are worth considering in advance.","title":"Examining the prodigal parameters"},{"location":"day3/ex12_gene_prediction/#output-files","text":"When running prodigal the default behaviour is to create a gbk file from your genome and write it to the stdout of your interface. This can either be captured using a redirect, or the output can instead be placed into a file using the -o flag. You can also change the format of the file using the -f flag. Since we often want to go straight from gene prediction to annotation, prodigal also has the option to create fastA files of the gene prediction ( -d ) and protein translation ( -a ) at the same time. This is an extremely helpful feature, and it is worth running all three outputs at the same time. Generally speaking, you will probably find that the amino acid sequence for your genes is all you need for most practical purposes, but having the corresponding nucleotide sequence can sometimes be useful if we want to mine other data sets.","title":"Output files"},{"location":"day3/ex12_gene_prediction/#modes-of-gene-prediction","text":"As mentioned in the introduction to this exercise, prodigal uses the profiles of genes it detects in your data set to better tune its prediction models and improve coding sequence recovery. It has three algorithms for how the training is performed which you must determine in advance: Parameter Mode Description Normal mode -p single Take the sequence(s) you provide and profiles the sequence(s) properties. Gene predictions are then made based upon those properties. Normal mode\u202fshould be used on finished genomes, reasonable quality draft genomes, and big viruses. Anonymous mode -p meta Apply pre-calculated training files to the provided input sequences. Anonymous mode\u202fshould be used on metagenomes, low quality draft genomes, small viruses, and small plasmids. Training mode -p train Works like normal mode, but prodigal saves a training file for future use. Anecdotally, when applied to a MAG or genome, anonymous mode ( -p meta ) will identify slightly fewer genes than normal mode ( -p single ). However, single mode can miss laterally transfered elements. There is not necesarily a best choice for which version to use and this is at the users discretion.","title":"Modes of gene prediction"},{"location":"day3/ex12_gene_prediction/#executing-prodigal","text":"We will now run prodigal over the 10 bins in anonymous mode. As usual, we can use a loop to get the predictions for all of our bins. Create a new script nano prodigal.sl Paste in the script (modifying <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name prodigal #SBATCH --res SummerSchool #SBATCH --time 00:10:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH --error prodigal.err #SBATCH --output prodigal.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Prodigal/2.6.3-GCC-9.2.0 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/ mkdir -p predictions/ for bin_file in filtered_bins/*.fna ; do pred_file = $( basename ${ bin_file } .fna ) prodigal -i ${ bin_file } -p meta \\ -d predictions/ ${ pred_file } .genes.fna \\ -a predictions/ ${ pred_file } .genes.faa \\ -o predictions/ ${ pred_file } .genes.gbk done Submit the script sbatch prodigal.sl Once prodigal has completed, let's check one of the output files: head -n8 predictions/bin_0.filtered.genes.faa # >bin_0_NODE_5_length_607162_cov_1.000802_1 # 1 # 1503 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.322 # RYLIMKKITEEDIIESIADACQYISFYHPEDFVKGMVEAYNVEKGEAAKNAIGQILINSK # MCAMGHRPLCQDTGSVNIFVKVGLNAPLDIKKELVDLLNEGVAKGYTDPDNTLRYSVVAD # PAGKRVNTKNNTPAVIHVSVDNSDEIDITVAAKGGGSENKSKFAVLNPSDSIYDWVMENV # RNMGAGWCPPGILGIGIGGNPEKSMLLAKESLMSHVDIHELKARGPKNALEELRLKLYED # INKVGIGAQGLGGITTVLDVKILDYPCHAASLPVAMIPNCAATRHIHFKLKGDGPAVFNK # PDLDLWPDIELPMDTIKRVNIEDLTKENLSQFKSGDTLLLSGKILTARDAAHKKIVEYKN # AGKDLPNGVKLEDRFIYYVGPVDPVRDEAVGPAGPTTSTRMDKFTKDMMEIGIMGMIGKA There are a few thing to unpack here. First lets look at the first line of the fastA file: >bin_0_NODE_5_length_607162_cov_1.000802_1 # 1 # 1503 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.322 | | | | | | | | | | | | | Are the gene boundaries complete or not | | | | | Unique gene ID | | | | Orientation | | | Stop position | | Start position | Gene suffix Contig name Here are the first few pieces of information in the fastA header identified by what they mean. prodigal names genes using the contig name followed by an underscore then the number of the gene along the contig. The next two pieces of information are the start and stop coordinates of the gene. Next, a 1 is report for a gene that is in the forward orientation (relative to the start of the contig) and a -1 for genes that are in reverse orientation. There is also a unique gene ID provided although this may not be necessary. As long as your contig names are unique, then all gene names generated from them will also be unique. The last option that is important to check is the partial parameter. This is reported as two digits which correspond to the start and end of the gene and report whether or not the gene has the expected amino acids for the start (M) and end of a gene (* in the protein file). A 0 indicates a complete gene edge, and 1 means partial. In this case, we have '10' which indicates the gene runs off the left edge of the contig. Alternate outcomes for this field are '00', '01', or '11'.","title":"Executing prodigal"},{"location":"day3/ex12_gene_prediction/#stripping-metadata","text":"While this header information can be very informative, its presence in the fastA files can lead to some downstream issues. The fastA file format specifies that the sequence name for each entry runs from the '>' character to the first space, and everything after the space is metadata. Some bioinformatic programs are aware of this convention and will strip the metadata when producing their outputs, but some tools do not do this. It's really easy to end up in situtations where your gene names are failing to match between analyses because of this inconsistency, so we recommend creating new fastA files with the metadata removed to preempt this problem. for pred_file in predictions/*.fna ; do file_base = $( basename ${ pred_file } .fna ) cut -f1 -d ' ' predictions/ ${ file_base } .fna > predictions/ ${ file_base } .no_metadata.fna cut -f1 -d ' ' predictions/ ${ file_base } .faa > predictions/ ${ file_base } .no_metadata.faa done head -n8 predictions/bin_0.filtered.genes.no_metadata.faa # >bin_0_NODE_5_length_607162_cov_1.000802_1 # RYLIMKKITEEDIIESIADACQYISFYHPEDFVKGMVEAYNVEKGEAAKNAIGQILINSK # MCAMGHRPLCQDTGSVNIFVKVGLNAPLDIKKELVDLLNEGVAKGYTDPDNTLRYSVVAD # PAGKRVNTKNNTPAVIHVSVDNSDEIDITVAAKGGGSENKSKFAVLNPSDSIYDWVMENV # RNMGAGWCPPGILGIGIGGNPEKSMLLAKESLMSHVDIHELKARGPKNALEELRLKLYED # INKVGIGAQGLGGITTVLDVKILDYPCHAASLPVAMIPNCAATRHIHFKLKGDGPAVFNK # PDLDLWPDIELPMDTIKRVNIEDLTKENLSQFKSGDTLLLSGKILTARDAAHKKIVEYKN # AGKDLPNGVKLEDRFIYYVGPVDPVRDEAVGPAGPTTSTRMDKFTKDMMEIGIMGMIGKA","title":"Stripping metadata"},{"location":"day3/ex12_gene_prediction/#predicting-rna-features-and-non-coding-regions","text":"","title":"Predicting RNA features and non-coding regions"},{"location":"day3/ex12_gene_prediction/#predicting-rrna-sequences","text":"While they will not be covered in great detail here, there are a few other prediction tools that are useful when working with metagenomic data. The first of these is MeTaxa2 , which can be used to predict ribosomal RNA sequences in a genome. Detection of these is a handy way to link your MAGs to the scientific literature and taxonomy, although recovery of ribosomal sequences like the 16S rRNA subunit is often not successful. To attempt to find the small (16S, SSU) and large (28S, LSU) ribosomal subunits in our data, use the following commands. Create a new script nano metaxa2.sl Paste in the script (replacing <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name metaxa2 #SBATCH --res SummerSchool #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error metaxa2.err #SBATCH --output metaxa2.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Metaxa2/2.1.3-gimkl-2017a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/ mkdir -p ribosomes/ for bin_file in filtered_bins/*.fna ; do pred_file = $( basename ${ bin_file } .fna ) metaxa2 --cpu 10 -g ssu -i ${ bin_file } -o ribosomes/ ${ pred_file } .ssu metaxa2 --cpu 10 -g lsu -i ${ bin_file } -o ribosomes/ ${ pred_file } .lsu done Submit the script sbatch metaxa2.sl The parameters here are fairly self-explanatory, so we won't discuss them in detail. Briefly, --cpu tells the program how many CPUs to use in sequence prediction, and the -g flag determines whether we are using the training data set for SSU or LSU regions. the -i and -o flags denote the input file and output prefix. The only other parameter that can be helpful is the -A flag. By default, MeTaxa2 will search your genome for the following ribosomal signatures: Bacteria Archaea Chloroplast Mitochondira Eukaryote It is usually worth letting it search for all options as detecting multiple rRNAs from different lineages can be a good sign of binning contamination. However, if you want to restrict the search or provide a custom training set this can be set with the -A flag.","title":"Predicting rRNA sequences"},{"location":"day3/ex12_gene_prediction/#predicting-trna-and-tmrna-sequences","text":"The MIMAG standard specifies that in order to reach particular quality criteria, a MAG must contain a certain number or tRNA sequences. We can search a MAG or genome for these using Aragorn ( link here ).","title":"Predicting tRNA and tmRNA sequences"},{"location":"day3/ex13_gene_annotation_part1/","text":"Gene annotation (part 1) \u00b6 Objectives \u00b6 BLAST -like gene annotations and domain annotations Overview of annotation databases Evaluating the quality of gene assignment Discussion: Differences in taxonomies (GTDB, NCBI etc) BLAST -like gene annotations and domain annotations \u00b6 Broadly speaking, there are two ways we perform gene annotations with protein sequences. Both compare our sequences of interest against a curated set of protein sequences for which function is known, or is strongly suspected. In each case, there are particular strenths to the approach and for particular research questions, one option may be favoured over another. BLAST-like annotation \u00b6 The first of these is the BLAST algorithm for sequence alignment. This approach performs pairwise alignment between the gene of interest (query sequence) and the sequences in the database (target sequence). BLAST searches each potential target sequence for k -mers identified in the query sequence. Where these k -mers are found in targets, the ends are extended out to try to create longer regions of highly similar sequence spans. Across this span, the tool identifies the longest span of characters (nucleotide or amino acid) that match within a scoring framework to return the length of the region (coverage) and the sequence identity over the span (identity). The original tool for performing this kind of analysis was the BLAST tool. While BLAST and its variants are still excellent tools for performing this kind of sequence annotation, they suffer from a slow runtime speed due to the need to test each query sequence against every target sequence in the database. For this reason, several tools have been published which take the basic approach of BLAST , but augment it with methods to reduce the number of pairwise comparisons needed to identify targets with high sequence similarity to the query. Two popular pieces of software are the tools usearch here and diamond here . HMM-profiling of domains \u00b6 An alternate method for attributing function to query sequences it to consider them as a collection of independently functioning protein folding domains. This is the approach used in the HMMer software, and the Pfam , TIGRfam , and PANTHER databases. In these analyses, the database consists not of individual sequences, but of Hidden Markov models built from a collection of proteins that share a common domain. These profiles build out a statistical map of the amino acid transitions (from position to position), variations (differences at a position), and insertions/deletions between positions in the domain across the different observations in the training database and apply these maps to the query data. These exercises will take place in the 10.gene_annotation/ folder. Annotating MAGs against the UniProt database with diamond \u00b6 For this exercise we are going to use diamond for performing our annotation. We have chosen to use this tool because it is faster than BLAST, and usearch comes with licencing restrictions that make it hard to work with in a shared computing environment like NeSI. For this exercise we have created a diamond-compatible database from the 2018 release of the UniProt database. For input files, the predictions/ results from the previous gene prediction exercise have been copied over to 10.gene_annotation/predictions/ . In general, diamond takes a simple pair of input files - the protein coding sequences we wish to annotate and the database we will use for this purpose. There are a few parameters that need to be tweaked for obtaining a useful output file, however. module purge module load DIAMOND/0.9.25-gimkl-2018b diamond help # diamond v0.9.25.126 | by Benjamin Buchfink <buchfink@gmail.com> # Licensed under the GNU GPL <https://www.gnu.org/licenses/gpl.txt> # Check http://github.com/bbuchfink/diamond for updates. # Syntax: diamond COMMAND [OPTIONS] # Commands: # ... # blastp Align amino acid query sequences against a protein reference database # ... # General options: # --threads (-p) number of CPU threads # --db (-d) database file # --out (-o) output file # --outfmt (-f) output format # ... There are two output formats we can chose from which are useful for our analysis. We will obtain our output in the BLAST tabular format, which provides the annotation information in a simple-to-parse text file that can be viewed in any text or spreadsheet viewing tool. This will allow us to investigate and evaluate the quality of our annotations. Awkwardly, diamond does not provide the headers for what the columns in the output table mean. This table is a handy reference for how to interpret the output. From here we can view important stastics for each query/target pairing such as the number of identify residues between sequences and the aligned length between query and target. Lets set up a slurm job to annotate each of our MAGs. Create a new script nano annotate_uniprot.sl Paste in the script (update <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name annotate_uniprot #SBATCH --res SummerSchool #SBATCH --time 02:00:00 #SBATCH --mem 20GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 20 #SBATCH --error annotate_uniprot_dmnd.err #SBATCH --output annotate_uniprot_dmnd.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load DIAMOND/0.9.25-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ mkdir -p gene_annotations/ for prot_file in predictions/*.genes.no_metadata.faa ; do out_file = $( basename ${ prot_file } .faa ) diamond blastp -p 20 --max-target-seqs 5 --evalue 0 .001 \\ --db /nesi/nobackup/nesi02659/MGSS_resources_2020/databases/uniprot_nr_200213.diamond \\ -q ${ prot_file } --outfmt 6 -o gene_annotations/ ${ out_file } .uniprot.txt done Submit the script sbatch annotate_uniprot.sl Annotating MAGs against the Pfam database with hmmer \u00b6 The standard software for performing HMM-profiling annotation is hmmer . Compared to BLAST , FASTA , and other sequence alignment and database search tools based on older scoring methodology, HMMER aims to be significantly more accurate and more able to detect remote homologs because of the strength of its underlying mathematical models. In the past, this strength came at significant computational expense, but in the new HMMER3 project, HMMER is now essentially as fast as BLAST . HMMER will search one or more profiles against a sequence database for sequence hommologs, and for making sequence alignments, implementing profile hidden Markov models. In this exercise, we will perform a search using hmmsearch . For each profile in hmmfile , HMMER uses that query profile to search the target database of sequences indicated in seqdb , and output ranked lists of the sequences with the most significant matches to the profile. hmmsearch accepts any fastA file as target database input. It also accepts EMBL/UniProtKB text format, and Genbank format. It will automatically determine what format your file is in so you don\u2019t have to specify it. As we did with diamond , we will also have to modify some parameters to get the desired ouotput. module load HMMER/3.1b2-gimkl-2017a hmmsearch -h # hmmsearch :: search profile(s) against a sequence database # HMMER 3.1b2 (February 2015); http://hmmer.org/ # Copyright (C) 2015 Howard Hughes Medical Institute. # Freely distributed under the GNU General Public License (GPLv3). # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Usage: hmmsearch [options] <hmmfile> <seqdb> # Basic options: # -h : show brief help on version and usage # Options directing output: # ... # --tblout <f> : save parseable table of per-sequence hits to file <f> # .... # Options controlling reporting thresholds: # ... # -E <x> : report sequences <= this E-value threshold in output [10.0] (x>0) # ... # Other expert options: # ... # --cpu <n> : number of parallel CPU workers to use for multithreads # ... We are now going to submit another slurm job to annotate our MAGs using the Pfam database . Matching sequences to a Pfam entry allows us to transfer the functional information from an experimentally characterised sequence to uncharacterised sequences in the same entry. Pfam then provides comprehensive annotation for each entry. Create a new script nano annotate_pfam.sl Paste in the script (update <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name annotate_pfam #SBATCH --res SummerSchool #SBATCH --time 02:00:00 #SBATCH --mem 5GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error annotate_pfam_hmm.err #SBATCH --output annotate_pfam_hmm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load HMMER/3.1b2-gimkl-2017a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ for prot_file in predictions/*.genes.no_metadata.faa ; do out_file = $( basename ${ prot_file } .faa ) hmmsearch --tblout gene_annotations/ ${ out_file } .pfam.txt -E 1e-3 --cpu 10 /nesi/nobackup/nesi02659/MGSS_resources_2020/databases/Pfam-A.hmm ${ prot_file } done Submit the script sbatch annotate_pfam.sl Evaluating the quality of gene assignment \u00b6 Determining how trustworthy a gene annotation is can be a very tricky process. How similar do protein sequences need to be to perform the same function? The answer is surprisingly low. A bioinformatic analysis performed in 1999 identified that proteins with as little as 20 - 35% sequence identity can still share the same function ( Rost, 1999 ), but this is not a universal occurrence. When evaluating annotations, consider the following questions: What is the amino acid identity along the aligned region? What is the amino acid similarity between the aligned region? What is the coverage as a percentage or the query and target genes? If we infer a phylogeny of this query gene with references from the target family, is a stable tree resolved? Does the inclusion of this gene function make sense in the context of the organism's taxonomy? Does the gene sit on a long contig that is core to the MAG, or is it a short contig carrying only a single gene? If we are uncertain of a particular annotation, does the predicted gene occur in an operon? If so, are the other genes present in the annotation? We must also remain aware of the potential for incorrectly annotated genes in the annotation database and that proteins can perform multiple functions (and may therefore be attributed multiple, inconsistent annotations). Furthermore, it is also important to consider exactly which part of the target gene the alignment is happening across. There are several catalytic centers of enzymes, such as the Fe-S cofactor, which are shared across many different proteins, and if your annotation is only spanning one of these regions then it may simply be the case that you are identifying a generic electron accepting or donating domain. Differences in taxonomies \u00b6 Another way to determine if a annotation 'belongs' in the MAG of interest is to consider the predicted taxonomy of the query gene with that of the MAG itself. For example, if you detect a Desulfovibrio -like dsrA seuqence in a bin that has been classified as belonging to the genus Desulfovibrio then it is probably a safe bet that the annotation is correct. However, when comparing taxonomic assignments, it is important to be aware of the differing taxonomic schemas that are circulating in the microbiological and bioinformatic literature and to know how to reconcile their differences. Similar to how the 16S rRNA gene taxonomies provided by SILVA , Greengenes , and RDP taxonomies all differ in some aspects, there are multiple competing taxonomies in protein databases. Examples of various genome-level and protein taxonomies \u00b6 NCBI Genome Taxonomy Database This problem exists despite the existance of a formal Code for the naming of bacteria and archaea, because There are no rules governing how we define the grouping of these names together, other than for type species Defunct synonyms and basonyms are not correctly purged from taxonomy lists (this is quite noticable with the NCBI taxonomy) Valid names cannot be assigned for uncultivate organisms, meaning there are many informal placeholder names in the literature. For example, clades like WPS-2, SAR324, and SAUL are widely cited in the literature despite having no official standing It is therefore important to periodically sanity check your taxonomic annotations in order to avoid splitting taxa based on spelling differences or the use of historic names that have since been reclassified.","title":"Gene annotation (part 1)"},{"location":"day3/ex13_gene_annotation_part1/#gene-annotation-part-1","text":"","title":"Gene annotation (part 1)"},{"location":"day3/ex13_gene_annotation_part1/#objectives","text":"BLAST -like gene annotations and domain annotations Overview of annotation databases Evaluating the quality of gene assignment Discussion: Differences in taxonomies (GTDB, NCBI etc)","title":"Objectives"},{"location":"day3/ex13_gene_annotation_part1/#blast-like-gene-annotations-and-domain-annotations","text":"Broadly speaking, there are two ways we perform gene annotations with protein sequences. Both compare our sequences of interest against a curated set of protein sequences for which function is known, or is strongly suspected. In each case, there are particular strenths to the approach and for particular research questions, one option may be favoured over another.","title":"BLAST-like gene annotations and domain annotations"},{"location":"day3/ex13_gene_annotation_part1/#blast-like-annotation","text":"The first of these is the BLAST algorithm for sequence alignment. This approach performs pairwise alignment between the gene of interest (query sequence) and the sequences in the database (target sequence). BLAST searches each potential target sequence for k -mers identified in the query sequence. Where these k -mers are found in targets, the ends are extended out to try to create longer regions of highly similar sequence spans. Across this span, the tool identifies the longest span of characters (nucleotide or amino acid) that match within a scoring framework to return the length of the region (coverage) and the sequence identity over the span (identity). The original tool for performing this kind of analysis was the BLAST tool. While BLAST and its variants are still excellent tools for performing this kind of sequence annotation, they suffer from a slow runtime speed due to the need to test each query sequence against every target sequence in the database. For this reason, several tools have been published which take the basic approach of BLAST , but augment it with methods to reduce the number of pairwise comparisons needed to identify targets with high sequence similarity to the query. Two popular pieces of software are the tools usearch here and diamond here .","title":"BLAST-like annotation"},{"location":"day3/ex13_gene_annotation_part1/#hmm-profiling-of-domains","text":"An alternate method for attributing function to query sequences it to consider them as a collection of independently functioning protein folding domains. This is the approach used in the HMMer software, and the Pfam , TIGRfam , and PANTHER databases. In these analyses, the database consists not of individual sequences, but of Hidden Markov models built from a collection of proteins that share a common domain. These profiles build out a statistical map of the amino acid transitions (from position to position), variations (differences at a position), and insertions/deletions between positions in the domain across the different observations in the training database and apply these maps to the query data. These exercises will take place in the 10.gene_annotation/ folder.","title":"HMM-profiling of domains"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-uniprot-database-with-diamond","text":"For this exercise we are going to use diamond for performing our annotation. We have chosen to use this tool because it is faster than BLAST, and usearch comes with licencing restrictions that make it hard to work with in a shared computing environment like NeSI. For this exercise we have created a diamond-compatible database from the 2018 release of the UniProt database. For input files, the predictions/ results from the previous gene prediction exercise have been copied over to 10.gene_annotation/predictions/ . In general, diamond takes a simple pair of input files - the protein coding sequences we wish to annotate and the database we will use for this purpose. There are a few parameters that need to be tweaked for obtaining a useful output file, however. module purge module load DIAMOND/0.9.25-gimkl-2018b diamond help # diamond v0.9.25.126 | by Benjamin Buchfink <buchfink@gmail.com> # Licensed under the GNU GPL <https://www.gnu.org/licenses/gpl.txt> # Check http://github.com/bbuchfink/diamond for updates. # Syntax: diamond COMMAND [OPTIONS] # Commands: # ... # blastp Align amino acid query sequences against a protein reference database # ... # General options: # --threads (-p) number of CPU threads # --db (-d) database file # --out (-o) output file # --outfmt (-f) output format # ... There are two output formats we can chose from which are useful for our analysis. We will obtain our output in the BLAST tabular format, which provides the annotation information in a simple-to-parse text file that can be viewed in any text or spreadsheet viewing tool. This will allow us to investigate and evaluate the quality of our annotations. Awkwardly, diamond does not provide the headers for what the columns in the output table mean. This table is a handy reference for how to interpret the output. From here we can view important stastics for each query/target pairing such as the number of identify residues between sequences and the aligned length between query and target. Lets set up a slurm job to annotate each of our MAGs. Create a new script nano annotate_uniprot.sl Paste in the script (update <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name annotate_uniprot #SBATCH --res SummerSchool #SBATCH --time 02:00:00 #SBATCH --mem 20GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 20 #SBATCH --error annotate_uniprot_dmnd.err #SBATCH --output annotate_uniprot_dmnd.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load DIAMOND/0.9.25-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ mkdir -p gene_annotations/ for prot_file in predictions/*.genes.no_metadata.faa ; do out_file = $( basename ${ prot_file } .faa ) diamond blastp -p 20 --max-target-seqs 5 --evalue 0 .001 \\ --db /nesi/nobackup/nesi02659/MGSS_resources_2020/databases/uniprot_nr_200213.diamond \\ -q ${ prot_file } --outfmt 6 -o gene_annotations/ ${ out_file } .uniprot.txt done Submit the script sbatch annotate_uniprot.sl","title":"Annotating MAGs against the UniProt database with diamond"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-pfam-database-with-hmmer","text":"The standard software for performing HMM-profiling annotation is hmmer . Compared to BLAST , FASTA , and other sequence alignment and database search tools based on older scoring methodology, HMMER aims to be significantly more accurate and more able to detect remote homologs because of the strength of its underlying mathematical models. In the past, this strength came at significant computational expense, but in the new HMMER3 project, HMMER is now essentially as fast as BLAST . HMMER will search one or more profiles against a sequence database for sequence hommologs, and for making sequence alignments, implementing profile hidden Markov models. In this exercise, we will perform a search using hmmsearch . For each profile in hmmfile , HMMER uses that query profile to search the target database of sequences indicated in seqdb , and output ranked lists of the sequences with the most significant matches to the profile. hmmsearch accepts any fastA file as target database input. It also accepts EMBL/UniProtKB text format, and Genbank format. It will automatically determine what format your file is in so you don\u2019t have to specify it. As we did with diamond , we will also have to modify some parameters to get the desired ouotput. module load HMMER/3.1b2-gimkl-2017a hmmsearch -h # hmmsearch :: search profile(s) against a sequence database # HMMER 3.1b2 (February 2015); http://hmmer.org/ # Copyright (C) 2015 Howard Hughes Medical Institute. # Freely distributed under the GNU General Public License (GPLv3). # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Usage: hmmsearch [options] <hmmfile> <seqdb> # Basic options: # -h : show brief help on version and usage # Options directing output: # ... # --tblout <f> : save parseable table of per-sequence hits to file <f> # .... # Options controlling reporting thresholds: # ... # -E <x> : report sequences <= this E-value threshold in output [10.0] (x>0) # ... # Other expert options: # ... # --cpu <n> : number of parallel CPU workers to use for multithreads # ... We are now going to submit another slurm job to annotate our MAGs using the Pfam database . Matching sequences to a Pfam entry allows us to transfer the functional information from an experimentally characterised sequence to uncharacterised sequences in the same entry. Pfam then provides comprehensive annotation for each entry. Create a new script nano annotate_pfam.sl Paste in the script (update <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name annotate_pfam #SBATCH --res SummerSchool #SBATCH --time 02:00:00 #SBATCH --mem 5GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH --error annotate_pfam_hmm.err #SBATCH --output annotate_pfam_hmm.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load HMMER/3.1b2-gimkl-2017a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ for prot_file in predictions/*.genes.no_metadata.faa ; do out_file = $( basename ${ prot_file } .faa ) hmmsearch --tblout gene_annotations/ ${ out_file } .pfam.txt -E 1e-3 --cpu 10 /nesi/nobackup/nesi02659/MGSS_resources_2020/databases/Pfam-A.hmm ${ prot_file } done Submit the script sbatch annotate_pfam.sl","title":"Annotating MAGs against the Pfam database with hmmer"},{"location":"day3/ex13_gene_annotation_part1/#evaluating-the-quality-of-gene-assignment","text":"Determining how trustworthy a gene annotation is can be a very tricky process. How similar do protein sequences need to be to perform the same function? The answer is surprisingly low. A bioinformatic analysis performed in 1999 identified that proteins with as little as 20 - 35% sequence identity can still share the same function ( Rost, 1999 ), but this is not a universal occurrence. When evaluating annotations, consider the following questions: What is the amino acid identity along the aligned region? What is the amino acid similarity between the aligned region? What is the coverage as a percentage or the query and target genes? If we infer a phylogeny of this query gene with references from the target family, is a stable tree resolved? Does the inclusion of this gene function make sense in the context of the organism's taxonomy? Does the gene sit on a long contig that is core to the MAG, or is it a short contig carrying only a single gene? If we are uncertain of a particular annotation, does the predicted gene occur in an operon? If so, are the other genes present in the annotation? We must also remain aware of the potential for incorrectly annotated genes in the annotation database and that proteins can perform multiple functions (and may therefore be attributed multiple, inconsistent annotations). Furthermore, it is also important to consider exactly which part of the target gene the alignment is happening across. There are several catalytic centers of enzymes, such as the Fe-S cofactor, which are shared across many different proteins, and if your annotation is only spanning one of these regions then it may simply be the case that you are identifying a generic electron accepting or donating domain.","title":"Evaluating the quality of gene assignment"},{"location":"day3/ex13_gene_annotation_part1/#differences-in-taxonomies","text":"Another way to determine if a annotation 'belongs' in the MAG of interest is to consider the predicted taxonomy of the query gene with that of the MAG itself. For example, if you detect a Desulfovibrio -like dsrA seuqence in a bin that has been classified as belonging to the genus Desulfovibrio then it is probably a safe bet that the annotation is correct. However, when comparing taxonomic assignments, it is important to be aware of the differing taxonomic schemas that are circulating in the microbiological and bioinformatic literature and to know how to reconcile their differences. Similar to how the 16S rRNA gene taxonomies provided by SILVA , Greengenes , and RDP taxonomies all differ in some aspects, there are multiple competing taxonomies in protein databases.","title":"Differences in taxonomies"},{"location":"day3/ex13_gene_annotation_part1/#examples-of-various-genome-level-and-protein-taxonomies","text":"NCBI Genome Taxonomy Database This problem exists despite the existance of a formal Code for the naming of bacteria and archaea, because There are no rules governing how we define the grouping of these names together, other than for type species Defunct synonyms and basonyms are not correctly purged from taxonomy lists (this is quite noticable with the NCBI taxonomy) Valid names cannot be assigned for uncultivate organisms, meaning there are many informal placeholder names in the literature. For example, clades like WPS-2, SAR324, and SAUL are widely cited in the literature despite having no official standing It is therefore important to periodically sanity check your taxonomic annotations in order to avoid splitting taxa based on spelling differences or the use of historic names that have since been reclassified.","title":"Examples of various genome-level and protein taxonomies"},{"location":"day3/ex14_gene_annotation_part2/","text":"Gene annotation (part 2) \u00b6 Objectives \u00b6 Gene prediction and annotation with DRAM Annotation of the MAGs with DRAM Gene prediction and annotation with DRAM (Distilled and Refined Annotation of Metabolism) \u00b6 DRAM is a tool designed to profile microbial (meta)genomes for metabolisms known to impact ecosystem functions across biomes. DRAM annotates MAGs and viral contigs using KEGG (if provided by user), UniRef90, PFAM, CAZy, dbCAN, RefSeq viral, VOGDB (Virus Orthologous Groups), and the MEROPS peptidase database. It is also highly customizable to other custom user databases. DRAM only uses assembly-derived fastA files input by the user. These input files may come from unbinned data (metagenome contig or scaffold files) or genome-resolved data from one or many organisms (isolate genomes, single-amplified genome (SAGs), MAGs). DRAM is run in two stages: annotation and distillation. Annotation \u00b6 The first step in DRAM is to annotate genes by assigning database identifiers to genes. Short contigs (default < 2,500 bp) are initially removed. Then, Prodigal is used to detect open reading frames (ORFs) and to predict their amino acid sequences. Next, DRAM searches all amino acid sequences against multiple databases, providing a single Raw output. When gene annotation is complete, all results are merged in a single tab-delimited annotation table, including the best hit for each database for user comparison. Distillation \u00b6 After genome annotation, a distill step follows with the aim to curate these annotations into useful functional categories, creating genome statistics and metabolism summary files, which are stored in the Distillate output. The genome statistics provides most genome quality information required for MIMAG standards, including GTDB-tk and checkM information if provided by the user. The summarised metabolism table includes the number of genes with specific metabolic function identifiers (KO, CAZY ID, etc) for each genome, with information obtained from multiple databases. The Distillate output is then further distilled into the Product , an html file displaying a heatmap, as well as the corresponding data table. We will investigate all these files later on. Annotation of the MAGs with DRAM \u00b6 Beyond annotation, DRAM aims to be a data compiler. For that reason, output files from both CheckM and GTDB_tk steps can be input to DRAM to provide both taxonomy and genome quality information of the MAGs. DRAM input files \u00b6 For these exercises, we have copied the relevant input files into the folder 10.gene_annotation/DRAM_input_files/ . gtdbtk.bac120.classification_pplacer.tsv was taken from the earlier 8.coverage_and_taxonomy/gtdbtk_out/ outputs, and checkm.txt from the result of re-running CheckM on the final refined filtered bins in 6.bin_refinement/filtered_bins . Navigate to the 10.gene_annotation/ folder cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ The CheckM output file ( checkm.txt ) can be input as it is. However, in order to use the file with the gtdb_tk taxonomy ( gtdbtk.bac120.classification_pplacer.tsv ) we should modify it first to include column headers 'bin_id' and 'classification' First, take a quick look at the current format of the taxonomy file using less less DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv #bin_3.filtered d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ #bin_8.filtered d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ #bin_2.filtered d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ #bin_5.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ #bin_9.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ #bin_4.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ #bin_7.filtered d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ #bin_0.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ #bin_1.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ #bin_6.filtered d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ Now, use sed to add the headers as a new line. The ^ character indicates to make the additions at the start of the line (in this case, the first line). \\t and \\n represent a tab space and a newline character, respectively. sed -i '1s/^/bin_id\\tclassification\\n/' DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv less DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv #bin_id classification #bin_3.filtered d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ #bin_8.filtered d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ #bin_2.filtered d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ #bin_5.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ #bin_9.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ #bin_4.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ #bin_7.filtered d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ #bin_0.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ #bin_1.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ #bin_6.filtered d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ DRAM annotation \u00b6 In default annotation mode, DRAM only requires as input the directory containing all the bins we would like to annotate in fastA format (either .fa or .fna). There are few parameters that can be modified if not using the default mode. Once the annotation step is complete, the mode distill is used to summarise the obtained results. NOTE: due to the increased memory requirements, UniRef90 database is not default and the flag \u2013use_uniref should be specified in order to search amino acid sequences against UniRef90. In this exercise, due to memory and time constraints, we won't be using the UniRef90 database. DRAM is not currently available as a NeSI module. Here, we will be running DRAM from within a conda environment . This is activated by entering the commands below before running the DRAM.py command. module purge module load Miniconda3/4.8.3 module load gimkl/2020a export CONDA_PKGS_DIRS = /nesi/project/nesi02659/.conda/pkgs export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs source activate DRAM DRAM.py --help # usage: DRAM.py [-h] {annotate,annotate_genes,distill,strainer,neighborhoods} ... # positional arguments: # {annotate,annotate_genes,distill,strainer,neighborhoods} # annotate Annotate genomes/contigs/bins/MAGs # annotate_genes Annotate already called genes, limited functionality compared to annotate # distill Summarize metabolic content of annotated genomes # strainer Strain annotations down to genes of interest # neighborhoods Find neighborhoods around genes of interest #optional arguments: # -h, --help show this help message and exit To deactivate the conda environment and return to the standard command prompt, enter: conda deactivate Submitting DRAM annotation as a slurm job \u00b6 To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. NOTE: Currently DRAM has to be run from the directory where DRAM-setup.py was run in order to work. That is why we start the slurm script with cd /nesi/project/nesi02659/.conda/dramdbsetup and pass absolute paths to each of the arguments included in the DRAM.py script. Create a new script nano dram_annnotation.sl Paste in the script (update all of the cases of <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name DRAM_annotation #SBATCH --res SummerSchool #SBATCH --time=5:00:00 #SBATCH --mem=20Gb #SBATCH --error slurm-DRAM_annot.%A-%a.err #SBATCH --output slurm-DRAM_annot.%A-%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/project/nesi02659/.conda/dramdbsetup module purge module load Miniconda3/4.8.3 module load gimkl/2020a export CONDA_PKGS_DIRS = /nesi/project/nesi02659/.conda/pkgs export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs source activate DRAM DRAM.py annotate -i '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/predictions/*.filtered.fna' \\ --checkm_quality /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/DRAM_input_files/checkm.txt \\ --gtdb_taxonomy /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv \\ -o /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/annotation_dram conda deactivate Submit the job sbatch dram_annnotation.sl The program will take 4-4.5 hours to run, so we will submit the jobs and inspect the results tomorrow morning. Select initial goal \u00b6 It is now time to select the goals to investigate the genomes you have been working with. We ask you to select one of the following goals: Denitrification (Nitrate or nitrite to nitrogen) Ammonia oxidation (Ammonia to nitrite or nitrate) Anammox (Ammonia and nitrite to nitrogen) Sulfur oxidation (SOX pathway, thiosulfate to sulfate) Sulfur reduction (DSR pathway, sulfate to sulfide) Photosynthetic carbon fixation Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl) Non-polar flagella expression due to a chromosomal deletion Plasmid-encoded antibiotic resistance Aerobic (versus anaerobic) metabolism Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them. To conduct this exersise, you should use the information generated with DRAM as well as the annotation files we created previously that will be available in the directory 10.gene_annotation/gene_annotations . Please note that we have also provided further annotation files within the directory 10.gene_annotation/example_annotation_tables that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here . These files were created by using an in-house python script designed to aggregate different annotations and as part of the environmental metagenomics worflow followed in Handley's lab. Information about using this script as well as the script is available here","title":"Gene annotation (part 2)"},{"location":"day3/ex14_gene_annotation_part2/#gene-annotation-part-2","text":"","title":"Gene annotation (part 2)"},{"location":"day3/ex14_gene_annotation_part2/#objectives","text":"Gene prediction and annotation with DRAM Annotation of the MAGs with DRAM","title":"Objectives"},{"location":"day3/ex14_gene_annotation_part2/#gene-prediction-and-annotation-with-dram-distilled-and-refined-annotation-of-metabolism","text":"DRAM is a tool designed to profile microbial (meta)genomes for metabolisms known to impact ecosystem functions across biomes. DRAM annotates MAGs and viral contigs using KEGG (if provided by user), UniRef90, PFAM, CAZy, dbCAN, RefSeq viral, VOGDB (Virus Orthologous Groups), and the MEROPS peptidase database. It is also highly customizable to other custom user databases. DRAM only uses assembly-derived fastA files input by the user. These input files may come from unbinned data (metagenome contig or scaffold files) or genome-resolved data from one or many organisms (isolate genomes, single-amplified genome (SAGs), MAGs). DRAM is run in two stages: annotation and distillation.","title":"Gene prediction and annotation with DRAM (Distilled and Refined Annotation of Metabolism)"},{"location":"day3/ex14_gene_annotation_part2/#annotation","text":"The first step in DRAM is to annotate genes by assigning database identifiers to genes. Short contigs (default < 2,500 bp) are initially removed. Then, Prodigal is used to detect open reading frames (ORFs) and to predict their amino acid sequences. Next, DRAM searches all amino acid sequences against multiple databases, providing a single Raw output. When gene annotation is complete, all results are merged in a single tab-delimited annotation table, including the best hit for each database for user comparison.","title":"Annotation"},{"location":"day3/ex14_gene_annotation_part2/#distillation","text":"After genome annotation, a distill step follows with the aim to curate these annotations into useful functional categories, creating genome statistics and metabolism summary files, which are stored in the Distillate output. The genome statistics provides most genome quality information required for MIMAG standards, including GTDB-tk and checkM information if provided by the user. The summarised metabolism table includes the number of genes with specific metabolic function identifiers (KO, CAZY ID, etc) for each genome, with information obtained from multiple databases. The Distillate output is then further distilled into the Product , an html file displaying a heatmap, as well as the corresponding data table. We will investigate all these files later on.","title":"Distillation"},{"location":"day3/ex14_gene_annotation_part2/#annotation-of-the-mags-with-dram","text":"Beyond annotation, DRAM aims to be a data compiler. For that reason, output files from both CheckM and GTDB_tk steps can be input to DRAM to provide both taxonomy and genome quality information of the MAGs.","title":"Annotation of the MAGs with DRAM"},{"location":"day3/ex14_gene_annotation_part2/#dram-input-files","text":"For these exercises, we have copied the relevant input files into the folder 10.gene_annotation/DRAM_input_files/ . gtdbtk.bac120.classification_pplacer.tsv was taken from the earlier 8.coverage_and_taxonomy/gtdbtk_out/ outputs, and checkm.txt from the result of re-running CheckM on the final refined filtered bins in 6.bin_refinement/filtered_bins . Navigate to the 10.gene_annotation/ folder cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ The CheckM output file ( checkm.txt ) can be input as it is. However, in order to use the file with the gtdb_tk taxonomy ( gtdbtk.bac120.classification_pplacer.tsv ) we should modify it first to include column headers 'bin_id' and 'classification' First, take a quick look at the current format of the taxonomy file using less less DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv #bin_3.filtered d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ #bin_8.filtered d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ #bin_2.filtered d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ #bin_5.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ #bin_9.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ #bin_4.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ #bin_7.filtered d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ #bin_0.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ #bin_1.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ #bin_6.filtered d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ Now, use sed to add the headers as a new line. The ^ character indicates to make the additions at the start of the line (in this case, the first line). \\t and \\n represent a tab space and a newline character, respectively. sed -i '1s/^/bin_id\\tclassification\\n/' DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv less DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv #bin_id classification #bin_3.filtered d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ #bin_8.filtered d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ #bin_2.filtered d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ #bin_5.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ #bin_9.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ #bin_4.filtered d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ #bin_7.filtered d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ #bin_0.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ #bin_1.filtered d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ #bin_6.filtered d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__","title":"DRAM input files"},{"location":"day3/ex14_gene_annotation_part2/#dram-annotation","text":"In default annotation mode, DRAM only requires as input the directory containing all the bins we would like to annotate in fastA format (either .fa or .fna). There are few parameters that can be modified if not using the default mode. Once the annotation step is complete, the mode distill is used to summarise the obtained results. NOTE: due to the increased memory requirements, UniRef90 database is not default and the flag \u2013use_uniref should be specified in order to search amino acid sequences against UniRef90. In this exercise, due to memory and time constraints, we won't be using the UniRef90 database. DRAM is not currently available as a NeSI module. Here, we will be running DRAM from within a conda environment . This is activated by entering the commands below before running the DRAM.py command. module purge module load Miniconda3/4.8.3 module load gimkl/2020a export CONDA_PKGS_DIRS = /nesi/project/nesi02659/.conda/pkgs export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs source activate DRAM DRAM.py --help # usage: DRAM.py [-h] {annotate,annotate_genes,distill,strainer,neighborhoods} ... # positional arguments: # {annotate,annotate_genes,distill,strainer,neighborhoods} # annotate Annotate genomes/contigs/bins/MAGs # annotate_genes Annotate already called genes, limited functionality compared to annotate # distill Summarize metabolic content of annotated genomes # strainer Strain annotations down to genes of interest # neighborhoods Find neighborhoods around genes of interest #optional arguments: # -h, --help show this help message and exit To deactivate the conda environment and return to the standard command prompt, enter: conda deactivate","title":"DRAM annotation"},{"location":"day3/ex14_gene_annotation_part2/#submitting-dram-annotation-as-a-slurm-job","text":"To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. NOTE: Currently DRAM has to be run from the directory where DRAM-setup.py was run in order to work. That is why we start the slurm script with cd /nesi/project/nesi02659/.conda/dramdbsetup and pass absolute paths to each of the arguments included in the DRAM.py script. Create a new script nano dram_annnotation.sl Paste in the script (update all of the cases of <YOUR FOLDER> ) #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name DRAM_annotation #SBATCH --res SummerSchool #SBATCH --time=5:00:00 #SBATCH --mem=20Gb #SBATCH --error slurm-DRAM_annot.%A-%a.err #SBATCH --output slurm-DRAM_annot.%A-%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/project/nesi02659/.conda/dramdbsetup module purge module load Miniconda3/4.8.3 module load gimkl/2020a export CONDA_PKGS_DIRS = /nesi/project/nesi02659/.conda/pkgs export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs source activate DRAM DRAM.py annotate -i '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/predictions/*.filtered.fna' \\ --checkm_quality /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/DRAM_input_files/checkm.txt \\ --gtdb_taxonomy /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/DRAM_input_files/gtdbtk.bac120.classification_pplacer.tsv \\ -o /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/annotation_dram conda deactivate Submit the job sbatch dram_annnotation.sl The program will take 4-4.5 hours to run, so we will submit the jobs and inspect the results tomorrow morning.","title":"Submitting DRAM annotation as a slurm job"},{"location":"day3/ex14_gene_annotation_part2/#select-initial-goal","text":"It is now time to select the goals to investigate the genomes you have been working with. We ask you to select one of the following goals: Denitrification (Nitrate or nitrite to nitrogen) Ammonia oxidation (Ammonia to nitrite or nitrate) Anammox (Ammonia and nitrite to nitrogen) Sulfur oxidation (SOX pathway, thiosulfate to sulfate) Sulfur reduction (DSR pathway, sulfate to sulfide) Photosynthetic carbon fixation Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl) Non-polar flagella expression due to a chromosomal deletion Plasmid-encoded antibiotic resistance Aerobic (versus anaerobic) metabolism Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them. To conduct this exersise, you should use the information generated with DRAM as well as the annotation files we created previously that will be available in the directory 10.gene_annotation/gene_annotations . Please note that we have also provided further annotation files within the directory 10.gene_annotation/example_annotation_tables that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here . These files were created by using an in-house python script designed to aggregate different annotations and as part of the environmental metagenomics worflow followed in Handley's lab. Information about using this script as well as the script is available here","title":"Select initial goal"},{"location":"day4/ex15_gene_annotation_part3/","text":"Gene annotation (part 3) \u00b6 Objectives \u00b6 Overview of DRAM.py annotate output DRAM Distillation step and visualization of results Tie findings to your initial goal Overview of DRAM.py annotate output \u00b6 The submitted job from the previous session should now be completed. If we examine the output directory 10.gene_annotation/annotation_dram/ we will see the following files: File name Description genes.faa and genes.fna Fasta files with all the genes called by Prodigal , with additional header information gained from the annotation as nucleotide and amino acid records, respectively genes.gff GFF3 file with the same annotation information as well as gene locations scaffolds.fna A collection of all scaffolds/contigs given as input to DRAM.py annotate with added bin information annotations.tsv This file includes all annotation information about every gene from all MAGs trnas.tsv Summary of the tRNAs found in each MAG rrnas.tsv Summary of the rRNAs found in each MAG If we inspect the head of the annotation file we will see the following cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ head annotation_dram/annotation.tsv #fasta scaffold gene_position start_position end_position strandedness rank kegg_id kegg_hit uniref_id uniref_hit uniref_taxonomy uniref_RBH uniref_identity uniref_bitScore uniref_eVal peptidase_id peptidase_family peptidase_hit peptidase_RBH peptidase_identity peptidase_bitScore peptidase_eVal pfam_hits cazy_hits vogdb_description vogdb_categories heme_regulatory_motif_count #bin_0_1f9359e86e6a75bcff340e6a8b60ef98_1 bin_0 1f9359e86e6a75bcff340e6a8b60ef98 1 205 1371 1 B K02338 DNA polymerase III subunit beta [EC:2.7.7.7] Q7V9E7_PROMM UniRef90_Q7V9E7 Beta sliding clamp n=10 Tax=Prochlorococcus TaxID=1218 RepID=Q7V9E7_PROMM Prochlorococcus True 0.8959999999999999 726.0 1.5509999999999984e-233 DNA polymerase III beta subunit, C-terminal domain [PF02768.16]; DNA polymerase III beta subunit, N-terminal domain [PF00712.20]; DNA polymerase III beta subunit, central domain [PF02767.17] sp|P9WNU1|DPO3B_MYCTU Beta sliding clamp; XhXr Xr;Xh 0 For each gene annotated, DRAM provides a summary rank (from A to E), representing the confidence of the annotation based on reciprocal best hits (RBH). The following figure briefly explains how this summary rank is calculated: DRAM distillation of the results \u00b6 After the annotation is finished, we will summarise and visualise these annotations with the so-called Distillation step. We do so by running the following command directly in the terminal. This will generate the distillate and liquor files. module purge module load Miniconda3/4.8.3 module load gimkl/2020a export CONDA_PKGS_DIRS = /nesi/project/nesi02659/.conda/pkgs export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs source activate DRAM cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ DRAM.py distill -i annotation_dram/annotations.tsv -o dram_distillation --trna_path annotation_dram/trnas.tsv --rrna_path annotation_dram/rrnas.tsv conda deactivate \u00b6 The distillation step generates the following files that can be found within the dram_distillation directory : File name Description genome_stats.tsv Genome quality information required for MIMAG metabolism_summary.xlsx Summarise metabolism table containing number of genes with specific metabolic function identifiers product.html HTML file displaying a heatmap summarising pathway coverage, electron transport chain component completion, and presence/absence of specific functions product.tsv Data table with product.html information First, let's have a look at the genome_stats.tsv file to check the assembly quality of our bins by double-clicking the file within the Jupyter environment, viewing from the terminal via less or cat , or downloading the files from here and opening locally (e.g. via excel). genome number of scaffolds taxonomy completeness score contamination score 5S rRNA 16S rRNA 23S rRNA tRNA count assembly quality bin_0 1 d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ 100 0.14 - 2 present 2 present 47 med bin_1 4 d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ 99.51 0.08 6 present 5 present 5 present 60 med bin_2 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ 96.45 0.11 3 present - - 43 med bin_3 3 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ 99.73 0.03 9 present 8 present 8 present 98 med bin_4 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ 99.97 0.74 bin_4, (74043, 74150) bin_4, (69143, 70676) bin_4, (71085, 73967) 42 high bin_5 1 d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ 99.8 0 bin_5, (643507, 643615) bin_5, (638306, 639791) bin_5, (640621, 643431) 49 high bin_6 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ 99.59 0.41 4 present 4 present 4 present 49 med bin_7 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ 99.59 2.98 4 present 4 present 4 present 54 med bin_8 19 d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ 99.41 0 2 present bin_8, (3744, 5289) bin_8, (379, 3300) 57 med bin_9 1 d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ 97.8 1.65 bin_9, (1066028, 1066130) bin_9, (1069811, 1071397) bin_9, (1066309, 1069302) 46 high To finish, we visualize the Product , an .HTML file produced in the Distillation step, by double-clicking on it in our Jupyter lab notebook or downloading from here . The Product has three primary parts: 1. Modules. Central metabolism pathways coverage. Completion of pathways is based on the structure of KEGG modules, with the pathway coverage calculated as the percent of steps with at least one gene present. ETC Complexes. Electron Transport Chain component completion Presence of specific functions, including CAZy, Nitrogen metabolism, Sulfur metabolism and Photosynthesis. Note that the taxonomic classification of each of the bins is also shown in the first figure Tie findings to your initial goal \u00b6 It is now time to explore the genomes and try to address your original goal! You were tasked with identfying one of the following. Denitrification (Nitrate or nitrite to nitrogen) Ammonia oxidation (Ammonia to nitrite or nitrate) Anammox (Ammonia and nitrite to nitrogen) Sulfur oxidation (SOX pathway, thiosulfate to sulfate) Sulfur reduction (DSR pathway, sulfate to sulfide) Photosynthetic carbon fixation Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl) Non-polar flagella expression due to a chromosomal deletion Plasmid-encoded antibiotic resistance Aerobic (versus anaerobic) metabolism Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them. To conduct this exersise, you should use the information generated with DRAM as well as the annotation files we created yesterday and that are available in the directory 10.gene_annotation/gene_annotations . Please note that we have also provided further annotation files within the directory 10.gene_annotation/example_annotation_tables that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here .","title":"Gene annotation (part 3)"},{"location":"day4/ex15_gene_annotation_part3/#gene-annotation-part-3","text":"","title":"Gene annotation (part 3)"},{"location":"day4/ex15_gene_annotation_part3/#objectives","text":"Overview of DRAM.py annotate output DRAM Distillation step and visualization of results Tie findings to your initial goal","title":"Objectives"},{"location":"day4/ex15_gene_annotation_part3/#overview-of-drampy-annotate-output","text":"The submitted job from the previous session should now be completed. If we examine the output directory 10.gene_annotation/annotation_dram/ we will see the following files: File name Description genes.faa and genes.fna Fasta files with all the genes called by Prodigal , with additional header information gained from the annotation as nucleotide and amino acid records, respectively genes.gff GFF3 file with the same annotation information as well as gene locations scaffolds.fna A collection of all scaffolds/contigs given as input to DRAM.py annotate with added bin information annotations.tsv This file includes all annotation information about every gene from all MAGs trnas.tsv Summary of the tRNAs found in each MAG rrnas.tsv Summary of the rRNAs found in each MAG If we inspect the head of the annotation file we will see the following cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/10.gene_annotation/ head annotation_dram/annotation.tsv #fasta scaffold gene_position start_position end_position strandedness rank kegg_id kegg_hit uniref_id uniref_hit uniref_taxonomy uniref_RBH uniref_identity uniref_bitScore uniref_eVal peptidase_id peptidase_family peptidase_hit peptidase_RBH peptidase_identity peptidase_bitScore peptidase_eVal pfam_hits cazy_hits vogdb_description vogdb_categories heme_regulatory_motif_count #bin_0_1f9359e86e6a75bcff340e6a8b60ef98_1 bin_0 1f9359e86e6a75bcff340e6a8b60ef98 1 205 1371 1 B K02338 DNA polymerase III subunit beta [EC:2.7.7.7] Q7V9E7_PROMM UniRef90_Q7V9E7 Beta sliding clamp n=10 Tax=Prochlorococcus TaxID=1218 RepID=Q7V9E7_PROMM Prochlorococcus True 0.8959999999999999 726.0 1.5509999999999984e-233 DNA polymerase III beta subunit, C-terminal domain [PF02768.16]; DNA polymerase III beta subunit, N-terminal domain [PF00712.20]; DNA polymerase III beta subunit, central domain [PF02767.17] sp|P9WNU1|DPO3B_MYCTU Beta sliding clamp; XhXr Xr;Xh 0 For each gene annotated, DRAM provides a summary rank (from A to E), representing the confidence of the annotation based on reciprocal best hits (RBH). The following figure briefly explains how this summary rank is calculated:","title":"Overview of DRAM.py annotate output"},{"location":"day4/ex15_gene_annotation_part3/#dram-distillation-of-the-results","text":"After the annotation is finished, we will summarise and visualise these annotations with the so-called Distillation step. We do so by running the following command directly in the terminal. This will generate the distillate and liquor files.","title":"DRAM distillation of the results"},{"location":"day4/ex15_gene_annotation_part3/#module-purge-module-load-miniconda3483-module-load-gimkl2020a-export-conda_pkgs_dirsnesiprojectnesi02659condapkgs-export-conda_envs_pathnesiprojectnesi02659condaenvs-source-activate-dram-cd-nesinobackupnesi02659mgss_uyour-folder10gene_annotation-drampy-distill-i-annotation_dramannotationstsv-o-dram_distillation-trna_path-annotation_dramtrnastsv-rrna_path-annotation_dramrrnastsv-conda-deactivate","text":"The distillation step generates the following files that can be found within the dram_distillation directory : File name Description genome_stats.tsv Genome quality information required for MIMAG metabolism_summary.xlsx Summarise metabolism table containing number of genes with specific metabolic function identifiers product.html HTML file displaying a heatmap summarising pathway coverage, electron transport chain component completion, and presence/absence of specific functions product.tsv Data table with product.html information First, let's have a look at the genome_stats.tsv file to check the assembly quality of our bins by double-clicking the file within the Jupyter environment, viewing from the terminal via less or cat , or downloading the files from here and opening locally (e.g. via excel). genome number of scaffolds taxonomy completeness score contamination score 5S rRNA 16S rRNA 23S rRNA tRNA count assembly quality bin_0 1 d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ 100 0.14 - 2 present 2 present 47 med bin_1 4 d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ 99.51 0.08 6 present 5 present 5 present 60 med bin_2 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ 96.45 0.11 3 present - - 43 med bin_3 3 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ 99.73 0.03 9 present 8 present 8 present 98 med bin_4 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ 99.97 0.74 bin_4, (74043, 74150) bin_4, (69143, 70676) bin_4, (71085, 73967) 42 high bin_5 1 d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ 99.8 0 bin_5, (643507, 643615) bin_5, (638306, 639791) bin_5, (640621, 643431) 49 high bin_6 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ 99.59 0.41 4 present 4 present 4 present 49 med bin_7 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ 99.59 2.98 4 present 4 present 4 present 54 med bin_8 19 d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ 99.41 0 2 present bin_8, (3744, 5289) bin_8, (379, 3300) 57 med bin_9 1 d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ 97.8 1.65 bin_9, (1066028, 1066130) bin_9, (1069811, 1071397) bin_9, (1066309, 1069302) 46 high To finish, we visualize the Product , an .HTML file produced in the Distillation step, by double-clicking on it in our Jupyter lab notebook or downloading from here . The Product has three primary parts: 1. Modules. Central metabolism pathways coverage. Completion of pathways is based on the structure of KEGG modules, with the pathway coverage calculated as the percent of steps with at least one gene present. ETC Complexes. Electron Transport Chain component completion Presence of specific functions, including CAZy, Nitrogen metabolism, Sulfur metabolism and Photosynthesis. Note that the taxonomic classification of each of the bins is also shown in the first figure","title":"module purge\nmodule load Miniconda3/4.8.3\nmodule load gimkl/2020a\n\nexport CONDA_PKGS_DIRS=/nesi/project/nesi02659/.conda/pkgs\nexport CONDA_ENVS_PATH=/nesi/project/nesi02659/.conda/envs\n\nsource activate DRAM\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation/\n\nDRAM.py distill -i annotation_dram/annotations.tsv -o dram_distillation --trna_path annotation_dram/trnas.tsv --rrna_path annotation_dram/rrnas.tsv\n\nconda deactivate\n"},{"location":"day4/ex15_gene_annotation_part3/#tie-findings-to-your-initial-goal","text":"It is now time to explore the genomes and try to address your original goal! You were tasked with identfying one of the following. Denitrification (Nitrate or nitrite to nitrogen) Ammonia oxidation (Ammonia to nitrite or nitrate) Anammox (Ammonia and nitrite to nitrogen) Sulfur oxidation (SOX pathway, thiosulfate to sulfate) Sulfur reduction (DSR pathway, sulfate to sulfide) Photosynthetic carbon fixation Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl) Non-polar flagella expression due to a chromosomal deletion Plasmid-encoded antibiotic resistance Aerobic (versus anaerobic) metabolism Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them. To conduct this exersise, you should use the information generated with DRAM as well as the annotation files we created yesterday and that are available in the directory 10.gene_annotation/gene_annotations . Please note that we have also provided further annotation files within the directory 10.gene_annotation/example_annotation_tables that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here .","title":"Tie findings to your initial goal"},{"location":"day4/ex16a_data_presentation_Intro/","text":"Presentation of data: Introduction \u00b6 Objectives \u00b6 Overview, RStudio , and using R in the Jupyter environment Data visualisation and accessibility Logging in to the NeSI Jupyter hub Overview, R/RStudio , and using R in the Jupyter environment \u00b6 There are a number of powerful packages within the R software environment which can be used to create high quality images of genomic and metagenomic data. While each of these packages comes with its own documentation, these documents and tutorials usually assume that your data is already in some already-prepared format. Our data will almost never be in this format, though, so these exercises show two brief examples of how we can scrape data from our existing files to create useful figures. As such, these examples are more complicated than what you would get reading the tutorials and manuals of the plotting tools, but will be transferable to your own work. In your own work, it may be preferable to download the relevant files from NeSI (e.g. via scp ... ) and work with them on a locally installed version of RStudio on your own machine. For today, to be able to run these R exercises in a stable environment within the NeSI platform, we will be running an R kernel from within a Jupyter environment. By now you should be very familiar with running the terminal window from within the NeSI Jupyter hub . In addition to the terminal, Jupyter Notebooks more generally also provide an interactive space that allows for mixing multiple languages within a single document, including Markdown , Python , and R (by default, Markdown and one coding language such as R can be used within one document, but there are add-ons available to expand this, should you wish to). Jupyter Notebooks can be extremely useful as a workspace that is the equivalent of an electronic \"lab book\". Today, we will be using it as an interactive space to run R . Note that, while the layout will be slightly different to RStudio , the commands we will be working through will work the same in both environments. These exercises will take place with files in the 11.data_presentation/ folder. Data visualisation and accessibility \u00b6 In this section, we will work through a number of example exercises for visualising various aspects of metagenomic data. As the fundamental point of data visualisation is communication , when building figures it is important to be mindful of aspects of your figures that might affect the accessibility of what you're trying to communicate (i.e. to maximise the number of people you will be communicating effectively with). A considerable number of your intended audience will be affected by one of the forms of colour vision deficiency (colour blindness). There are a number of useful resources available online for both selecting and testing the appropriateness of your colour selection. Some include: ColorBrewer2 (select 'colorblindsafe') chroma.js Selecting and checking your colour choice using Viz Palette Blog post by Brian Connelly. Several useful colour palettes designed by Martin Krzywinski are available here We have been mindful to make appropriate colour selections throughout these examples, but please do let us know if you spot any we might have overlooked. Getting started: logging in to the NeSI Jupyter hub \u00b6 To get started, if you're not already, log back in to NeSI's Jupyter hub . Within the Jupyter launcher, this time open a Notebook running the R 4.0.1 module as the kernel. All of the required packages for these exercises are already installed within the R 4.0.1 (gimkl-2020a) kernel we are running. If you need to install these on your local R or RStudio , this can be done via the install.packages() command within R or RStudio : install.packages ( 'ade4' ) install.packages ( 'genoPlotR' ) install.packages ( 'pheatmap' ) install.packages ( 'gplots' ) install.packages ( 'vegan' ) # Install 'tidyverse' (includes: ggplot2, tibble, tidyr, readr, purrr, dplyr, string4, forcats) install.packages ( 'tidyverse' ) # Install 'pathview' package (part of Bioconductor) if ( ! require ( BiocManager )) { install.packages ( \"BiocManager\" ) BiocManager :: install ( \"pathview\" , update = FALSE ) } # Install 'KEGGREST' package (part of Bioconductor) if ( ! require ( BiocManager )) { install.packages ( \"BiocManager\" ) BiocManager :: install ( \"KEGGREST\" , update = FALSE ) } Spend a few minutes familiarising yourself with the Jupyter Notebook workspace, and how it differs to the standard terminal we've been working in. You'll see the coding language kernel running in the background of this Notebook in the top right of the pane. The main document works in blocks ; click the + button to add additional blocks. The Code drop-down menu allows you to select between whether the current block is a Markdown or code (in this case, R ) block. To start, create a title and description for this Notebook. Create a title Click on the first code block use the drop-down menu to convert the block to Markdown enter a title in the block preceeded by one or more # symbols (e.g. # MGSS2020: Data presentation ). In Markdown , the # symbol denotes this to be rendered as a title. Click on the block and then press <shift> + <enter> to 'run' the block (in this case, to render the Markdown text). Create a comment or sub-title Create a second block convert to Markdown and enter a short description of the workshop (e.g. Data presentation exercises for the Genomics Aotearoa Metagenomics Workshop, 2020 ). as above, press + to render the text NOTE: for Markdown blocks, simply double click on a block to be able to re-edit the rendered section. NOTE: To run a code block, use the same method of clicking on the block and pressing <shift> + <enter> to interactively run the code for that block (in this case, to run our R code).","title":"Presentation of data: Introduction"},{"location":"day4/ex16a_data_presentation_Intro/#presentation-of-data-introduction","text":"","title":"Presentation of data: Introduction"},{"location":"day4/ex16a_data_presentation_Intro/#objectives","text":"Overview, RStudio , and using R in the Jupyter environment Data visualisation and accessibility Logging in to the NeSI Jupyter hub","title":"Objectives"},{"location":"day4/ex16a_data_presentation_Intro/#overview-rrstudio-and-using-r-in-the-jupyter-environment","text":"There are a number of powerful packages within the R software environment which can be used to create high quality images of genomic and metagenomic data. While each of these packages comes with its own documentation, these documents and tutorials usually assume that your data is already in some already-prepared format. Our data will almost never be in this format, though, so these exercises show two brief examples of how we can scrape data from our existing files to create useful figures. As such, these examples are more complicated than what you would get reading the tutorials and manuals of the plotting tools, but will be transferable to your own work. In your own work, it may be preferable to download the relevant files from NeSI (e.g. via scp ... ) and work with them on a locally installed version of RStudio on your own machine. For today, to be able to run these R exercises in a stable environment within the NeSI platform, we will be running an R kernel from within a Jupyter environment. By now you should be very familiar with running the terminal window from within the NeSI Jupyter hub . In addition to the terminal, Jupyter Notebooks more generally also provide an interactive space that allows for mixing multiple languages within a single document, including Markdown , Python , and R (by default, Markdown and one coding language such as R can be used within one document, but there are add-ons available to expand this, should you wish to). Jupyter Notebooks can be extremely useful as a workspace that is the equivalent of an electronic \"lab book\". Today, we will be using it as an interactive space to run R . Note that, while the layout will be slightly different to RStudio , the commands we will be working through will work the same in both environments. These exercises will take place with files in the 11.data_presentation/ folder.","title":"Overview, R/RStudio, and using R in the Jupyter environment"},{"location":"day4/ex16a_data_presentation_Intro/#data-visualisation-and-accessibility","text":"In this section, we will work through a number of example exercises for visualising various aspects of metagenomic data. As the fundamental point of data visualisation is communication , when building figures it is important to be mindful of aspects of your figures that might affect the accessibility of what you're trying to communicate (i.e. to maximise the number of people you will be communicating effectively with). A considerable number of your intended audience will be affected by one of the forms of colour vision deficiency (colour blindness). There are a number of useful resources available online for both selecting and testing the appropriateness of your colour selection. Some include: ColorBrewer2 (select 'colorblindsafe') chroma.js Selecting and checking your colour choice using Viz Palette Blog post by Brian Connelly. Several useful colour palettes designed by Martin Krzywinski are available here We have been mindful to make appropriate colour selections throughout these examples, but please do let us know if you spot any we might have overlooked.","title":"Data visualisation and accessibility"},{"location":"day4/ex16a_data_presentation_Intro/#getting-started-logging-in-to-the-nesi-jupyter-hub","text":"To get started, if you're not already, log back in to NeSI's Jupyter hub . Within the Jupyter launcher, this time open a Notebook running the R 4.0.1 module as the kernel. All of the required packages for these exercises are already installed within the R 4.0.1 (gimkl-2020a) kernel we are running. If you need to install these on your local R or RStudio , this can be done via the install.packages() command within R or RStudio : install.packages ( 'ade4' ) install.packages ( 'genoPlotR' ) install.packages ( 'pheatmap' ) install.packages ( 'gplots' ) install.packages ( 'vegan' ) # Install 'tidyverse' (includes: ggplot2, tibble, tidyr, readr, purrr, dplyr, string4, forcats) install.packages ( 'tidyverse' ) # Install 'pathview' package (part of Bioconductor) if ( ! require ( BiocManager )) { install.packages ( \"BiocManager\" ) BiocManager :: install ( \"pathview\" , update = FALSE ) } # Install 'KEGGREST' package (part of Bioconductor) if ( ! require ( BiocManager )) { install.packages ( \"BiocManager\" ) BiocManager :: install ( \"KEGGREST\" , update = FALSE ) } Spend a few minutes familiarising yourself with the Jupyter Notebook workspace, and how it differs to the standard terminal we've been working in. You'll see the coding language kernel running in the background of this Notebook in the top right of the pane. The main document works in blocks ; click the + button to add additional blocks. The Code drop-down menu allows you to select between whether the current block is a Markdown or code (in this case, R ) block. To start, create a title and description for this Notebook. Create a title Click on the first code block use the drop-down menu to convert the block to Markdown enter a title in the block preceeded by one or more # symbols (e.g. # MGSS2020: Data presentation ). In Markdown , the # symbol denotes this to be rendered as a title. Click on the block and then press <shift> + <enter> to 'run' the block (in this case, to render the Markdown text). Create a comment or sub-title Create a second block convert to Markdown and enter a short description of the workshop (e.g. Data presentation exercises for the Genomics Aotearoa Metagenomics Workshop, 2020 ). as above, press + to render the text NOTE: for Markdown blocks, simply double click on a block to be able to re-edit the rendered section. NOTE: To run a code block, use the same method of clicking on the block and pressing <shift> + <enter> to interactively run the code for that block (in this case, to run our R code).","title":"Getting started: logging in to the NeSI Jupyter hub"},{"location":"day4/ex16b_data_presentation_Coverage/","text":"Presentation of data: Per-sample coverage heatmaps \u00b6 Objectives \u00b6 Building a heatmap of MAG coverage per sample Building a heatmap of viral contigs per sample Build a heatmap of average coverage per sample using R \u00b6 One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? In the previous coverage and taxonomy exercises we generated per-sample coverage tables by mapping the quality-filtered unassembled reads back to the refined bins and the viral contigs to then generate coverage profiles for each. As a reminder: Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. A simple way to present this information is via a heatmap. In this exercise we will build a clustered heatmap of these coverage profiles in R . Since we also have tables of taxonomy assignments (via gtdb-tk for MAGs) and/or predictions (via vContact2 for viral contigs), we will also use these to add taxonomy information to the plot. The coverage and taxonomy tables generated in earlier exercises have been copied to 11.data_presentation/coverage/ a for use in these exercises. In addition to this, a simple mapping file has also been created ( 11.data_presentation/coverage/mapping_file.txt ). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column ( Group_A , Group_B , and Group_C ). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination, or correlation analyses) you could also add new columns to this file and load them at the same time. NOTE: As discussed in the coverage and taxonomy exercises , it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the **normalised* coverage tables into the steps outlined below.* Part 1 - Building a heatmap of MAG coverage per sample \u00b6 To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). 1.1 Set working directory, load R libraries, and import data \u00b6 First, set the working directory and load the required libraries. setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( tidyr ) library ( dplyr ) library ( readr ) library ( stringr ) library ( tibble ) # Other libraries library ( gplots ) library ( vegan ) NOTE: after copying this code into a code block in Jupyter , remember that, to run the code, press <shift> + <enter> with the code block selected. Import the coverage and taxonomy tables. When importing the files, we will select only the information of use here and will do a few basic first data wrangling steps. For the coverage table, we will select the contigName column and each of the columns of coverage values (columns sample[1-4].bam ). For taxonomy, we will select the user_genome column (converted to Bin ID) and classification (coverated to taxonomy ), and will also use gsub to extract just the taxonomic ranks of interest (in this case, we will extract the class to colour code MAGs in the heat map, and genus to add to MAG names in the plot), and to add Unassigned to any MAGs not assigned at these ranks. ( NOTE: to view a different taxonomic rank, you will need to change the two mutate(taxonomy_class = gsub(...)) rows below accordingly ). # coverage table cov_MAG <- read_tsv ( \"coverage/bins_cov_table.txt\" ) %>% select ( c ( 'contigName' , ends_with ( '.bam' ))) # taxonomy table tax_MAG <- read_tsv ( \"coverage/gtdbtk.bac120.summary.tsv\" ) %>% mutate ( Bin = gsub ( \"(.*).filtered\" , \"\\\\1\" , . $ user_genome )) %>% mutate ( taxonomy = gsub ( \".*;c(.*);o.*\" , \"class\\\\1\" , classification )) %>% mutate ( taxonomy = gsub ( \"^class__$\" , \"Unassigned\" , taxonomy )) %>% mutate ( taxonomy_genus = gsub ( \".*;g__(.*);.*\" , \"\\\\1\" , classification )) %>% mutate ( taxonomy_genus = gsub ( \"^$\" , \"Unassigned\" , taxonomy_genus )) %>% select ( c ( 'Bin' , 'taxonomy' , 'taxonomy_genus' )) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" ) 1.2 wrangle data \u00b6 As noted during the coverage and taxonomy exercises, it is important to remember that we currently have a table of coverage values for all contigs contained within each MAG. Since we're aiming to present coverage for each MAG , we need to reduce these contig coverages into a single mean coverage value per MAG per sample. In the following code, we first strip the .bam extensions off of our sample names. We will then leverage the fact that we added bin IDs to each of the contig headers earlier to re-extract the bin ID for each using gsub , use the group_by() function to group by Bin , and the summarise() function to return the per-sample mean coverage of each set of contigs contained within each bin. # Extract BinID, group by Bin, calculate mean coverages for each set of contigs per Bin cov_MAG <- cov_MAG %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) %>% mutate ( Bin = gsub ( \"(.*)_NODE.*\" , \"\\\\1\" , . $ contigName )) %>% group_by ( Bin ) %>% summarise ( across ( where ( is.numeric ), mean )) Finally, collate the coverage and taxonomy tables into a single table for downstream use, and rename bins to include the genus name. # Collate coverage and taxonomy collate_data_MAG <- cov_MAG %>% left_join ( tax_MAG ) %>% mutate ( Bin = paste ( Bin , taxonomy_genus , sep = \"_\" )) %>% select ( - taxonomy_genus ) %>% mutate ( taxonomy = replace_na ( taxonomy , \"Unassigned\" )) In many real-life cases, to enhance the visualisation across a wide range of coverage values, you may wish to perform a transformation on your data. Perform a log(2)-transformation on the coverage data (those values in the columns that contains(\"sample\") ). Note, here we are calculating log2(x + 1) to allow for any cases where coverage values are 0 in any of the samples (log2(0 + 1) = 0). # Log(2)-transform coverage data collate_data_MAG_log2 <- collate_data_MAG collate_data_MAG_log2[,names(select(collate_data_MAG_log2, contains(\"sample\")))] <- log2(collate_data_MAG_log2[,names(select(collate_data_MAG_log2, contains(\"sample\")))] + 1) To have the data in a format that is ready for heatmap2 we will perform a few final data wrangling steps. Here we are re-ordering the file by row sums, selecting only the coverage columns ( contains(\"sample\") ) and taxonomy column, ensuring taxonomy is set as a factor, and setting the rownames based on the Bin column. coverage.heatmap.data.MAG <- collate_data_MAG_log2 %>% mutate ( sum = rowSums ( select ( collate_data_MAG_log2 , contains ( \"sample\" )))) %>% arrange ( desc ( sum )) %>% select ( \"Bin\" , contains ( \"sample\" ), \"taxonomy\" ) %>% mutate ( taxonomy = factor ( taxonomy )) %>% droplevels () %>% column_to_rownames ( var = \"Bin\" ) %>% as.data.frame () As there are only 10 bins in our mock metagenome data, spread over 7 different classes (the taxonomic rank we've chosen), we can include colour codes for all classes here. If you had many more cases of the selected taxonomic rank, it would be necessary to select a subset to colour code (e.g. the top 10 taxa, and grouping all others into Other ) so as not to have so many colours that things become unintelligible. We don't need to run this code block today (in fact, it will return an error due to having too few bins compared to the number we're trying to subset). But if you wished to do this for your own work, you could run something like the following to identify the 10 most abundant taxa (at the selected rank), and change the taxonomy of all others to Other . ## Identify most abundant Phyla in MAG data # Aggregate rows based on taxonomic assignments, reorder by overall relative abundance cov_MAG.tax <- collate_data_MAG_log2 %>% replace_na ( list ( taxonomy = \"Unassigned\" )) %>% mutate ( sum = rowSums ( select ( collate_data_MAG_log2 , contains ( \"sample\" )))) %>% group_by ( taxonomy ) %>% summarise_if ( is.numeric , list ( sum = sum )) %>% arrange ( desc ( sum_sum )) %>% mutate ( tax_summary = factor ( c ( taxonomy [ 1 : 10 ], rep ( \"Other\" , length ( taxonomy ) -10 )))) ## Add summaries of top 14 taxa to coverage.heatmap.data.MAG coverage.heatmap.data.MAG <- coverage.heatmap.data.MAG %>% rownames_to_column ( \"Bin\" ) %>% left_join ( select ( cov_MAG.tax , c ( \"taxonomy\" , \"tax_summary\" ))) %>% mutate ( taxonomy = factor ( tax_summary )) %>% column_to_rownames ( \"Bin\" ) 1.3 Calculate hierarchical clustering of columns (samples) and rows (MAGs). \u00b6 It can be useful to arrange our rows and/or columns by some form of clustering. The plotting function heatmap2 can do this for us. However, here we will first perform this ourselves to be able to view the clustering output separately. We can then pass the same clustering to heatmap2 using the as.dendrogram() function within the heatmap2 command. Some data sets will encounter an issue with the clustering calculation due to some variables having insufficient variance. Let's first perform a filter to remove any potential problem rows from the data. # Subset the relevant columns coverage.heatmap.data.MAG.filt <- select ( coverage.heatmap.data.MAG , contains ( \"sample\" ), \"taxonomy\" ) # Filter out zero-variance rows coverage.heatmap.data.MAG.filt <- coverage.heatmap.data.MAG.filt [ ! apply ( select ( coverage.heatmap.data.MAG.filt , - \"taxonomy\" ) == select ( coverage.heatmap.data.MAG.filt , - \"taxonomy\" )[[ 1 ]], 1 , all ),] Run the hierarchical clustering calculations based on Bray-Curtis dissimilarity by column and row. cov_clus.avg.col <- hclust ( vegdist ( t ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" ))), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) cov_clus.avg.row <- hclust ( vegdist ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" )), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) Let's have a quick look at the outputs of each using the basic plot function. plot ( cov_clus.avg.col , hang = -1 , cex = 1.5 ) plot ( cov_clus.avg.row , hang = -1 , cex = 1.5 ) If you wish to write these to file, we can wrap them in the png(...) and dev.off() lines, as below (this is true of all of the figures we will be generating): png ( \"coverage/MAGs_BC_hclust_Samples.png\" , width = 17 , height = 10 , units = \"cm\" , res = 300 ) plot ( cov_clus.avg.col , hang = -1 , cex = 0.5 ) dev.off () png ( \"coverage/MAGs_BC_hclust_MAGs.png\" , width = 17 , height = 10 , units = \"cm\" , res = 300 ) plot ( cov_clus.avg.row , hang = -1 , cex = 0.5 ) dev.off () 1.4 Set the colour palettes \u00b6 Before generating the heat map, let's set some colour palettes to use within the plot. We will create a greyscale for the coverage values in the heat map, one colour palette to colour the rows by the taxonomy of the MAG, and an additional palette for the columns (sample groups). NOTE: The list of colours for the taxonomy palette are taken from a 15-colour colour blind-friendly palette available here . The Group.col palette was selected using Chroma.js . # Load greyscale palette scalegreys <- colorRampPalette ( c ( \"white\" , \"black\" ), space = \"Lab\" )( 100 ) # Taxonomy colour palette MAG.cols <- c ( \"#006DDB\" , \"#FF6DB6\" , \"#DBD100\" , \"#FFB677\" , \"#004949\" , \"#009292\" , \"#FFFF6D\" , \"#924900\" , \"#490092\" , \"#24FF24\" , \"#920000\" , \"#B6DBFF\" , \"#B66DFF\" , \"#6DB6FF\" , \"#000000\" ) # Data.frame containing 'sample groups' colour palette Group.col <- data.frame ( Group = factor ( levels ( map.df $ Group )), col = c ( '#71dfdf' , '#3690b8' , '#00429d' ), stringsAsFactors = FALSE ) Finally, join the Group.col colours to the mapping file ( map.df ) so that we can call them in the heatmap.2 command. # Update map.df with `Group` colour code for each sample map.col <- data.frame ( full_join ( map.df , Group.col )) 1.5 Build the heat map \u00b6 Output the full heat map with sample (column) and variable (row) clustering. NOTE: Un-comment (remove the # symbol from) the lines png(...) and dev.off() before and after the heat map code will write the plot to file A number of options for visual parameters within heatmap2 can be set. Of the ones provided below, the first six (from Colv to ColSideColors ) are likely to be the most of interest at this stage. Colv and Rowv set the clustering (using the Bray-Curtis hierarcical clustering we calculated above. The cex... commands set the font sizes. The ...SideColors commands set the colours placed adjacent to the rows or columns to add further information to the plot (in this case, we are using this to add MAG taxonomy information and sample grouping information). In some cases, you may wish to omit the column clustering (for example, if your samples are from a set transect and you want to preserve the order of samples in the plot). In this case, you can set Colv = FALSE . NOTE: Here we are using gplot::legend to add two legends to the bottom of the plot for row-side and column-side colours. This can sometimes look a little clunky (and often requires experimenting with the margins settings). For final publication-quality figures, it may be preferable to output each element (heatmap, legend1, and legend2) as separate image files, and then compile in a program such as Adobe Illustrator. #png(\"coverage/MAGs_heatmap.png\",width=17,height=21,units=\"cm\",res=300) heatmap.2 ( as.matrix ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" ))), Colv = as.dendrogram ( cov_clus.avg.col ), Rowv = as.dendrogram ( cov_clus.avg.row ), cexCol = 1.2 , cexRow = 1.2 , RowSideColors = MAG.cols [ coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]], ColSideColors = map.col $ col , margins = c ( 28 , 16 ), lwid = c ( 1 , 8 ), lhei = c ( 1 , 8 ), col = scalegreys , na.color = \"white\" , scale = 'none' , srtCol = 45 , density.info = \"none\" , xlab = NA , ylab = NA , trace = c ( 'none' ), offsetRow = 0.2 , offsetCol = 0.2 , key.title = NA , key.xlab = \"log2(coverage)\" , key.par = list ( mgp = c ( 1.5 , 0.5 , 0 ), mar = c ( 3 , 1 , 3 , 0 ), cex = 0.5 ), ) legend ( \"bottomleft\" , legend = levels ( coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]), border = NULL , col = MAG.cols [ 1 : length ( levels ( coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]))], lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"Y-axis key:\\nMAG taxonomy\" ) legend ( \"bottomright\" , legend = levels ( map.col $ Group ), border = NULL , col = unique ( map.col $ col ), lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"X-axis key:\\nSample groups\" ) #dev.off() Some quick take-aways from looking at this plot: We can see that three MAGs are from the same class ( Gammaproteobacteria ) Sample 4 (from sample group Group_C ) is somewhat of an outlier, with only two bins (MAGs) recovered. Furthermore bin_7_Nitrobacter was only recovered from this sample. Samples from sample groups A and B each recovered the same sets of bins. With more samples, we would also be able to better discern co-occurence patterns of the MAGs across the samples by examining the clustering patterns in the dendrogram on the left. Feel free to experiment with some of the settings above. For example: What happens if you set Colv = TRUE, Rowv = TRUE instead of providing our pre-calculated Bray-Curtis dissimilarity-based clustering? Why might this cluster differently? (Hint: have a look at the documentation for heatmap.2 to see it's default settings for clustering: ?heatmap.2() ). NOTE: in the Jupyter environment, leaving a manual for a function open can be cumbersome. To clear the manual, right-click on the code block containing ?heatmap.2() and click Clear Outputs . Try removing the column clustering (but retaining the row clustering) by setting Colv = FALSE, Rowv = as.dendrogram(cov_clus.avg.row) . Play around with the margins = ... settings to see what difference each makes. Part 2 - Building a heatmap of viral contigs per sample \u00b6 We can run through the same process for the viral contigs. Many of the steps are as outlined above, so we will work through these a bit quicker and with less commentary along the way. However, we will highlight a handful of differences compared to the commands for the MAGs above, for example: steps for selecting and/or formatting the taxonomy; importing the quality output from CheckV ; and the (optional) addition of filtering out low quality contigs. 2.1 Set working directory, load R libraries, and import data \u00b6 In this case, import the coverage, taxonomy, mapping, and checkv files. For taxonomy we will select the Genome column (converted to Contig ), Order_VC_predicted (converted to taxonomy ), and Genus_VC_predicted (converted to taxonomy_genus ) (we will use the Order_VC_predicted to colour code viral contigs in the heat map, and Genus_VC_predicted to add to viral contig names in the plot). setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( tidyr ) library ( dplyr ) library ( readr ) library ( stringr ) library ( tibble ) # Other libraries library ( gplots ) library ( vegan ) # coverage table cov_vir <- read_tsv ( \"coverage/viruses_cov_table.txt\" ) %>% mutate ( Contig = contigName ) %>% select ( c ( 'Contig' , ends_with ( '.bam' ))) # taxonomy table tax_vir <- read_tsv ( \"coverage/vcontact2_tax_predict_table.txt\" ) %>% mutate ( Contig = Genome ) %>% mutate ( taxonomy = factor ( Order_VC_predicted )) %>% mutate ( taxonomy_genus = factor ( Genus_VC_predicted )) %>% select ( c ( 'Contig' , 'taxonomy' , 'taxonomy_genus' )) # CheckV quality checkv <- read_tsv ( \"coverage/checkv_quality_summary.tsv\" ) %>% mutate ( Contig = contig_id ) %>% select ( c ( 'Contig' , 'checkv_quality' )) %>% mutate ( checkv_quality = factor ( checkv_quality , levels = c ( \"Not-determined\" , \"Low-quality\" , \"Medium-quality\" , \"High-quality\" , \"Complete\" ))) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" ) 2.2 wrangle data \u00b6 This time we will collate the coverage, checkv, and taxonomy tables into a single table for downstream use, and create vir_ID_* unique identifers. # Strip .bam from sample names cov_vir <- cov_vir %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) # Collate data, add unique 'vir_ID' identifer, add 'vir_ID_tax' column collate_data_vir <- cov_vir %>% left_join ( tax_vir , by = 'Contig' ) %>% left_join ( checkv , by = 'Contig' ) %>% mutate ( vir_ID = paste ( \"vir\" , seq ( 1 , length ( Contig )), sep = \"_\" )) %>% relocate ( 'vir_ID' ) %>% mutate ( vir_ID_tax = paste ( vir_ID , taxonomy_genus , sep = \"_\" )) %>% filter ( ! is.na ( checkv_quality )) %>% select ( - taxonomy_genus ) # Log(2)-transform coverage data collate_data_vir_log2 <- collate_data_vir collate_data_vir_log2 [, names ( select ( collate_data_vir_log2 , contains ( \"sample\" )))] <- log2 ( collate_data_vir_log2 [, names ( select ( collate_data_vir_log2 , contains ( \"sample\" )))] + 1 ) Now let's add an (optional) filtering step here to remove any contigs that CheckV flagged as poor quality. First, take a look at all the categories of checkv_quality by using the levels() function. levels ( collate_data_vir_log2 $ checkv_quality ) # 'Not-determined''Low-quality''Medium-quality''High-quality''Complete' Let's filter out anything with a quality category of Not-determined or Low-quality . We will use != in the filter() function here to return only those rows that do not include 'Not-determined' or 'Low-quality'). collate_data_vir_log2 <- collate_data_vir_log2 %>% filter ( checkv_quality != 'Not-determined' & checkv_quality != 'Low-quality' ) A few final data wrangling steps: coverage.heatmap.data.vir <- collate_data_vir_log2 %>% mutate ( sum = rowSums ( select ( collate_data_vir_log2 , contains ( \"sample\" )))) %>% arrange ( desc ( sum )) %>% select ( \"vir_ID\" , contains ( \"sample\" ), \"taxonomy\" ) %>% mutate ( taxonomy = factor ( taxonomy )) %>% droplevels () %>% column_to_rownames ( var = \"vir_ID\" ) %>% as.data.frame () NOTE: In this case, the script only keeps the vir_ID* identifiers without the genus taxonomy appended for use in the plots, since the the long labels (due to numerous genus predictions for some contigs) make the plots illegible otherwise. To retain the taxonomy, change the two vir_ID entries above to vir_ID_tax . 2.3 Calculate hierarchical clustering of columns (samples) and rows (viral contigs). \u00b6 Filter to remove any potential problem rows from the data, and then run clustering and plot the dendrograms. # Subset the relevant columns coverage.heatmap.data.vir.filt <- select ( coverage.heatmap.data.vir , contains ( \"sample\" ), \"taxonomy\" ) # Filter out zero-variance rows coverage.heatmap.data.vir.filt <- coverage.heatmap.data.vir.filt [ ! apply ( select ( coverage.heatmap.data.vir.filt , - \"taxonomy\" ) == select ( coverage.heatmap.data.vir.filt , - \"taxonomy\" )[[ 1 ]], 1 , all ),] # hclust vir_cov_clus.avg.col <- hclust ( vegdist ( t ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" ))), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) vir_cov_clus.avg.row <- hclust ( vegdist ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" )), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) # plot #png(\"coverage/Viruses_BC_hclust_Samples.png\", width=17, height=10, units=\"cm\", res=300) plot ( vir_cov_clus.avg.col , hang = -1 , cex = 1.5 ) #dev.off() #png(\"coverage/Viruses_BC_hclust_MAGs.png\", width=17, height=10, units=\"cm\", res=300) plot ( vir_cov_clus.avg.row , hang = -1 , cex = 1.5 ) #dev.off() 2.4 Set the colour palettes \u00b6 # Load greyscale palette scalegreys <- colorRampPalette ( c ( \"white\" , \"black\" ), space = \"Lab\" )( 100 ) # Taxonomy colour palette vir.cols <- c ( \"#006DDB\" , \"#FF6DB6\" , \"#DBD100\" , \"#FFB677\" , \"#004949\" , \"#009292\" , \"#FFFF6D\" , \"#924900\" , \"#490092\" , \"#24FF24\" , \"#920000\" , \"#B6DBFF\" , \"#B66DFF\" , \"#6DB6FF\" , \"#000000\" ) # Data.frame containing 'sample groups' colour palette Group.col <- data.frame ( Group = factor ( levels ( map.df $ Group )), col = c ( '#71dfdf' , '#3690b8' , '#00429d' ), stringsAsFactors = FALSE ) # Update map.df with `Group` colour code for each sample map.df <- data.frame ( full_join ( map.df , Group.col )) 2.5 Build the heat map \u00b6 Output the full heat map with sample (column) and variable (row) clustering. NOTE: Un-comment (remove the # symbol from) the lines png(...) and dev.off() before and after the heat map code will write the plot to file #png(\"coverage/Viruses_heatmap.png\",width=17,height=21,units=\"cm\",res=300) heatmap.2 ( as.matrix ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" ))), Colv = as.dendrogram ( vir_cov_clus.avg.col ), Rowv = as.dendrogram ( vir_cov_clus.avg.row ), cexCol = 1.2 , cexRow = 1.2 , RowSideColors = vir.cols [ coverage.heatmap.data.vir.filt [, \"taxonomy\" ]], ColSideColors = map.df $ col , margins = c ( 28 , 16 ), lwid = c ( 1 , 8 ), lhei = c ( 1 , 8 ), col = scalegreys , na.color = \"white\" , scale = 'none' , srtCol = 45 , density.info = \"none\" , xlab = NA , ylab = NA , trace = c ( 'none' ), offsetRow = 0.2 , offsetCol = 0.2 , key.title = NA , key.xlab = \"log2(coverage)\" , key.par = list ( mgp = c ( 1.5 , 0.5 , 0 ), mar = c ( 3 , 1 , 3 , 0 ), cex = 0.5 ), ) legend ( \"bottomleft\" , legend = levels ( coverage.heatmap.data.vir.filt [, \"taxonomy\" ]), border = NULL , col = vir.cols [ 1 : length ( levels ( coverage.heatmap.data.vir.filt [, \"taxonomy\" ]))], lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"Y-axis key:\\nVirus taxonomy (Order)\" ) legend ( \"bottomright\" , legend = levels ( map.df $ Group ), border = NULL , col = unique ( map.df $ col ), lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"X-axis key:\\nSample groups\" ) #dev.off()","title":"Presentation of data: Per-sample coverage heatmaps"},{"location":"day4/ex16b_data_presentation_Coverage/#presentation-of-data-per-sample-coverage-heatmaps","text":"","title":"Presentation of data: Per-sample coverage heatmaps"},{"location":"day4/ex16b_data_presentation_Coverage/#objectives","text":"Building a heatmap of MAG coverage per sample Building a heatmap of viral contigs per sample","title":"Objectives"},{"location":"day4/ex16b_data_presentation_Coverage/#build-a-heatmap-of-average-coverage-per-sample-using-r","text":"One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? In the previous coverage and taxonomy exercises we generated per-sample coverage tables by mapping the quality-filtered unassembled reads back to the refined bins and the viral contigs to then generate coverage profiles for each. As a reminder: Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. A simple way to present this information is via a heatmap. In this exercise we will build a clustered heatmap of these coverage profiles in R . Since we also have tables of taxonomy assignments (via gtdb-tk for MAGs) and/or predictions (via vContact2 for viral contigs), we will also use these to add taxonomy information to the plot. The coverage and taxonomy tables generated in earlier exercises have been copied to 11.data_presentation/coverage/ a for use in these exercises. In addition to this, a simple mapping file has also been created ( 11.data_presentation/coverage/mapping_file.txt ). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column ( Group_A , Group_B , and Group_C ). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination, or correlation analyses) you could also add new columns to this file and load them at the same time. NOTE: As discussed in the coverage and taxonomy exercises , it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the **normalised* coverage tables into the steps outlined below.*","title":"Build a heatmap of average coverage per sample using R"},{"location":"day4/ex16b_data_presentation_Coverage/#part-1-building-a-heatmap-of-mag-coverage-per-sample","text":"To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)).","title":"Part 1 - Building a heatmap of MAG coverage per sample"},{"location":"day4/ex16b_data_presentation_Coverage/#11-set-working-directory-load-r-libraries-and-import-data","text":"First, set the working directory and load the required libraries. setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( tidyr ) library ( dplyr ) library ( readr ) library ( stringr ) library ( tibble ) # Other libraries library ( gplots ) library ( vegan ) NOTE: after copying this code into a code block in Jupyter , remember that, to run the code, press <shift> + <enter> with the code block selected. Import the coverage and taxonomy tables. When importing the files, we will select only the information of use here and will do a few basic first data wrangling steps. For the coverage table, we will select the contigName column and each of the columns of coverage values (columns sample[1-4].bam ). For taxonomy, we will select the user_genome column (converted to Bin ID) and classification (coverated to taxonomy ), and will also use gsub to extract just the taxonomic ranks of interest (in this case, we will extract the class to colour code MAGs in the heat map, and genus to add to MAG names in the plot), and to add Unassigned to any MAGs not assigned at these ranks. ( NOTE: to view a different taxonomic rank, you will need to change the two mutate(taxonomy_class = gsub(...)) rows below accordingly ). # coverage table cov_MAG <- read_tsv ( \"coverage/bins_cov_table.txt\" ) %>% select ( c ( 'contigName' , ends_with ( '.bam' ))) # taxonomy table tax_MAG <- read_tsv ( \"coverage/gtdbtk.bac120.summary.tsv\" ) %>% mutate ( Bin = gsub ( \"(.*).filtered\" , \"\\\\1\" , . $ user_genome )) %>% mutate ( taxonomy = gsub ( \".*;c(.*);o.*\" , \"class\\\\1\" , classification )) %>% mutate ( taxonomy = gsub ( \"^class__$\" , \"Unassigned\" , taxonomy )) %>% mutate ( taxonomy_genus = gsub ( \".*;g__(.*);.*\" , \"\\\\1\" , classification )) %>% mutate ( taxonomy_genus = gsub ( \"^$\" , \"Unassigned\" , taxonomy_genus )) %>% select ( c ( 'Bin' , 'taxonomy' , 'taxonomy_genus' )) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" )","title":"1.1 Set working directory, load R libraries, and import data"},{"location":"day4/ex16b_data_presentation_Coverage/#12-wrangle-data","text":"As noted during the coverage and taxonomy exercises, it is important to remember that we currently have a table of coverage values for all contigs contained within each MAG. Since we're aiming to present coverage for each MAG , we need to reduce these contig coverages into a single mean coverage value per MAG per sample. In the following code, we first strip the .bam extensions off of our sample names. We will then leverage the fact that we added bin IDs to each of the contig headers earlier to re-extract the bin ID for each using gsub , use the group_by() function to group by Bin , and the summarise() function to return the per-sample mean coverage of each set of contigs contained within each bin. # Extract BinID, group by Bin, calculate mean coverages for each set of contigs per Bin cov_MAG <- cov_MAG %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) %>% mutate ( Bin = gsub ( \"(.*)_NODE.*\" , \"\\\\1\" , . $ contigName )) %>% group_by ( Bin ) %>% summarise ( across ( where ( is.numeric ), mean )) Finally, collate the coverage and taxonomy tables into a single table for downstream use, and rename bins to include the genus name. # Collate coverage and taxonomy collate_data_MAG <- cov_MAG %>% left_join ( tax_MAG ) %>% mutate ( Bin = paste ( Bin , taxonomy_genus , sep = \"_\" )) %>% select ( - taxonomy_genus ) %>% mutate ( taxonomy = replace_na ( taxonomy , \"Unassigned\" )) In many real-life cases, to enhance the visualisation across a wide range of coverage values, you may wish to perform a transformation on your data. Perform a log(2)-transformation on the coverage data (those values in the columns that contains(\"sample\") ). Note, here we are calculating log2(x + 1) to allow for any cases where coverage values are 0 in any of the samples (log2(0 + 1) = 0). # Log(2)-transform coverage data collate_data_MAG_log2 <- collate_data_MAG collate_data_MAG_log2[,names(select(collate_data_MAG_log2, contains(\"sample\")))] <- log2(collate_data_MAG_log2[,names(select(collate_data_MAG_log2, contains(\"sample\")))] + 1) To have the data in a format that is ready for heatmap2 we will perform a few final data wrangling steps. Here we are re-ordering the file by row sums, selecting only the coverage columns ( contains(\"sample\") ) and taxonomy column, ensuring taxonomy is set as a factor, and setting the rownames based on the Bin column. coverage.heatmap.data.MAG <- collate_data_MAG_log2 %>% mutate ( sum = rowSums ( select ( collate_data_MAG_log2 , contains ( \"sample\" )))) %>% arrange ( desc ( sum )) %>% select ( \"Bin\" , contains ( \"sample\" ), \"taxonomy\" ) %>% mutate ( taxonomy = factor ( taxonomy )) %>% droplevels () %>% column_to_rownames ( var = \"Bin\" ) %>% as.data.frame () As there are only 10 bins in our mock metagenome data, spread over 7 different classes (the taxonomic rank we've chosen), we can include colour codes for all classes here. If you had many more cases of the selected taxonomic rank, it would be necessary to select a subset to colour code (e.g. the top 10 taxa, and grouping all others into Other ) so as not to have so many colours that things become unintelligible. We don't need to run this code block today (in fact, it will return an error due to having too few bins compared to the number we're trying to subset). But if you wished to do this for your own work, you could run something like the following to identify the 10 most abundant taxa (at the selected rank), and change the taxonomy of all others to Other . ## Identify most abundant Phyla in MAG data # Aggregate rows based on taxonomic assignments, reorder by overall relative abundance cov_MAG.tax <- collate_data_MAG_log2 %>% replace_na ( list ( taxonomy = \"Unassigned\" )) %>% mutate ( sum = rowSums ( select ( collate_data_MAG_log2 , contains ( \"sample\" )))) %>% group_by ( taxonomy ) %>% summarise_if ( is.numeric , list ( sum = sum )) %>% arrange ( desc ( sum_sum )) %>% mutate ( tax_summary = factor ( c ( taxonomy [ 1 : 10 ], rep ( \"Other\" , length ( taxonomy ) -10 )))) ## Add summaries of top 14 taxa to coverage.heatmap.data.MAG coverage.heatmap.data.MAG <- coverage.heatmap.data.MAG %>% rownames_to_column ( \"Bin\" ) %>% left_join ( select ( cov_MAG.tax , c ( \"taxonomy\" , \"tax_summary\" ))) %>% mutate ( taxonomy = factor ( tax_summary )) %>% column_to_rownames ( \"Bin\" )","title":"1.2 wrangle data"},{"location":"day4/ex16b_data_presentation_Coverage/#13-calculate-hierarchical-clustering-of-columns-samples-and-rows-mags","text":"It can be useful to arrange our rows and/or columns by some form of clustering. The plotting function heatmap2 can do this for us. However, here we will first perform this ourselves to be able to view the clustering output separately. We can then pass the same clustering to heatmap2 using the as.dendrogram() function within the heatmap2 command. Some data sets will encounter an issue with the clustering calculation due to some variables having insufficient variance. Let's first perform a filter to remove any potential problem rows from the data. # Subset the relevant columns coverage.heatmap.data.MAG.filt <- select ( coverage.heatmap.data.MAG , contains ( \"sample\" ), \"taxonomy\" ) # Filter out zero-variance rows coverage.heatmap.data.MAG.filt <- coverage.heatmap.data.MAG.filt [ ! apply ( select ( coverage.heatmap.data.MAG.filt , - \"taxonomy\" ) == select ( coverage.heatmap.data.MAG.filt , - \"taxonomy\" )[[ 1 ]], 1 , all ),] Run the hierarchical clustering calculations based on Bray-Curtis dissimilarity by column and row. cov_clus.avg.col <- hclust ( vegdist ( t ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" ))), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) cov_clus.avg.row <- hclust ( vegdist ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" )), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) Let's have a quick look at the outputs of each using the basic plot function. plot ( cov_clus.avg.col , hang = -1 , cex = 1.5 ) plot ( cov_clus.avg.row , hang = -1 , cex = 1.5 ) If you wish to write these to file, we can wrap them in the png(...) and dev.off() lines, as below (this is true of all of the figures we will be generating): png ( \"coverage/MAGs_BC_hclust_Samples.png\" , width = 17 , height = 10 , units = \"cm\" , res = 300 ) plot ( cov_clus.avg.col , hang = -1 , cex = 0.5 ) dev.off () png ( \"coverage/MAGs_BC_hclust_MAGs.png\" , width = 17 , height = 10 , units = \"cm\" , res = 300 ) plot ( cov_clus.avg.row , hang = -1 , cex = 0.5 ) dev.off ()","title":"1.3 Calculate hierarchical clustering of columns (samples) and rows (MAGs)."},{"location":"day4/ex16b_data_presentation_Coverage/#14-set-the-colour-palettes","text":"Before generating the heat map, let's set some colour palettes to use within the plot. We will create a greyscale for the coverage values in the heat map, one colour palette to colour the rows by the taxonomy of the MAG, and an additional palette for the columns (sample groups). NOTE: The list of colours for the taxonomy palette are taken from a 15-colour colour blind-friendly palette available here . The Group.col palette was selected using Chroma.js . # Load greyscale palette scalegreys <- colorRampPalette ( c ( \"white\" , \"black\" ), space = \"Lab\" )( 100 ) # Taxonomy colour palette MAG.cols <- c ( \"#006DDB\" , \"#FF6DB6\" , \"#DBD100\" , \"#FFB677\" , \"#004949\" , \"#009292\" , \"#FFFF6D\" , \"#924900\" , \"#490092\" , \"#24FF24\" , \"#920000\" , \"#B6DBFF\" , \"#B66DFF\" , \"#6DB6FF\" , \"#000000\" ) # Data.frame containing 'sample groups' colour palette Group.col <- data.frame ( Group = factor ( levels ( map.df $ Group )), col = c ( '#71dfdf' , '#3690b8' , '#00429d' ), stringsAsFactors = FALSE ) Finally, join the Group.col colours to the mapping file ( map.df ) so that we can call them in the heatmap.2 command. # Update map.df with `Group` colour code for each sample map.col <- data.frame ( full_join ( map.df , Group.col ))","title":"1.4 Set the colour palettes"},{"location":"day4/ex16b_data_presentation_Coverage/#15-build-the-heat-map","text":"Output the full heat map with sample (column) and variable (row) clustering. NOTE: Un-comment (remove the # symbol from) the lines png(...) and dev.off() before and after the heat map code will write the plot to file A number of options for visual parameters within heatmap2 can be set. Of the ones provided below, the first six (from Colv to ColSideColors ) are likely to be the most of interest at this stage. Colv and Rowv set the clustering (using the Bray-Curtis hierarcical clustering we calculated above. The cex... commands set the font sizes. The ...SideColors commands set the colours placed adjacent to the rows or columns to add further information to the plot (in this case, we are using this to add MAG taxonomy information and sample grouping information). In some cases, you may wish to omit the column clustering (for example, if your samples are from a set transect and you want to preserve the order of samples in the plot). In this case, you can set Colv = FALSE . NOTE: Here we are using gplot::legend to add two legends to the bottom of the plot for row-side and column-side colours. This can sometimes look a little clunky (and often requires experimenting with the margins settings). For final publication-quality figures, it may be preferable to output each element (heatmap, legend1, and legend2) as separate image files, and then compile in a program such as Adobe Illustrator. #png(\"coverage/MAGs_heatmap.png\",width=17,height=21,units=\"cm\",res=300) heatmap.2 ( as.matrix ( select ( coverage.heatmap.data.MAG.filt , contains ( \"sample\" ))), Colv = as.dendrogram ( cov_clus.avg.col ), Rowv = as.dendrogram ( cov_clus.avg.row ), cexCol = 1.2 , cexRow = 1.2 , RowSideColors = MAG.cols [ coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]], ColSideColors = map.col $ col , margins = c ( 28 , 16 ), lwid = c ( 1 , 8 ), lhei = c ( 1 , 8 ), col = scalegreys , na.color = \"white\" , scale = 'none' , srtCol = 45 , density.info = \"none\" , xlab = NA , ylab = NA , trace = c ( 'none' ), offsetRow = 0.2 , offsetCol = 0.2 , key.title = NA , key.xlab = \"log2(coverage)\" , key.par = list ( mgp = c ( 1.5 , 0.5 , 0 ), mar = c ( 3 , 1 , 3 , 0 ), cex = 0.5 ), ) legend ( \"bottomleft\" , legend = levels ( coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]), border = NULL , col = MAG.cols [ 1 : length ( levels ( coverage.heatmap.data.MAG.filt [, \"taxonomy\" ]))], lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"Y-axis key:\\nMAG taxonomy\" ) legend ( \"bottomright\" , legend = levels ( map.col $ Group ), border = NULL , col = unique ( map.col $ col ), lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"X-axis key:\\nSample groups\" ) #dev.off() Some quick take-aways from looking at this plot: We can see that three MAGs are from the same class ( Gammaproteobacteria ) Sample 4 (from sample group Group_C ) is somewhat of an outlier, with only two bins (MAGs) recovered. Furthermore bin_7_Nitrobacter was only recovered from this sample. Samples from sample groups A and B each recovered the same sets of bins. With more samples, we would also be able to better discern co-occurence patterns of the MAGs across the samples by examining the clustering patterns in the dendrogram on the left. Feel free to experiment with some of the settings above. For example: What happens if you set Colv = TRUE, Rowv = TRUE instead of providing our pre-calculated Bray-Curtis dissimilarity-based clustering? Why might this cluster differently? (Hint: have a look at the documentation for heatmap.2 to see it's default settings for clustering: ?heatmap.2() ). NOTE: in the Jupyter environment, leaving a manual for a function open can be cumbersome. To clear the manual, right-click on the code block containing ?heatmap.2() and click Clear Outputs . Try removing the column clustering (but retaining the row clustering) by setting Colv = FALSE, Rowv = as.dendrogram(cov_clus.avg.row) . Play around with the margins = ... settings to see what difference each makes.","title":"1.5 Build the heat map"},{"location":"day4/ex16b_data_presentation_Coverage/#part-2-building-a-heatmap-of-viral-contigs-per-sample","text":"We can run through the same process for the viral contigs. Many of the steps are as outlined above, so we will work through these a bit quicker and with less commentary along the way. However, we will highlight a handful of differences compared to the commands for the MAGs above, for example: steps for selecting and/or formatting the taxonomy; importing the quality output from CheckV ; and the (optional) addition of filtering out low quality contigs.","title":"Part 2 - Building a heatmap of viral contigs per sample"},{"location":"day4/ex16b_data_presentation_Coverage/#21-set-working-directory-load-r-libraries-and-import-data","text":"In this case, import the coverage, taxonomy, mapping, and checkv files. For taxonomy we will select the Genome column (converted to Contig ), Order_VC_predicted (converted to taxonomy ), and Genus_VC_predicted (converted to taxonomy_genus ) (we will use the Order_VC_predicted to colour code viral contigs in the heat map, and Genus_VC_predicted to add to viral contig names in the plot). setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( tidyr ) library ( dplyr ) library ( readr ) library ( stringr ) library ( tibble ) # Other libraries library ( gplots ) library ( vegan ) # coverage table cov_vir <- read_tsv ( \"coverage/viruses_cov_table.txt\" ) %>% mutate ( Contig = contigName ) %>% select ( c ( 'Contig' , ends_with ( '.bam' ))) # taxonomy table tax_vir <- read_tsv ( \"coverage/vcontact2_tax_predict_table.txt\" ) %>% mutate ( Contig = Genome ) %>% mutate ( taxonomy = factor ( Order_VC_predicted )) %>% mutate ( taxonomy_genus = factor ( Genus_VC_predicted )) %>% select ( c ( 'Contig' , 'taxonomy' , 'taxonomy_genus' )) # CheckV quality checkv <- read_tsv ( \"coverage/checkv_quality_summary.tsv\" ) %>% mutate ( Contig = contig_id ) %>% select ( c ( 'Contig' , 'checkv_quality' )) %>% mutate ( checkv_quality = factor ( checkv_quality , levels = c ( \"Not-determined\" , \"Low-quality\" , \"Medium-quality\" , \"High-quality\" , \"Complete\" ))) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" )","title":"2.1 Set working directory, load R libraries, and import data"},{"location":"day4/ex16b_data_presentation_Coverage/#22-wrangle-data","text":"This time we will collate the coverage, checkv, and taxonomy tables into a single table for downstream use, and create vir_ID_* unique identifers. # Strip .bam from sample names cov_vir <- cov_vir %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) # Collate data, add unique 'vir_ID' identifer, add 'vir_ID_tax' column collate_data_vir <- cov_vir %>% left_join ( tax_vir , by = 'Contig' ) %>% left_join ( checkv , by = 'Contig' ) %>% mutate ( vir_ID = paste ( \"vir\" , seq ( 1 , length ( Contig )), sep = \"_\" )) %>% relocate ( 'vir_ID' ) %>% mutate ( vir_ID_tax = paste ( vir_ID , taxonomy_genus , sep = \"_\" )) %>% filter ( ! is.na ( checkv_quality )) %>% select ( - taxonomy_genus ) # Log(2)-transform coverage data collate_data_vir_log2 <- collate_data_vir collate_data_vir_log2 [, names ( select ( collate_data_vir_log2 , contains ( \"sample\" )))] <- log2 ( collate_data_vir_log2 [, names ( select ( collate_data_vir_log2 , contains ( \"sample\" )))] + 1 ) Now let's add an (optional) filtering step here to remove any contigs that CheckV flagged as poor quality. First, take a look at all the categories of checkv_quality by using the levels() function. levels ( collate_data_vir_log2 $ checkv_quality ) # 'Not-determined''Low-quality''Medium-quality''High-quality''Complete' Let's filter out anything with a quality category of Not-determined or Low-quality . We will use != in the filter() function here to return only those rows that do not include 'Not-determined' or 'Low-quality'). collate_data_vir_log2 <- collate_data_vir_log2 %>% filter ( checkv_quality != 'Not-determined' & checkv_quality != 'Low-quality' ) A few final data wrangling steps: coverage.heatmap.data.vir <- collate_data_vir_log2 %>% mutate ( sum = rowSums ( select ( collate_data_vir_log2 , contains ( \"sample\" )))) %>% arrange ( desc ( sum )) %>% select ( \"vir_ID\" , contains ( \"sample\" ), \"taxonomy\" ) %>% mutate ( taxonomy = factor ( taxonomy )) %>% droplevels () %>% column_to_rownames ( var = \"vir_ID\" ) %>% as.data.frame () NOTE: In this case, the script only keeps the vir_ID* identifiers without the genus taxonomy appended for use in the plots, since the the long labels (due to numerous genus predictions for some contigs) make the plots illegible otherwise. To retain the taxonomy, change the two vir_ID entries above to vir_ID_tax .","title":"2.2 wrangle data"},{"location":"day4/ex16b_data_presentation_Coverage/#23-calculate-hierarchical-clustering-of-columns-samples-and-rows-viral-contigs","text":"Filter to remove any potential problem rows from the data, and then run clustering and plot the dendrograms. # Subset the relevant columns coverage.heatmap.data.vir.filt <- select ( coverage.heatmap.data.vir , contains ( \"sample\" ), \"taxonomy\" ) # Filter out zero-variance rows coverage.heatmap.data.vir.filt <- coverage.heatmap.data.vir.filt [ ! apply ( select ( coverage.heatmap.data.vir.filt , - \"taxonomy\" ) == select ( coverage.heatmap.data.vir.filt , - \"taxonomy\" )[[ 1 ]], 1 , all ),] # hclust vir_cov_clus.avg.col <- hclust ( vegdist ( t ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" ))), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) vir_cov_clus.avg.row <- hclust ( vegdist ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" )), method = \"bray\" , binary = FALSE , diag = FALSE , upper = FALSE , na.rm = FALSE ), \"aver\" ) # plot #png(\"coverage/Viruses_BC_hclust_Samples.png\", width=17, height=10, units=\"cm\", res=300) plot ( vir_cov_clus.avg.col , hang = -1 , cex = 1.5 ) #dev.off() #png(\"coverage/Viruses_BC_hclust_MAGs.png\", width=17, height=10, units=\"cm\", res=300) plot ( vir_cov_clus.avg.row , hang = -1 , cex = 1.5 ) #dev.off()","title":"2.3 Calculate hierarchical clustering of columns (samples) and rows (viral contigs)."},{"location":"day4/ex16b_data_presentation_Coverage/#24-set-the-colour-palettes","text":"# Load greyscale palette scalegreys <- colorRampPalette ( c ( \"white\" , \"black\" ), space = \"Lab\" )( 100 ) # Taxonomy colour palette vir.cols <- c ( \"#006DDB\" , \"#FF6DB6\" , \"#DBD100\" , \"#FFB677\" , \"#004949\" , \"#009292\" , \"#FFFF6D\" , \"#924900\" , \"#490092\" , \"#24FF24\" , \"#920000\" , \"#B6DBFF\" , \"#B66DFF\" , \"#6DB6FF\" , \"#000000\" ) # Data.frame containing 'sample groups' colour palette Group.col <- data.frame ( Group = factor ( levels ( map.df $ Group )), col = c ( '#71dfdf' , '#3690b8' , '#00429d' ), stringsAsFactors = FALSE ) # Update map.df with `Group` colour code for each sample map.df <- data.frame ( full_join ( map.df , Group.col ))","title":"2.4 Set the colour palettes"},{"location":"day4/ex16b_data_presentation_Coverage/#25-build-the-heat-map","text":"Output the full heat map with sample (column) and variable (row) clustering. NOTE: Un-comment (remove the # symbol from) the lines png(...) and dev.off() before and after the heat map code will write the plot to file #png(\"coverage/Viruses_heatmap.png\",width=17,height=21,units=\"cm\",res=300) heatmap.2 ( as.matrix ( select ( coverage.heatmap.data.vir.filt , contains ( \"sample\" ))), Colv = as.dendrogram ( vir_cov_clus.avg.col ), Rowv = as.dendrogram ( vir_cov_clus.avg.row ), cexCol = 1.2 , cexRow = 1.2 , RowSideColors = vir.cols [ coverage.heatmap.data.vir.filt [, \"taxonomy\" ]], ColSideColors = map.df $ col , margins = c ( 28 , 16 ), lwid = c ( 1 , 8 ), lhei = c ( 1 , 8 ), col = scalegreys , na.color = \"white\" , scale = 'none' , srtCol = 45 , density.info = \"none\" , xlab = NA , ylab = NA , trace = c ( 'none' ), offsetRow = 0.2 , offsetCol = 0.2 , key.title = NA , key.xlab = \"log2(coverage)\" , key.par = list ( mgp = c ( 1.5 , 0.5 , 0 ), mar = c ( 3 , 1 , 3 , 0 ), cex = 0.5 ), ) legend ( \"bottomleft\" , legend = levels ( coverage.heatmap.data.vir.filt [, \"taxonomy\" ]), border = NULL , col = vir.cols [ 1 : length ( levels ( coverage.heatmap.data.vir.filt [, \"taxonomy\" ]))], lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"Y-axis key:\\nVirus taxonomy (Order)\" ) legend ( \"bottomright\" , legend = levels ( map.df $ Group ), border = NULL , col = unique ( map.df $ col ), lty = 1 , lwd = 6 , bty = \"n\" , ncol = 1 , cex = 0.8 , title = \"X-axis key:\\nSample groups\" ) #dev.off()","title":"2.5 Build the heat map"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/","text":"( Optional ) Presentation of data: Ordinations \u00b6 Objectives \u00b6 Import and wrangle data in R Calculate weighted and unweighted Bray-Curtis dissimilarity metrics and nMDS analyses Build nMDS plots in R using the ggplot2 package Discussion: Follow-up analyses Introduction \u00b6 A common method to investigate the relatedness of samples to one another is to calculate ordinations and to visualise this in the form of a principal components analysis (PCA) or non-metric multidimensional scaling (nMDS) plot. In this exercise, we will calculate ordinations based on weighted and unweighted (binary) Bray-Curtis dissimilarity and present these in nMDS plots. The coverage tables generated in earlier exercises have been copied to 11.data_presentation/coverage/ a for use in these exercises. In addition to this, a simple mapping file has also been created ( 11.data_presentation/coverage/mapping_file.txt ). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column ( Group_A , Group_B , and Group_C ). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination) you could also add new columns to this file and load them at the same time. NOTE: As discussed in the coverage and taxonomy exercises , it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the **normalised* coverage tables into the steps outlined below.* 1. Import and wrangle data in R \u00b6 To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). NOTE: You will recognise that the first few steps will follow the same process as the previous exercise on generating coverage heatmaps . In practice, these two workflows can be combined to reduce the repititive aspects. 1.1 Set working directory, load R libraries, and import data \u00b6 First, set the working directory and load the required libraries. setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( dplyr ) library ( readr ) library ( stringr ) library ( ggplot2 ) # Other libraries library ( vegan ) Import the coverage tables and mapping file. When importing the files, we will select only the information of use here and will do a few basic first data wrangling steps. From the coverage table, we will select the contigName column and each of the columns of coverage values (columns sample[1-4].bam ). # bins coverage table cov_MAG <- read_tsv ( \"coverage/bins_cov_table.txt\" ) %>% select ( c ( 'contigName' , ends_with ( '.bam' ))) # viruses coverage table cov_vir <- read_tsv ( \"coverage/viruses_cov_table.txt\" ) %>% mutate ( Contig = contigName ) %>% select ( c ( 'Contig' , ends_with ( '.bam' ))) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" ) 1.2 wrangle data \u00b6 As noted during the coverage and taxonomy exercises, for the bin data it is important to remember that we currently have a table of coverage values for all contigs contained within each bin (MAG). Since we're aiming to present coverage for each MAG , we need to reduce these contig coverages into a single mean coverage value per MAG per sample. In the following code, we first strip the .bam extensions off of our sample names. For the bin data, we will then leverage the fact that we added bin IDs to each of the contig headers earlier to re-extract the bin ID for each using gsub , use the group_by() function to group by Bin , and the summarise() function to return the per-sample mean coverage of each set of contigs contained within each bin (these grouping steps are not necessary for the viral contig data). # Bins data: Extract BinID, group by Bin, calculate mean coverages for each set of contigs per Bin cov_MAG <- cov_MAG %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) %>% mutate ( Bin = gsub ( \"(.*)_NODE.*\" , \"\\\\1\" , . $ contigName )) %>% group_by ( Bin ) %>% summarise ( across ( where ( is.numeric ), mean )) # Viral contigs data: Strip .bam from sample names cov_vir <- cov_vir %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) Finally, generate a data.frame in the format required to calculate the Bray-Curtis dissimilarities via the vegan function vegdist . Here we are selecting only the coverage columns ( contains(\"sample\") ), and then transposing so that each sample is a row and variables (bins or viral contigs) are in columns. # Make data.frame for nMDS: MAGS coverage.nmds.data.MAG <- t ( as.data.frame ( select ( cov_MAG , contains ( \"sample\" )))) # Make data.frame for nMDS: viruses coverage.nmds.data.vir <- t ( as.data.frame ( select ( cov_vir , contains ( \"sample\" )))) 2. Calculate weighted and unweighted Bray-Curtis dissimilarity and nMDS using R \u00b6 It is often useful to examine ordinations based on both weighted and unweighted (binary) dissimilarity (or distance) metrics. Weighted metrics take into account the proportions or abundances of each variable (in our case, the coverage value of each bin or viral contig). This can be particularly useful for visualising broad shifts in overall community structure (while the membership of the community may remain relatively unchanged). Unweighted metrics are based on presence/absence alone, and can be useful for highlighting cases where the actual membership of communities differs (ignoring their relative proportions within the communities). Here we will use the functions vegdist() and metaMDS() from the R package vegan to generate weighted and unweighted Bray-Curtis dissimilarity matrices and nMDS solutions for the microbial bin data and viral contigs data. NOTE: you may also wish to make use of the set.seed() function before each calculation to ensure that you obtain consistent results if the same commands are re-run at a later date. NOTE: in the Jupyter environment, these commands will create long outputs below each code block. If need be, these outputs can be cleared from the screeen via right-click on the code block > Clear Outputs . # Bins: weighted Bray-Curtis cov.bray.MAG <- vegdist ( coverage.nmds.data.MAG, method = \"bray\" , binary = FALSE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.sol.MAG <- metaMDS ( cov.bray.MAG, k = 2 , trymax = 999 ) # Bins: unweighted (binary) Bray-Curtis cov.bray.binary.MAG <- vegdist ( coverage.nmds.data.MAG, method = \"bray\" , binary = TRUE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.binary.sol.MAG <- metaMDS ( cov.bray.binary.MAG, k = 2 , trymax = 999 ) # Viruses: weighted Bray-Curtis cov.bray.vir <- vegdist ( coverage.nmds.data.vir, method = \"bray\" , binary = FALSE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.sol.vir <- metaMDS ( cov.bray.vir, k = 2 , trymax = 999 ) # Viruses: unweighted (binary) Bray-Curtis cov.bray.binary.vir <- vegdist ( coverage.nmds.data.vir, method = \"bray\" , binary = TRUE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.binary.sol.vir <- metaMDS ( cov.bray.binary.vir, k = 2 , trymax = 999 ) 3. Build nMDS plots in R using the ggplot2 package \u00b6 3.1 Set the ggplot plot theme and sample groups colour palette \u00b6 We will build the nMDS plot using the ggplot2 package. Note that ggplot s are based on the principle of layering various aspects of a plot on top of each other in sequential calls. Getting familair with the functionality of ggplot s is incredibly useful for visualising many different types of data sets in a variety of different formats. First, set the plot theme elements (via the theme() function), and the colour palette for the sample grouping. theme_Ordination <- theme( axis.text = element_text(size=7), axis.title = element_text(size=7), axis.line = element_line(color=\"grey40\", size=0.3, linetype=1), axis.ticks = element_line(color=\"grey40\", size=0.3, linetype=1), legend.text = element_text(size=7), legend.title = element_blank(), legend.key = element_blank(), legend.position=\"bottom\", panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill=\"white\", colour=\"grey40\", size=0.3, linetype=1), plot.title = element_text(size=7, face=\"bold\"), strip.background = element_rect(fill=\"grey90\", colour=\"grey40\", size=0.3, linetype=0), strip.text.x = element_text(size=7), panel.spacing = unit(0.2, \"lines\"), plot.margin=unit(c(5, 5, 5, 5), \"points\") ) group.cols = c('#71dfdf', '#3690b8', '#00429d') 3.2 Select the data set \u00b6 From here, the process is identical for each of the different analyses calculated above (based on weighted and unweighted Bray-Curtis for both microbial bin (MAG) data and viral contig data). In this first step, we are simply setting up which data set we are wishing to plot (bin or viral data, weighted or unweighted Bray-Curtis dissimilarity). The same process can then be re-run for each data set by changing both of the data.frames being read into these bray.dist and bray.sol variables (copy the relevant data.frame names from the section above, or un-comment the relevant lines below). bray.dist <- cov.bray.MAG bray.sol <- cov.bray.sol.MAG #bray.dist <- cov.bray.binary.MAG #bray.sol <- cov.bray.binary.sol.MAG #bray.dist <- cov.bray.vir #bray.sol <- cov.bray.sol.vir #bray.dist <- cov.bray.binary.vir #bray.sol <- cov.bray.binary.sol.vir 3.3 Create the nMDS data.frame \u00b6 Now, create a data.frame that includes each of the nMDS X and Y points (NMDS1 and NMDS2 scores) for each sample, and merge with the mapping file to add the sample group information (i.e. whether samples are from GroupA , GroupB , or GroupC ). sol.scrs <- data.frame ( SampleID = dimnames ( bray.sol $points )[[ 1 ]] , NMDS1 = bray.sol $point [ ,1 ] , NMDS2 = bray.sol $point [ ,2 ] ) %>% merge ( ., map.df, by = \"SampleID\" , type = \"full\" ) 3.4 build the plot \u00b6 Finally, build the plot using ggplot . In this function, we: load the data into ggplot() , setting x and y axes data as the NMDS1 and NMDS2 columns from the sol.scrs data.frame plot the sample points using the geom_point() function, and also set the colour aesthetic to be based on the sample group ( Group ) from the mapping file information set coord_fixed() to ensure the the scales of the x- and y-axes are kept consistent use the geom_text function to add a text label of the stress value for this ordination (extracted from bray.sol$stress ) to the top right corner of the plot set the colours for the sample groups using the scale_colour_manual() function. Here we will pass the group.cols variable generated above (which is set to have a consistent colour scheme with the figures generated in the previous exercise on coverage heatmaps). Add x- and y-axis labels Modify various visual aspects of the plot using the theme() function (as set in the theme_Ordination variable above) Save this into the variable NMDS.plot NOTE: here we are also surrounding the entire call in parentheses ( (...) ). This tells R to both save the plot in the variable NMDS.plot and **also* output the plot to the visualisation pane in RStudio or the Jupyter Notebook . Omitting the parentheses would result in saving to NMDS.plot but not viewing the plot.* ( NMDS.plot <- ggplot ( sol.scrs, aes ( NMDS1, NMDS2 )) + geom_point ( size = 2 .5, aes ( col = Group )) + coord_fixed () + geom_text ( data = NULL, x = Inf, y = Inf, vjust = 2 .4, hjust = 1 .2, size = 2 .5, colour = \"black\" , fontface = \"italic\" , label = paste ( \"Stress =\" , round ( bray.sol $stress , digits = 4 ))) + scale_color_manual ( values = group.cols ) + ylab ( \"nMDS2\" ) + xlab ( \"nMDS1\" ) + theme_Ordination ) 3.4 Save the plot to file \u00b6 NOTE: change the file output name in the first line below for each different data set run through the above commands png ( \"nMDS_MAGs_weighted.png\" , width = 17 , height = 17 , units = \"cm\" , res = 300 ) NMDS.plot dev.off () Repeat the steps above (from 3.2 onwards), each time inputting a different data set (bin or viral data, weighted or unweighted Bray-Curtis dissimilarity) into the bray.dist and bray.sol variables in the section \" 3.2 Select the data set \". nMDS figures: example outputs \u00b6 NOTE: How informative these types of analyses are depends in part on the number of samples you actually have and the degree of variation between the samples. As you can see in the nMDS plots based on unweighted (binary) Bray-Curtis dissimilarities (especially for the MAGs data) there are not enough differences between any of the samples (in this case, in terms of community membership, rather than relative abundances) for this to result in a particularly meaningful or useful plot in these cases. Follow-up analyses \u00b6 It is often valuable to follow these visualisations up with tests for beta-dispersion (whether or not sample groups have a comparable spread to one another) and, provided that beta-dispersion is not significantly different between groups, PERMANOVA tests (the extent to which the variation in the data can be explained by a given variable (such as sample groups or other environmental factors, based on differences between the centroids of each group). Beta-dispersion can be calculated using the betadisper() function from the vegan package (passing the bray.dist data and the map$Group variable to group by), followed by anova() , permutest() , or TukeyHSD() tests of differences between the groups (by inputting the generated betadisper output). PERMANOVA tests can be conducted via the adonis() function from the vegan package (for example, via: adonis(bray.dist ~ Group, data=map, permutations=999, method=\"bray\") .","title":"(*Optional*) Presentation of data: Ordinations"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#optional-presentation-of-data-ordinations","text":"","title":"(Optional) Presentation of data: Ordinations"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#objectives","text":"Import and wrangle data in R Calculate weighted and unweighted Bray-Curtis dissimilarity metrics and nMDS analyses Build nMDS plots in R using the ggplot2 package Discussion: Follow-up analyses","title":"Objectives"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#introduction","text":"A common method to investigate the relatedness of samples to one another is to calculate ordinations and to visualise this in the form of a principal components analysis (PCA) or non-metric multidimensional scaling (nMDS) plot. In this exercise, we will calculate ordinations based on weighted and unweighted (binary) Bray-Curtis dissimilarity and present these in nMDS plots. The coverage tables generated in earlier exercises have been copied to 11.data_presentation/coverage/ a for use in these exercises. In addition to this, a simple mapping file has also been created ( 11.data_presentation/coverage/mapping_file.txt ). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column ( Group_A , Group_B , and Group_C ). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination) you could also add new columns to this file and load them at the same time. NOTE: As discussed in the coverage and taxonomy exercises , it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the **normalised* coverage tables into the steps outlined below.*","title":"Introduction"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#1-import-and-wrangle-data-in-r","text":"To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). NOTE: You will recognise that the first few steps will follow the same process as the previous exercise on generating coverage heatmaps . In practice, these two workflows can be combined to reduce the repititive aspects.","title":"1. Import and wrangle data in R"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#11-set-working-directory-load-r-libraries-and-import-data","text":"First, set the working directory and load the required libraries. setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/' ) # tidyverse libraries library ( dplyr ) library ( readr ) library ( stringr ) library ( ggplot2 ) # Other libraries library ( vegan ) Import the coverage tables and mapping file. When importing the files, we will select only the information of use here and will do a few basic first data wrangling steps. From the coverage table, we will select the contigName column and each of the columns of coverage values (columns sample[1-4].bam ). # bins coverage table cov_MAG <- read_tsv ( \"coverage/bins_cov_table.txt\" ) %>% select ( c ( 'contigName' , ends_with ( '.bam' ))) # viruses coverage table cov_vir <- read_tsv ( \"coverage/viruses_cov_table.txt\" ) %>% mutate ( Contig = contigName ) %>% select ( c ( 'Contig' , ends_with ( '.bam' ))) # mapping file (import both columns as factors: col_types set to factor, factor) map.df <- read_tsv ( \"coverage/mapping_file.txt\" , col_types = \"ff\" )","title":"1.1 Set working directory, load R libraries, and import data"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#12-wrangle-data","text":"As noted during the coverage and taxonomy exercises, for the bin data it is important to remember that we currently have a table of coverage values for all contigs contained within each bin (MAG). Since we're aiming to present coverage for each MAG , we need to reduce these contig coverages into a single mean coverage value per MAG per sample. In the following code, we first strip the .bam extensions off of our sample names. For the bin data, we will then leverage the fact that we added bin IDs to each of the contig headers earlier to re-extract the bin ID for each using gsub , use the group_by() function to group by Bin , and the summarise() function to return the per-sample mean coverage of each set of contigs contained within each bin (these grouping steps are not necessary for the viral contig data). # Bins data: Extract BinID, group by Bin, calculate mean coverages for each set of contigs per Bin cov_MAG <- cov_MAG %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) %>% mutate ( Bin = gsub ( \"(.*)_NODE.*\" , \"\\\\1\" , . $ contigName )) %>% group_by ( Bin ) %>% summarise ( across ( where ( is.numeric ), mean )) # Viral contigs data: Strip .bam from sample names cov_vir <- cov_vir %>% rename_at ( vars ( contains ( \"sample\" )), list ( ~ str_replace ( . , \".bam\" , \"\" )) ) Finally, generate a data.frame in the format required to calculate the Bray-Curtis dissimilarities via the vegan function vegdist . Here we are selecting only the coverage columns ( contains(\"sample\") ), and then transposing so that each sample is a row and variables (bins or viral contigs) are in columns. # Make data.frame for nMDS: MAGS coverage.nmds.data.MAG <- t ( as.data.frame ( select ( cov_MAG , contains ( \"sample\" )))) # Make data.frame for nMDS: viruses coverage.nmds.data.vir <- t ( as.data.frame ( select ( cov_vir , contains ( \"sample\" ))))","title":"1.2 wrangle data"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#2-calculate-weighted-and-unweighted-bray-curtis-dissimilarity-and-nmds-using-r","text":"It is often useful to examine ordinations based on both weighted and unweighted (binary) dissimilarity (or distance) metrics. Weighted metrics take into account the proportions or abundances of each variable (in our case, the coverage value of each bin or viral contig). This can be particularly useful for visualising broad shifts in overall community structure (while the membership of the community may remain relatively unchanged). Unweighted metrics are based on presence/absence alone, and can be useful for highlighting cases where the actual membership of communities differs (ignoring their relative proportions within the communities). Here we will use the functions vegdist() and metaMDS() from the R package vegan to generate weighted and unweighted Bray-Curtis dissimilarity matrices and nMDS solutions for the microbial bin data and viral contigs data. NOTE: you may also wish to make use of the set.seed() function before each calculation to ensure that you obtain consistent results if the same commands are re-run at a later date. NOTE: in the Jupyter environment, these commands will create long outputs below each code block. If need be, these outputs can be cleared from the screeen via right-click on the code block > Clear Outputs . # Bins: weighted Bray-Curtis cov.bray.MAG <- vegdist ( coverage.nmds.data.MAG, method = \"bray\" , binary = FALSE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.sol.MAG <- metaMDS ( cov.bray.MAG, k = 2 , trymax = 999 ) # Bins: unweighted (binary) Bray-Curtis cov.bray.binary.MAG <- vegdist ( coverage.nmds.data.MAG, method = \"bray\" , binary = TRUE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.binary.sol.MAG <- metaMDS ( cov.bray.binary.MAG, k = 2 , trymax = 999 ) # Viruses: weighted Bray-Curtis cov.bray.vir <- vegdist ( coverage.nmds.data.vir, method = \"bray\" , binary = FALSE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.sol.vir <- metaMDS ( cov.bray.vir, k = 2 , trymax = 999 ) # Viruses: unweighted (binary) Bray-Curtis cov.bray.binary.vir <- vegdist ( coverage.nmds.data.vir, method = \"bray\" , binary = TRUE, diag = FALSE, upper = FALSE, na.rm = FALSE ) cov.bray.binary.sol.vir <- metaMDS ( cov.bray.binary.vir, k = 2 , trymax = 999 )","title":"2. Calculate weighted and unweighted Bray-Curtis dissimilarity and nMDS using R"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#3-build-nmds-plots-in-r-using-the-ggplot2-package","text":"","title":"3. Build nMDS plots in R using the ggplot2 package"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#31-set-the-ggplot-plot-theme-and-sample-groups-colour-palette","text":"We will build the nMDS plot using the ggplot2 package. Note that ggplot s are based on the principle of layering various aspects of a plot on top of each other in sequential calls. Getting familair with the functionality of ggplot s is incredibly useful for visualising many different types of data sets in a variety of different formats. First, set the plot theme elements (via the theme() function), and the colour palette for the sample grouping. theme_Ordination <- theme( axis.text = element_text(size=7), axis.title = element_text(size=7), axis.line = element_line(color=\"grey40\", size=0.3, linetype=1), axis.ticks = element_line(color=\"grey40\", size=0.3, linetype=1), legend.text = element_text(size=7), legend.title = element_blank(), legend.key = element_blank(), legend.position=\"bottom\", panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill=\"white\", colour=\"grey40\", size=0.3, linetype=1), plot.title = element_text(size=7, face=\"bold\"), strip.background = element_rect(fill=\"grey90\", colour=\"grey40\", size=0.3, linetype=0), strip.text.x = element_text(size=7), panel.spacing = unit(0.2, \"lines\"), plot.margin=unit(c(5, 5, 5, 5), \"points\") ) group.cols = c('#71dfdf', '#3690b8', '#00429d')","title":"3.1 Set the ggplot plot theme and sample groups colour palette"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#32-select-the-data-set","text":"From here, the process is identical for each of the different analyses calculated above (based on weighted and unweighted Bray-Curtis for both microbial bin (MAG) data and viral contig data). In this first step, we are simply setting up which data set we are wishing to plot (bin or viral data, weighted or unweighted Bray-Curtis dissimilarity). The same process can then be re-run for each data set by changing both of the data.frames being read into these bray.dist and bray.sol variables (copy the relevant data.frame names from the section above, or un-comment the relevant lines below). bray.dist <- cov.bray.MAG bray.sol <- cov.bray.sol.MAG #bray.dist <- cov.bray.binary.MAG #bray.sol <- cov.bray.binary.sol.MAG #bray.dist <- cov.bray.vir #bray.sol <- cov.bray.sol.vir #bray.dist <- cov.bray.binary.vir #bray.sol <- cov.bray.binary.sol.vir","title":"3.2 Select the data set"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#33-create-the-nmds-dataframe","text":"Now, create a data.frame that includes each of the nMDS X and Y points (NMDS1 and NMDS2 scores) for each sample, and merge with the mapping file to add the sample group information (i.e. whether samples are from GroupA , GroupB , or GroupC ). sol.scrs <- data.frame ( SampleID = dimnames ( bray.sol $points )[[ 1 ]] , NMDS1 = bray.sol $point [ ,1 ] , NMDS2 = bray.sol $point [ ,2 ] ) %>% merge ( ., map.df, by = \"SampleID\" , type = \"full\" )","title":"3.3 Create the nMDS data.frame"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#34-build-the-plot","text":"Finally, build the plot using ggplot . In this function, we: load the data into ggplot() , setting x and y axes data as the NMDS1 and NMDS2 columns from the sol.scrs data.frame plot the sample points using the geom_point() function, and also set the colour aesthetic to be based on the sample group ( Group ) from the mapping file information set coord_fixed() to ensure the the scales of the x- and y-axes are kept consistent use the geom_text function to add a text label of the stress value for this ordination (extracted from bray.sol$stress ) to the top right corner of the plot set the colours for the sample groups using the scale_colour_manual() function. Here we will pass the group.cols variable generated above (which is set to have a consistent colour scheme with the figures generated in the previous exercise on coverage heatmaps). Add x- and y-axis labels Modify various visual aspects of the plot using the theme() function (as set in the theme_Ordination variable above) Save this into the variable NMDS.plot NOTE: here we are also surrounding the entire call in parentheses ( (...) ). This tells R to both save the plot in the variable NMDS.plot and **also* output the plot to the visualisation pane in RStudio or the Jupyter Notebook . Omitting the parentheses would result in saving to NMDS.plot but not viewing the plot.* ( NMDS.plot <- ggplot ( sol.scrs, aes ( NMDS1, NMDS2 )) + geom_point ( size = 2 .5, aes ( col = Group )) + coord_fixed () + geom_text ( data = NULL, x = Inf, y = Inf, vjust = 2 .4, hjust = 1 .2, size = 2 .5, colour = \"black\" , fontface = \"italic\" , label = paste ( \"Stress =\" , round ( bray.sol $stress , digits = 4 ))) + scale_color_manual ( values = group.cols ) + ylab ( \"nMDS2\" ) + xlab ( \"nMDS1\" ) + theme_Ordination )","title":"3.4 build the plot"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#34-save-the-plot-to-file","text":"NOTE: change the file output name in the first line below for each different data set run through the above commands png ( \"nMDS_MAGs_weighted.png\" , width = 17 , height = 17 , units = \"cm\" , res = 300 ) NMDS.plot dev.off () Repeat the steps above (from 3.2 onwards), each time inputting a different data set (bin or viral data, weighted or unweighted Bray-Curtis dissimilarity) into the bray.dist and bray.sol variables in the section \" 3.2 Select the data set \".","title":"3.4 Save the plot to file"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#nmds-figures-example-outputs","text":"NOTE: How informative these types of analyses are depends in part on the number of samples you actually have and the degree of variation between the samples. As you can see in the nMDS plots based on unweighted (binary) Bray-Curtis dissimilarities (especially for the MAGs data) there are not enough differences between any of the samples (in this case, in terms of community membership, rather than relative abundances) for this to result in a particularly meaningful or useful plot in these cases.","title":"nMDS figures: example outputs"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#follow-up-analyses","text":"It is often valuable to follow these visualisations up with tests for beta-dispersion (whether or not sample groups have a comparable spread to one another) and, provided that beta-dispersion is not significantly different between groups, PERMANOVA tests (the extent to which the variation in the data can be explained by a given variable (such as sample groups or other environmental factors, based on differences between the centroids of each group). Beta-dispersion can be calculated using the betadisper() function from the vegan package (passing the bray.dist data and the map$Group variable to group by), followed by anova() , permutest() , or TukeyHSD() tests of differences between the groups (by inputting the generated betadisper output). PERMANOVA tests can be conducted via the adonis() function from the vegan package (for example, via: adonis(bray.dist ~ Group, data=map, permutations=999, method=\"bray\") .","title":"Follow-up analyses"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/","text":"Presentation of data: KEGG pathway maps \u00b6 Objectives \u00b6 Build a KEGG pathway map using R Import and wrangle data in R Subset the data for KO IDs of interest Reshape the data for input into pathview Creating pathway map of genes related to nitrogen metabolism Build a KEGG pathway map using R \u00b6 In this exercise, we will generate KEGG a pathways map from genome annotations to visualize potential pathways present in our assembled, binned genome sequences. The key package used here is pathview , available from Bioconductor (for installation on your local version of RStudio , see the previous ex16a_data_presentation_Intro.md section). pathview is mainly used in transcriptomic analyses but can be adapted for any downstream analyses that utilise the KEGG database. For this exercise, we are interested in visualising the prevalence of genes that we have annotated in a pathway of interest. To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). Set the working directory and load packages and files into R \u00b6 Set the working directory and load the required packages setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/kegg_map' ) library ( pathview ) library ( KEGGREST ) # tidyverse libraries library ( readr ) library ( tibble ) library ( dplyr ) library ( tidyr ) library ( purrr ) library ( stringr ) Load your files into R. Here, we are loading them into a list. Given that there are ten files, it is easier to load, clean, and analyze the data using list methods available in tidyverse. tmp_files <- dir ( pattern = \"*.aa\" ) tmp_names <- str_replace ( tmp_files , \"(.*).annotation.aa\" , \"\\\\1\" ) bin_list <- as.list ( tmp_files ) %>% map ( function ( x ) { read_delim ( file = x , delim = \"\\t\" ) } ) names ( bin_list ) <- tmp_names rm ( list = ls ( pattern = \"tmp*\" )) NOTE: R will print some warning messages about column types. However, we do not need all the columns for this analysis and it is not necessary to reformat them. Subset the data \u00b6 For this section, we only require the KEGG orthology IDs (KO numbers). Use the code below to subset your data based on these IDs. Check data headers to determine which columns to subset: names ( bin_list $ bin_0 ) We can see that the headers related to KEGG annotations are in columns 22 to 27. We will subset the relevant columns and concatenate the rows. kegg_annotations <- map ( bin_list , function ( x ) { x [, 22 : 27 ] } ) %>% bind_rows ( .id = \"bin\" ) We also need the KO numbers. Here, we will create a column with that information where available (it is in the column Description_2). Note that some annotations do not have KO numbers attached to them. In these cases, we will filter these data out. kegg_annotations <- kegg_annotations %>% mutate ( \"KO\" = str_replace ( Description_2 , \"([^:]+):.*\" , \"\\\\1\" ) ) kegg_filtered_annotations <- kegg_annotations %>% filter ( str_detect ( KO , \"^K\\\\d+\" ) ) Check if each query was only annotated with a single KO number. The length of KO numbers are 6 (K followed by 5 numbers). Multiple annotations are possible and will need to be separated into individual rows. For this exercise, we are only interested in identifying the genomic potential for pathways. You can choose to perform further refinement of the annotations later. Let's check for any cases where the $KO variable is longer than six characters long: length ( which ( str_length ( kegg_filtered_annotations $ KO ) > 6 )) We can see that there are a total of 66 genes with multiple annotations. We will split them into their own rows. kegg_filtered_annotations <- kegg_filtered_annotations %>% separate_rows ( KO , sep = \",\" ) Reshaping the data for input into pathview \u00b6 pathview needs the data as a numeric matrix with IDs as row names and samples/experiments/bins as column names. Here, we will reshape the data into a matrix of counts per KO number in each bin. We first create a data frame of counts of KO in each bin. kegg_tally <- kegg_filtered_annotations %>% select ( bin , KO ) %>% group_by ( bin , KO ) %>% tally () Turn the tally into a numeric matrix with KO numbers as row names. Do not worry about NAs, pathview can deal with that. kegg_matrix <- kegg_tally %>% pivot_wider ( names_from = \"bin\" , values_from = \"n\" ) %>% column_to_rownames ( \"KO\" ) %>% as.matrix () Creating pathway map of genes related to nitrogen metabolism \u00b6 Now we can generate images of the KEGG pathway maps using the matrix we just made. For this section, we will try to find genes related nitrogen metabolism. We need to first identify the KEGG pathway ID. This is where KEGGREST comes in handy. KEGGREST can also help you identify other information stored in the KEGG database. For more information, the KEGGREST vignette can be viewed using the vignette function in R : vignette(\"KEGGREST-vignette\") Use the keggFind function to identify the pathway ID for nitrogen metabolism: keggFind ( database = \"pathway\" , query = \"nitrogen\" ) The result tells us that the nitrogen metabolism pathway ID is 00910 . First, let's generate a map using just the bin_0 data. The output will be a png image file in your working directory. pv_bin_0 <- pathview ( gene.data = kegg_matrix [, colnames ( kegg_matrix ) == \"bin_0\" ], pathway.id = \"00910\" , species = \"ko\" , kegg.native = T , limit = list ( gene = c ( 0 , max ( kegg_matrix [, 1 ], na.rm = T )), cpd = c ( 0 , max ( kegg_matrix [, 1 ], na.rm = T )) ), bins = list ( gene = 4 , cpd = 4 ), both.dirs = list ( gene = F , cpd = F ), out.suffix = \"nitrogen_metabolism.bin_0\" , na.col = \"white\" , low = '#ffdfdc' , mid = '#d57c69' , high = '#980000' ) You can modify the bins and limit arguments to make it look more sensible for your data. In the meantime, here we've just set it from zero to the maximum of the data. We can create maps for all bins at the same time using R 's implementation of a for loop, looping over all of the entries in our kegg_matrix R object (where each column relates to one of the bins). pv_all_bins <- list () for ( i in 1 : ncol ( kegg_matrix )) { out.suffix <- paste0 ( \"nitrogen_metabolism.\" , colnames ( kegg_matrix )[ i ]) pv_all_bins [[ colnames ( kegg_matrix )[ i ]]] <- pathview ( gene.data = kegg_matrix [, i ], pathway.id = \"00910\" , species = \"ko\" , kegg.native = T , limit = list ( gene = c ( 0 , max ( kegg_matrix , na.rm = T )), cpd = c ( 0 , max ( kegg_matrix , na.rm = T )) ), bins = list ( gene = 4 , cpd = 4 ), both.dirs = list ( gene = F , cpd = F ), out.suffix = out.suffix , na.col = \"white\" , low = '#ffdfdc' , mid = '#d57c69' , high = '#980000' ) rm ( i , out.suffix ) } The above code will generate one png image file per bin, and will save it to the current working directory. The list pv_all_bins in the first line contains some plotting information. Importantly, it has the KO numbers it used to match the data provided to the pathway. If you wish to, you can subset your original data to only obtain those KO numbers related to the pathway that you have plotted for further analyses with other annotations. A few example outputs for bins 3 and 5: Bin 3 nitrogen metabolism KEGG map \u00b6 Bin 5 nitrogen metabolism KEGG map \u00b6","title":"Presentation of data: KEGG pathway maps"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#presentation-of-data-kegg-pathway-maps","text":"","title":"Presentation of data: KEGG pathway maps"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#objectives","text":"Build a KEGG pathway map using R Import and wrangle data in R Subset the data for KO IDs of interest Reshape the data for input into pathview Creating pathway map of genes related to nitrogen metabolism","title":"Objectives"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#build-a-kegg-pathway-map-using-r","text":"In this exercise, we will generate KEGG a pathways map from genome annotations to visualize potential pathways present in our assembled, binned genome sequences. The key package used here is pathview , available from Bioconductor (for installation on your local version of RStudio , see the previous ex16a_data_presentation_Intro.md section). pathview is mainly used in transcriptomic analyses but can be adapted for any downstream analyses that utilise the KEGG database. For this exercise, we are interested in visualising the prevalence of genes that we have annotated in a pathway of interest. To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)).","title":"Build a KEGG pathway map using R"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#set-the-working-directory-and-load-packages-and-files-into-r","text":"Set the working directory and load the required packages setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/kegg_map' ) library ( pathview ) library ( KEGGREST ) # tidyverse libraries library ( readr ) library ( tibble ) library ( dplyr ) library ( tidyr ) library ( purrr ) library ( stringr ) Load your files into R. Here, we are loading them into a list. Given that there are ten files, it is easier to load, clean, and analyze the data using list methods available in tidyverse. tmp_files <- dir ( pattern = \"*.aa\" ) tmp_names <- str_replace ( tmp_files , \"(.*).annotation.aa\" , \"\\\\1\" ) bin_list <- as.list ( tmp_files ) %>% map ( function ( x ) { read_delim ( file = x , delim = \"\\t\" ) } ) names ( bin_list ) <- tmp_names rm ( list = ls ( pattern = \"tmp*\" )) NOTE: R will print some warning messages about column types. However, we do not need all the columns for this analysis and it is not necessary to reformat them.","title":"Set the working directory and load packages and files into R"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#subset-the-data","text":"For this section, we only require the KEGG orthology IDs (KO numbers). Use the code below to subset your data based on these IDs. Check data headers to determine which columns to subset: names ( bin_list $ bin_0 ) We can see that the headers related to KEGG annotations are in columns 22 to 27. We will subset the relevant columns and concatenate the rows. kegg_annotations <- map ( bin_list , function ( x ) { x [, 22 : 27 ] } ) %>% bind_rows ( .id = \"bin\" ) We also need the KO numbers. Here, we will create a column with that information where available (it is in the column Description_2). Note that some annotations do not have KO numbers attached to them. In these cases, we will filter these data out. kegg_annotations <- kegg_annotations %>% mutate ( \"KO\" = str_replace ( Description_2 , \"([^:]+):.*\" , \"\\\\1\" ) ) kegg_filtered_annotations <- kegg_annotations %>% filter ( str_detect ( KO , \"^K\\\\d+\" ) ) Check if each query was only annotated with a single KO number. The length of KO numbers are 6 (K followed by 5 numbers). Multiple annotations are possible and will need to be separated into individual rows. For this exercise, we are only interested in identifying the genomic potential for pathways. You can choose to perform further refinement of the annotations later. Let's check for any cases where the $KO variable is longer than six characters long: length ( which ( str_length ( kegg_filtered_annotations $ KO ) > 6 )) We can see that there are a total of 66 genes with multiple annotations. We will split them into their own rows. kegg_filtered_annotations <- kegg_filtered_annotations %>% separate_rows ( KO , sep = \",\" )","title":"Subset the data"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#reshaping-the-data-for-input-into-pathview","text":"pathview needs the data as a numeric matrix with IDs as row names and samples/experiments/bins as column names. Here, we will reshape the data into a matrix of counts per KO number in each bin. We first create a data frame of counts of KO in each bin. kegg_tally <- kegg_filtered_annotations %>% select ( bin , KO ) %>% group_by ( bin , KO ) %>% tally () Turn the tally into a numeric matrix with KO numbers as row names. Do not worry about NAs, pathview can deal with that. kegg_matrix <- kegg_tally %>% pivot_wider ( names_from = \"bin\" , values_from = \"n\" ) %>% column_to_rownames ( \"KO\" ) %>% as.matrix ()","title":"Reshaping the data for input into pathview"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#creating-pathway-map-of-genes-related-to-nitrogen-metabolism","text":"Now we can generate images of the KEGG pathway maps using the matrix we just made. For this section, we will try to find genes related nitrogen metabolism. We need to first identify the KEGG pathway ID. This is where KEGGREST comes in handy. KEGGREST can also help you identify other information stored in the KEGG database. For more information, the KEGGREST vignette can be viewed using the vignette function in R : vignette(\"KEGGREST-vignette\") Use the keggFind function to identify the pathway ID for nitrogen metabolism: keggFind ( database = \"pathway\" , query = \"nitrogen\" ) The result tells us that the nitrogen metabolism pathway ID is 00910 . First, let's generate a map using just the bin_0 data. The output will be a png image file in your working directory. pv_bin_0 <- pathview ( gene.data = kegg_matrix [, colnames ( kegg_matrix ) == \"bin_0\" ], pathway.id = \"00910\" , species = \"ko\" , kegg.native = T , limit = list ( gene = c ( 0 , max ( kegg_matrix [, 1 ], na.rm = T )), cpd = c ( 0 , max ( kegg_matrix [, 1 ], na.rm = T )) ), bins = list ( gene = 4 , cpd = 4 ), both.dirs = list ( gene = F , cpd = F ), out.suffix = \"nitrogen_metabolism.bin_0\" , na.col = \"white\" , low = '#ffdfdc' , mid = '#d57c69' , high = '#980000' ) You can modify the bins and limit arguments to make it look more sensible for your data. In the meantime, here we've just set it from zero to the maximum of the data. We can create maps for all bins at the same time using R 's implementation of a for loop, looping over all of the entries in our kegg_matrix R object (where each column relates to one of the bins). pv_all_bins <- list () for ( i in 1 : ncol ( kegg_matrix )) { out.suffix <- paste0 ( \"nitrogen_metabolism.\" , colnames ( kegg_matrix )[ i ]) pv_all_bins [[ colnames ( kegg_matrix )[ i ]]] <- pathview ( gene.data = kegg_matrix [, i ], pathway.id = \"00910\" , species = \"ko\" , kegg.native = T , limit = list ( gene = c ( 0 , max ( kegg_matrix , na.rm = T )), cpd = c ( 0 , max ( kegg_matrix , na.rm = T )) ), bins = list ( gene = 4 , cpd = 4 ), both.dirs = list ( gene = F , cpd = F ), out.suffix = out.suffix , na.col = \"white\" , low = '#ffdfdc' , mid = '#d57c69' , high = '#980000' ) rm ( i , out.suffix ) } The above code will generate one png image file per bin, and will save it to the current working directory. The list pv_all_bins in the first line contains some plotting information. Importantly, it has the KO numbers it used to match the data provided to the pathway. If you wish to, you can subset your original data to only obtain those KO numbers related to the pathway that you have plotted for further analyses with other annotations. A few example outputs for bins 3 and 5:","title":"Creating pathway map of genes related to nitrogen metabolism"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#bin-3-nitrogen-metabolism-kegg-map","text":"","title":"Bin 3 nitrogen metabolism KEGG map"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#bin-5-nitrogen-metabolism-kegg-map","text":"","title":"Bin 5 nitrogen metabolism KEGG map"},{"location":"day4/ex16e_data_presentation_Gene_synteny/","text":"Presentation of data: Gene synteny \u00b6 Objectives \u00b6 Build a sulfur assimilation gene alignment figure to investigate gene synteny using R Parsing files in bash Import and wrangle data in R Create a comparison table and build the plot in R Build a sulfur assimilation gene alignment figure to investigate gene synteny using R \u00b6 When investigating the evolution of genomes, we sometimes want to consider not only the presence/absence of genes in a genome, but also how they are arranged in an operon. For this exercise, we are going to visualise several sulfur assimilation genes from bin_4, bin_5 and bin_7, comparing their arrangement among the organisms. For this exercise, navigate to the folder 11.data_presentation/gene_synteny/ . You have been provided with a copy of the prodigal gene predictions for each of the bins ( .faa files), an annotation output table using multiple databases ( .aa files), a small table of the annotation of some key genes of interest ( cys.txt files), and blastn output ( blast*.txt ) comparing the genes of interest from these organisms. The annotation files were created by manually searching the annotations obtained in the previous exercises. NOTE: Refer to gene_synteny_grab_GOI.md for detailed information on how the cys.txt files were produced. NOTE: Refer to gene_synteny_Generate_blast_files.md for detailed information on how the blast files were produced. Part 1 - Parsing files in bash \u00b6 We will be performing this exercise in two stages. Firstly, in bash , we will use cut and tail to pull out the genes of interest listed in the *cys.txt files from the prodigal files. The gene names will then be used to create a table of gene coordinates from the prodigal output using grep , cut , and sed . For these bash steps, we will need to return to our logged in NeSI terminal. Switch over to a NeSI Jupyter hub terminal or log in to a fresh session in a new terminal. Navigate to the 11.data_presentation/gene_synteny/ folder, and then perform the cut and tail steps outlined above. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/ cut -f3 bin_4_cys.txt | tail -n+2 > bin_4_cys.genes cut -f3 bin_5_cys.txt | tail -n+2 > bin_5_cys.genes cut -f3 bin_7_cys.txt | tail -n+2 > bin_7_cys.genes We now have three new files, ending with the .genes suffix which are simply a list of the genes that we wish to extract from the prodigal files. We can then use each of these files as input for grep to pull out the fastA entries that correspond to these genes. grep -f bin_4_cys.genes bin_4.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_4_cys.coords grep -f bin_5_cys.genes bin_5.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_5_cys.coords grep -f bin_7_cys.genes bin_7.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_7_cys.coords As previously, we will quickly go through the steps of this command: grep -f bin_4_cys.genes bin_4.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_4_cys.coords | | | | | | | | | Write the output | | | | | | | Replace each '#' character with tab | | | | | For each line, replace the '>' with empty text | | | Split the results into columns delimited by the # character, | then take columns 1 - 4 . | Select the lines of the bin_4_cys.faa file that contain entries found in the bin_4_cys.genes file. Check the content of the .coords files now. You should see something like the following: cat bin_4_cys.coords # bin_4_NODE_55_length_158395_cov_1.135272_128 135928 136935 1 # bin_4_NODE_55_length_158395_cov_1.135272_129 136994 137299 1 # bin_4_NODE_55_length_158395_cov_1.135272_130 137411 138322 1 # bin_4_NODE_55_length_158395_cov_1.135272_131 138413 139201 1 # bin_4_NODE_55_length_158395_cov_1.135272_132 139267 140100 1 # bin_4_NODE_55_length_158395_cov_1.135272_133 140110 140988 1 # bin_4_NODE_55_length_158395_cov_1.135272_134 140985 142073 1 If you recall from the previous exercise on gene prediction, we have taken the first four entries from each line of the prodigal output, which consists of: The gene name, written as [CONTIG]_[GENE] The start position of the gene The stop position of the gene The orienation of the gene We will now use these tables, together with the annotation tables to create the gene synteny view in R . Part 2 - Producing the figure in R \u00b6 First, move back to the Jupyter hub Notebook pane where we have R running as the kernel (or relaunch a new R 4.0.1 Notebook). (Outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). There are two R libaries we need to load for this exercise. Set working directory and load R libraries \u00b6 setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/' ) library ( dplyr ) library ( genoPlotR ) Part 2.1 - Load coordinate files \u00b6 We can now begin importing the data. First, we will import the .coords files, and set column names to the files. bin_4_coords = read.table ( 'bin_4_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_4_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) bin_5_coords = read.table ( 'bin_5_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_5_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) bin_7_coords = read.table ( 'bin_7_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_7_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) Take a look at the content of each of these data.frames, by entering their names into the terminal. You should notice that the coordinates occur at quite different positions between the genomes. If we were looking at complete genomes, this would indicate their position relative to the origin of replication , but as these are unclosed genomes obtained from MAGs, they reflect the coordinates upon their particular contig . We now parse these data.frames into the dna_seg data class, which is defined by the genoPlotR library. bin_4_ds = dna_seg ( bin_4_coords ) bin_5_ds = dna_seg ( bin_5_coords ) bin_7_ds = dna_seg ( bin_7_coords ) bin_5_ds A dna_seg: 4 \u00d7 11 name start end strand col fill lty lwd pch cex gene_type <chr> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <chr> bin_5_NODE_95_length_91726_cov_0.379357_18 16268 17257 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_19 17261 18130 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_20 18141 18959 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_21 19121 20119 -1 blue blue 1 1 8 1 arrows By looking at the table, we can see that the genes in bin_5 are in reversed order (strand -1), so we might want to change the color of the gene to make sure they are different than bin_4 and bin_7. bin_5_ds $ col = \"#1a535c\" bin_5_ds $ fill = \"#1a535c\" Part 2.2 - Load annotation tables \u00b6 Then, we can load the annotation tables we have into R and take a look at them. bin_4_ann = read.table ( 'bin_4_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_4_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_5_ann = read.table ( 'bin_5_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_5_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_7_ann = read.table ( 'bin_7_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_7_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_5_ann A data.frame: 4 \u00d7 4 Annotation start1 end1 strand1 <chr> <dbl> <dbl> <dbl> sbp; sulfate-binding protein 16268 17257 -1 cysT; sulfate transporter CysT 17261 18130 -1 cysW; sulfate transporter CysW 18141 18959 -1 cysA; sulfate.thiosulfate ABC transporter ATP-binding protein CysA 19121 20119 -1 We need to create one more table with descriptive information. This is an annotation object, which contains the name of each gene in the figure along with the coordinates to write the name. x1 = starts of the gene, x2 = end of the gene annot1 <- annotation ( x1 = c ( bin_4_ds $ start [ 1 ], bin_4_ds $ start [ 2 ], bin_4_ds $ start [ 5 ], bin_4_ds $ start [ 6 ], bin_4_ds $ start [ 7 ]), x2 = c ( bin_4_ds $ end [ 1 ], bin_4_ds $ end [ 4 ], bin_4_ds $ end [ 5 ], bin_4_ds $ end [ 6 ], bin_4_ds $ end [ 7 ]), text = c ( \"sbp\" , \"unknown domain\" , \"cysU\" , \"cysW\" , \"cysA\" ), rot = 0 , col = \"black\" ) annot2 <- annotation ( x1 = c ( bin_7_ds $ start [ 1 ], bin_7_ds $ start [ 2 ], bin_7_ds $ start [ 3 ], bin_7_ds $ start [ 4 ]), x2 = c ( bin_7_ds $ end [ 1 ], bin_7_ds $ end [ 2 ], bin_7_ds $ end [ 3 ], bin_7_ds $ end [ 4 ]), text = c ( \"sbp\" , \"cysT\" , \"cysW\" , \"cysA\" ), rot = 0 , col = \"black\" ) Part 2.3 - Creating a comparison table and building the plot in R \u00b6 Then, we can parse the blast output as comparison file among the genomes. genoPlotR can read tabular files, either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by genoPlotR, the -m 8 or 9 option should be used in blastall, or -outfmt 6 or 7 with the BLAST+ suite. blast1 = read_comparison_from_blast ( \"blast_bin4_bin5.txt\" ) blast2 = read_comparison_from_blast ( \"blast_bin5_bin7.txt\" ) What it does here is to set the line color according to the direction of the gene match and the color gradient is based on the percent identity of the matches. Lighter color indicates weaker match and darker color indicates strong match. Now we can generate the plot. Running this command in RStudio or our Jupyter Notebook running the R kernel interactively loads the figure. NOTE: the commented-out lines below (the two lines starting with # ) will not run. Un-comment these if you wish to save the figure to file rather than opening it in the Jupyter or RStudio viewer. (N.b. alternatives using a similar command are also available other formats, including tiff and png ). #pdf(\"genoplot.pdf\",colormodel = \"cmyk\",width = 8,height = 4,paper = 'special') plot_gene_map ( dna_segs = list ( bin_4_ds , bin_5_ds , bin_7_ds ), gene_type = \"arrows\" , dna_seg_labels = c ( \"bin_4\" , \"bin_5\" , \"bin_7\" ), comparisons = list ( blast1 , blast2 ), dna_seg_scale = TRUE , override_color_schemes = FALSE , annotations = list ( annot1 , NULL , annot2 ), annotation_height = 1.7 , dna_seg_label_cex = 1 , main = \"Sulfur assimilation\" ) #dev.off() NOTE: While we do have control over the colours of the arrows via setting the bin_n_ds$col and bin_n_ds$fill parameters for each contig (as above), unfortunately there appears to be little flexibility within the plot_gene_map() function for setting the colours of the segments joining the arrows (the current options are limited to 'red_blue', 'blue_red', and 'grey'). Careful analysis would be needed to determine whether this is a genuine rearrangement relative to the rest of the genome, as these are draft genomes and contig orientation can either be forward or reverse. In this case, you can see that genes in bin_5 are in reversed order relative to the other bin contigs, hence, we can manually rotate the contig. Rotate the contig and update the annotation \u00b6 Rotate the orientation of the contig from bin_5: blast1 = mutate ( blast1 , direction = 1 ) blast2 = mutate ( blast2 , direction = 1 ) #annot3 <- annotation(x1 = c(bin_4_ds$start[1], bin_4_ds$start[2],bin_4_ds$start[3],bin_4_ds$start[4],bin_4_ds$start[7]), # x2 = c(bin_4_ds$end[1], bin_4_ds$end[2], bin_4_ds$end[3], bin_4_ds$end[6], bin_4_ds$end[7]), # text = c(\"cysA\",\"cysW\",\"cysU\",\"unknown domain\" ,\"sbp\"), # rot = 0, col = \"black\") #edit the color bin_5_ds $ col = \"blue\" bin_5_ds $ fill = \"blue\" Regenerate the figure: #AFTER ROTATING #pdf(\"genoplot_rotated.pdf\",colormodel = \"cmyk\",width = 8, height = 4, paper = 'special') plot_gene_map ( dna_segs = list ( bin_4_ds , bin_5_ds , bin_7_ds ), gene_type = \"arrows\" , dna_seg_labels = c ( \"bin_4\" , \"bin_5\" , \"bin_7\" ), comparisons = list ( blast1 , blast2 ), dna_seg_scale = TRUE , override_color_schemes = TRUE , annotations = list ( annot1 , NULL , annot2 ), annotation_height = 1.7 , dna_seg_label_cex = 1 , xlims = list ( NULL , c ( Inf , - Inf ), NULL ), main = \"Sulfur assimilation\" ) #dev.off() All done! We can see here that compared to bin_5 and bin_7 the following differences are apparent in the gene operon for bin_4: Three genes with unknown domain/function present in bin_4 in between cysU and SBP cysW and cysA are highly conserved among the genomes and have higher similarity compared to cysU and SBP .","title":"Presentation of data: Gene synteny"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#presentation-of-data-gene-synteny","text":"","title":"Presentation of data: Gene synteny"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#objectives","text":"Build a sulfur assimilation gene alignment figure to investigate gene synteny using R Parsing files in bash Import and wrangle data in R Create a comparison table and build the plot in R","title":"Objectives"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#build-a-sulfur-assimilation-gene-alignment-figure-to-investigate-gene-synteny-using-r","text":"When investigating the evolution of genomes, we sometimes want to consider not only the presence/absence of genes in a genome, but also how they are arranged in an operon. For this exercise, we are going to visualise several sulfur assimilation genes from bin_4, bin_5 and bin_7, comparing their arrangement among the organisms. For this exercise, navigate to the folder 11.data_presentation/gene_synteny/ . You have been provided with a copy of the prodigal gene predictions for each of the bins ( .faa files), an annotation output table using multiple databases ( .aa files), a small table of the annotation of some key genes of interest ( cys.txt files), and blastn output ( blast*.txt ) comparing the genes of interest from these organisms. The annotation files were created by manually searching the annotations obtained in the previous exercises. NOTE: Refer to gene_synteny_grab_GOI.md for detailed information on how the cys.txt files were produced. NOTE: Refer to gene_synteny_Generate_blast_files.md for detailed information on how the blast files were produced.","title":"Build a sulfur assimilation gene alignment figure to investigate gene synteny using R"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-1-parsing-files-in-bash","text":"We will be performing this exercise in two stages. Firstly, in bash , we will use cut and tail to pull out the genes of interest listed in the *cys.txt files from the prodigal files. The gene names will then be used to create a table of gene coordinates from the prodigal output using grep , cut , and sed . For these bash steps, we will need to return to our logged in NeSI terminal. Switch over to a NeSI Jupyter hub terminal or log in to a fresh session in a new terminal. Navigate to the 11.data_presentation/gene_synteny/ folder, and then perform the cut and tail steps outlined above. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/ cut -f3 bin_4_cys.txt | tail -n+2 > bin_4_cys.genes cut -f3 bin_5_cys.txt | tail -n+2 > bin_5_cys.genes cut -f3 bin_7_cys.txt | tail -n+2 > bin_7_cys.genes We now have three new files, ending with the .genes suffix which are simply a list of the genes that we wish to extract from the prodigal files. We can then use each of these files as input for grep to pull out the fastA entries that correspond to these genes. grep -f bin_4_cys.genes bin_4.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_4_cys.coords grep -f bin_5_cys.genes bin_5.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_5_cys.coords grep -f bin_7_cys.genes bin_7.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_7_cys.coords As previously, we will quickly go through the steps of this command: grep -f bin_4_cys.genes bin_4.filtered.genes.faa | cut -f1,2,3,4 -d \"#\" | sed 's/>//g' | sed 's/#/\\t/g' > bin_4_cys.coords | | | | | | | | | Write the output | | | | | | | Replace each '#' character with tab | | | | | For each line, replace the '>' with empty text | | | Split the results into columns delimited by the # character, | then take columns 1 - 4 . | Select the lines of the bin_4_cys.faa file that contain entries found in the bin_4_cys.genes file. Check the content of the .coords files now. You should see something like the following: cat bin_4_cys.coords # bin_4_NODE_55_length_158395_cov_1.135272_128 135928 136935 1 # bin_4_NODE_55_length_158395_cov_1.135272_129 136994 137299 1 # bin_4_NODE_55_length_158395_cov_1.135272_130 137411 138322 1 # bin_4_NODE_55_length_158395_cov_1.135272_131 138413 139201 1 # bin_4_NODE_55_length_158395_cov_1.135272_132 139267 140100 1 # bin_4_NODE_55_length_158395_cov_1.135272_133 140110 140988 1 # bin_4_NODE_55_length_158395_cov_1.135272_134 140985 142073 1 If you recall from the previous exercise on gene prediction, we have taken the first four entries from each line of the prodigal output, which consists of: The gene name, written as [CONTIG]_[GENE] The start position of the gene The stop position of the gene The orienation of the gene We will now use these tables, together with the annotation tables to create the gene synteny view in R .","title":"Part 1 - Parsing files in bash"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-2-producing-the-figure-in-r","text":"First, move back to the Jupyter hub Notebook pane where we have R running as the kernel (or relaunch a new R 4.0.1 Notebook). (Outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). There are two R libaries we need to load for this exercise.","title":"Part 2 - Producing the figure in R"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#set-working-directory-and-load-r-libraries","text":"setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/' ) library ( dplyr ) library ( genoPlotR )","title":"Set working directory and load R libraries"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-21-load-coordinate-files","text":"We can now begin importing the data. First, we will import the .coords files, and set column names to the files. bin_4_coords = read.table ( 'bin_4_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_4_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) bin_5_coords = read.table ( 'bin_5_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_5_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) bin_7_coords = read.table ( 'bin_7_cys.coords' , header = F , sep = '\\t' , stringsAsFactors = F ) colnames ( bin_7_coords ) = c ( 'name' , 'start' , 'end' , 'strand' ) Take a look at the content of each of these data.frames, by entering their names into the terminal. You should notice that the coordinates occur at quite different positions between the genomes. If we were looking at complete genomes, this would indicate their position relative to the origin of replication , but as these are unclosed genomes obtained from MAGs, they reflect the coordinates upon their particular contig . We now parse these data.frames into the dna_seg data class, which is defined by the genoPlotR library. bin_4_ds = dna_seg ( bin_4_coords ) bin_5_ds = dna_seg ( bin_5_coords ) bin_7_ds = dna_seg ( bin_7_coords ) bin_5_ds A dna_seg: 4 \u00d7 11 name start end strand col fill lty lwd pch cex gene_type <chr> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <chr> bin_5_NODE_95_length_91726_cov_0.379357_18 16268 17257 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_19 17261 18130 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_20 18141 18959 -1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379357_21 19121 20119 -1 blue blue 1 1 8 1 arrows By looking at the table, we can see that the genes in bin_5 are in reversed order (strand -1), so we might want to change the color of the gene to make sure they are different than bin_4 and bin_7. bin_5_ds $ col = \"#1a535c\" bin_5_ds $ fill = \"#1a535c\"","title":"Part 2.1 - Load coordinate files"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-22-load-annotation-tables","text":"Then, we can load the annotation tables we have into R and take a look at them. bin_4_ann = read.table ( 'bin_4_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_4_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_5_ann = read.table ( 'bin_5_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_5_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_7_ann = read.table ( 'bin_7_cys.txt' , header = T , sep = '\\t' , stringsAsFactors = F ) %>% cbind ( . , bin_7_coords ) %>% select ( Annotation , start1 = start , end1 = end , strand1 = strand ) bin_5_ann A data.frame: 4 \u00d7 4 Annotation start1 end1 strand1 <chr> <dbl> <dbl> <dbl> sbp; sulfate-binding protein 16268 17257 -1 cysT; sulfate transporter CysT 17261 18130 -1 cysW; sulfate transporter CysW 18141 18959 -1 cysA; sulfate.thiosulfate ABC transporter ATP-binding protein CysA 19121 20119 -1 We need to create one more table with descriptive information. This is an annotation object, which contains the name of each gene in the figure along with the coordinates to write the name. x1 = starts of the gene, x2 = end of the gene annot1 <- annotation ( x1 = c ( bin_4_ds $ start [ 1 ], bin_4_ds $ start [ 2 ], bin_4_ds $ start [ 5 ], bin_4_ds $ start [ 6 ], bin_4_ds $ start [ 7 ]), x2 = c ( bin_4_ds $ end [ 1 ], bin_4_ds $ end [ 4 ], bin_4_ds $ end [ 5 ], bin_4_ds $ end [ 6 ], bin_4_ds $ end [ 7 ]), text = c ( \"sbp\" , \"unknown domain\" , \"cysU\" , \"cysW\" , \"cysA\" ), rot = 0 , col = \"black\" ) annot2 <- annotation ( x1 = c ( bin_7_ds $ start [ 1 ], bin_7_ds $ start [ 2 ], bin_7_ds $ start [ 3 ], bin_7_ds $ start [ 4 ]), x2 = c ( bin_7_ds $ end [ 1 ], bin_7_ds $ end [ 2 ], bin_7_ds $ end [ 3 ], bin_7_ds $ end [ 4 ]), text = c ( \"sbp\" , \"cysT\" , \"cysW\" , \"cysA\" ), rot = 0 , col = \"black\" )","title":"Part 2.2 - Load annotation tables"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-23-creating-a-comparison-table-and-building-the-plot-in-r","text":"Then, we can parse the blast output as comparison file among the genomes. genoPlotR can read tabular files, either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by genoPlotR, the -m 8 or 9 option should be used in blastall, or -outfmt 6 or 7 with the BLAST+ suite. blast1 = read_comparison_from_blast ( \"blast_bin4_bin5.txt\" ) blast2 = read_comparison_from_blast ( \"blast_bin5_bin7.txt\" ) What it does here is to set the line color according to the direction of the gene match and the color gradient is based on the percent identity of the matches. Lighter color indicates weaker match and darker color indicates strong match. Now we can generate the plot. Running this command in RStudio or our Jupyter Notebook running the R kernel interactively loads the figure. NOTE: the commented-out lines below (the two lines starting with # ) will not run. Un-comment these if you wish to save the figure to file rather than opening it in the Jupyter or RStudio viewer. (N.b. alternatives using a similar command are also available other formats, including tiff and png ). #pdf(\"genoplot.pdf\",colormodel = \"cmyk\",width = 8,height = 4,paper = 'special') plot_gene_map ( dna_segs = list ( bin_4_ds , bin_5_ds , bin_7_ds ), gene_type = \"arrows\" , dna_seg_labels = c ( \"bin_4\" , \"bin_5\" , \"bin_7\" ), comparisons = list ( blast1 , blast2 ), dna_seg_scale = TRUE , override_color_schemes = FALSE , annotations = list ( annot1 , NULL , annot2 ), annotation_height = 1.7 , dna_seg_label_cex = 1 , main = \"Sulfur assimilation\" ) #dev.off() NOTE: While we do have control over the colours of the arrows via setting the bin_n_ds$col and bin_n_ds$fill parameters for each contig (as above), unfortunately there appears to be little flexibility within the plot_gene_map() function for setting the colours of the segments joining the arrows (the current options are limited to 'red_blue', 'blue_red', and 'grey'). Careful analysis would be needed to determine whether this is a genuine rearrangement relative to the rest of the genome, as these are draft genomes and contig orientation can either be forward or reverse. In this case, you can see that genes in bin_5 are in reversed order relative to the other bin contigs, hence, we can manually rotate the contig.","title":"Part 2.3 - Creating a comparison table and building the plot in R"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#rotate-the-contig-and-update-the-annotation","text":"Rotate the orientation of the contig from bin_5: blast1 = mutate ( blast1 , direction = 1 ) blast2 = mutate ( blast2 , direction = 1 ) #annot3 <- annotation(x1 = c(bin_4_ds$start[1], bin_4_ds$start[2],bin_4_ds$start[3],bin_4_ds$start[4],bin_4_ds$start[7]), # x2 = c(bin_4_ds$end[1], bin_4_ds$end[2], bin_4_ds$end[3], bin_4_ds$end[6], bin_4_ds$end[7]), # text = c(\"cysA\",\"cysW\",\"cysU\",\"unknown domain\" ,\"sbp\"), # rot = 0, col = \"black\") #edit the color bin_5_ds $ col = \"blue\" bin_5_ds $ fill = \"blue\" Regenerate the figure: #AFTER ROTATING #pdf(\"genoplot_rotated.pdf\",colormodel = \"cmyk\",width = 8, height = 4, paper = 'special') plot_gene_map ( dna_segs = list ( bin_4_ds , bin_5_ds , bin_7_ds ), gene_type = \"arrows\" , dna_seg_labels = c ( \"bin_4\" , \"bin_5\" , \"bin_7\" ), comparisons = list ( blast1 , blast2 ), dna_seg_scale = TRUE , override_color_schemes = TRUE , annotations = list ( annot1 , NULL , annot2 ), annotation_height = 1.7 , dna_seg_label_cex = 1 , xlims = list ( NULL , c ( Inf , - Inf ), NULL ), main = \"Sulfur assimilation\" ) #dev.off() All done! We can see here that compared to bin_5 and bin_7 the following differences are apparent in the gene operon for bin_4: Three genes with unknown domain/function present in bin_4 in between cysU and SBP cysW and cysA are highly conserved among the genomes and have higher similarity compared to cysU and SBP .","title":"Rotate the contig and update the annotation"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/","text":"Presentation of data: Optional exercise : CAZy annotations heatmap \u00b6 Objectives \u00b6 Build a basic heatmap from BLAST data using R Import and wrangle data in R Build the plot in R Build a basic heatmap from annotation data using R \u00b6 To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). For this exercise, set 11.data_presentation/cazy_heatmap/ as the working directory. These annotation files we will be using have been pre-computed by annotating the prodigal gene predictions against the CAZy database using the dbCAN resource. Briefly, each annotation was made by: Annotating each prodigal output against the dbCAN database using hmmer Converting the raw hmmer output into a table using the hmmscan-parser.py script that bundles with dbCAN Import the data into an R data.frame \u00b6 The first thing we need to do is import these annotations into R . We will do this using the following workflow Import tidyverse libraries for manipulating data tables Create an empty data.frame (master data.frame) Importing each of the text file into an R , then Append a new column to the text file, consisting of the name of the bin Select only the bin name and gene annotation from the imported table Append the text table to the master data.frame First, we import our R libraries with the library() command. For this workflow, we need three libraries from the tidyverse package: setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/cazy_heatmap/' ) library ( dplyr ) library ( tidyr ) library ( tibble ) We can then import our data using the list.files() function to loop over each text file, and the mutate , select , and pipe ( %>% ) functions from the dplyr library. cazy_files = list.files ( '.' ) # For each file, import it, drop unneeded columns, and add a column recording the bin name cazy_df = data.frame () for ( cazy_file in cazy_files ) { df = read.table ( cazy_file , sep = '\\t' , stringsAsFactors = F , header = F ) %>% mutate ( 'Bin' = cazy_file ) %>% select ( CAZy = V3 , Bin ) cazy_df = rbind ( cazy_df , df ) } We can inspect the final data.frame using the head command: head ( cazy_df ) # CAZy Bin # 1 AA3.hmm bin_0.txt # 2 GH13_9.hmm bin_0.txt # 3 GH104.hmm bin_0.txt # 4 GT5.hmm bin_0.txt # 5 GH116.hmm bin_0.txt # 6 GT41.hmm bin_0.txt We can also confirm that we have imported all of the text files by looking at the unique entries in the Bin column: unique ( cazy_df $ Bin ) # [1] \"bin_0.txt\" \"bin_1.txt\" \"bin_2.txt\" \"bin_3.txt\" \"bin_4.txt\" \"bin_5.txt\" # [7] \"bin_6.txt\" \"bin_7.txt\" \"bin_8.txt\" \"bin_9.txt\" We will now perform a summarising step, aggregating instances of multiple genes with the same annotation into a single count for each genome. We do this by For each bin in the data.frame For each annotation in the bin Count the number of times the annotation is observed For the majority of cases this will probably be one, but there will be a few cases where multiple annotations have been seen. This process is done using the group_by and tally functions from dplyr , again using pipes to pass the data between functions. cazy_counts = cazy_df %>% group_by ( Bin , CAZy ) %>% tally () cazy_counts # A tibble: 253 x 3 # Groups: Bin [10] # Bin CAZy n # <chr> <chr> <int> # 1 bin_0.txt AA3.hmm 1 # 2 bin_0.txt AA7.hmm 1 # 3 bin_0.txt CE11.hmm 1 # 4 bin_0.txt CE9.hmm 1 # 5 bin_0.txt GH100.hmm 1 # 6 bin_0.txt GH104.hmm 1 # 7 bin_0.txt GH116.hmm 1 # 8 bin_0.txt GH13_11.hmm 1 # 9 bin_0.txt GH13_20.hmm 1 #10 bin_0.txt GH13_9.hmm 1 # \u2026 with 243 more rows We now have a data.frame-like object (a tibble ) with three columns. We can convert this into a gene matrix using the pivot_wider function from the tidyr library to create a genome x gene matrix in the following form: Bin CAZy_1 CAZy_2 ... CAZy_n bin_0 N. of genes N. of genes ... N. of genes bin_1 N. of genes ... ... ... ... ... ... ... ... bin_9 N. of genes ... ... ... cazy_matrix = cazy_counts %>% pivot_wider ( id_cols = Bin , names_from = CAZy , values_from = n , values_fill = list ( n = 0 )) Build the plot in R \u00b6 Finally, we create the actual plot by passing this matrix into the pheatmap library. Before doing this, we need to take the text column Bin from the matrix and move it into the rownames, as this is how pheatmap infers the names of our samples. Also, if we left text data in the numeric input for a heatmap, the function would crash. We can quickly transfer the Bin column to the rownames using the column_to_rownames function from the tibble library. library ( pheatmap ) colours <- colorRampPalette ( c ( \"#fff9e7\" , \"#920000\" ), space = \"Lab\" )( 100 ) cazy_matrix %>% column_to_rownames ( 'Bin' ) %>% as.matrix ( . ) %>% pheatmap ( . , col = colours ) And there we go. This is a pretty basic heatmap, so there are a number of presentation issues with it. If you have time, try to do the following fixes to the heatmap by exploring the manual for pheatmap or other tidyverse and R functions. Replace the column-wise clustering with alphabetic arrangement of the gene columns Change the colour palette of the heatmap Reduce the font size in the column (gene) labels Remove the .hmm extensions from the gene names, and the .txt extensions from the bin names Add colouring to the row (bins), marking bins belonging to the same phyla","title":"Presentation of data: *Optional exercise*: CAZy annotations heatmap"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#presentation-of-data-optional-exercise-cazy-annotations-heatmap","text":"","title":"Presentation of data: Optional exercise: CAZy annotations heatmap"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#objectives","text":"Build a basic heatmap from BLAST data using R Import and wrangle data in R Build the plot in R","title":"Objectives"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#build-a-basic-heatmap-from-annotation-data-using-r","text":"To get started, if you're not already, log back in to NeSI's Jupyter hub and open a Notebook running the R 4.0.1 module as the kernel (or, outside the context of this workshop, open RStudio with the required packages installed (see the data presentation intro docs for more information)). For this exercise, set 11.data_presentation/cazy_heatmap/ as the working directory. These annotation files we will be using have been pre-computed by annotating the prodigal gene predictions against the CAZy database using the dbCAN resource. Briefly, each annotation was made by: Annotating each prodigal output against the dbCAN database using hmmer Converting the raw hmmer output into a table using the hmmscan-parser.py script that bundles with dbCAN","title":"Build a basic heatmap from annotation data using R"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#import-the-data-into-an-r-dataframe","text":"The first thing we need to do is import these annotations into R . We will do this using the following workflow Import tidyverse libraries for manipulating data tables Create an empty data.frame (master data.frame) Importing each of the text file into an R , then Append a new column to the text file, consisting of the name of the bin Select only the bin name and gene annotation from the imported table Append the text table to the master data.frame First, we import our R libraries with the library() command. For this workflow, we need three libraries from the tidyverse package: setwd ( '/nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/cazy_heatmap/' ) library ( dplyr ) library ( tidyr ) library ( tibble ) We can then import our data using the list.files() function to loop over each text file, and the mutate , select , and pipe ( %>% ) functions from the dplyr library. cazy_files = list.files ( '.' ) # For each file, import it, drop unneeded columns, and add a column recording the bin name cazy_df = data.frame () for ( cazy_file in cazy_files ) { df = read.table ( cazy_file , sep = '\\t' , stringsAsFactors = F , header = F ) %>% mutate ( 'Bin' = cazy_file ) %>% select ( CAZy = V3 , Bin ) cazy_df = rbind ( cazy_df , df ) } We can inspect the final data.frame using the head command: head ( cazy_df ) # CAZy Bin # 1 AA3.hmm bin_0.txt # 2 GH13_9.hmm bin_0.txt # 3 GH104.hmm bin_0.txt # 4 GT5.hmm bin_0.txt # 5 GH116.hmm bin_0.txt # 6 GT41.hmm bin_0.txt We can also confirm that we have imported all of the text files by looking at the unique entries in the Bin column: unique ( cazy_df $ Bin ) # [1] \"bin_0.txt\" \"bin_1.txt\" \"bin_2.txt\" \"bin_3.txt\" \"bin_4.txt\" \"bin_5.txt\" # [7] \"bin_6.txt\" \"bin_7.txt\" \"bin_8.txt\" \"bin_9.txt\" We will now perform a summarising step, aggregating instances of multiple genes with the same annotation into a single count for each genome. We do this by For each bin in the data.frame For each annotation in the bin Count the number of times the annotation is observed For the majority of cases this will probably be one, but there will be a few cases where multiple annotations have been seen. This process is done using the group_by and tally functions from dplyr , again using pipes to pass the data between functions. cazy_counts = cazy_df %>% group_by ( Bin , CAZy ) %>% tally () cazy_counts # A tibble: 253 x 3 # Groups: Bin [10] # Bin CAZy n # <chr> <chr> <int> # 1 bin_0.txt AA3.hmm 1 # 2 bin_0.txt AA7.hmm 1 # 3 bin_0.txt CE11.hmm 1 # 4 bin_0.txt CE9.hmm 1 # 5 bin_0.txt GH100.hmm 1 # 6 bin_0.txt GH104.hmm 1 # 7 bin_0.txt GH116.hmm 1 # 8 bin_0.txt GH13_11.hmm 1 # 9 bin_0.txt GH13_20.hmm 1 #10 bin_0.txt GH13_9.hmm 1 # \u2026 with 243 more rows We now have a data.frame-like object (a tibble ) with three columns. We can convert this into a gene matrix using the pivot_wider function from the tidyr library to create a genome x gene matrix in the following form: Bin CAZy_1 CAZy_2 ... CAZy_n bin_0 N. of genes N. of genes ... N. of genes bin_1 N. of genes ... ... ... ... ... ... ... ... bin_9 N. of genes ... ... ... cazy_matrix = cazy_counts %>% pivot_wider ( id_cols = Bin , names_from = CAZy , values_from = n , values_fill = list ( n = 0 ))","title":"Import the data into an R data.frame"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#build-the-plot-in-r","text":"Finally, we create the actual plot by passing this matrix into the pheatmap library. Before doing this, we need to take the text column Bin from the matrix and move it into the rownames, as this is how pheatmap infers the names of our samples. Also, if we left text data in the numeric input for a heatmap, the function would crash. We can quickly transfer the Bin column to the rownames using the column_to_rownames function from the tibble library. library ( pheatmap ) colours <- colorRampPalette ( c ( \"#fff9e7\" , \"#920000\" ), space = \"Lab\" )( 100 ) cazy_matrix %>% column_to_rownames ( 'Bin' ) %>% as.matrix ( . ) %>% pheatmap ( . , col = colours ) And there we go. This is a pretty basic heatmap, so there are a number of presentation issues with it. If you have time, try to do the following fixes to the heatmap by exploring the manual for pheatmap or other tidyverse and R functions. Replace the column-wise clustering with alphabetic arrangement of the gene columns Change the colour palette of the heatmap Reduce the font size in the column (gene) labels Remove the .hmm extensions from the gene names, and the .txt extensions from the bin names Add colouring to the row (bins), marking bins belonging to the same phyla","title":"Build the plot in R"},{"location":"figures/","text":"README \u00b6 This is the location for all figures presented in the workshop materials.","title":"Index"},{"location":"figures/#readme","text":"This is the location for all figures presented in the workshop materials.","title":"README"},{"location":"resources/APPENDIX_ex11_Normalise_coverage_example/","text":"APPENDIX (ex11) : Normalise per-sample coverage values by average library size (example) \u00b6 Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by calculating something like RPKM to normalise to both library size (sample sequencing depth) and gene length, or simply by normalising to minimum or average library size alone. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size. Normalise to average read depth via the python script norm_jgi_cov_table.py \u00b6 The script norm_jgi_cov_table.py is available in the folder 8.coverage_and_taxonomy/ , and is also available for download for future reference at this link . NOTE: This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected. In brief, this python script leverages the fact that the standard error output from bowtie2 includes read counts for each sample. This has been saved in mapping_filtered_bins.err , as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that bowtie2 processed the samples (based on the loop we provided to bowtie2 : for i in sample1 sample2 sample3 sample4 ), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the *.bam columns) are normalised for each sample based on: (coverage / sample_read_depth) * average_read_depth . Finally, this is saved to the new file with the prefix 'normalised_' (e.g. normalised_bins_cov_table.txt ). NOTE: In your own work, if you alternatively chose to use BBMap (and BBMap s covstats output) for the previous coverage calculation step, read counts can similarly be extracted from the scafstats output by searching for the line \"Reads Used: ...\". norm_jgi_cov_table.py requires two input arguments, and takes an additional optional output path argument. -c : Coverage table generated by jgi_summarize_bam_contig_depths -e : Standard error file created by read mapping via bowtie2 -o : (Optional) Path to output directory (must already exist) (default is the current directory). Run norm_jgi_cov_table.py for microbial bins data and viral contigs, inputting: A. the bins_cov_table.txt and mapping_filtered_bins.err files B. the viruses_cov_table.txt and mapping_filtered_viruses.err files. This will generate the outputs normalised_bins_cov_table.txt and normalised_viruses_cov_table.txt . NOTE: If this python script is in the directory you are currently in, you can call it simply by adding ./ in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path. module purge module load Python/3.8.2-gimkl-2020a ./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins.err ./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses.err","title":"APPENDIX ex11 Normalise coverage example"},{"location":"resources/APPENDIX_ex11_Normalise_coverage_example/#appendix-ex11-normalise-per-sample-coverage-values-by-average-library-size-example","text":"Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by calculating something like RPKM to normalise to both library size (sample sequencing depth) and gene length, or simply by normalising to minimum or average library size alone. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the cov_table.txt output generated by jgi_summarize_bam_contig_depths above could then be normalised based on average library size.","title":"APPENDIX (ex11): Normalise per-sample coverage values by average library size (example)"},{"location":"resources/APPENDIX_ex11_Normalise_coverage_example/#normalise-to-average-read-depth-via-the-python-script-norm_jgi_cov_tablepy","text":"The script norm_jgi_cov_table.py is available in the folder 8.coverage_and_taxonomy/ , and is also available for download for future reference at this link . NOTE: This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected. In brief, this python script leverages the fact that the standard error output from bowtie2 includes read counts for each sample. This has been saved in mapping_filtered_bins.err , as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that bowtie2 processed the samples (based on the loop we provided to bowtie2 : for i in sample1 sample2 sample3 sample4 ), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the *.bam columns) are normalised for each sample based on: (coverage / sample_read_depth) * average_read_depth . Finally, this is saved to the new file with the prefix 'normalised_' (e.g. normalised_bins_cov_table.txt ). NOTE: In your own work, if you alternatively chose to use BBMap (and BBMap s covstats output) for the previous coverage calculation step, read counts can similarly be extracted from the scafstats output by searching for the line \"Reads Used: ...\". norm_jgi_cov_table.py requires two input arguments, and takes an additional optional output path argument. -c : Coverage table generated by jgi_summarize_bam_contig_depths -e : Standard error file created by read mapping via bowtie2 -o : (Optional) Path to output directory (must already exist) (default is the current directory). Run norm_jgi_cov_table.py for microbial bins data and viral contigs, inputting: A. the bins_cov_table.txt and mapping_filtered_bins.err files B. the viruses_cov_table.txt and mapping_filtered_viruses.err files. This will generate the outputs normalised_bins_cov_table.txt and normalised_viruses_cov_table.txt . NOTE: If this python script is in the directory you are currently in, you can call it simply by adding ./ in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path. module purge module load Python/3.8.2-gimkl-2020a ./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins.err ./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses.err","title":"Normalise to average read depth via the python script norm_jgi_cov_table.py"},{"location":"resources/APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/","text":"Appendix (ex11): viral taxonomy prediction via vContact2 \u00b6 NOTE: these steps are based on having vContact2 set up as a conda environment. This documentation will be updated should vContact2 become available as a NeSI module. Further information for installing and running of vContact2 can also be found here . 1. Predict genes via prodigal cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy Example slurm script: #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J prodigal #SBATCH --time 00:05:00 #SBATCH --mem=1GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH -e prodigal.err #SBATCH -o prodigal.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load dependencies module purge module load Prodigal/2.6.3-GCC-9.2.0 # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy mkdir -p viral_taxonomy # Run main analyses srun prodigal -p meta -q \\ -i checkv_combined.fna \\ -a viral_taxonomy/checkv_combined.faa 2. Generate required mapping file for vContact2 Use vContact2 's vcontact2_gene2genome script to generate the required mapping file from the output of prodigal . NOTE: update /path/to/conda/envs/vContact2/bin in the below script to the appropraite path. # activate vcontact2 conda environment module purge module load Miniconda3 source activate vContact2 # Load dependencies export PATH = \"/path/to/conda/envs/vContact2/bin: $PATH \" module load DIAMOND/0.9.32-GCC-9.2.0 module load MCL/14.137-gimkl-2020a # run vcontact2_gene2genome vcontact2_gene2genome -p viral_taxonomy/checkv_combined.faa -o viral_taxonomy/viral_genomes_g2g.csv -s 'Prodigal-FAA' # deactivate conda environment conda deactivate 3. Run vContact2 Example slurm script: NOTE: update /path/to/conda/envs/vContact2/bin and /path/to/conda/envs/vContact2/bin/cluster_one-1.0.jar in the below script to the appropraite paths. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J vcontact2 #SBATCH --time 02:00:00 #SBATCH --mem=20GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=20 #SBATCH -e vcontact2.err #SBATCH -o vcontact2.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/viral_taxonomy/ # activate vcontact2 conda environment module purge module load Miniconda3 source activate vContact2 # Load dependencies export PATH = \"/path/to/conda/envs/vContact2/bin: $PATH \" module load DIAMOND/0.9.32-GCC-9.2.0 module load MCL/14.137-gimkl-2020a # Run vcontact2 srun vcontact2 \\ -t 20 \\ --raw-proteins checkv_combined.faa \\ --rel-mode Diamond \\ --proteins-fp viral_genomes_g2g.csv \\ --db 'ProkaryoticViralRefSeq201-Merged' \\ --c1-bin /path/to/conda/envs/vContact2/bin/cluster_one-1.0.jar \\ --output-dir vConTACT2_Results # deactivate conda environment conda deactivate 4. Predict taxonomy of viral contigs based on ouput of vContact2 vContact2 doesn't actually assign taxonomy to your input viral contigs. It instead provides an output outlining which reference viral genomes your viral contigs clustered with (if they clustered with any at all). Based on how closely they clustered with any reference genome(s), you can then use this to predict the likely taxonomy of the contig. From the vContact2 online docs: One important note is that the taxonomic information is not included for user sequences. This means that each user will need to find their genome(s) of interest and check to see if reference genomes are located in the same VC. If the user genome is within the same VC subcluster as a reference genome, then there's a very high probability that the user genome is part of the same genus. If the user genome is in the same VC but not the same subcluster as a reference, then it's highly likely the two genomes are related at roughly genus-subfamily level. If there are no reference genomes in the same VC or VC subcluster, then it's likely that they are not related at the genus level at all. The summary output of vContact2 is the file vConTACT2_Results/genome_by_genome_overview.csv . As the comment above notes, one approach would be to search this file for particular contigs of interest, and see if any reference genomes fall into the same viral cluster (VC), using this reference to predict the taxonomy of the contig of interest. The following python script is effectively an automated version of this for all input contigs ( Note: this script has not been widely tested, and so should be used with some degree of caution ). This script groups together contigs (and reference genomes) that fall into each VC, and then for each, outputs a list of all taxonomies (at the ranks of 'Order', 'Family', and 'Genus', separately) that were found in that cluster. The predictions (i.e. the list of all taxonomies found in the same VC) for each rank and each contig is output to the table tax_predict_table.txt . NOTE: The taxonomies are deliberately enclosed in square brackets ( [ ] ) to highlight the fact that these are **predictions* , rather than definitive taxonomy assignments .* For future reference, a copy of this script is available for download here module purge module load Python/3.8.2-gimkl-2020a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ ./vcontact2_tax_predict.py \\ -i viral_taxonomy/vConTACT2_Results/genome_by_genome_overview.csv \\ -o viral_taxonomy/","title":"APPENDIX ex11 viral taxonomy prediction via vContact2"},{"location":"resources/APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/#appendix-ex11-viral-taxonomy-prediction-via-vcontact2","text":"NOTE: these steps are based on having vContact2 set up as a conda environment. This documentation will be updated should vContact2 become available as a NeSI module. Further information for installing and running of vContact2 can also be found here . 1. Predict genes via prodigal cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy Example slurm script: #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J prodigal #SBATCH --time 00:05:00 #SBATCH --mem=1GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH -e prodigal.err #SBATCH -o prodigal.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Load dependencies module purge module load Prodigal/2.6.3-GCC-9.2.0 # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy mkdir -p viral_taxonomy # Run main analyses srun prodigal -p meta -q \\ -i checkv_combined.fna \\ -a viral_taxonomy/checkv_combined.faa 2. Generate required mapping file for vContact2 Use vContact2 's vcontact2_gene2genome script to generate the required mapping file from the output of prodigal . NOTE: update /path/to/conda/envs/vContact2/bin in the below script to the appropraite path. # activate vcontact2 conda environment module purge module load Miniconda3 source activate vContact2 # Load dependencies export PATH = \"/path/to/conda/envs/vContact2/bin: $PATH \" module load DIAMOND/0.9.32-GCC-9.2.0 module load MCL/14.137-gimkl-2020a # run vcontact2_gene2genome vcontact2_gene2genome -p viral_taxonomy/checkv_combined.faa -o viral_taxonomy/viral_genomes_g2g.csv -s 'Prodigal-FAA' # deactivate conda environment conda deactivate 3. Run vContact2 Example slurm script: NOTE: update /path/to/conda/envs/vContact2/bin and /path/to/conda/envs/vContact2/bin/cluster_one-1.0.jar in the below script to the appropraite paths. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J vcontact2 #SBATCH --time 02:00:00 #SBATCH --mem=20GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=20 #SBATCH -e vcontact2.err #SBATCH -o vcontact2.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/viral_taxonomy/ # activate vcontact2 conda environment module purge module load Miniconda3 source activate vContact2 # Load dependencies export PATH = \"/path/to/conda/envs/vContact2/bin: $PATH \" module load DIAMOND/0.9.32-GCC-9.2.0 module load MCL/14.137-gimkl-2020a # Run vcontact2 srun vcontact2 \\ -t 20 \\ --raw-proteins checkv_combined.faa \\ --rel-mode Diamond \\ --proteins-fp viral_genomes_g2g.csv \\ --db 'ProkaryoticViralRefSeq201-Merged' \\ --c1-bin /path/to/conda/envs/vContact2/bin/cluster_one-1.0.jar \\ --output-dir vConTACT2_Results # deactivate conda environment conda deactivate 4. Predict taxonomy of viral contigs based on ouput of vContact2 vContact2 doesn't actually assign taxonomy to your input viral contigs. It instead provides an output outlining which reference viral genomes your viral contigs clustered with (if they clustered with any at all). Based on how closely they clustered with any reference genome(s), you can then use this to predict the likely taxonomy of the contig. From the vContact2 online docs: One important note is that the taxonomic information is not included for user sequences. This means that each user will need to find their genome(s) of interest and check to see if reference genomes are located in the same VC. If the user genome is within the same VC subcluster as a reference genome, then there's a very high probability that the user genome is part of the same genus. If the user genome is in the same VC but not the same subcluster as a reference, then it's highly likely the two genomes are related at roughly genus-subfamily level. If there are no reference genomes in the same VC or VC subcluster, then it's likely that they are not related at the genus level at all. The summary output of vContact2 is the file vConTACT2_Results/genome_by_genome_overview.csv . As the comment above notes, one approach would be to search this file for particular contigs of interest, and see if any reference genomes fall into the same viral cluster (VC), using this reference to predict the taxonomy of the contig of interest. The following python script is effectively an automated version of this for all input contigs ( Note: this script has not been widely tested, and so should be used with some degree of caution ). This script groups together contigs (and reference genomes) that fall into each VC, and then for each, outputs a list of all taxonomies (at the ranks of 'Order', 'Family', and 'Genus', separately) that were found in that cluster. The predictions (i.e. the list of all taxonomies found in the same VC) for each rank and each contig is output to the table tax_predict_table.txt . NOTE: The taxonomies are deliberately enclosed in square brackets ( [ ] ) to highlight the fact that these are **predictions* , rather than definitive taxonomy assignments .* For future reference, a copy of this script is available for download here module purge module load Python/3.8.2-gimkl-2020a cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/ ./vcontact2_tax_predict.py \\ -i viral_taxonomy/vConTACT2_Results/genome_by_genome_overview.csv \\ -o viral_taxonomy/","title":"Appendix (ex11): viral taxonomy prediction via vContact2"},{"location":"resources/APPENDIX_ex15_gene_synteny_Generate_blast_files/","text":"How to generate the blast files provided in ex15_gene_synteny \u00b6 There are several ways of getting the blast files. genoplotR can read tabular files: either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by genoPlotR, the -m 8 or 9 option should be used in blastall, or -outfmt 6 or 7 with the BLAST+ suite. In this exercise, we are using tblastx on the NCBI website . Alternatively, you can use the command line version of tblastx in BLAST suite to get the same output (but remember to create the database first). Firstly, we will need to get the input .fna files for blast. Navigate to the 11.data_presentation/gene_synteny/ folder, then we can grab the node of interest and load seqtk on jupyter to grab the fastA sequence. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/ #grab node name for i in *cys.txt ; do grep 'bin_' $i | sed 's/.*bin/bin/g;s/cov_\\(.*\\)_.*/cov_\\1/g' | uniq > node_ $i ; done #grab sequence using seqtk export dir = /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/filtered_bins/ module load seqtk for i in { 4 ,5,7 } ; do seqtk subseq ${ dir } /bin_ ${ i } .filtered.fna node_bin_ ${ i } _cys.txt > bin_ ${ i } _cys.fna ; done Download the *cys.fna files to your local computer and then upload them to the NCBI website for blasting between bin 4 and bin 5, and then again between bin 5 and bin 7. You will get the output that looks like this. Click download all , and select Hit Table (text) . That's it! Now you will have downloaded two files (one comparing between bin 4 and bin 5, and another between bin 5 and bin 7).","title":"APPENDIX ex15 gene synteny Generate blast files"},{"location":"resources/APPENDIX_ex15_gene_synteny_Generate_blast_files/#how-to-generate-the-blast-files-provided-in-ex15_gene_synteny","text":"There are several ways of getting the blast files. genoplotR can read tabular files: either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by genoPlotR, the -m 8 or 9 option should be used in blastall, or -outfmt 6 or 7 with the BLAST+ suite. In this exercise, we are using tblastx on the NCBI website . Alternatively, you can use the command line version of tblastx in BLAST suite to get the same output (but remember to create the database first). Firstly, we will need to get the input .fna files for blast. Navigate to the 11.data_presentation/gene_synteny/ folder, then we can grab the node of interest and load seqtk on jupyter to grab the fastA sequence. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/11.data_presentation/gene_synteny/ #grab node name for i in *cys.txt ; do grep 'bin_' $i | sed 's/.*bin/bin/g;s/cov_\\(.*\\)_.*/cov_\\1/g' | uniq > node_ $i ; done #grab sequence using seqtk export dir = /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/9.gene_prediction/filtered_bins/ module load seqtk for i in { 4 ,5,7 } ; do seqtk subseq ${ dir } /bin_ ${ i } .filtered.fna node_bin_ ${ i } _cys.txt > bin_ ${ i } _cys.fna ; done Download the *cys.fna files to your local computer and then upload them to the NCBI website for blasting between bin 4 and bin 5, and then again between bin 5 and bin 7. You will get the output that looks like this. Click download all , and select Hit Table (text) . That's it! Now you will have downloaded two files (one comparing between bin 4 and bin 5, and another between bin 5 and bin 7).","title":"How to generate the blast files provided in ex15_gene_synteny"},{"location":"resources/APPENDIX_ex15_gene_synteny_grab_GOI/","text":"Grab Gene of Interest (generate the cys.txt files provided in ex15_gene_synteny ) \u00b6 Below is the example documentation for how the files of genes that we were interested in ( *cys.txt files) were generated. Load annotation files in R and find genes of interest. setwd ( 'path/to/input/files' ) library ( \"readr\" ) library ( \"dplyr\" ) library ( \"tidyr\" ) dataFiles <- lapply ( Sys.glob ( \"*.aa\" ), read.delim , sep = \"\\t\" ) Here, we are looking for the gene cluster that consists of sbp (sulfur binding protein) and cysUWA using KEGG databases (thier KO numbers are: K02048,K02046,K02047,K02045) GOI_4 <- data.frame () GOI_5 <- data.frame () GOI_7 <- data.frame () for ( keggid in c ( \"K02048\" , \"K02046\" , \"K02047\" , \"K02045\" )) { GOI_4 <- rbind ( GOI_4 , dataFiles [[ 1 ]][ grep ( keggid , dataFiles [[ 1 ]] $ Description.2 ),]) GOI_5 <- rbind ( GOI_5 , dataFiles [[ 2 ]][ grep ( keggid , dataFiles [[ 2 ]] $ Description.2 ),]) GOI_7 <- rbind ( GOI_7 , dataFiles [[ 3 ]][ grep ( keggid , dataFiles [[ 3 ]] $ Description.2 ),])} GOI_4 A data.frame: 4 \u00d7 33 Query.Gene GC. Contig.name Start.position Stop.position Orientation Query.sequence Signalling Signal.confidence Target.gene..UniProt. \u22ef Coverage.2 E.value.2 Description.2 Taxonomy.2 Target.gene..Pfam. E.value.3 Description.3 Target.gene..TIGRfam. E.value.4 Description.4 <fct> <dbl> <fct> <int> <int> <fct> <fct> <fct> <fct> <fct> \u22ef <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> 1433 bin_4_NODE_55_length_158395_cov_1.135272_128 49.90 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MKFNIWLSVGLLTFALAGASQAFADRTLLNVSYDPTRELYEEYNREFIRYWQEKTGEKITVKQSHGGSGKQARAVIDGVPADIVTLALAHDIDAISEQSGLIPAEWQKRLPNNSSPYLSTIVLLVRKDNPKGIRDWDDLIKPGVAVITPNPKTSGGARWNYLAAWGFALKKYDGDEAKAQEFLTRLYKNVSILDSGARGSTINFIQRGIGDVLISWENEAFLALKEYGPEKFEIIIPSISILAEPPVAVVDKNVDKHGVRNIAQAYLEYLYDKKGQEIAARNFYRPSDPEIAKKYAHQFPAINLFTINEVFGGWPQAQSIHFKDGGLFDKIYVNQ* - - A0A1H9YF74 \u22ef 99.701493 -1.000000 K02048: sbp1; Prokaryotic sulfate-/thiosulfate-binding protein Nitrosomonas europaea PF12849.7; PF01547.25; PF13531.6; PF13343.6 0.000200; 0.000000; 0.000000; 0.000000 PBP_like_2; SBP_bac_1; SBP_bac_11; SBP_bac_6 TIGR00971; TIGR03261 0.000000; 0.000001 TIGR00971; TIGR03261 1600 bin_4_NODE_55_length_158395_cov_1.135272_132 52.88 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MSTFKQYSVLPGFNLALGFTLLYLSLVVLIPLSAAFIHSAKLTWPEFWSTVTAPRVVASYRLTFGASFAAAVVNTFFGLLVAWVLVRYPFPGKRLVDALIDLPFALPTSVAGITLTAIYAGNGWLGQYLEPLGIKVAFTPVGVFVALTFIGLPFVVRTVQPVLEDIEKELEEAAAMLGATRWQTFRYVIFPAVLPALTTGFALAFARAIGEYGSVIFIAGNIPMVSEITPLLIITKLEQYDYAGATAIAVVMLVISFILLLIINLLQWWVRHRSIKA* - - A0A1H9YF01 \u22ef 99.638989 -1.000000 K02046: cysU; cysU; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 0.000000; 0.000000; 0.000001; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000220 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 1599 bin_4_NODE_55_length_158395_cov_1.135272_133 54.27 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTAVPLLQTSPAPHKVVTQESPWARWTLILLALSFLSLFLLLPLVAVFFEALRKGWEVYLAAITEPDALAAIRLTLIAAAIAVPLNLIFGIAAAWAIAKFEFRGKSILTSLIDLPFSVSPVVAGLIYVLIFGLQGWIGPWLREHDLSIIFAVPGIVLATIFVTVPFIARELIPLMQAQGSEEEEAAIILGASGWQTLWYVTLPNIKWGLLYGTILCNARAMGEFGAVSVVSGHIRGLTNTLSLHVEILYNEYNFVAAFAVASLLALLALITLVLKTLVEAKVAQQTSRGNHS* - - A0A1H9YF25 \u22ef 99.657534 -1.000000 K02047: cysW; cysW; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 0.000000; 0.000000; 0.000420; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000019; 0.000000 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 1606 bin_4_NODE_55_length_158395_cov_1.135272_134 53.35 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTIEIHDLSKQFGSFTALNDINLKVNPGELLALLGPSGSGKTTLLRVIAGLETADSGQVLFNEEDSTDKHIRDRHVGFVFQHYALFRNMTIFENVAFGLRVRPRKQRPNAPEINHRVTELLQLVQLDWLADRYPHQLSGGQRQRIALARALAVEPSVLLLDEPFGALDAKVRKELRAWLRKLHDDMHITSVFVTHDQEEALEVADRIVVMNRGRIEQIGTPDEVYEKPANPFVYEFLGHVNLFHGRVHQGHAWIGDLEVDAPEYSEAEDLSAIAYVRPHDIEVDRTLNGEPALAAHIVHILAIGPVVRLELAGKDNQSTNSIYAEISKERFRELQLARGDQVFIKPRKLDLFPNHAQNGSIH* - - Q82WT5 \u22ef 99.723757 -1.000000 K02045: cysA; cysA; sulfate transport ATP-binding ABC transporter protein Nitrosomonas europaea PF00004.29; PF13191.6; PF13304.6; PF13401.6; PF13476.6; PF13555.6; PF00005.27; PF17850.1; PF05729.12; PF02463.19; PF05621.11; PF03459.17; PF12857.7 0.000500; 0.000035; 0.000000; 0.000150; 0.000200; 0.000096; 0.000000; 0.000000; 0.000930; 0.000000; 0.000770; 0.000009; 0.000000 AAA; AAA_16; AAA_21; AAA_22; AAA_23; AAA_29; ABC_tran; CysA_C_terminal; NACHT; SMC_N; TniB; TOBE; TOBE_3 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 0.000020; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 As bin_4 has additional unknown genes in between the sulfur assimilation gene cluster, we will manually add them into the dataframe by looking in the .aa file (protein sequence bin_4_NODE_55_length_158395_cov_1.135272_129, bin_4_NODE_55_length_158395_cov_1.135272_130 and bin_4_NODE_55_length_158395_cov_1.135272_131). for ( bin_node in c ( \"bin_4_NODE_55_length_158395_cov_1.135272_129\" , \"bin_4_NODE_55_length_158395_cov_1.135272_130\" , \"bin_4_NODE_55_length_158395_cov_1.135272_131\" )) { GOI_4 <- rbind ( GOI_4 , dataFiles [[ 1 ]][ grep ( bin_node , dataFiles [[ 1 ]] $ Query.Gene ),])} ( GOI_4 ) A data.frame: 7 \u00d7 33 Query.Gene GC. Contig.name Start.position Stop.position Orientation Query.sequence Signalling Signal.confidence Target.gene..UniProt. \u22ef Coverage.2 E.value.2 Description.2 Taxonomy.2 Target.gene..Pfam. E.value.3 Description.3 Target.gene..TIGRfam. E.value.4 Description.4 <fct> <dbl> <fct> <int> <int> <fct> <fct> <fct> <fct> <fct> \u22ef <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> 1433 bin_4_NODE_55_length_158395_cov_1.135272_128 49.90 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MKFNIWLSVGLLTFALAGASQAFADRTLLNVSYDPTRELYEEYNREFIRYWQEKTGEKITVKQSHGGSGKQARAVIDGVPADIVTLALAHDIDAISEQSGLIPAEWQKRLPNNSSPYLSTIVLLVRKDNPKGIRDWDDLIKPGVAVITPNPKTSGGARWNYLAAWGFALKKYDGDEAKAQEFLTRLYKNVSILDSGARGSTINFIQRGIGDVLISWENEAFLALKEYGPEKFEIIIPSISILAEPPVAVVDKNVDKHGVRNIAQAYLEYLYDKKGQEIAARNFYRPSDPEIAKKYAHQFPAINLFTINEVFGGWPQAQSIHFKDGGLFDKIYVNQ* - - A0A1H9YF74 \u22ef 99.701493 -1.000000 K02048: sbp1; Prokaryotic sulfate-/thiosulfate-binding protein Nitrosomonas europaea PF12849.7; PF01547.25; PF13531.6; PF13343.6 0.000200; 0.000000; 0.000000; 0.000000 PBP_like_2; SBP_bac_1; SBP_bac_11; SBP_bac_6 TIGR00971; TIGR03261 0.000000; 0.000001 TIGR00971; TIGR03261 1600 bin_4_NODE_55_length_158395_cov_1.135272_132 52.88 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MSTFKQYSVLPGFNLALGFTLLYLSLVVLIPLSAAFIHSAKLTWPEFWSTVTAPRVVASYRLTFGASFAAAVVNTFFGLLVAWVLVRYPFPGKRLVDALIDLPFALPTSVAGITLTAIYAGNGWLGQYLEPLGIKVAFTPVGVFVALTFIGLPFVVRTVQPVLEDIEKELEEAAAMLGATRWQTFRYVIFPAVLPALTTGFALAFARAIGEYGSVIFIAGNIPMVSEITPLLIITKLEQYDYAGATAIAVVMLVISFILLLIINLLQWWVRHRSIKA* - - A0A1H9YF01 \u22ef 99.638989 -1.000000 K02046: cysU; cysU; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 0.000000; 0.000000; 0.000001; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000220 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 1599 bin_4_NODE_55_length_158395_cov_1.135272_133 54.27 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTAVPLLQTSPAPHKVVTQESPWARWTLILLALSFLSLFLLLPLVAVFFEALRKGWEVYLAAITEPDALAAIRLTLIAAAIAVPLNLIFGIAAAWAIAKFEFRGKSILTSLIDLPFSVSPVVAGLIYVLIFGLQGWIGPWLREHDLSIIFAVPGIVLATIFVTVPFIARELIPLMQAQGSEEEEAAIILGASGWQTLWYVTLPNIKWGLLYGTILCNARAMGEFGAVSVVSGHIRGLTNTLSLHVEILYNEYNFVAAFAVASLLALLALITLVLKTLVEAKVAQQTSRGNHS* - - A0A1H9YF25 \u22ef 99.657534 -1.000000 K02047: cysW; cysW; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 0.000000; 0.000000; 0.000420; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000019; 0.000000 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 1606 bin_4_NODE_55_length_158395_cov_1.135272_134 53.35 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTIEIHDLSKQFGSFTALNDINLKVNPGELLALLGPSGSGKTTLLRVIAGLETADSGQVLFNEEDSTDKHIRDRHVGFVFQHYALFRNMTIFENVAFGLRVRPRKQRPNAPEINHRVTELLQLVQLDWLADRYPHQLSGGQRQRIALARALAVEPSVLLLDEPFGALDAKVRKELRAWLRKLHDDMHITSVFVTHDQEEALEVADRIVVMNRGRIEQIGTPDEVYEKPANPFVYEFLGHVNLFHGRVHQGHAWIGDLEVDAPEYSEAEDLSAIAYVRPHDIEVDRTLNGEPALAAHIVHILAIGPVVRLELAGKDNQSTNSIYAEISKERFRELQLARGDQVFIKPRKLDLFPNHAQNGSIH* - - Q82WT5 \u22ef 99.723757 -1.000000 K02045: cysA; cysA; sulfate transport ATP-binding ABC transporter protein Nitrosomonas europaea PF00004.29; PF13191.6; PF13304.6; PF13401.6; PF13476.6; PF13555.6; PF00005.27; PF17850.1; PF05729.12; PF02463.19; PF05621.11; PF03459.17; PF12857.7 0.000500; 0.000035; 0.000000; 0.000150; 0.000200; 0.000096; 0.000000; 0.000000; 0.000930; 0.000000; 0.000770; 0.000009; 0.000000 AAA; AAA_16; AAA_21; AAA_22; AAA_23; AAA_29; ABC_tran; CysA_C_terminal; NACHT; SMC_N; TniB; TOBE; TOBE_3 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 0.000020; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 1434 bin_4_NODE_55_length_158395_cov_1.135272_129 46.41 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTVLIVGGDYIASLKQRITAHGYSRIEHWNGRKKGFNKRALPGRTKLVVIIYDYVSHNLANSVKDQASRIGIPMIFCRHAMHEIDTIFDEKKAEESCCNFV* - - A0A1H9YF98 \u22ef 99.009901 -1.000000 possible predicted diverged CheY-domain Nitrosomonas europaea PF10087.9 0.000000 DUF2325 Not annotated - - 1602 bin_4_NODE_55_length_158395_cov_1.135272_130 44.85 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MLPSTLKTLVHGDYMIRTIERIITSTGFSLQHSDNGEITGLFYHCKLSSAYQPVFEAARNNIIGQEARIRADLNGNGEIQLSPWHIFALAPDDEQLIKLDRLCRTIHALNFLDNITNKQMKLFVSVQPRLLESVKDDHGSAFEQILDIIGIRTSRIVIEIPVEANRDWRLLKRVIMNYRAHGYLISVNYSGTNNDRISELGKLYPNVIRIDARDLLRRSVLDPLIVTAHNQGADVLVGNIETSYQLTDAIHAGADLLQGNFLARYSRKADNMLTPFSWQIPDERNGYRFDNESLQENQPYRSI* - - A0A1H9YFC5 \u22ef 99.653979 -1.000000 Domain of unknown function 2 Nitrosomonas europaea PF00563.20 0.000000 EAL Not annotated - - 1601 bin_4_NODE_55_length_158395_cov_1.135272_131 46.51 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MNRTTAKPFTVMDEFELIKAATDYSLEQDENGRISGNFYHCKLTSAFQVILDIHENNIIGHAAYIRSESNGEVSLWPWQVFALASKDEQLIDLDRLCRAIHALNYFKKNFDTNTGQLFLSVHPRLLSSIQNEHGRTFKDFLDLTGISTSRIVIEIPPALNHDWKLLQKLIINYRSYGYQIALNFSSTNGHWLLGTDDLHPDILIVQAHELLHYQLNDYPEDNSTRDAGFRLHVRKIETQEQLTAAKQAGAHYLQGNFLGKTV* - - A0A1H9YEZ8 \u22ef 99.618321 -1.000000 Domain of unknown function 2 Nitrosomonas europaea PF00563.20 0.000000 EAL Not annotated - - We found 2 genes match to K02048 for bin 5: the sulfate binding protein, but as we are only interested at the gene cluster, we will look at the one from the same contig as the other genes. Next, we will create the tables ( *cys.txt ) to be used for 'genoPlotR'. GOI4 = GOI_4 [ 1 : 7 ,] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) GOI5 = GOI_5 [ c ( 1 , 3 : 5 ),] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) GOI7 = GOI_7 [ 1 : 4 ,] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) rownames ( GOI4 ) <- c () rownames ( GOI5 ) <- c () rownames ( GOI7 ) <- c () head ( GOI4 ) Warning message: \u201cExpected 2 pieces. Missing pieces filled with `NA` in 3 rows [5, 6, 7].\u201d A data.frame: 6 \u00d7 3 KO Annotation Query.Gene <chr> <chr> <fct> K02048 sbp1; Prokaryotic sulfate-/thiosulfate-binding protein bin_4_NODE_55_length_158395_cov_1.135272_128 K02046 cysU; cysU; sulfate transport ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_132 K02047 cysW; cysW; sulfate transport ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_133 K02045 cysA; cysA; sulfate transport ATP-binding ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_134 possible predicted diverged CheY-domain NA bin_4_NODE_55_length_158395_cov_1.135272_129 Domain of unknown function 2 NA bin_4_NODE_55_length_158395_cov_1.135272_130 Edit the gene annotation for the unknown KO in bin_4 to get the table below GOI4 = mutate ( GOI4 , Annotation = case_when ( Annotation == \"NA\" ~ \"\" , TRUE ~ as.character ( KO ))) GOI4 A data.frame: 7 \u00d7 3 KO Annotation Query.Gene <chr> <chr> <fct> K02048 K02048 bin_4_NODE_55_length_158395_cov_1.135272_128 K02046 K02046 bin_4_NODE_55_length_158395_cov_1.135272_132 K02047 K02047 bin_4_NODE_55_length_158395_cov_1.135272_133 K02045 K02045 bin_4_NODE_55_length_158395_cov_1.135272_134 possible predicted diverged CheY-domain possible predicted diverged CheY-domain bin_4_NODE_55_length_158395_cov_1.135272_129 Domain of unknown function 2 Domain of unknown function 2 bin_4_NODE_55_length_158395_cov_1.135272_130 Domain of unknown function 2 Domain of unknown function 2 bin_4_NODE_55_length_158395_cov_1.135272_131 Export the files. write_delim ( GOI4 , \"bin_4_cys.txt\" , delim = \"\\t\" ) write_delim ( GOI5 , \"bin_5_cys.txt\" , delim = \"\\t\" ) write_delim ( GOI7 , \"bin_7_cys.txt\" , delim = \"\\t\" )","title":"APPENDIX ex15 gene synteny grab GOI"},{"location":"resources/APPENDIX_ex15_gene_synteny_grab_GOI/#grab-gene-of-interest-generate-the-cystxt-files-provided-in-ex15_gene_synteny","text":"Below is the example documentation for how the files of genes that we were interested in ( *cys.txt files) were generated. Load annotation files in R and find genes of interest. setwd ( 'path/to/input/files' ) library ( \"readr\" ) library ( \"dplyr\" ) library ( \"tidyr\" ) dataFiles <- lapply ( Sys.glob ( \"*.aa\" ), read.delim , sep = \"\\t\" ) Here, we are looking for the gene cluster that consists of sbp (sulfur binding protein) and cysUWA using KEGG databases (thier KO numbers are: K02048,K02046,K02047,K02045) GOI_4 <- data.frame () GOI_5 <- data.frame () GOI_7 <- data.frame () for ( keggid in c ( \"K02048\" , \"K02046\" , \"K02047\" , \"K02045\" )) { GOI_4 <- rbind ( GOI_4 , dataFiles [[ 1 ]][ grep ( keggid , dataFiles [[ 1 ]] $ Description.2 ),]) GOI_5 <- rbind ( GOI_5 , dataFiles [[ 2 ]][ grep ( keggid , dataFiles [[ 2 ]] $ Description.2 ),]) GOI_7 <- rbind ( GOI_7 , dataFiles [[ 3 ]][ grep ( keggid , dataFiles [[ 3 ]] $ Description.2 ),])} GOI_4 A data.frame: 4 \u00d7 33 Query.Gene GC. Contig.name Start.position Stop.position Orientation Query.sequence Signalling Signal.confidence Target.gene..UniProt. \u22ef Coverage.2 E.value.2 Description.2 Taxonomy.2 Target.gene..Pfam. E.value.3 Description.3 Target.gene..TIGRfam. E.value.4 Description.4 <fct> <dbl> <fct> <int> <int> <fct> <fct> <fct> <fct> <fct> \u22ef <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> 1433 bin_4_NODE_55_length_158395_cov_1.135272_128 49.90 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MKFNIWLSVGLLTFALAGASQAFADRTLLNVSYDPTRELYEEYNREFIRYWQEKTGEKITVKQSHGGSGKQARAVIDGVPADIVTLALAHDIDAISEQSGLIPAEWQKRLPNNSSPYLSTIVLLVRKDNPKGIRDWDDLIKPGVAVITPNPKTSGGARWNYLAAWGFALKKYDGDEAKAQEFLTRLYKNVSILDSGARGSTINFIQRGIGDVLISWENEAFLALKEYGPEKFEIIIPSISILAEPPVAVVDKNVDKHGVRNIAQAYLEYLYDKKGQEIAARNFYRPSDPEIAKKYAHQFPAINLFTINEVFGGWPQAQSIHFKDGGLFDKIYVNQ* - - A0A1H9YF74 \u22ef 99.701493 -1.000000 K02048: sbp1; Prokaryotic sulfate-/thiosulfate-binding protein Nitrosomonas europaea PF12849.7; PF01547.25; PF13531.6; PF13343.6 0.000200; 0.000000; 0.000000; 0.000000 PBP_like_2; SBP_bac_1; SBP_bac_11; SBP_bac_6 TIGR00971; TIGR03261 0.000000; 0.000001 TIGR00971; TIGR03261 1600 bin_4_NODE_55_length_158395_cov_1.135272_132 52.88 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MSTFKQYSVLPGFNLALGFTLLYLSLVVLIPLSAAFIHSAKLTWPEFWSTVTAPRVVASYRLTFGASFAAAVVNTFFGLLVAWVLVRYPFPGKRLVDALIDLPFALPTSVAGITLTAIYAGNGWLGQYLEPLGIKVAFTPVGVFVALTFIGLPFVVRTVQPVLEDIEKELEEAAAMLGATRWQTFRYVIFPAVLPALTTGFALAFARAIGEYGSVIFIAGNIPMVSEITPLLIITKLEQYDYAGATAIAVVMLVISFILLLIINLLQWWVRHRSIKA* - - A0A1H9YF01 \u22ef 99.638989 -1.000000 K02046: cysU; cysU; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 0.000000; 0.000000; 0.000001; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000220 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 1599 bin_4_NODE_55_length_158395_cov_1.135272_133 54.27 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTAVPLLQTSPAPHKVVTQESPWARWTLILLALSFLSLFLLLPLVAVFFEALRKGWEVYLAAITEPDALAAIRLTLIAAAIAVPLNLIFGIAAAWAIAKFEFRGKSILTSLIDLPFSVSPVVAGLIYVLIFGLQGWIGPWLREHDLSIIFAVPGIVLATIFVTVPFIARELIPLMQAQGSEEEEAAIILGASGWQTLWYVTLPNIKWGLLYGTILCNARAMGEFGAVSVVSGHIRGLTNTLSLHVEILYNEYNFVAAFAVASLLALLALITLVLKTLVEAKVAQQTSRGNHS* - - A0A1H9YF25 \u22ef 99.657534 -1.000000 K02047: cysW; cysW; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 0.000000; 0.000000; 0.000420; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000019; 0.000000 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 1606 bin_4_NODE_55_length_158395_cov_1.135272_134 53.35 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTIEIHDLSKQFGSFTALNDINLKVNPGELLALLGPSGSGKTTLLRVIAGLETADSGQVLFNEEDSTDKHIRDRHVGFVFQHYALFRNMTIFENVAFGLRVRPRKQRPNAPEINHRVTELLQLVQLDWLADRYPHQLSGGQRQRIALARALAVEPSVLLLDEPFGALDAKVRKELRAWLRKLHDDMHITSVFVTHDQEEALEVADRIVVMNRGRIEQIGTPDEVYEKPANPFVYEFLGHVNLFHGRVHQGHAWIGDLEVDAPEYSEAEDLSAIAYVRPHDIEVDRTLNGEPALAAHIVHILAIGPVVRLELAGKDNQSTNSIYAEISKERFRELQLARGDQVFIKPRKLDLFPNHAQNGSIH* - - Q82WT5 \u22ef 99.723757 -1.000000 K02045: cysA; cysA; sulfate transport ATP-binding ABC transporter protein Nitrosomonas europaea PF00004.29; PF13191.6; PF13304.6; PF13401.6; PF13476.6; PF13555.6; PF00005.27; PF17850.1; PF05729.12; PF02463.19; PF05621.11; PF03459.17; PF12857.7 0.000500; 0.000035; 0.000000; 0.000150; 0.000200; 0.000096; 0.000000; 0.000000; 0.000930; 0.000000; 0.000770; 0.000009; 0.000000 AAA; AAA_16; AAA_21; AAA_22; AAA_23; AAA_29; ABC_tran; CysA_C_terminal; NACHT; SMC_N; TniB; TOBE; TOBE_3 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 0.000020; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 As bin_4 has additional unknown genes in between the sulfur assimilation gene cluster, we will manually add them into the dataframe by looking in the .aa file (protein sequence bin_4_NODE_55_length_158395_cov_1.135272_129, bin_4_NODE_55_length_158395_cov_1.135272_130 and bin_4_NODE_55_length_158395_cov_1.135272_131). for ( bin_node in c ( \"bin_4_NODE_55_length_158395_cov_1.135272_129\" , \"bin_4_NODE_55_length_158395_cov_1.135272_130\" , \"bin_4_NODE_55_length_158395_cov_1.135272_131\" )) { GOI_4 <- rbind ( GOI_4 , dataFiles [[ 1 ]][ grep ( bin_node , dataFiles [[ 1 ]] $ Query.Gene ),])} ( GOI_4 ) A data.frame: 7 \u00d7 33 Query.Gene GC. Contig.name Start.position Stop.position Orientation Query.sequence Signalling Signal.confidence Target.gene..UniProt. \u22ef Coverage.2 E.value.2 Description.2 Taxonomy.2 Target.gene..Pfam. E.value.3 Description.3 Target.gene..TIGRfam. E.value.4 Description.4 <fct> <dbl> <fct> <int> <int> <fct> <fct> <fct> <fct> <fct> \u22ef <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> <fct> 1433 bin_4_NODE_55_length_158395_cov_1.135272_128 49.90 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MKFNIWLSVGLLTFALAGASQAFADRTLLNVSYDPTRELYEEYNREFIRYWQEKTGEKITVKQSHGGSGKQARAVIDGVPADIVTLALAHDIDAISEQSGLIPAEWQKRLPNNSSPYLSTIVLLVRKDNPKGIRDWDDLIKPGVAVITPNPKTSGGARWNYLAAWGFALKKYDGDEAKAQEFLTRLYKNVSILDSGARGSTINFIQRGIGDVLISWENEAFLALKEYGPEKFEIIIPSISILAEPPVAVVDKNVDKHGVRNIAQAYLEYLYDKKGQEIAARNFYRPSDPEIAKKYAHQFPAINLFTINEVFGGWPQAQSIHFKDGGLFDKIYVNQ* - - A0A1H9YF74 \u22ef 99.701493 -1.000000 K02048: sbp1; Prokaryotic sulfate-/thiosulfate-binding protein Nitrosomonas europaea PF12849.7; PF01547.25; PF13531.6; PF13343.6 0.000200; 0.000000; 0.000000; 0.000000 PBP_like_2; SBP_bac_1; SBP_bac_11; SBP_bac_6 TIGR00971; TIGR03261 0.000000; 0.000001 TIGR00971; TIGR03261 1600 bin_4_NODE_55_length_158395_cov_1.135272_132 52.88 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MSTFKQYSVLPGFNLALGFTLLYLSLVVLIPLSAAFIHSAKLTWPEFWSTVTAPRVVASYRLTFGASFAAAVVNTFFGLLVAWVLVRYPFPGKRLVDALIDLPFALPTSVAGITLTAIYAGNGWLGQYLEPLGIKVAFTPVGVFVALTFIGLPFVVRTVQPVLEDIEKELEEAAAMLGATRWQTFRYVIFPAVLPALTTGFALAFARAIGEYGSVIFIAGNIPMVSEITPLLIITKLEQYDYAGATAIAVVMLVISFILLLIINLLQWWVRHRSIKA* - - A0A1H9YF01 \u22ef 99.638989 -1.000000 K02046: cysU; cysU; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 0.000000; 0.000000; 0.000001; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000220 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262; TIGR03416 1599 bin_4_NODE_55_length_158395_cov_1.135272_133 54.27 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTAVPLLQTSPAPHKVVTQESPWARWTLILLALSFLSLFLLLPLVAVFFEALRKGWEVYLAAITEPDALAAIRLTLIAAAIAVPLNLIFGIAAAWAIAKFEFRGKSILTSLIDLPFSVSPVVAGLIYVLIFGLQGWIGPWLREHDLSIIFAVPGIVLATIFVTVPFIARELIPLMQAQGSEEEEAAIILGASGWQTLWYVTLPNIKWGLLYGTILCNARAMGEFGAVSVVSGHIRGLTNTLSLHVEILYNEYNFVAAFAVASLLALLALITLVLKTLVEAKVAQQTSRGNHS* - - A0A1H9YF25 \u22ef 99.657534 -1.000000 K02047: cysW; cysW; sulfate transport ABC transporter protein Nitrosomonas europaea PF00528.22 0.000000 BPD_transp_1 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 0.000000; 0.000000; 0.000420; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000019; 0.000000 TIGR00969; TIGR00974; TIGR01097; TIGR01253; TIGR01581; TIGR02138; TIGR02139; TIGR02140; TIGR02141; TIGR03226; TIGR03255; TIGR03262 1606 bin_4_NODE_55_length_158395_cov_1.135272_134 53.35 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTIEIHDLSKQFGSFTALNDINLKVNPGELLALLGPSGSGKTTLLRVIAGLETADSGQVLFNEEDSTDKHIRDRHVGFVFQHYALFRNMTIFENVAFGLRVRPRKQRPNAPEINHRVTELLQLVQLDWLADRYPHQLSGGQRQRIALARALAVEPSVLLLDEPFGALDAKVRKELRAWLRKLHDDMHITSVFVTHDQEEALEVADRIVVMNRGRIEQIGTPDEVYEKPANPFVYEFLGHVNLFHGRVHQGHAWIGDLEVDAPEYSEAEDLSAIAYVRPHDIEVDRTLNGEPALAAHIVHILAIGPVVRLELAGKDNQSTNSIYAEISKERFRELQLARGDQVFIKPRKLDLFPNHAQNGSIH* - - Q82WT5 \u22ef 99.723757 -1.000000 K02045: cysA; cysA; sulfate transport ATP-binding ABC transporter protein Nitrosomonas europaea PF00004.29; PF13191.6; PF13304.6; PF13401.6; PF13476.6; PF13555.6; PF00005.27; PF17850.1; PF05729.12; PF02463.19; PF05621.11; PF03459.17; PF12857.7 0.000500; 0.000035; 0.000000; 0.000150; 0.000200; 0.000096; 0.000000; 0.000000; 0.000930; 0.000000; 0.000770; 0.000009; 0.000000 AAA; AAA_16; AAA_21; AAA_22; AAA_23; AAA_29; ABC_tran; CysA_C_terminal; NACHT; SMC_N; TniB; TOBE; TOBE_3 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 0.000020; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000; 0.000000 TIGR00611; TIGR00630; TIGR00954; TIGR00955; TIGR00956; TIGR00957; TIGR00958; TIGR00968; TIGR00972; TIGR01166; TIGR01184; TIGR01186; TIGR01187; TIGR01188; TIGR01189; TIGR01192; TIGR01193; TIGR01194; TIGR01257; TIGR01271; TIGR01277; TIGR01288; TIGR01842; TIGR01846; TIGR01978; TIGR02142; TIGR02203; TIGR02204; TIGR02211; TIGR02314; TIGR02315; TIGR02323; TIGR02324; TIGR02633; TIGR02673; TIGR02769; TIGR02770; TIGR02857; TIGR02868; TIGR02982; TIGR03005; TIGR03258; TIGR03265; TIGR03269; TIGR03375; TIGR03410; TIGR03411; TIGR03415; TIGR03522; TIGR03608; TIGR03719; TIGR03740; TIGR03771; TIGR03796; TIGR03797; TIGR03864; TIGR03873; TIGR04406 1434 bin_4_NODE_55_length_158395_cov_1.135272_129 46.41 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MTVLIVGGDYIASLKQRITAHGYSRIEHWNGRKKGFNKRALPGRTKLVVIIYDYVSHNLANSVKDQASRIGIPMIFCRHAMHEIDTIFDEKKAEESCCNFV* - - A0A1H9YF98 \u22ef 99.009901 -1.000000 possible predicted diverged CheY-domain Nitrosomonas europaea PF10087.9 0.000000 DUF2325 Not annotated - - 1602 bin_4_NODE_55_length_158395_cov_1.135272_130 44.85 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MLPSTLKTLVHGDYMIRTIERIITSTGFSLQHSDNGEITGLFYHCKLSSAYQPVFEAARNNIIGQEARIRADLNGNGEIQLSPWHIFALAPDDEQLIKLDRLCRTIHALNFLDNITNKQMKLFVSVQPRLLESVKDDHGSAFEQILDIIGIRTSRIVIEIPVEANRDWRLLKRVIMNYRAHGYLISVNYSGTNNDRISELGKLYPNVIRIDARDLLRRSVLDPLIVTAHNQGADVLVGNIETSYQLTDAIHAGADLLQGNFLARYSRKADNMLTPFSWQIPDERNGYRFDNESLQENQPYRSI* - - A0A1H9YFC5 \u22ef 99.653979 -1.000000 Domain of unknown function 2 Nitrosomonas europaea PF00563.20 0.000000 EAL Not annotated - - 1601 bin_4_NODE_55_length_158395_cov_1.135272_131 46.51 bin_4_NODE_55_length_158395_cov_1.135272 0 0 Reverse MNRTTAKPFTVMDEFELIKAATDYSLEQDENGRISGNFYHCKLTSAFQVILDIHENNIIGHAAYIRSESNGEVSLWPWQVFALASKDEQLIDLDRLCRAIHALNYFKKNFDTNTGQLFLSVHPRLLSSIQNEHGRTFKDFLDLTGISTSRIVIEIPPALNHDWKLLQKLIINYRSYGYQIALNFSSTNGHWLLGTDDLHPDILIVQAHELLHYQLNDYPEDNSTRDAGFRLHVRKIETQEQLTAAKQAGAHYLQGNFLGKTV* - - A0A1H9YEZ8 \u22ef 99.618321 -1.000000 Domain of unknown function 2 Nitrosomonas europaea PF00563.20 0.000000 EAL Not annotated - - We found 2 genes match to K02048 for bin 5: the sulfate binding protein, but as we are only interested at the gene cluster, we will look at the one from the same contig as the other genes. Next, we will create the tables ( *cys.txt ) to be used for 'genoPlotR'. GOI4 = GOI_4 [ 1 : 7 ,] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) GOI5 = GOI_5 [ c ( 1 , 3 : 5 ),] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) GOI7 = GOI_7 [ 1 : 4 ,] %>% select ( \"Description.2\" , \"Query.Gene\" ) %>% separate ( col = Description.2 , into = c ( \"KO\" , \"Annotation\" ), sep = \":\" ) rownames ( GOI4 ) <- c () rownames ( GOI5 ) <- c () rownames ( GOI7 ) <- c () head ( GOI4 ) Warning message: \u201cExpected 2 pieces. Missing pieces filled with `NA` in 3 rows [5, 6, 7].\u201d A data.frame: 6 \u00d7 3 KO Annotation Query.Gene <chr> <chr> <fct> K02048 sbp1; Prokaryotic sulfate-/thiosulfate-binding protein bin_4_NODE_55_length_158395_cov_1.135272_128 K02046 cysU; cysU; sulfate transport ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_132 K02047 cysW; cysW; sulfate transport ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_133 K02045 cysA; cysA; sulfate transport ATP-binding ABC transporter protein bin_4_NODE_55_length_158395_cov_1.135272_134 possible predicted diverged CheY-domain NA bin_4_NODE_55_length_158395_cov_1.135272_129 Domain of unknown function 2 NA bin_4_NODE_55_length_158395_cov_1.135272_130 Edit the gene annotation for the unknown KO in bin_4 to get the table below GOI4 = mutate ( GOI4 , Annotation = case_when ( Annotation == \"NA\" ~ \"\" , TRUE ~ as.character ( KO ))) GOI4 A data.frame: 7 \u00d7 3 KO Annotation Query.Gene <chr> <chr> <fct> K02048 K02048 bin_4_NODE_55_length_158395_cov_1.135272_128 K02046 K02046 bin_4_NODE_55_length_158395_cov_1.135272_132 K02047 K02047 bin_4_NODE_55_length_158395_cov_1.135272_133 K02045 K02045 bin_4_NODE_55_length_158395_cov_1.135272_134 possible predicted diverged CheY-domain possible predicted diverged CheY-domain bin_4_NODE_55_length_158395_cov_1.135272_129 Domain of unknown function 2 Domain of unknown function 2 bin_4_NODE_55_length_158395_cov_1.135272_130 Domain of unknown function 2 Domain of unknown function 2 bin_4_NODE_55_length_158395_cov_1.135272_131 Export the files. write_delim ( GOI4 , \"bin_4_cys.txt\" , delim = \"\\t\" ) write_delim ( GOI5 , \"bin_5_cys.txt\" , delim = \"\\t\" ) write_delim ( GOI7 , \"bin_7_cys.txt\" , delim = \"\\t\" )","title":"Grab Gene of Interest (generate the cys.txt files provided in ex15_gene_synteny)"},{"location":"resources/APPENDIX_ex8_Dereplication/","text":"Dereplicating data from multiple assemblies \u00b6 Objectives \u00b6 Understand the common issues with using dRep and CheckM Use CheckM and dRep together to dereplicate a set of genomes De-duplicate viral contigs using BBMap 's dedupe.sh Using dRep and CheckM \u00b6 Before we begin to use dRep , it is important to understand the workflow that it applies to a data set. The basic idea of dRep is that genomes or MAGs are processed as follows: Genomes are scored for completeness and contamination estimates using CheckM Genomes are assigned to primary clusters using a quick and rough average nucleotide identidy (ANI) calculation Clusters of genomes sharing greater than 90% ANI are grouped together and ANI is calculated using a more sensitive method Where groups of genomes sharing >99% ANI are found, the best (determined by completeness and contamination statistics) is picked as a representative of the cluster When run on its own, dRep will automatically try to run CheckM in the background. There are two problems with this approach, namely: dRep is written in version 3.6 of the python language, and the version of CheckM avaiable on NeSI is written in version 2.7. These are not compatible with each other There are two parameters in CheckM which speed up the workflow through multithreading, but dRep only has access to one of them For these reasons, when working on NeSI we run CheckM on our data set first, and then pass the results directly into dRep , avoiding the need for dRep to try to call CheckM . Use CheckM and dRep together to dereplicate a set of MAGs \u00b6 For this exercise, we will be working with a different set of MAGs to the mock community, as there is not enough strain-level variation in the mock metagenome for dRep to actually remove any MAGs. We will write a single slurm script to run all necessary commands, then analyse the content. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J checkm_drep #SBATCH --res SummerSchool #SBATCH --time 2:00:00 #SBATCH --mem 80GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH -e checkm_drep.err #SBATCH -o checkm_drep.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/12.drep_example/ # Step 1 module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 checkm lineage_wf -t 10 --pplacer_threads 10 -x fa --tab_table -f checkm.txt \\ input_bins/ checkm_out/ # Step 2 echo \"genome,completeness,contamination\" > dRep.genomeInfo cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | \\ tail -n+2 >> dRep.genomeInfo # Step 3 module purge module load drep/2.3.2-gimkl-2017a-Python-3.6.3 MUMmer/3.23-gimkl-2017a dRep dereplicate --genomeInfo dRep.genomeInfo -g input_bins/*.fa -p 10 drep_output/ Walking through this script, step by step, we are performing the following tasks: Step 1 \u00b6 This should look familiar to you. Here we are simply loading CheckM , then running it over a set of MAGs to get the completeness and contamination estimates for our data. Step 2 \u00b6 When running dRep , we have the option to either let dRep execute CheckM in the background, or we can pass a comma-separated file of the MAG name and its statistics. Unfortunately, dRep does not take the CheckM output itself, so we must use some shell commands to reformat the data. To achieve this, we use the following steps: echo \"genome,completeness,contamination\" > dRep.genomeInfo This line creates the header row for the dRep file, which we are calling dRep.genomeInfo . cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | tail -n+2 >> dRep.genomeInfo This line cuts the columns 1, 12, and 13 from the CheckM output table, which correspond to the MAG name and completeness/contamination estimates. We then redirect these columns using the | character and use them as input in a sed command. Because CheckM reports our MAG names without their fastA file extension, but dRep requires the extension to be present in the MAG name, we add the trailing .fa with our sed command. This also gives us an opportunity to replace the tab-delimiting character from the CheckM output with the comma character that dRep uses for marking columns in the table. We then pass the output into a second sed command to replace the tab between columns 12 and 13 with a comma. We then use another redirect ( | ) to pass the resulting text stream to the tail command. The way we are calling tail here will return every row in the text stream except for the first, which means that we are getting all the MAG rows but not their column names. We remove these names because `dRep uses a different naming convention for specifying columns. We append the MAG statistics to the end of the dRep.genomeInfo file, whih contains the correct column names for dRep . Step 3 \u00b6 Here we simply load the modules required for dRep , then execute the command. Because of the compatibility issues between the python version required by CheckM and dRep , we use the module purge command to unload all current modules before loading dRep . This removes the CheckM library, and its python2.7 dependency, allowing the dRep and python3.6 to load correctly. The parameters for dRep are as follows: Parameter Function dereplicate Activate the dereplicate workflow from dRep --genomeInfo ... Skip quality checking via CheckM , instead use the values in the table provided -g ... List of MAGs to dereplicate, passed by wildcard -p ... Number of processors to use drep_output/ Output folder for all outputs When dRep finishes running, there are a few useful outputs to examine: drep_output/dereplicated_genomes/ # The representative set of MAGs drep_output/figures/ # Dendrograms to visualise the clustering of genomes drep_output/data_tables/ # The primary and secondary clustering of the MAGs, and scoring information De-duplicate viral contigs using BBMap 's dedupe.sh \u00b6 Part of the process for dRep includes measures specific to prokaryotes. Hence, the above approach will not be appropriate for dereplicating viral contigs derived from different assemblies. dedupe.sh from the BBMap suite of tools is one alternative to achieve a similar process for these data. dedupe.sh takes a comma separated list of assembly fastA files as input, and filters out any contigs that are either full duplicates of another contig, or fully contained within another (longer) contig (i.e. matching alignment within another longer contig). minidentity=... sets the minimum identity threshold, and out=... results in a single deduplicated set of contigs as output. An example of how dedupe.sh might be run on multiple fastA files of assembled viral contigs (e.g. those output by tools such as VIBRANT or VirSorter ) is as follows: cd /path/to/viral/contigs/from/multiple/assemblies/ mkdir -p dedupe # load BBMap module load BBMap/38.81-gimkl-2020a # Set infiles infiles = \"assembly_viral_1.fna,assembly_viral_2.fna,assembly_viral_3.fna,assembly_viral_4.fna\" # Run main analyses dedupe.sh threads = 1 in = ${ infiles } \\ minidentity = 98 exact = f sort = length mergenames = t mergedelimiter = ___ overwrite = t \\ out = dedupe/dedupe.fa NOTE: dedupe.sh will dereplicate contigs that are duplicates or are fully contained by another contig, but unforunately not those that share a partial overlap (i.e. sharing an overlapping region, but with non-overlapping sections hanging off the ends). dedupe.sh does include the functionality to **cluster* these contigs together (via c and mo ) and output as separate fastA files, but not to then merge these sequences together into a single representative (this appears to have been a \"to do\" item for a number of years). One option in this case could be to develop a method that: outputs all of the clusters, aligns sequences within each cluster, generates a consensus sequence from the alignment (i.e. effectively performing new mini-assemblies on each of the clusters of overlapping contigs), and then adds this back to the deduplicated fasta output from dedupe.sh (n.b. this is unfortunately a less trivial process than it sounds...)*","title":"Dereplicating data from multiple assemblies"},{"location":"resources/APPENDIX_ex8_Dereplication/#dereplicating-data-from-multiple-assemblies","text":"","title":"Dereplicating data from multiple assemblies"},{"location":"resources/APPENDIX_ex8_Dereplication/#objectives","text":"Understand the common issues with using dRep and CheckM Use CheckM and dRep together to dereplicate a set of genomes De-duplicate viral contigs using BBMap 's dedupe.sh","title":"Objectives"},{"location":"resources/APPENDIX_ex8_Dereplication/#using-drep-and-checkm","text":"Before we begin to use dRep , it is important to understand the workflow that it applies to a data set. The basic idea of dRep is that genomes or MAGs are processed as follows: Genomes are scored for completeness and contamination estimates using CheckM Genomes are assigned to primary clusters using a quick and rough average nucleotide identidy (ANI) calculation Clusters of genomes sharing greater than 90% ANI are grouped together and ANI is calculated using a more sensitive method Where groups of genomes sharing >99% ANI are found, the best (determined by completeness and contamination statistics) is picked as a representative of the cluster When run on its own, dRep will automatically try to run CheckM in the background. There are two problems with this approach, namely: dRep is written in version 3.6 of the python language, and the version of CheckM avaiable on NeSI is written in version 2.7. These are not compatible with each other There are two parameters in CheckM which speed up the workflow through multithreading, but dRep only has access to one of them For these reasons, when working on NeSI we run CheckM on our data set first, and then pass the results directly into dRep , avoiding the need for dRep to try to call CheckM .","title":"Using dRep and CheckM"},{"location":"resources/APPENDIX_ex8_Dereplication/#use-checkm-and-drep-together-to-dereplicate-a-set-of-mags","text":"For this exercise, we will be working with a different set of MAGs to the mock community, as there is not enough strain-level variation in the mock metagenome for dRep to actually remove any MAGs. We will write a single slurm script to run all necessary commands, then analyse the content. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J checkm_drep #SBATCH --res SummerSchool #SBATCH --time 2:00:00 #SBATCH --mem 80GB #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH -e checkm_drep.err #SBATCH -o checkm_drep.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/12.drep_example/ # Step 1 module load CheckM/1.0.13-gimkl-2018b-Python-2.7.16 checkm lineage_wf -t 10 --pplacer_threads 10 -x fa --tab_table -f checkm.txt \\ input_bins/ checkm_out/ # Step 2 echo \"genome,completeness,contamination\" > dRep.genomeInfo cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | \\ tail -n+2 >> dRep.genomeInfo # Step 3 module purge module load drep/2.3.2-gimkl-2017a-Python-3.6.3 MUMmer/3.23-gimkl-2017a dRep dereplicate --genomeInfo dRep.genomeInfo -g input_bins/*.fa -p 10 drep_output/ Walking through this script, step by step, we are performing the following tasks:","title":"Use CheckM and dRep together to dereplicate a set of MAGs"},{"location":"resources/APPENDIX_ex8_Dereplication/#step-1","text":"This should look familiar to you. Here we are simply loading CheckM , then running it over a set of MAGs to get the completeness and contamination estimates for our data.","title":"Step 1"},{"location":"resources/APPENDIX_ex8_Dereplication/#step-2","text":"When running dRep , we have the option to either let dRep execute CheckM in the background, or we can pass a comma-separated file of the MAG name and its statistics. Unfortunately, dRep does not take the CheckM output itself, so we must use some shell commands to reformat the data. To achieve this, we use the following steps: echo \"genome,completeness,contamination\" > dRep.genomeInfo This line creates the header row for the dRep file, which we are calling dRep.genomeInfo . cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | tail -n+2 >> dRep.genomeInfo This line cuts the columns 1, 12, and 13 from the CheckM output table, which correspond to the MAG name and completeness/contamination estimates. We then redirect these columns using the | character and use them as input in a sed command. Because CheckM reports our MAG names without their fastA file extension, but dRep requires the extension to be present in the MAG name, we add the trailing .fa with our sed command. This also gives us an opportunity to replace the tab-delimiting character from the CheckM output with the comma character that dRep uses for marking columns in the table. We then pass the output into a second sed command to replace the tab between columns 12 and 13 with a comma. We then use another redirect ( | ) to pass the resulting text stream to the tail command. The way we are calling tail here will return every row in the text stream except for the first, which means that we are getting all the MAG rows but not their column names. We remove these names because `dRep uses a different naming convention for specifying columns. We append the MAG statistics to the end of the dRep.genomeInfo file, whih contains the correct column names for dRep .","title":"Step 2"},{"location":"resources/APPENDIX_ex8_Dereplication/#step-3","text":"Here we simply load the modules required for dRep , then execute the command. Because of the compatibility issues between the python version required by CheckM and dRep , we use the module purge command to unload all current modules before loading dRep . This removes the CheckM library, and its python2.7 dependency, allowing the dRep and python3.6 to load correctly. The parameters for dRep are as follows: Parameter Function dereplicate Activate the dereplicate workflow from dRep --genomeInfo ... Skip quality checking via CheckM , instead use the values in the table provided -g ... List of MAGs to dereplicate, passed by wildcard -p ... Number of processors to use drep_output/ Output folder for all outputs When dRep finishes running, there are a few useful outputs to examine: drep_output/dereplicated_genomes/ # The representative set of MAGs drep_output/figures/ # Dendrograms to visualise the clustering of genomes drep_output/data_tables/ # The primary and secondary clustering of the MAGs, and scoring information","title":"Step 3"},{"location":"resources/APPENDIX_ex8_Dereplication/#de-duplicate-viral-contigs-using-bbmaps-dedupesh","text":"Part of the process for dRep includes measures specific to prokaryotes. Hence, the above approach will not be appropriate for dereplicating viral contigs derived from different assemblies. dedupe.sh from the BBMap suite of tools is one alternative to achieve a similar process for these data. dedupe.sh takes a comma separated list of assembly fastA files as input, and filters out any contigs that are either full duplicates of another contig, or fully contained within another (longer) contig (i.e. matching alignment within another longer contig). minidentity=... sets the minimum identity threshold, and out=... results in a single deduplicated set of contigs as output. An example of how dedupe.sh might be run on multiple fastA files of assembled viral contigs (e.g. those output by tools such as VIBRANT or VirSorter ) is as follows: cd /path/to/viral/contigs/from/multiple/assemblies/ mkdir -p dedupe # load BBMap module load BBMap/38.81-gimkl-2020a # Set infiles infiles = \"assembly_viral_1.fna,assembly_viral_2.fna,assembly_viral_3.fna,assembly_viral_4.fna\" # Run main analyses dedupe.sh threads = 1 in = ${ infiles } \\ minidentity = 98 exact = f sort = length mergenames = t mergedelimiter = ___ overwrite = t \\ out = dedupe/dedupe.fa NOTE: dedupe.sh will dereplicate contigs that are duplicates or are fully contained by another contig, but unforunately not those that share a partial overlap (i.e. sharing an overlapping region, but with non-overlapping sections hanging off the ends). dedupe.sh does include the functionality to **cluster* these contigs together (via c and mo ) and output as separate fastA files, but not to then merge these sequences together into a single representative (this appears to have been a \"to do\" item for a number of years). One option in this case could be to develop a method that: outputs all of the clusters, aligns sequences within each cluster, generates a consensus sequence from the alignment (i.e. effectively performing new mini-assemblies on each of the clusters of overlapping contigs), and then adds this back to the deduplicated fasta output from dedupe.sh (n.b. this is unfortunately a less trivial process than it sounds...)*","title":"De-duplicate viral contigs using BBMap's dedupe.sh"},{"location":"resources/APPENDIX_ex9_Generating_input_files_for_VizBin/","text":"Appendix (ex9): Generating input files for VizBin from DAS_Tool curated bins \u00b6 The final bins that we obtained in the previous step (output from DAS_Tool ) have been copied into 6.bin_refinement/dastool_out/_DASTool_bins/ 1. Generalise bin naming and add bin IDs to sequence headers We will first modify the names of our bins to be simply numbered 1 to n bins. We will use a loop to do this, using the wildcard ( * ) to loop over all files in the DASTool_bins folder, copying to the new example_data_unchopped/ folder and renaming as bin [1-n].fna. The sed command then adds the bin ID to the start of sequence headers in each of the new bin files (this will be handy information to have in the sequence headers for downstream processing). cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Make a new directory the renamed bins mkdir example_data_unchopped/ # Copy and rename bins into generic <bin_[0-n].fna> filenames i = 0 for file in dastool_out/_DASTool_bins/* ; do # Copy and rename bin file cp ${ file } example_data_unchopped/bin_ ${ i } .fna # extract bin ID binID = $( basename example_data_unchopped/bin_ ${ i } .fna .fna ) # Add bin ID to sequence headers sed -i -e \"s/>/> ${ binID } _/g\" example_data_unchopped/bin_ ${ i } .fna # Increment i (( i += 1 )) done 2. Fragment contigs Using the cut_up_fasta.py script that comes with the binning tool CONCOCT , cut contigs into 20k fragments to add better density to the cluster. # Load CONCONCT module module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16 # Make directory to add chopped bin data into mkdir example_data_20k/ # loop over .fna files to generate chopped (fragmented) files using CONCONT's cut_up_fasta.py for bin_file in example_data_unchopped/* ; do bin_name = $( basename ${ bin_file } .fna ) cut_up_fasta.py -c 20000 -o 0 --merge_last ${ bin_file } > example_data_20k/ ${ bin_name } .chopped.fna done 3. Concatenate fragmented bins Concatenate chopped bins into a single fastA file. Concatenate chopped bins into a single all_bins.fna file to use as input for both subcontig read mapping via Bowtie2 and visualisation via VizBin . cat example_data_20k/*.fna > all_bins.fna 4. Read mapping of subcontigs (fragmented contigs based on 20k length) 4a. Build mapping index Build Bowtie2 mapping index based on the concatenated chopped bins. mkdir -p read_mapping/ module load Bowtie2/2.3.5-GCC-7.4.0 bowtie2-build all_bins.fna read_mapping/bw_bins 4b. Map sample reads to index Map quality filtered reads to the index using Bowtie2 . Example slurm script: #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J 6.bin_refinement_mapping #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH -e 6.bin_refinement_mapping.err #SBATCH -o 6.bin_refinement_mapping.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x read_mapping/bw_bins \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S read_mapping/ ${ i } .sam # Step 3 samtools sort -@ 10 -o read_mapping/ ${ i } .bam read_mapping/ ${ i } .sam done Note: These settings are appropriate for this workshop's mock data. Full data sets will likely require considerably greater memory and time allocations. 5. Generate coverage table of subcontigs (contigs fragmented based on 20k length) Use MetaBAT 's jgi_summarise_bam_contig_depths to generate a coverage table. # Load module module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth example_data_20k_cov.txt read_mapping/sample*.bam 6. Generate annotation table for VizBin Using the chopped bin files ( example_data_20k/ ) and the coverage table generated above ( example_data_20k_cov.txt ), we can use the following script to generate an annotation table in the format that VizBin is expecting. Note that here we are including columns for per-sub-contig coverage based on sample1 (see note at the start of this exercise), label (bin ID), and length values, and storing this in all_bins.sample1.vizbin.ann . What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information for sample1 which is associated with that contig in the example_data_20k_cov.txt file, and adding that as a new column to the file. (If you wished to instead look based on sample2, you would need to modify the cut command in the line sample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4) accordingly (e.g. cut -f6 ). # Set up annotation file headers echo \"coverage,label,length\" > all_bins.sample1.vizbin.ann # loop through bin .fna files for bin_file in example_data_20k/*.fna ; do # extract bin ID binID = $( basename ${ bin_file } .fna ) # loop through each sequence header in bin_file for header in ` grep \">\" ${ bin_file } ` ; do contigID = $( echo ${ header } | sed 's/>//g' ) # identify this line from the coverage table (example_data_20k_cov.txt), and extract contigLen (column 2) and coverage for sample1.bam (column 4) contigLen = $( grep -P \" ${ contigID } \\t\" example_data_20k_cov.txt | cut -f2 ) sample1_cov = $( grep -P \" ${ contigID } \\t\" example_data_20k_cov.txt | cut -f4 ) # Add to vizbin.ann file echo \" ${ sample1_cov } , ${ binID } , ${ contigLen } \" >> all_bins.sample1.vizbin.ann done done We now have the all_bins.fna and all_bins.sample1.vizbin.ann files that were provided at the start of the VizBin exercise .","title":"APPENDIX ex9 Generating input files for VizBin"},{"location":"resources/APPENDIX_ex9_Generating_input_files_for_VizBin/#appendix-ex9-generating-input-files-for-vizbin-from-das_tool-curated-bins","text":"The final bins that we obtained in the previous step (output from DAS_Tool ) have been copied into 6.bin_refinement/dastool_out/_DASTool_bins/ 1. Generalise bin naming and add bin IDs to sequence headers We will first modify the names of our bins to be simply numbered 1 to n bins. We will use a loop to do this, using the wildcard ( * ) to loop over all files in the DASTool_bins folder, copying to the new example_data_unchopped/ folder and renaming as bin [1-n].fna. The sed command then adds the bin ID to the start of sequence headers in each of the new bin files (this will be handy information to have in the sequence headers for downstream processing). cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Make a new directory the renamed bins mkdir example_data_unchopped/ # Copy and rename bins into generic <bin_[0-n].fna> filenames i = 0 for file in dastool_out/_DASTool_bins/* ; do # Copy and rename bin file cp ${ file } example_data_unchopped/bin_ ${ i } .fna # extract bin ID binID = $( basename example_data_unchopped/bin_ ${ i } .fna .fna ) # Add bin ID to sequence headers sed -i -e \"s/>/> ${ binID } _/g\" example_data_unchopped/bin_ ${ i } .fna # Increment i (( i += 1 )) done 2. Fragment contigs Using the cut_up_fasta.py script that comes with the binning tool CONCOCT , cut contigs into 20k fragments to add better density to the cluster. # Load CONCONCT module module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16 # Make directory to add chopped bin data into mkdir example_data_20k/ # loop over .fna files to generate chopped (fragmented) files using CONCONT's cut_up_fasta.py for bin_file in example_data_unchopped/* ; do bin_name = $( basename ${ bin_file } .fna ) cut_up_fasta.py -c 20000 -o 0 --merge_last ${ bin_file } > example_data_20k/ ${ bin_name } .chopped.fna done 3. Concatenate fragmented bins Concatenate chopped bins into a single fastA file. Concatenate chopped bins into a single all_bins.fna file to use as input for both subcontig read mapping via Bowtie2 and visualisation via VizBin . cat example_data_20k/*.fna > all_bins.fna 4. Read mapping of subcontigs (fragmented contigs based on 20k length) 4a. Build mapping index Build Bowtie2 mapping index based on the concatenated chopped bins. mkdir -p read_mapping/ module load Bowtie2/2.3.5-GCC-7.4.0 bowtie2-build all_bins.fna read_mapping/bw_bins 4b. Map sample reads to index Map quality filtered reads to the index using Bowtie2 . Example slurm script: #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J 6.bin_refinement_mapping #SBATCH --time 00:05:00 #SBATCH --mem 1GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 10 #SBATCH -e 6.bin_refinement_mapping.err #SBATCH -o 6.bin_refinement_mapping.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/ # Step 1 for i in sample1 sample2 sample3 sample4 ; do # Step 2 bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\ -x read_mapping/bw_bins \\ -1 ../3.assembly/ ${ i } _R1.fastq.gz -2 ../3.assembly/ ${ i } _R2.fastq.gz \\ -S read_mapping/ ${ i } .sam # Step 3 samtools sort -@ 10 -o read_mapping/ ${ i } .bam read_mapping/ ${ i } .sam done Note: These settings are appropriate for this workshop's mock data. Full data sets will likely require considerably greater memory and time allocations. 5. Generate coverage table of subcontigs (contigs fragmented based on 20k length) Use MetaBAT 's jgi_summarise_bam_contig_depths to generate a coverage table. # Load module module load MetaBAT/2.13-GCC-7.4.0 # calculate coverage table jgi_summarize_bam_contig_depths --outputDepth example_data_20k_cov.txt read_mapping/sample*.bam 6. Generate annotation table for VizBin Using the chopped bin files ( example_data_20k/ ) and the coverage table generated above ( example_data_20k_cov.txt ), we can use the following script to generate an annotation table in the format that VizBin is expecting. Note that here we are including columns for per-sub-contig coverage based on sample1 (see note at the start of this exercise), label (bin ID), and length values, and storing this in all_bins.sample1.vizbin.ann . What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information for sample1 which is associated with that contig in the example_data_20k_cov.txt file, and adding that as a new column to the file. (If you wished to instead look based on sample2, you would need to modify the cut command in the line sample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4) accordingly (e.g. cut -f6 ). # Set up annotation file headers echo \"coverage,label,length\" > all_bins.sample1.vizbin.ann # loop through bin .fna files for bin_file in example_data_20k/*.fna ; do # extract bin ID binID = $( basename ${ bin_file } .fna ) # loop through each sequence header in bin_file for header in ` grep \">\" ${ bin_file } ` ; do contigID = $( echo ${ header } | sed 's/>//g' ) # identify this line from the coverage table (example_data_20k_cov.txt), and extract contigLen (column 2) and coverage for sample1.bam (column 4) contigLen = $( grep -P \" ${ contigID } \\t\" example_data_20k_cov.txt | cut -f2 ) sample1_cov = $( grep -P \" ${ contigID } \\t\" example_data_20k_cov.txt | cut -f4 ) # Add to vizbin.ann file echo \" ${ sample1_cov } , ${ binID } , ${ contigLen } \" >> all_bins.sample1.vizbin.ann done done We now have the all_bins.fna and all_bins.sample1.vizbin.ann files that were provided at the start of the VizBin exercise .","title":"Appendix (ex9): Generating input files for VizBin from DAS_Tool curated bins"},{"location":"resources/command_line_shortcuts/","text":"Useful command line shortcuts \u00b6 Overview \u00b6 Bash Nano SLURM commands SBATCH flags Bash commands \u00b6 Command Function Example cat Read and print contents of a file cat my_file.txt grep Search a file for lines containing text of interest grep \"words\" my_file.txt sed Find and replace text in a file sed 's/words/pictures/' my_file.txt head Print the first n lines from a file head -n 10 my_file.txt tail Print the last n lines from a file tail -n 10 my_file.tx t less Partially open a text file for viewing, reading lines as the user scrolls less my_file.txt cut Retrieve particular columns of a file. Uses tab as the delimiting character, can change with -d parameter cut -f1,2,5 -d \",\" my_file.txt paste Opposite of cut , stitch together files in a row-wise manner paste my_file.txt my_other_file.txt Nano shortcuts \u00b6 Command Function Ctrl + X Close the file Ctrl + O Save the file Ctrl + W Search for a string in the file Ctrl + Space Go forward one word in a line Alt + Space Go backwards one word in a line Ctrl + A Go to the beginning of a line Ctrl + E Go to the end of a line Ctrl + W , Ctrl + V Go to the last line of the file Ctrl + K Cut the current line and save it to the clipboard Ctrl + U Paste (**U**n-cut) the text in the clipboard Ctrl + Shift + 6 Select text SLURM commands \u00b6 Command Function Example sbatch Submits a SLURM script to the queue manager sbatch job.sl squeue Display jobs in the queue Display jobs belonging to user usr9999 Display jobs on the long partition squeue squeue -u usr9999 squeue -p long sacct Displays all the jobs run by you that day Display job 123456789 sacct sacct -j 123456789 scancel Cancel a queued or running job Cancel all jobs belonging to usr9999 scancel 123456789 scancel -u usr9999 sshare Shows the Fair Share scores for all projects of which usr9999 is a member sshare -U usr9999 sinfo Shows the current state of the SLURM partitions sinfo sview Launches a GUI for monitoring SLURM jobs sview SBATCH flags \u00b6 Most of these commands have a single character shortcut that you can use instead. These can be found by running man sbatch While logged into NeSI. Command Description Example --job-name The name that will appear when using squeue or sacct #SBATCH --job-name=MyJob --account The account to which your core hours will be 'charged' #SBATCH --account=nesi9999 --time Job max walltime. After this duration, the job will be terminated #SBATCH --time=DD-HH:MM:SS --mem Memory required by the job. If exceeded the job will be termintated #SBATCH --mem=30GB --cpu Number of CPUs to use for multithreading #SBATCH --cpu=20 --partition NeSI partition for job to run (default is large ) #SBATCH --partition=long --output Path and name of standard output file #SBATCH --output=output.out --error Path and name of standard error file #SBATCH --output=output.err --mail-user Address to send mail notifications #SBATCH --mail-user=bob123@gmail.com --mail-type When to send mail notifications Will send a mail notification at BEGIN/END/FAIL Will send message at 80% walltime #SBATCH --mail-type=ALL #SBATCH --mail-type=TIME_LIMIT_80 --ntasks Number of MPI tasks to use It is very rare to use this option in bioinformatics","title":"Useful command line shortcuts"},{"location":"resources/command_line_shortcuts/#useful-command-line-shortcuts","text":"","title":"Useful command line shortcuts"},{"location":"resources/command_line_shortcuts/#overview","text":"Bash Nano SLURM commands SBATCH flags","title":"Overview"},{"location":"resources/command_line_shortcuts/#bash-commands","text":"Command Function Example cat Read and print contents of a file cat my_file.txt grep Search a file for lines containing text of interest grep \"words\" my_file.txt sed Find and replace text in a file sed 's/words/pictures/' my_file.txt head Print the first n lines from a file head -n 10 my_file.txt tail Print the last n lines from a file tail -n 10 my_file.tx t less Partially open a text file for viewing, reading lines as the user scrolls less my_file.txt cut Retrieve particular columns of a file. Uses tab as the delimiting character, can change with -d parameter cut -f1,2,5 -d \",\" my_file.txt paste Opposite of cut , stitch together files in a row-wise manner paste my_file.txt my_other_file.txt","title":"Bash commands"},{"location":"resources/command_line_shortcuts/#nano-shortcuts","text":"Command Function Ctrl + X Close the file Ctrl + O Save the file Ctrl + W Search for a string in the file Ctrl + Space Go forward one word in a line Alt + Space Go backwards one word in a line Ctrl + A Go to the beginning of a line Ctrl + E Go to the end of a line Ctrl + W , Ctrl + V Go to the last line of the file Ctrl + K Cut the current line and save it to the clipboard Ctrl + U Paste (**U**n-cut) the text in the clipboard Ctrl + Shift + 6 Select text","title":"Nano shortcuts"},{"location":"resources/command_line_shortcuts/#slurm-commands","text":"Command Function Example sbatch Submits a SLURM script to the queue manager sbatch job.sl squeue Display jobs in the queue Display jobs belonging to user usr9999 Display jobs on the long partition squeue squeue -u usr9999 squeue -p long sacct Displays all the jobs run by you that day Display job 123456789 sacct sacct -j 123456789 scancel Cancel a queued or running job Cancel all jobs belonging to usr9999 scancel 123456789 scancel -u usr9999 sshare Shows the Fair Share scores for all projects of which usr9999 is a member sshare -U usr9999 sinfo Shows the current state of the SLURM partitions sinfo sview Launches a GUI for monitoring SLURM jobs sview","title":"SLURM commands"},{"location":"resources/command_line_shortcuts/#sbatch-flags","text":"Most of these commands have a single character shortcut that you can use instead. These can be found by running man sbatch While logged into NeSI. Command Description Example --job-name The name that will appear when using squeue or sacct #SBATCH --job-name=MyJob --account The account to which your core hours will be 'charged' #SBATCH --account=nesi9999 --time Job max walltime. After this duration, the job will be terminated #SBATCH --time=DD-HH:MM:SS --mem Memory required by the job. If exceeded the job will be termintated #SBATCH --mem=30GB --cpu Number of CPUs to use for multithreading #SBATCH --cpu=20 --partition NeSI partition for job to run (default is large ) #SBATCH --partition=long --output Path and name of standard output file #SBATCH --output=output.out --error Path and name of standard error file #SBATCH --output=output.err --mail-user Address to send mail notifications #SBATCH --mail-user=bob123@gmail.com --mail-type When to send mail notifications Will send a mail notification at BEGIN/END/FAIL Will send message at 80% walltime #SBATCH --mail-type=ALL #SBATCH --mail-type=TIME_LIMIT_80 --ntasks Number of MPI tasks to use It is very rare to use this option in bioinformatics","title":"SBATCH flags"}]}