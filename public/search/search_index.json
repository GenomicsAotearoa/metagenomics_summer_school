{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Metagenomics Summer School Day Lesson overview   Day 1   1. Intro Session I (pre-summer school): Shell 2. Intro Session II. : HPC and HPC Job Scheduler3. Quality filtering raw reads4. Assembly5. Assembly (part 2)6. Evaluating the assemblies   Day 2   1. Introduction to binning2. Binning (continued)3. Bin dereplication4. Manually refining bins   Day 3   1. Identifying viral contigs in metagenomic data2. Assigning taxonomy to refined prokaryotic bins3. Gene prediction4. Gene annotation (part 1)5. Gene annotation (part 2) and coverage calculation   Day 4   1. Gene annotation (part 3)2. Presentation of data - Intro3. Presentation of data: Per-sample coverage heatmaps4. Presentation of data: Ordinations5. Presentation of data: KEGG pathway maps6. Presentation of data: Gene synteny7. Presentation of data: CAZY annotations heatmap Timetable 2022 Day 1 : 29th NovDay 2 : 30th NovDay 3 : 1st DecDay 4 : 2nd Dec Time Event Session leader 9:00 am \u2013 9:30 am Introduction- Welcome- Logging into NeSI Jian Sheng Boey 9:30 am \u2013 10:30 am TASK: Bash scripting and introduction to Slurm scheduler Dinindu Senanayake 10:30 am \u2013 10:50 am Morning tea break 10:50 am \u2013 11:10 am TASK: Bash scripting (continued) Dinindu Senanayake 11:10 am \u2013 11:40 pm TALK: The metagenomics decision tree Kim Handley 11:40 am \u2013 12:00 pm DISCUSSION Guided discussions on approaches to metagenomics analyses Kim Handley 12:00 pm \u2013 12:45 pm Break for lunch (lunch not provided) 12:45 pm \u2013 1:45 pm TALK: Quality filtering raw readsTASK: Visualisation with FastQCTASK: Read trimming and adapter removalTASK: Diagnosing poor librariesTALK: Common issues and best practiceTASK (Optional): Filtering out host DNA Annie West 1:45 pm \u2013 3:00 pm TALK: Assembly- Choice of assemblers- Considerations for parameters, and when to stop!TASK: Prepare data for assemblyTASK: Exploring assembler optionsTASK: Submitting jobs to NeSI via slurmTASK: Run SPAdes and IDBA-UD assemblyTASK (Optional): Submitting variant assemblies to NeSI Kim Handley 3:00 pm \u2013 3:20 pm Afternoon tea break (Tea, coffee, and snacks provided) 3:20 pm \u2013 5:00 pm TALK: Future considerations - co-assembly vs. single assembliesTASK: Assembly evaluationTASK: Short contig removal Jian Sheng Boey Kim Handley Time Event Session leader 9:00 am \u2013 9:15 am Introduction- Overview of yesterday, questions- Overview of today Annie West 9:15 am \u2013 9:30 am RECORDED PRESENTATION: Genomic adaptations enabling Acidithiobacillus distribution across wide ranging hot spring temperatures and pHs (Chanenath Sriaporn)  Kim Handley 9:30 am \u2013 10:30 am Binning (part 1)TALK: Overview of binning history- Key parameters and strategies for binningTASK: Read mapping Kim Handley 10:30 am \u2013 10:50 am Morning tea break (Tea, coffee, and snacks provided) 10:50 am \u2013 11:20 am Binning (part 2)TASK: Multi-binning strategy (Metabat and Maxbin) Cesar Facimoto 11:20 am \u2013 12:00 pm TALK: Overview of binning history (continued)- Key parameters and strategies for binning Kim Handley 12:00 pm \u2013 12:45 pm Break for lunch (lunch not provided) 12:45 pm \u2013 2:00 pm Binning (part 3)TASK: Bin dereplication via DAS_ToolTASK: Evaluating bins using CheckM Cesar Facimoto 2:00 pm - 2:30 pm Binning (part 4)TALK: Discuss additional dereplication strategies, such as dRepTALK: How to work with viral and eukaryotic binsTALK: Dealing with organisms which possess minimal genomes Jian Sheng Boey Annie West 2:30 pm - 3:00 pm TALK: Bin refinement- Refinement strategies 3:00 pm \u2013 3:20 pm Afternoon tea break (Tea, coffee, and snacks provided) 3:20 pm \u2013 5:00 pm TASK: Working with VizBin Jian Sheng Boey Annie West Time Event Session leader 9:00 am \u2013 9:20 am Introduction- Overview of yesterday, questions- Overview of today Jian Sheng Boey 9:20 am \u2013 10:30 am TALK: Bin taxonomic classification- Bin and species determinationTASK: Taxonomic classification using GTDB-TkTASK: View phylogenetic trait distribution using ANNOTREE David Waite 10:30 am \u2013 10:50 am Morning tea break (Tea, coffee, and snacks provided) 10:50 am \u2013 12:00 pm TALK: Identifying viruses from metagenomic dataTASK: Identifying viral contigs using VIBRANTRECORDED PRESENTATION: Genetic exchange in ultra-small Patescibacteria (Emilie Gios)TASK: QC of viral contigs using CheckVTASK: Taxonomic classification of viruses using vContact2 Jian Sheng BoeyAnnie West 12:00 pm \u2013 12:45 pm Break for lunch (lunch not provided) 12:45 pm - 1:30 pm TALK: Gene prediction using prodigal and other tools (RNAmer, Aragorn, etc)TASK: Predict open reading frames and protein sequences Jian Sheng Boey  Cesar Facimoto 1:30 pm - 2:00 pm TALK: Gene annotation (part 1) - MethodsTASK: Gene annotation using DIAMOND and HMMER3Discussion: Evaluating the quality of gene assignment Jian Sheng BoeyCesar Facimoto 2:00 pm \u2013 3:00 pm TALK: Gene annotation (part 2)- Using online resources (e.g. KEGG, BioCyc, MetaCyc, HydDB, PSORT)TASK: View KEGG annotation in KEGG website Jian Sheng BoeyCesar Facimoto 3:00 pm \u2013 3:20 pm Afternoon tea break 3:20 pm \u2013 4:30 pm TASK: MAG annotation with DRAMTASK: Coverage calculation using Bowtie2TASK: Introduce group project goalsTASK: Dividing into working groups / get a group nameTASK: Select a goal from your project Annie WestKim Handley 4:30 pm \u2013 5:00 pm End of day wrap up Kim Handley Time Event Session leader 9:00 am \u2013 9:15 am Introduction- Overview of yesterday, questions- Overview of today Jian Sheng Boey 9:15 am \u2013 10:00 am Presentation of dataTALK: DRAM results overviewTASK: Explore DRAM results Annie West 10:00 am \u2013 10:30 am Presentation of data (genome distributions)TALK: Visualising findings (environmental distribution)TASK: Coverage heatmap and nMDS ordination Kim Handley 10:30 am \u2013 10:50 am Morning tea break (Tea, coffee, and snacks provided)TASK: Workshop survey 10:50 am \u2013 12:00 pm Presentation of data (metabolism)TALK: Visualising findings (metabolic maps, genome features, metabolic schematics, and gene trees)TASK: KEGG metabolic pathwaysTASK: Gene syntenyTASK: CAZy heatmaps Cesar FacimotoJian Sheng Boey 12:00 pm \u2013 12:45 pm Break for lunch (lunch not provided) 12:45 pm \u2013 2:30 pm TASK: Analyse data for group workTASK: Prepare group presentation Kim Handley 2:30 pm \u2013 3:00 pm Present and discuss findingsTASK: Each group to give an informal presentation of their data Kim Handley 3:00 pm \u2013 3:20 pm Afternoon tea break (Tea, coffee, and snacks provided) 3:20 pm \u2013 3:40 pm Present and discuss findings (continued)TASK: Each group to give an informal presentation of their data Annie West 3:40 pm \u2013 4:00 pm End of day wrap up- Final discussion Kim HandleyJian Sheng Boey Appendices Appendix ID Appendix 1 Dereplicating data from multiple assemblies Appendix 2 Generating input files for \"VizBin\" from \"DAS_Tool\" curated bins Appendix 3 Normalise per-sample coverage values by average library size Appendix 4 Viral taxonomy prediction via vContact2 Appendix 5 Preparing input files for visualising gene synteny <p>Post-workshop survey</p> <p>Thank you for attending Metagenomics Summer School 2022! We would like your feedback on how we have done and what we can improve on. You can provide feedback here.</p> <p></p> <p>License</p> <p>Genomics Aotearoa / New Zealand eScience Infrastructure/ University of Auckland Metagenomics Summer School material is licensed under the GNU General Public License v3.0, 29 June 2007 . (Follow this link for more information)</p> <p></p> <p>Slides for workshop</p> <p>You can find a copy of the slides presented during the workshop, with published figures removed, in the slides/ folder.</p> <p></p> <p>Snapshots of results to download</p> <p>If you are having trouble downloading files using <code>scp</code>, we are providing exemplar output files, which you can download through your browser, here, or via the following links: </p> <ul> <li>FastQC results</li> <li>Quast results and required references</li> <li>Input files for VizBin</li> <li>Gene annotation tables</li> <li>DRAM output files</li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/","title":"Introduction to shell","text":"<p>This lesson will be covered/referred during pre-Summer School sessions. We will start Day 1 with Introduction to HPC &amp; HPC job scheduler</p> <p>Objectives</p> <ul> <li>Navigating your file system</li> <li>Copying, Moving, Renaming and Removing files</li> <li>Examining file contents</li> <li>Redirection, manipulation and extraction</li> <li>Text and file manipulation</li> <li>Loops</li> <li>Shell scripting</li> <li>Moving files between your laptop and NeSI</li> <li>Introduction to slurm</li> </ul> <p>Quick recap</p> <ul> <li>Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material</li> <li>Then open a Jupyter Terminal Session </li> <li>We do recommend referring to NeSI File system,..Symlinks Supplementary material first </li> <li>This lesson is a quick recap on basic/essential linux commands and will be covered during the pre-summer school sessions. </li> <li>If you would like to follow through and learn a bit more on this topic, refer to  Intermediate Shell for Bioinformatics material (or welcome to attend that workshop which will be offered two or three times per year)</li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#navigating-your-file-system","title":"Navigating your file system","text":"<ul> <li> <p>Check the current working directory.(terminal session  will be land on <code>/home</code> directory) <pre><code>pwd\n# /home/UserName/\n</code></pre></p> </li> <li> <p>Switch to individual working directory on nobackup ( below ) <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/$USER\n</code></pre></p> </li> <li> <p>OR you can navigate to above by using the symlink created as per instructions on  Supplementary material with just <code>cd ~/mgss</code></p> </li> <li> <p>Change the directory to <code>MGSS_Intro</code> <pre><code>cd MGSS_Intro/\n</code></pre></p> </li> <li> <p>Run the <code>ls</code> command to list the contents of the current directory. Check whether there are two .fastq files.</p> </li> <li> <p>The <code>mkdir</code> command (make directory) is used to make a directory. Enter <code>mkdir</code> followed by a space, then the directory name you want to create <pre><code>mkdir backup/\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#copying-moving-renaming-and-removing-files","title":"Copying, Moving, Renaming and Removing files","text":"<ul> <li> <p>Make a second copy of <code>SRR097977.fastq</code> and rename it as <code>Test_1_backup.fastq</code>. <pre><code>cp SRR097977.fastq Test_1_backup.fastq\n</code></pre></p> </li> <li> <p>Then move that file to <code>backup/</code> directory. <pre><code>mv Test_1_backup.fastq backup\n</code></pre></p> </li> <li> <p>Navigate to <code>backup/</code> directory and use <code>mv</code> command to rename and move <code>Test_1_backup.fastq</code> as <code>Test_1_copy.fastq</code> to the directory immediately above. <pre><code>cd backup/\n</code></pre> <pre><code>mv Test_1_backup.fastq ../Test_1_copy.fastq\n</code></pre></p> </li> <li> <p>Return to the directory immediately above, check whether the <code>Test_1_copy.fastq</code> was moved and renamed as instructed and remove it by using the <code>rm</code> command. <pre><code>cd ..\n\nrm Test_1_copy.fastq\n</code></pre></p> </li> <li> <p>See whether you can remove the <code>backup/</code> directory by using the <code>rm</code> command as well.  <pre><code>rm backup/\n# rm : can not remove 'backup/': Is a directory\n</code></pre></p> </li> <li> <p>By default, <code>rm</code> will not delete directories. This can be done by using <code>-r</code> (recursive) option. <pre><code>rm -r backup\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#examining-file-contents","title":"Examining file contents","text":"<ul> <li> <p>There are a number of ways to examine the content of a file. <code>cat</code> and <code>less</code> are two commonly used programs for a quick look. Check the content of <code>SRR097977.fastq</code> by using these commands. Take a note of the differences.  <pre><code>cat SRR097977.fastq\n# less SRR097977.fastq\n</code></pre></p> </li> <li> <p>A few useful shortcuts for navigating in <code>less</code></p> </li> </ul> <p></p> <ul> <li> <p>There are ways to take a look at parts of a file. For example, the <code>head</code> and <code>tail</code> commands will scan the beginning and end of a file, respectively.  <pre><code>head SRR097977.fastq\n\ntail SRR097977.fastq\n</code></pre></p> </li> <li> <p>Adding <code>-n</code> option to either of these commands will print the first or last n lines of a file. <pre><code>head -n 1 SRR097977.fastq\n# @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#redirection-and-extraction","title":"Redirection and extraction","text":"<ul> <li> <p>Although using <code>cat</code> and <code>less</code> commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is <code>grep</code>. Let's use this command to search for the string <code>NNNNNNNNNN</code> in <code>SRR098026.fastq</code> file. <pre><code>grep NNNNNNNNNN SRR098026.fastq\n</code></pre></p> </li> <li> <p>Retrieve and discuss the output you get when <code>grep</code> was executed with the <code>-B1</code> and <code>-A1</code> flags. <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq\n</code></pre></p> </li> <li> <p>In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is <code>&gt;</code>. Redirecting the string of bad reads that was searched using the <code>grep</code> command to a file named <code>bad_reads.txt</code> can be done with <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt\n</code></pre></p> </li> <li> <p>Use the <code>wc</code> command to count the number of words, lines and characters in the <code>bad_reads.txt</code> file. <pre><code>wc bad_reads.txt\n</code></pre></p> </li> <li> <p>Add <code>-l</code> flag to <code>wc</code> command and compare the number with the above output <pre><code>wc -l bad_reads.txt\n</code></pre></p> </li> <li> <p>In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of <code>&gt;&gt;</code> (append redirect) command which will append the new output to the end of the file, rather than overwriting it.  <pre><code>grep -B1 -A2 NNNNNNNNNN SRR097977.fastq &gt;&gt; bad_reads.txt\n</code></pre></p> </li> <li> <p>Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards, which are symbols or special characters that represent other characters. For an example. Using <code>*</code> wildcard, we can run the previous <code>grep</code> command on both files at the same time. <pre><code>grep -B1 -A2 NNNNNNNNNN *.fastq &gt;&gt; bad_reads.txt\nwc -l bad_reads.txt\n</code></pre></p> </li> <li> <p>The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The <code>|</code> (pipe) command is a commonly used method to apply an operation for an output without creating intermediate files. It takes the output generated by one command and uses it as the input to another command.  <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#text-and-file-manipulation","title":"Text and file manipulation","text":"<p>There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below.</p>"},{"location":"day1/ex1_bash_and_scheduler/#cut","title":"Cut","text":"<p>The <code>cut</code> command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that <code>cut</code> uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the <code>-d</code> parameter.</p> <p>See what results you get from the file <code>names.txt</code>.</p> <pre><code>cat names.txt\n\ncut -d \" \" -f 1 names.txt\ncut -d \" \" -f 1-3 names.txt\ncut -d \" \" -f 1,3 names.txt\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#basename","title":"basename","text":"<p><code>basename</code> is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use <code>basename</code> to remove the .fastq extension from the files that we've been working with.</p> <pre><code>basename SRR097977.fastq .fastq\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#sed","title":"sed","text":"<p><code>sed</code> is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the <code>sed</code> command in UNIX is for substitution or for find and replace. By using <code>sed</code> you can edit files even without opening them, which is extremely important when working with large files.</p> <ul> <li>Some find and replace examples</li> </ul> <p>Find and replace all <code>chr</code> to <code>chromosome</code> in the example.bed file and append the the edit to a new file named example_chromosome.bed</p> <p><pre><code>sed 's/chr/chromosome/g' example.bed &gt; example_chromosome.bed\n</code></pre> Find and replace <code>chr</code> to <code>chromosome</code>, only if you also find 40 in the line</p> <p><pre><code>sed '/40/s/chr/chromosome/g' example.bed &gt; example_40.bed\n</code></pre> Find and replace directly on the input, but save an old version too</p> <pre><code>sed -i.old 's/chr/chromosome/g' example.bed\n</code></pre> <p><code>-i</code> to edit files in-place instead of printing to standard output</p> <ul> <li>Print specific lines of the file</li> </ul> <p>To print a specific line you can use the address function. Note that by default, <code>sed</code> will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option <code>-n</code> </p> <p><code>-n</code>, <code>--quiet</code>, <code>--silent</code> = suppress automatic printing of pattern space</p> <p>print 5th line of example.bed</p> <pre><code>sed -n '5p' example.bed\n</code></pre> <p>We can provide any number of additional lines to print using <code>-e</code> option. Let's print line 2 and 5, </p> <pre><code>sed -n -e '2p' -e '5p' example.bed\n</code></pre> <p>It also accepts range, using <code>,</code>. Let's print line 2-6,</p> <pre><code>sed -n '2,6p' example.bed\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#loops","title":"Loops","text":"<p>Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in <code>bash</code> scripting,</p> Types of Loops <code>for</code> <code>while</code><code>until</code> <p>iterates over a list of items and performs the given set of commands <pre><code>for item in [LIST]\ndo\n    [COMMANDS]\ndone\n</code></pre> For most of our uses, a <code>for loop</code> is sufficient for our needs, so that is what we will be focusing on for this exercise.</p> <p>Shell identifies the <code>for</code> command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the <code>do</code> and <code>done</code> keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the <code>$</code> character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example</p> <pre><code>i=\"MG Summer School\"\necho i\n# i\necho $i\n# MG Summer School\necho ${i}\n# MG Summer School\n</code></pre> <p>This prevents the shell interpreter from treating <code>i</code> as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files:</p> <p><pre><code>for filename in *.fastq\ndo\n    head -n 2 ${filename}\ndone\n</code></pre> Another useful command to be used with <code>for</code> loops is <code>basename</code> which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed).</p> <pre><code>basename SRR097977.fastq .fastq\n</code></pre> <p><code>basename</code> is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things</p> <pre><code>for filename in *.fastq\ndo\n    name=$(basename ${filename} .fastq)\n    echo ${name}\ndone\n</code></pre> <p>Performs a given set of commands an unknown number of times as long as the given condition evaluates is true <pre><code>while [CONDITION]\ndo\n    [COMMANDS]\ndone\n</code></pre></p> <p>Execute a given set of commands as longs as the given condition evaluates to false</p>"},{"location":"day1/ex1_bash_and_scheduler/#scripts","title":"Scripts","text":"<p>Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal.</p> <p>The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as <code>nano</code>, <code>emacs</code>, and <code>vi</code> provide the platforms to write scripts. For this workshop we will use <code>nano</code> to create a file named <code>ForLoop.sh</code>.</p> <pre><code>nano ForLoop.sh\n</code></pre> <p>Add the following for-loop to the script (note the header <code>#!/bin/bash</code>).</p> <pre><code>#!/bin/bash\nfor filename in *.fastq\ndo\nhead -n 2 ${filename}\ndone\n</code></pre> <p>Because <code>nano</code> is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing <code>Ctrl + O</code>, then exit <code>nano</code> using <code>Ctrl + X</code>. If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word.</p> <p>Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. </p> <pre><code>bash ForLoop.sh\n</code></pre> <p>Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the <code>ls -l ForLoop.sh</code> command and evaluate the first part of the output</p> <pre><code>ls -l ForLoop.sh # -rw-rw-r-- 1 user user 88 Dec  6 19:52 ForLoop.sh\n</code></pre> <p>There are three file permission flags that a file we create on NeSI can possess. Two of these, the read (<code>r</code>) and write (<code>w</code>) are marked for the <code>ForLoop.sh</code> file .The third flag, executable (<code>x</code>) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using <code>chmod</code> command. Add the executable permissions (<code>+x</code>) to <code>ForLoop.sh</code> and run <code>ls</code> again to see what has changed.</p> <pre><code>chmod +x ForLoop.sh\nls -l ForLoop.sh # -rwxrwxr-x 1 user user 88 Dec  6 19:52 ForLoop.sh\n</code></pre> <p>Re-open the file in <code>nano</code> and append the output to TwoLines.txt, save and exit</p> <pre><code>#!/bin/bash\nfor filename in *.fastq\ndo\nhead -n 2 ${filename} &gt;&gt; TwoLines.txt\ndone\n</code></pre> <p>Execute the file <code>ForLoop.sh</code>. We'll need to put <code>./</code> at the beginning so the computer knows to look here in this directory for the program.</p> <pre><code>./ForLoop.sh\n</code></pre> <p>Cheat sheet</p> Key commands for navigating around the filesystem are:Other useful commands: <ul> <li><code>ls</code> - list the contents of the current directory</li> <li><code>ls -l</code> - list the contents of the current directory in more detail</li> <li><code>pwd</code> - show the location of the current directory</li> <li><code>cd DIR</code> - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type <code>ls</code> OR you need to specify either a full or relative path to DIR)</li> <li><code>cd -</code> - change back to the last directory you were in</li> <li><code>cd</code> (also <code>cd ~/</code>) - change to your home directory</li> <li><code>cd ..</code> - change to the directory one level above</li> </ul> <ul> <li><code>mv</code> - move files or directories</li> <li><code>cp</code> - copy files or directories</li> <li><code>rm</code> - delete files or directories</li> <li><code>mkdir</code> - create a new directory</li> <li><code>cat</code> - concatenate and print text files to screen</li> <li><code>more</code> - show contents of text files on screen</li> <li><code>less</code> - cooler version of <code>more</code>. Allows searching (use <code>/</code>)</li> <li><code>tree</code> - tree view of directory structure</li> <li><code>head</code> - view lines from the start of a file</li> <li><code>tail</code> - view lines from the end of a file</li> <li><code>grep</code> - find patterns within files</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/","title":"Introduction to HPC &amp; HPC job scheduler","text":""},{"location":"day1/ex2_1_intro_to_scheduler/#defining-high-performance-computing","title":"Defining high-performance computing","text":"<p>The simplest way of defining high-performance computing is by saying that it is the using of high-performance computers (HPC). However, this leads to our next question what is a HPC .</p> <p>HPC</p> <p>A high-performance computer is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer.</p> <p></p> <p>While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time.</p> <p>One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster.</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#hpc-architectures","title":"HPC architectures","text":"<p>Most HPC systems follow the ideas described above of taking many computers and linking them via network switches.  described above is:</p> <p></p> <p>What distinguishes a high-performance computer from the computer clusters</p> <ul> <li>The number of computers/nodes </li> <li>The strength of each individual computer/node </li> <li>The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit.</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#nesi-mahuika-cluster-architecture","title":"NeSI Mahuika Cluster architecture","text":"<p>NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> </ul> Overview of HPC ArchitectureComposition of a nodeIn reality <p> </p> <p> </p> <p> </p>"},{"location":"day1/ex2_1_intro_to_scheduler/#from-hardware-to-software","title":"From Hardware to Software","text":"<p>Over 90% HPCs &amp; supercomputers employ Linux as their operating system.  Linux has four essential properties which make it an excellent operating system for the HPCs &amp;  science community:</p> PerformanceFunctionalityFlexibilityPortability <p>Performance of the operating system can be optimized for specific tasks such as running small portable devices or large supercomputers.</p> <p>A number of community-driven scientific applications and libraries have been developed under Linux such as molecular dynamics, linear algebra, and fast-Fourier transforms.</p> <p>The system is flexible enough to allow users to build applications with a wide array of support tools such as compilers, scientific libraries, debuggers, and network monitors.</p> <p>The operating system, utilities, and libraries have been ported to a wide variety of devices including desktops, clusters, supercomputers, mainframes, embedded systems, and smart phones.</p> <p>The Linux operating system is made up of three parts; the kernel, the shell and the software</p> <p>Kernel \u2212 The kernel is the heart of the operating system. It interacts with the hardware and most of the tasks like memory management, task scheduling and file management.</p> <p>Shell \u2212 The shell is the utility that processes your requests (acts as an interface between the user and the kernel). When you type in a command at your terminal, the shell interprets (operating as in interpreter) the command and calls the program that you want. The shell uses standard syntax for all commands. The shell recognizes a limited set of commands, and you must give commands to the shell in a way that it understands: Each shell command consists of a command name, followed by command options (if any are desired) and command arguments (if any are desired). The command name, options, and arguments, are separated by blank space. </p> <ul> <li>An interpreter operates in a simple loop: It accepts a command, interprets the command, executes the command, and then waits for another command. The shell displays a \"prompt,\" to notify you that it is ready to accept your command.  </li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#accessing-software-via-modules","title":"Accessing software via modules","text":"<p>On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run.</p> <p>Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are:</p> <ul> <li>software incompatibilities</li> <li>versioning</li> <li>dependencies</li> </ul> <p>One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</p> <p>There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is <code>Lmod</code> where the <code>module</code> command is used to interact with environment modules.</p> <p>Viewing, Accessing and Deploying software with <code>module</code> command\"</p> <ul> <li>View available modules</li> </ul> <pre><code>#View all modules\n$ module avail\n\n# View all modules which match the keyword in their name\n$ module avail KEYWORD\n\n# View all modules which match the keyword in their name or description\n$ module spider KEYWORD\n</code></pre> <ul> <li> <p>Load a specific program</p> <p>All module names on NeSI Software stack have a version and toolchain/environment suffixes. If none is specified, then the default version of the software is loaded. The default version can be seen with the <code>module avail modulename</code> command (corresponding module name will have <code>(D)</code> suffix)</p> </li> </ul> <pre><code>$ module load Module_Name\n</code></pre> <ul> <li>Unload all current modules</li> </ul> <pre><code>$ module purge\n</code></pre> <p>Please do not use <code>module --force purge</code></p> <ul> <li>Swap a currently loaded module for a different one</li> </ul> <pre><code>$ module switch CURRENT_MODULE DESIRED_MODULE\n</code></pre>"},{"location":"day1/ex2_1_intro_to_scheduler/#working-with-job-scheduler","title":"Working with job scheduler","text":"<p>An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a </p> <ul> <li>Mechanism to control access by many users to shared computing resources</li> <li>Queuing / scheduling system for users\u2019 jobs</li> <li>Manages the reservation of resources and job execution on these resources </li> <li>Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d)</li> </ul> <p>Why do we need a scheduler ?</p> <ul> <li>To ensure the machine is utilised as fully as possible</li> <li>To ensure all users get a fair chance to use compute resources (demand usually exceeds supply)</li> <li>To track usage - for accounting and budget control</li> <li>To mediate access to other resources e.g. software licenses</li> </ul> <p>Commonly used schedulers</p> <ul> <li>Slurm</li> <li>PBS , Torque</li> <li>Grid Engine</li> <li>LSF \u2013 IBM Systems</li> </ul> <p></p> <p>Researchers can not communicate directly to  Compute nodes from the login node. Only way to establish a connection OR send scripts to compute nodes is to use scheduler as the carrier/manager</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#life-cycle-of-a-slurm-job","title":"Life cycle of a slurm job","text":"Commonly used Slurm commands Command Function <code>sbatch</code> Submit non-interactive (batch) jobs to the scheduler <code>squeue</code> List jobs in the queue <code>scancel</code> Cancel a job <code>sacct</code> Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database <code>srun</code> Slurm directive for parallel computing <code>sinfo</code> Query the current state of nodes <code>salloc</code> Submit interactive jobs to the scheduler <p>About</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0","text":"<p>As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command (<code>sbatch</code> in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to <code>sbatch</code> should be prepended with <code>#SBATCH</code>.</p> <p> </p> <p>Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The <code>#SBATCH</code> parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with <code>#</code>. This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the <code>#!</code>, then uses the program at the subsequent path to interpret the script, in our case <code>/bin/bash</code> (the program <code>bash</code> found in the /bin directory</p> Slurm variables header use description --job-name <code>#SBATCH --job-name=MyJob</code> The name that will appear when using <code>squeue</code> or <code>sacct</code>. --account <code>#SBATCH --account=nesi12345</code> The account your core hours will be 'charged' to. --time <code>#SBATCH --time=DD-HH:MM:SS</code> Job max walltime. --mem <code>#SBATCH --mem=512MB</code> Memory required per node. --cpus-per-task <code>#SBATCH --cpus-per-task=10</code> Will request 10 logical CPUs per task. --output <code>#SBATCH --output=%j_output.out</code> Path and name of standard output file. <code>%j</code> will be replaced by the job ID. --mail-user <code>#SBATCH --mail-user=me23@gmail.com</code> address to send mail notifications. --mail-type <code>#SBATCH --mail-type=ALL</code> Will send a mail notification at BEGIN END FAIL. <code>#SBATCH --mail-type=TIME_LIMIT_80</code> Will send message at 80% walltime. <p></p> <p>Assigning values to Slurm variables</p> <p></p> <p></p> <p></p> Exercise <ul> <li>Make sure you are in your working directory </li> <li>There is a <code>Slurm_Intro</code> directory which has the following directory structure </li> </ul> <p></p> <ul> <li>We will work from 1. to 4. . Objective is to review, submit, check and review Slurm jobs. </li> </ul> <p></p>"},{"location":"day1/ex2_quality_filtering/","title":"Quality filtering raw reads","text":"<p>Objectives</p> <ul> <li>Visualising raw reads with <code>FastQC</code></li> <li>Read trimming and adapter removal with <code>trimmomatic</code></li> <li>Diagnosing poor libraries</li> <li>Understand common issues and best practices</li> <li>Optional: Filtering out host DNA with <code>BBMap</code></li> </ul> <p> </p>"},{"location":"day1/ex2_quality_filtering/#visualising-raw-reads","title":"Visualising raw reads","text":"<p><code>FastQC</code> is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues:</p> <ol> <li>Adapter/barcode sequences</li> <li>Low quality regions of sequence</li> <li>Quality drop-off towards the end of read-pair sequence</li> </ol>"},{"location":"day1/ex2_quality_filtering/#loading-fastqc","title":"Loading FastQC","text":"<p>These exercises will take place in the <code>2.fastqc/</code> directory. First, navigate to this directory. Copy the command below into your terminal (logged in to NeSI), replacing <code>&lt;YOUR FOLDER&gt;</code>, and then running the command.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/\n</code></pre> <p>To activate <code>FastQC</code> on NeSI, you need to first load the module using the command</p> <pre><code>module purge\nmodule load FastQC/0.11.9\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#running-fastqc","title":"Running FastQC","text":"<p>We will run <code>FastQC</code> from the command line as follows:</p> <pre><code>fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#viewing-the-outputs-from-fastqc","title":"Viewing the outputs from FastQC","text":"<p><code>FastQC</code> generates output reports in <code>.html</code> files that can be viewed in a standard web browser. </p> <p>Fortunately, if you're currently using the terminal within <code>Jupyter hub</code> for today's session, we can open the <code>.html</code> files directly from here:</p> <ul> <li>Click on the directory icon in the top left to open the directory navigator pane (if not already open).</li> <li>The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). </li> <li>Click through <code>MGSS_U</code>, into your directory, and then into the <code>2.fastqc/</code> directory. </li> <li>Double click on the output <code>...fastqc.html</code> files to open them in the a new tab within the <code>Jupyter hub</code>.</li> </ul> <p>Examples of the output files are also available for download here.</p> <p>Note that <code>FastQC</code> does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. </p> <p> </p> <p>At a first glance, we can see the follow statistics:</p> <ol> <li>The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming.</li> <li>There are 100,000 reads in the file</li> <li>The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing.</li> </ol> <p>Have a quick look through the menu sidebar on the left. As you can see, the data has passed most of the basic parameters.</p> <p>Per-base sequence quality</p> <p> </p> <p>Per base sequence content</p> <p> </p> <p>Adapter Content</p> <p> </p> <p>Per sequence GC content</p> <p> </p> <p>The only aspect of the data that <code>FastQC</code> is flagging as potentially problematic is the GC% content of the data set. This observation is expected as we deal with a mixed community and organisms. Therefore, it is unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of <code>FastQC</code>, but completely consistent with the biology of the system.</p>"},{"location":"day1/ex2_quality_filtering/#fastqc-outputs-for-libraries-with-errors","title":"FastQC outputs for libraries with errors","text":"<p>Lets take a look at a library with significant errors. Process the sequence file <code>mock_R1.adapter_decay.fastq</code> with <code>FastQC</code>.</p> <pre><code>fastqc mock_R1.adapter_decay.fastq.gz\n</code></pre> <p>Compare the results with the <code>mock_R1.good.fastq.gz</code> file.</p> <p>Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not?</p> <p>Per-base sequence quality</p> <p> </p>"},{"location":"day1/ex2_quality_filtering/#read-trimming-and-adapter-removal-with-trimmomatic","title":"Read trimming and adapter removal with trimmomatic","text":"<p>There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use <code>trimmomatic</code>, but this should in no way be interpreted as an endorsement of <code>trimmomatic</code> over equivalent tools like <code>BBMap</code>, <code>sickle</code>, <code>cutadapt</code> or any other.</p> <p>For a first run with <code>trimmomatic</code>, type the following commands into your console:</p> <pre><code>module load Trimmomatic/0.39-Java-1.8.0_144\n\ntrimmomatic PE -threads 2 -phred33 \\\nmock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\\nmock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\\nHEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80\n</code></pre> <p>There is a lot going on in this command, so here is a breakdown of the parameters in the command above</p> Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 10 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used mock_R1.adapter_decay.fastq.gz / mock_R2.adapter_decay.fastq.gz positional The paired forward and reverse reads to trim mock_R1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner also passed mock_s1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) mock_R2.qc.fastq.gz / mock_s2.qc.fastq.gz positional The reverse-sequence equivalent of above HEADCROP:10 positional Adapter trimming command. Remove the first 10 positions in the sequence SLIDINGWINDOW:4:30 positional Quality filtering command. Analyse each sequence in a 4 base pair sliding window and then truncate if the average quality drops below Q30 MINLEN:80 positional Length filtering command. Discard sequences that are shorter than 80 base pairs after trimming <p>Terminal output</p> <pre><code>TrimmomaticPE: Started with arguments:\n -threads 10 -phred33 mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz HEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80\nInput Read Pairs: 100000 Both Surviving: 75684 (75.68%) Forward Only Surviving: 12783 (12.78%) Reverse Only Surviving: 2311 (2.31%) Dropped: 9222 (9.22%)\nTrimmomaticPE: Completed successfully\n</code></pre> <p>Running the trimmed files back through <code>FastQC</code>, we can see that this signifcantly improves the output.</p> <p>Quality of reads</p> <p> </p> <p>Nucleotide distribution</p> <p> </p>"},{"location":"day1/ex2_quality_filtering/#considerations-when-working-with-trimmomatic","title":"Considerations when working with trimmomatic","text":"<p>Order of operations</p> <p>The basic format for a <code>trimmomatic</code> command is</p> <pre><code>trimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; &lt;trimming parameters&gt;\n</code></pre> <p>The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users.</p> <p>For example, consider these two scenarios:</p> <pre><code>trimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; SLIDINGWINDOW:4:30 MINLEN:80\n\ntrimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; MINLEN:80 SLIDINGWINDOW:4:30\n</code></pre> <p>In the first run, we would not expect any sequence shorter than 80 base pairs to exist in the output files. However, we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming, the <code>MINLEN</code> filtering does not execute a second time. In the first instance, we do not perform trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the <code>MINLEN</code> run.</p> <p>A more subtle consequence of this behaviour is the interplay between adapter removal and quality filtering. In the command we ran, we try to identify adapters before quality trimming. Why do you think this is?</p>"},{"location":"day1/ex2_quality_filtering/#optional-working-with-the-illuminaclip-command","title":"Optional: Working with the ILLUMINACLIP command","text":"<p><code>Trimmomatic</code> also provides the <code>ILLUMINACLIP</code> command, which can be used to pass a FASTA file of adapter and barcode sequences to be found and removed from your data. The format for the <code>ILLUMINACLIP</code> parameter can be quite confusing to work with and so we generally favour a <code>HEADCROP</code> where possible. If you wish to work with the <code>ILLUMINACLIP</code> command, then you can see its use here in the <code>trimmomatic</code> manual.</p> <pre><code>ILLUMINACLIP:iua.fna:1:25:7\n             |       | |  |\n|       | |  |\n|       | |  Simple clip threshold\n             |       | Palindrome clip threshold\n             |       Seed mismatches\n             File of expected sequences\n</code></pre> <p>There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. Where possible, we favour the use of simple positional trimming.</p>"},{"location":"day1/ex2_quality_filtering/#diagnosing-poor-libraries","title":"Diagnosing poor libraries","text":"<p>Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating <code>FastQC</code>:</p> <ol> <li>Does the sequencing length match what you ordered from the facility?</li> <li>If the sequences are shorter than expected, is adapter read-through a concern?</li> <li>What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation?</li> <li>Are adapters and/or barcodes removed? <ul> <li>Look at the Per base sequence content to diagnose this.</li> </ul> </li> <li>Is there unexpected sequence duplication? <ul> <li>This can occur when low-input library preparations are used.</li> </ul> </li> <li>Are over-represented k-mers present? <ul> <li>This can be a sign of adapter and barcode contamination.</li> </ul> </li> </ol>"},{"location":"day1/ex2_quality_filtering/#understand-common-issues-and-best-practices","title":"Understand common issues and best practices","text":"<ol> <li>Do I need to remove (rare) adapters?</li> <li>I don\u2019t know if adapters have been removed or not</li> <li>How do I identify and remove adapter read-through</li> <li>Identifying incomplete barcode/adapter removal</li> <li>Over aggressive trimming</li> <li>GC skew is outside of expected range</li> </ol>"},{"location":"day1/ex2_quality_filtering/#optional-filtering-out-host-dna","title":"Optional: Filtering out host DNA","text":"<p>Metagenome data derived from host-associated microbial communities should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species.</p> <p>There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. </p> <p>Note</p> <p>This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via <code>Kraken2</code> and then filtering out all reads that map to the phylum or kingdom of your host taxa).</p> <p>This exercise provides an example using <code>BBMap</code> to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping the quality-filtered reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that:</p> <ul> <li>are presumed microbial contaminant in the reference</li> <li>have high homology to microbial genes/genomes (e.g. ribosomes)</li> <li>those that are of low complexity </li> </ul> <p>This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed.</p> <p>Note</p> <p>The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using <code>bbmask</code>. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread.*</p>"},{"location":"day1/ex2_quality_filtering/#downloading-the-masked-human-reference-genome","title":"Downloading the masked human reference genome","text":"<p>The masked reference genome is available via Google drive and Zenodo. For this workshop, we have provided you with the file within the <code>4.evaluation/BBMask_human_reference/</code> directory.</p> Downloading your own copy of the masked genome <p>We can use <code>gdown</code> to download this file from Google drive via the command line.</p> <p>To install <code>gdown</code>, we can use <code>pip</code>. </p> <pre><code># Install gdown (for downloading from google drive)\nmodule purge\nmodule load Python/3.10.5-gimkl-2022a\npip install --user gdown\n</code></pre> <p>Next, download the reference. It will also be necessary to first add your local <code>bin</code> location to the <code>PATH</code> variable via the <code>export PATH=...</code> command, as this is where <code>gdown</code> is located (modify <code>&lt;your_username&gt;</code> before running the code below).</p> <pre><code>mkdir BBMask_human_reference/\ncd BBMask_human_reference/\n\nexport PATH=\"/home/&lt;your_username&gt;/.local/bin:$PATH\"\ngdown https://drive.google.com/uc?id=0B3llHR93L14wd0pSSnFULUlhcUk\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#indexing-the-reference-genome-and-read-mapping-with-bbmap","title":"Indexing the reference genome and read mapping with BBMap","text":"<p>We will cover more about read mapping in later exercises. For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a <code>BBMap</code> index, and then use <code>BBMap</code> to map the quality-filtered reads to that index, ultimately retaining only those reads that do not map to the index.</p> <p>Build index reference via <code>BBMap</code>. We will do this by submitting the job via slurm. </p> <p>Note</p> <p>See Preparing an assembly job for slurm for more information about how to submit a job via slurm.</p> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own directory.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      host_filt_bbmap_index\n#SBATCH --time          00:20:00\n#SBATCH --mem           32GB\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/BBMask_human_reference/\n\n# Load BBMap module\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Build indexed reference file via BBMap\nbbmap.sh ref=hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz\n</code></pre> <p>Finally, map the quality-filtered reads to the reference via <code>BBMap</code>. Here we will submit the job as a slurm array, with one array job per sample. </p> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own directory.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      host_filt_bbmap_map\n#SBATCH --time          01:00:00\n#SBATCH --mem           27GB\n#SBATCH --array         1-4\n#SBATCH --cpus-per-task 20\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Set up working directories\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/\nmkdir -p host_filtered_reads/\n\n# Load BBMap module\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Run bbmap\nsrun bbmap.sh -Xmx27g -t=$SLURM_CPUS_PER_TASK \\\nminid=0.95 maxindel=3 bwr=0.16 bw=12 quickmatch fast minhits=2 qtrim=rl trimq=10 untrim \\\nin1=../3.assembly/sample${SLURM_ARRAY_TASK_ID}_R1.fastq.gz \\\nin2=../3.assembly/sample${SLURM_ARRAY_TASK_ID}_R2.fastq.gz \\\npath=BBMask_human_reference/ \\\noutu1=host_filtered_reads/sample${SLURM_ARRAY_TASK_ID}_R1_hostFilt.fastq \\\noutu2=host_filtered_reads/sample${SLURM_ARRAY_TASK_ID}_R2_hostFilt.fastq\n</code></pre> <p>Breaking down this command a little:</p> <ul> <li>We pass the path to the <code>ref</code> directory (the reference we just built) to <code>path=...</code>.</li> <li>Provide quality-filtered reads as input (i.e. output of the <code>trimmomatic</code> process above). In this case, we will provide the FASTQ files located in <code>../3.assembly/</code> which have been processed via <code>trimmomatic</code> in the same manner as the exercise above. These are four sets of paired reads (representing metagenome data from four \"samples\") that the remainder of the workshop exercises will be working with.</li> <li>The flags <code>-Xmx27g</code> and <code>-t=$SLURM_CPUS_PER_TASK</code> set the maximum memory and thread (AKA CPUs) allocations, and must match the <code>--mem</code> and <code>--cpus_per_task</code> allocations in the slurm headers at the top of the script.</li> <li>The rest of the settings in the <code>BBMap</code> call here are as per the recommendations found within this thread about processing data to remove host reads.</li> <li>Finally, the filtered output FASTQ files for downstream use are written to the <code>host_filtered_reads/</code> directory (taken from the outputs <code>outu1=</code> and <code>outu2=</code>, which include only those reads that did not map to the host reference genome).</li> </ul> <p>Note</p> <p>Slurm array jobs automatically create a variable <code>SLURM_ARRAY_TASK_ID</code> for that job, which contains the array task number (i.e. between 1 and 4 in the case above). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" (<code>sample${SLURM_ARRAY_TASK_ID}</code> is read in as <code>sample3</code>).</p> <p>The filtered reads are now available in <code>host_filtered_reads/</code> for downstream use.</p>"},{"location":"day1/ex3_assembly/","title":"Assembly","text":"<p>Objectives</p> <ul> <li>Become familiar with the standard input files for <code>SPAdes</code> and <code>IDBA-UD</code></li> <li>Understand the basic parameters that should be modified when using these assemblers</li> <li>Prepare an assembly job to run under slurm</li> </ul> <p> </p> <p>All work for this exercise will occur in the <code>3.assembly/</code> directory.</p>"},{"location":"day1/ex3_assembly/#the-standard-input-files-for-spades-and-idba-ud","title":"The standard input files for SPAdes and IDBA-UD","text":"<p>Although they both make use of the same types of data, both <code>SPAdes</code> and <code>IDBA-UD</code> have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by <code>SPAdes</code>:</p> <pre><code>module purge\nmodule load SPAdes/3.15.4-gimkl-2022a-Python-3.10.5\n</code></pre> <code>spades -h</code> <pre><code>spades.py -h\n# ...\n# Input data:\n#   --12 &lt;filename&gt;             file with interlaced forward and reverse paired-end reads\n#   -1 &lt;filename&gt;               file with forward paired-end reads\n#   -2 &lt;filename&gt;               file with reverse paired-end reads\n#   -s &lt;filename&gt;               file with unpaired reads\n#   --merged &lt;filename&gt;         file with merged forward and reverse paired-end reads\n#   --pe-12 &lt;#&gt; &lt;filename&gt;      file with interlaced reads for paired-end library number &lt;#&gt;.\n#                               Older deprecated syntax is -pe&lt;#&gt;-12 &lt;filename&gt;\n#   --pe-1 &lt;#&gt; &lt;filename&gt;       file with forward reads for paired-end library number &lt;#&gt;.\n#                               Older deprecated syntax is -pe&lt;#&gt;-1 &lt;filename&gt;\n#   --pe-2 &lt;#&gt; &lt;filename&gt;       file with reverse reads for paired-end library number &lt;#&gt;.\n#                               Older deprecated syntax is -pe&lt;#&gt;-2 &lt;filename&gt;\n#   --pe-s &lt;#&gt; &lt;filename&gt;       file with unpaired reads for paired-end library number &lt;#&gt;.\n#                               Older deprecated syntax is -pe&lt;#&gt;-s &lt;filename&gt;\n#   --pe-m &lt;#&gt; &lt;filename&gt;       file with merged reads for paired-end library number &lt;#&gt;.\n#                               Older deprecated syntax is -pe&lt;#&gt;-m &lt;filename&gt;\n#   --pe-or &lt;#&gt; &lt;or&gt;            orientation of reads for paired-end library number &lt;#&gt;\n#                               (&lt;or&gt; = fr, rf, ff).\n#                               Older deprecated syntax is -pe&lt;#&gt;-&lt;or&gt;\n#   --s &lt;#&gt; &lt;filename&gt;          file with unpaired reads for single reads library number &lt;#&gt;.\n#                               Older deprecated syntax is --s&lt;#&gt; &lt;filename&gt;\n#   --mp-12 &lt;#&gt; &lt;filename&gt;      file with interlaced reads for mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -mp&lt;#&gt;-12 &lt;filename&gt;\n#   --mp-1 &lt;#&gt; &lt;filename&gt;       file with forward reads for mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -mp&lt;#&gt;-1 &lt;filename&gt;\n#   --mp-2 &lt;#&gt; &lt;filename&gt;       file with reverse reads for mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -mp&lt;#&gt;-2 &lt;filename&gt;\n#   --mp-s &lt;#&gt; &lt;filename&gt;       file with unpaired reads for mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -mp&lt;#&gt;-s &lt;filename&gt;\n#   --mp-or &lt;#&gt; &lt;or&gt;            orientation of reads for mate-pair library number &lt;#&gt;\n#                               (&lt;or&gt; = fr, rf, ff).\n#                               Older deprecated syntax is -mp&lt;#&gt;-&lt;or&gt;\n#   --hqmp-12 &lt;#&gt; &lt;filename&gt;    file with interlaced reads for high-quality mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -hqmp&lt;#&gt;-12 &lt;filename&gt;\n#   --hqmp-1 &lt;#&gt; &lt;filename&gt;     file with forward reads for high-quality mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -hqmp&lt;#&gt;-1 &lt;filename&gt;\n#   --hqmp-2 &lt;#&gt; &lt;filename&gt;     file with reverse reads for high-quality mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -hqmp&lt;#&gt;-2 &lt;filename&gt;\n#   --hqmp-s &lt;#&gt; &lt;filename&gt;     file with unpaired reads for high-quality mate-pair library number &lt;#&gt;.\n#                               Older deprecated syntax is -hqmp&lt;#&gt;-s &lt;filename&gt;\n#   --hqmp-or &lt;#&gt; &lt;or&gt;          orientation of reads for high-quality mate-pair library number &lt;#&gt;\n#                               (&lt;or&gt; = fr, rf, ff).\n#                               Older deprecated syntax is -hqmp&lt;#&gt;-&lt;or&gt;\n#   --sanger &lt;filename&gt;         file with Sanger reads\n#   --pacbio &lt;filename&gt;         file with PacBio reads\n#   --nanopore &lt;filename&gt;       file with Nanopore reads\n#   --trusted-contigs &lt;filename&gt;\n#                               file with trusted contigs\n#   --untrusted-contigs &lt;filename&gt;\n#                               file with untrusted contigs\n# ...\n</code></pre> <p>At a glance, you could provide any of the following data types to <code>SPAdes</code> and have it perform an assembly:</p> <ol> <li>Illumina paired-end sequencing data, either as standard library or Mate Pairs</li> <li>Sanger sequences</li> <li>PacBio reads</li> <li>Oxford Nanopore reads</li> <li>Pre-assembled scaffolds for guiding the assembly</li> </ol> <p>Awkwardly, while <code>SPAdes</code> accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the <code>-meta</code> flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing.</p> <pre><code>cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz &gt; for_spades_R1.fq.gz\ncat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz &gt; for_spades_R2.fq.gz\n</code></pre> <p>Note that these FASTQ files are compressed, yet we can concatenate them together with the <code>cat</code> command regardless. This is a nice feature of <code>.gz</code> files that is handy to remember.</p> <p>By contrast, what does <code>IDBA-UD</code> accept?</p> <pre><code>module purge\nmodule load IDBA-UD/1.1.3-GCC-11.3.0\n</code></pre> <code>idba_ud --help</code> <pre><code>  -o, --out arg (=out)                   output directory\n  -r, --read arg                         fasta read file (&lt;=128)\n--read_level_2 arg                 paired-end reads fasta for second level scaffolds\n      --read_level_3 arg                 paired-end reads fasta for third level scaffolds\n      --read_level_4 arg                 paired-end reads fasta for fourth level scaffolds\n      --read_level_5 arg                 paired-end reads fasta for fifth level scaffolds\n  -l, --long_read arg                    fasta long read file (&gt;128)\n--mink arg (=20)                   minimum k value (&lt;=124)\n--maxk arg (=100)                  maximum k value (&lt;=124)\n--step arg (=20)                   increment of k-mer of each iteration\n      --inner_mink arg (=10)             inner minimum k value\n      --inner_step arg (=5)              inner increment of k-mer\n      --prefix arg (=3)                  prefix length used to build sub k-mer table\n      --min_count arg (=2)               minimum multiplicity for filtering k-mer when building the graph\n      --min_support arg (=1)             minimum supoort in each iteration\n      --num_threads arg (=0)             number of threads\n      --seed_kmer arg (=30)              seed kmer size for alignment\n      --min_contig arg (=200)            minimum size of contig\n      --similar arg (=0.95)              similarity for alignment\n      --max_mismatch arg (=3)            max mismatch of error correction\n      --min_pairs arg (=3)               minimum number of pairs\n      --no_bubble                        do not merge bubble\n      --no_local                         do not use local assembly\n      --no_coverage                      do not iterate on coverage\n      --no_correct                       do not do correction\n      --pre_correction                   perform pre-correction before assembly\n</code></pre> <p>'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using <code>IDBA-UD</code> we will need to pool the paired-end data into a single, interleaved FASTA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example</p> <pre><code># Paired-end file, forward\n&gt;read1_1\n...\n&gt;read2_1\n...\n\n# Paired-end file, forward\n&gt;read1_2\n...\n&gt;read2_2\n...\n\n# Interleaved file\n&gt;read1_1\n...\n&gt;read1_2\n...\n&gt;read2_1\n...\n&gt;read2_2\n...\n</code></pre> <p>Fortunately, the <code>IDBA</code> set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first.</p> <pre><code>module load pigz/2.7\n\nfor i in sample1 sample2 sample3 sample4;\ndo\npigz --keep --decompress ${i}_R1.fastq.gz ${i}_R2.fastq.gz\n  fq2fa --merge ${i}_R1.fastq ${i}_R2.fastq ${i}.fna\ndone\ncat sample1.fna sample2.fna sample3.fna sample4.fna &gt; for_idba.fna    </code></pre>"},{"location":"day1/ex3_assembly/#basic-assembly-parameters","title":"Basic assembly parameters","text":"<p>For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be.</p> <p>Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include:</p> <ol> <li>k-mer sizes to be assembled over, and step size if using a range</li> <li>Number of threads to use during assembly</li> <li>Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space</li> </ol>"},{"location":"day1/ex3_assembly/#setting-the-k-mer-size","title":"Setting the k-mer size","text":"<p>Depending on which assembler you are using, the commands for choosing the k-mer sizes for the assembly vary slightly, but they are recognisable between programs. In <code>SPAdes</code>, you can set the k-mer size using either</p> <pre><code>spades.py -k 21,33,55,77,99,121 ...\n\nspades.py -k auto ...\n</code></pre> <p>The first command lets us specify the k-mers ourselves, or we are letting <code>SPAdes</code> automatically pick the most appropriate size. For <code>IDBA-UD</code>, we can select the k-mer size using</p> <pre><code>idba_ud --mink 21 --maxk 121 --step 22\n</code></pre> <p>Unlike <code>SPAdes</code>, we do not have fine-scale control over the k-mer sizes used in the assembly. We instead provide <code>IDBA-UD</code> with the first and last k-mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k-mer of odd lengths in order to avoid the creation of palindromic k-mers.</p>"},{"location":"day1/ex3_assembly/#specifying-the-number-of-threads","title":"Specifying the number of threads","text":"<p>This is simple in either assembler:</p> <pre><code>spades.py -t 20 ...\n\nidba_ud --num_threads 20 ...\n</code></pre> <p>The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, <code>SPAdes</code> will assemble with 16 threads. <code>IDBA-UD</code> will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm.</p>"},{"location":"day1/ex3_assembly/#setting-a-memory-limit","title":"Setting a memory limit","text":"<p>By far, the worst feature of <code>SPAdes</code> is the high memory requirement for performing an assembly. In the absence of monitoring, <code>SPAdes</code> will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing <code>SPAdes</code>:</p> <pre><code>spades.py -m 400GB ...\n</code></pre> <p>No such parameter exists in <code>IDBA-UD</code>, but it requires far less RAM than <code>SPAdes</code>, so you are less likely to need it.</p>"},{"location":"day1/ex3_assembly/#preparing-an-assembly-job-for-slurm","title":"Preparing an assembly job for slurm","text":"<p>NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to (lander02) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm. The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs.</p>"},{"location":"day1/ex3_assembly/#submitting-a-spades-job-to-nesi-using-slurm","title":"Submitting a SPAdes job to NeSI using slurm","text":"<p>To begin, we need to open a text file using the <code>nano</code> text editor. </p> <pre><code>cd 3.assembly/\n\nnano spades_assembly.sl\n</code></pre> <p>Into this file, either write or copy/paste the following commands:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_assembly\n#SBATCH --time          00:30:00\n#SBATCH --mem           10GB\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load SPAdes/3.15.4-gimkl-2022a-Python-3.10.5\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/3.assembly\n\n# Run SPAdes\nspades.py --meta -k 33,55,77,99,121 -t $SLURM_CPUS_PER_TASK \\\n-1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz \\\n-o spades_assembly/\n</code></pre> <p>To save your file, use <code>Ctrl + O</code> to save the file, then <code>Ctrl + X</code> to exit <code>nano</code>. Going through those lines one by one;</p> Review above submission script: Slurm parameters and their functions Slurm parameter Function #!/bin/bash -e Header for the file, letting NeSI know how to interpret the following commands. The <code>-e</code> flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) --account The name of the project account to run the job under. You are provided with this when you create a project on NeSI --job-name The name of the job, to display when using the <code>squeue</code> command --res This is a parameter that we need due to a system reservation for this workshop. For your own work, ignore this flag --time Maximum run time for the job before it is killed --mem Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated --cpus-per-task number of processing cores to assign to the job. This should match with the number used by your assembler --error File to log the standard error stream of the program.This is typically used to prove error reports, or to just inform the user of job progress --output File to log the standard output stream of the program.This is typically used to inform the user of job progress and supply messages <p>The <code>module load</code> command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that</p> <ol> <li>There is no chance of having the job fail immediately because it cannot find the relevant files</li> <li>When looking back through your slurm logs, you know where the data is meant to be</li> </ol> <p>When executing the <code>SPAdes</code> command, there are a few parameters to note here:</p> Parameter Function --meta Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions -k k-mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly -t Number of threads (see above). Here, we use a special SLURM environment variable: <code>$SLURM_CPUS_PER_TASK</code> to tell the programme to use the same number of threads allocated for this job via <code>--cpus-per-task</code>. -1 Forward reads, matched to their reverse partners -2 Reverse reads, matched to their forward partners -o Output directory for all files <p>Note that we also prefix the command (<code>spades.py</code>) with the <code>srun</code> command. This is a command specific to slurm and allows NeSI to track the resource usage of the <code>SPAdes</code> job.</p> <p>We don't explicitly set memory or thread counts for this job, simply for the sake of keeping the command uncluttered. The default memory limit of <code>SPAdes</code> (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and <code>SPAdes</code> will terminate the assembly. We have also left the number of threads at the default value of 16, which matches the number specified in the slurm header.</p> <p>It is a good idea to match your number of threads request in the slurm script with what you intend to use with <code>SPAdes</code> because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e. request 10 in slurm, set thread count to 30 in <code>SPAdes</code>) will result in reduced performance, as your <code>SPAdes</code> job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post.</p> <p>Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command</p> <pre><code>sbatch spades_assembly.sl\n</code></pre> <p>You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise.</p>"},{"location":"day1/ex3_assembly/#monitoring-job-progress","title":"Monitoring job progress","text":"<p>You can view the status of your current jobs using the command</p> <pre><code>squeue --me\n\nJOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    31491555      jboe440  nesi02659 spawner-jupy   2      4G infill  2022-11-23T1     7:44:17 RUNNING  wbl001              31491999      jboe440  nesi02659 spades_assem  12     10G large   2022-11-23T1       30:00 PENDING  wbn069  </code></pre> <p>We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At this stage the <code>START_TIME</code> is an estimation of when the resources are expected to become available. When they do, the output will change to</p> <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    31491555      jboe440  nesi02659 spawner-jupy   2      4G infill  2022-11-23T1     7:44:15 RUNNING  wbl001              31491999      jboe440  nesi02659 spades_assem  12     10G large   2022-11-23T1       29:58 RUNNING  wbn069          </code></pre> <p>Which allows us to track how far into our run we are, and see the remaining time for the job. The <code>START_TIME</code> column now reports the time the job actually began.</p>"},{"location":"day1/ex3_assembly/#submitting-an-idba-ud-job-to-nesi-using-slurm","title":"Submitting an IDBA-UD job to NeSI using slurm","text":"<p>To run an equivalent assembly with <code>IDBA-UD</code>, create a new slurm script as follows</p> <pre><code>nano idbaud_assembly.sl\n</code></pre> <p>Paste or type in the following:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      idbaud_assembly\n#SBATCH --time          00:20:00\n#SBATCH --mem           4GB\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Prepare modules\nmodule purge\nmodule load IDBA-UD/1.1.3-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/3.assembly\n\n# Run IDBA-UD\nidba_ud --num_threads $SLURM_CPUS_PER_TASK --mink 33 --maxk 99 --step 22 \\\n-r for_idba.fna -o idbaud_assembly/\n</code></pre> <p>And submit the script as a slurm job:</p> <pre><code>sbatch idbaud_assembly.sl\n</code></pre> <p>When your job starts running, files with suffixes <code>.err</code> and <code>.out</code> will be created in the directory from where you submitted your job. These files will have have your job name and job identification number as file names.</p>"},{"location":"day1/ex4_assembly/","title":"Assembly (part 2)","text":"<p>Objectives</p> <ul> <li>Examine the effect of changing parameters for assembly</li> </ul> <p>All work for this exercise will occur in the <code>3.assembly/</code> directory.</p>"},{"location":"day1/ex4_assembly/#examine-the-effect-of-changing-assembly-parameters","title":"Examine the effect of changing assembly parameters","text":"<p>For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the assembly parameters. You will have a chance to compare the effects of these changes tomorrow.</p>"},{"location":"day1/ex4_assembly/#spades-parameters","title":"SPAdes parameters","text":"<p>Make a few copies of your <code>SPAdes</code> slurm script like so;</p> <pre><code>cp spades_assembly.sl spades_assembly_var1.sl\n</code></pre> <p>Change a few of the parameters for run time. Some potential options include</p> <ol> <li>Change the k-mer sizes to either a different specification, or change to the <code>auto</code> option</li> <li>Disable error correction</li> <li>Assemble without the <code>--meta</code> flag</li> <li>Employ a coverage cut-off for assembling</li> </ol>"},{"location":"day1/ex4_assembly/#idba-ud-parameters","title":"IDBA-UD parameters","text":"<p>Make variants of your <code>IDBA-UD</code> assembly script and change some parameters. Some potential options include</p> <ol> <li>Change the minimum/maximum k-mer sizes, or the k-mer step size</li> <li>Change the alignment similarity parameter</li> <li>Adjust the prefix length for the k-mer sub-table </li> </ol> <p>Submit two or three jobs per variation.</p> <p>Outputs per variant job</p> <p>For all variations of your assemblies, please remember to modify <code>-o</code> argument from <code>-o spades_assembly/</code> and <code>-o idbaud_assembly/</code> in your slurm scripts to another name (e.g. <code>-o spades_assembly_var_kmer/</code>)</p>"},{"location":"day1/ex5_evaluating_assemblies/","title":"Evaluating the assemblies","text":"<p>Objectives</p> <ul> <li>Evaluate the resource consumption of various assemblies</li> <li>Evaluate the assemblies</li> <li>Future considerations</li> </ul> <p> </p>"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-resource-consumption-of-various-assemblies","title":"Evaluate the resource consumption of various assemblies","text":"<p>Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the <code>squeue</code> command.</p> <pre><code>squeue --me\n\n# JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n# 31491555      jboe440  nesi02659 spawner-jupy   2      4G infill  2022-11-23T1     7:14:40 RUNNING  wbl001          \n</code></pre> <p>If there are no jobs besides your Jupyter session listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the <code>sacct</code> command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying.</p> <pre><code>sacct -S 2022-11-23\n\n# JobID           JobName          Alloc     Elapsed     TotalCPU  ReqMem   MaxRSS State      \n# --------------- ---------------- ----- ----------- ------------ ------- -------- ---------- \n# 31491555        spawner-jupyter+     2    00:45:50     00:00:00      4G          RUNNING    \n# 31491555.batch  batch                2    00:45:50     00:00:00                  RUNNING    \n# 31491555.extern extern               2    00:45:50     00:00:00                  RUNNING    \n# 31491999        spades_assembly     12    00:12:00     01:37:43     10G          COMPLETED  \n# 31491999.batch  batch               12    00:12:00     01:37:43         9564240K COMPLETED  \n# 31491999.extern extern              12    00:12:00    00:00.001                0 COMPLETED\n</code></pre> <p>Each job has been broken up into several lines, but the main ones to keep an eye on are the base JobID values. </p> Using <code>srun</code> <p>If you use <code>srun</code>, the JobID will have values suffixed with .0. The first of these references the complete job. The later (and any subsequent suffixes like .1, .2) are the individual steps in the script that were called with the <code>srun</code> command.</p> <p>We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the <code>seff</code> command</p> <pre><code>seff 31491999\n# Job ID: 31491999\n# Cluster: mahuika\n# User/Group: jboe440/jboe440\n# State: COMPLETED (exit code 0)\n# Nodes: 1\n# Cores per node: 12\n# CPU Utilized: 01:37:43\n# CPU Efficiency: 67.86% of 02:24:00 core-walltime\n# Job Wall-clock time: 00:12:00\n# Memory Utilized: 9.12 GB\n# Memory Efficiency: 91.21% of 10.00 GB\n</code></pre> <p>Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was somewhat efficient but had high memory efficiency. In the future, I can request less time and retain the same RAM and still had the job run to completion.</p> <p>CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like <code>bowtie</code> and <code>BBMap</code> can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in <code>SPAdes</code> go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down.</p>"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-assemblies","title":"Evaluate the assemblies","text":"<p>Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true.</p> <p>A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use <code>seqmagick</code> for performing the length filtering, and then just count sequence numbers using <code>grep</code>.</p> <p>These steps will take place in the <code>4.evaluation/</code> folder, which contains copies of our <code>SPAdes</code> and <code>IDBA-UD</code> assemblies.</p> <pre><code>module purge\nmodule load seqmagick/0.8.4-gimkl-2020a-Python-3.8.2\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation/\n\nseqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\\nspades_assembly/spades_assembly.m1000.fna\ngrep -c '&gt;' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna\n# spades_assembly/spades_assembly.fna:1318\n# spades_assembly/spades_assembly.m1000.fna:933\nseqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\\nidbaud_assembly/idbaud_assembly.m1000.fna\ngrep -c '&gt;' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna\n# idbaud_assembly/idbaud_assembly.fna:5057\n# idbaud_assembly/idbaud_assembly.m1000.fna:1996\n</code></pre> <p>If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands.</p> <pre><code>seqmagick convert --min-length 1000 ../3.assembly/spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna\n\nseqmagick convert --min-length 1000 ../3.assembly/idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna\n</code></pre> <p>Note</p> <p>The tool <code>seqtk</code> is also available on NeSI and performs many of the same functions as <code>seqmagick</code>. My choice of <code>seqmagick</code> is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either <code>seqtk</code> or <code>seqmagick</code> as both perform a lot of common fastA and fastQ file manipulations.*</p> <p>As we can see here, the <code>SPAdes</code> assembly has completed with fewer contigs assembled than the <code>IDBA-UD</code>, both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has <code>SPAdes</code> managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 (see more information about this statistic here) of the assembly with <code>BBMap</code>.</p> <pre><code>module load BBMap/39.01-GCC-11.3.0\n\nstats.sh in=spades_assembly/spades_assembly.m1000.fna\n</code></pre> <p>This gives quite a verbose output:</p> <pre><code>A       C       G       T       N       IUPAC   Other   GC      GC_stdev\n0.2536  0.2466  0.2462  0.2536  0.0019  0.0000  0.0000  0.4928  0.0960\n\nMain genome scaffold total:             933\nMain genome contig total:               2710\nMain genome scaffold sequence total:    34.300 MB\nMain genome contig sequence total:      34.236 MB       0.186% gap\nMain genome scaffold N/L50:             52/158.668 KB\nMain genome contig N/L50:               107/72.463 KB\nMain genome scaffold N/L90:             302/15.818 KB\nMain genome contig N/L90:               816/4.654 KB\nMax scaffold length:                    1.221 MB\nMax contig length:                      1.045 MB\nNumber of scaffolds &gt; 50 KB:            151\n% main genome in scaffolds &gt; 50 KB:     76.76%\n\nMinimum         Number          Number          Total           Total           Scaffold\nScaffold        of              of              Scaffold        Contig          Contig  Length          Scaffolds       Contigs         Length          Length          Coverage\n--------        --------------  --------------  --------------  --------------  --------\n    All                    933           2,710      34,299,647      34,235,702    99.81%\n   1 KB                    933           2,710      34,299,647      34,235,702    99.81%\n 2.5 KB                    745           2,458      33,980,511      33,921,524    99.83%\n   5 KB                    579           2,142      33,383,109      33,329,777    99.84%\n  10 KB                    396           1,605      32,059,731      32,022,009    99.88%\n  25 KB                    237             936      29,559,828      29,540,698    99.94%\n  50 KB                    151             593      26,330,017      26,317,988    99.95%\n 100 KB                     91             411      22,108,846      22,100,263    99.96%\n 250 KB                     29             141      12,338,782      12,335,701    99.98%\n 500 KB                      7              38       5,611,200       5,610,890    99.99%\n   1 MB                      1               2       1,221,431       1,221,421   100.00%\n</code></pre> <p>N50 and L50 in BBMap</p> <p>Unfortunately, the N50 and L50 values generated by <code>stats.sh</code> are switched. N50 should be a length and L50 should be a count. The results table below shows the corrected values based on <code>stats.sh</code> outputs.</p> <p>But what we can highlight here is that the statistics for the <code>SPAdes</code> assembly, with short contigs removed, yielded an N50 of 72.5 kbp at the contig level. We will now compute those same statistics from the other assembly options.</p> <pre><code>stats.sh in=spades_assembly/spades_assembly.fna\n\nstats.sh in=idbaud_assembly/idbaud_assembly.m1000.fna\nstats.sh in=idbaud_assembly/idbaud_assembly.fna\n</code></pre> Assembly N50 (contig) L50 (contig) SPAdes (filtered) 72.5 kbp 107 SPAdes (unfiltered) 72.1 kbp 108 IDBA-UD (filtered) 103.9 kbp 82 IDBA-UD (unfiltered) 96.6 kbp 88"},{"location":"day1/ex5_evaluating_assemblies/#optional-evaluating-assemblies-using-metaquast","title":"Optional: Evaluating assemblies using MetaQUAST","text":"<p>For more genome-informed evaluation of the assembly, we can use the <code>MetaQUAST</code> tool to view our assembled metagenome. This is something of an optional step because, like <code>QUAST</code>, <code>MetaQUAST</code> aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a <code>MetaQUAST</code> evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection:</p> <ol> <li>Kraken2 (DNA based, k-mer classification)</li> <li>CLARK (DNA based. k-mer classification)</li> <li>Kaiju (Protein based, BLAST classification)</li> <li>Centrifuge (DNA based, sequence alignment classification)</li> <li>MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification)</li> <li>MetaPhlAn2 (DNA based, clade-specific marker gene classification)</li> </ol> <p>A good summary and comparison of these tools (and more) was recently published by Ye et al..</p> <p>However, since we do know the composition of the original communities used to build this mock metagenome, <code>MetaQUAST</code> will work very well for us today. In your <code>4.evaluation/</code> directory you will find a file called <code>ref_genomes.txt</code>. This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for <code>MetaQUAST</code>.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      metaquast\n#SBATCH --time          00:15:00\n#SBATCH --mem           4GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load module\nmodule purge\nmodule load QUAST/5.0.2-gimkl-2018b\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation\n\n# Run metaquast    \nmetaquast.py --references-list ref_genomes.txt --max-ref-number 21 \\\n-t $SLURM_CPUS_PER_TASK \\\n--labels SPAdes,SPAdes.m1000,IDBAUD,IDBAUD.m1000 \\\n--output-dir metaquast_results/ \\\nspades_assembly/spades_assembly.fna \\\nspades_assembly/spades_assembly.m1000.fna \\\nidbaud_assembly/idbaud_assembly.fna \\\nidbaud_assembly/idbaud_assembly.m1000.fna\n</code></pre> <p>By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the <code>--max-ref-number</code> flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 taxa in the file <code>ref_genomes.txt</code> (10 prokaryote species and 11 viruses), <code>MetaQUAST</code> will download one reference genome for each. If we increase  the <code>--max-ref-number</code> flag we will start to get multiple reference genomes per taxa provided which is usually desirable.</p> <p>We will now look at a few interesting assembly comparisons.</p> <p>If you are working from the NeSI <code>Jupyter hub</code> environment today, the html viewer within the NeSI <code>Jupyter hub</code> does not currently support this (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the <code>4.evaluation/quast_results/</code> folder, right-click <code>report.html/</code> and select download. The downloaded file will then open within a new tab in the browser. (NOTE: rendering the full report requires the other folders from within <code>quast_results/</code> to also be downloaded and available in the same directory as <code>report.html</code>. Unfortunately, the <code>Jupyter hub</code> environment does not appear to currently support downloading entire folders using this method.)</p> <p>An example of the <code>MetaQUAST</code> output files are also available for download. You will need to download both references and results. Unzip both within the same directory.</p>"},{"location":"day1/ex5_evaluating_assemblies/#brief-summary-of-assemblies","title":"Brief summary of assemblies","text":""},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-nga50-between-assemblies","title":"Comparison of NGA50 between assemblies","text":""},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-aligned-contigs","title":"Comparison of aligned contigs","text":""},{"location":"day1/ex5_evaluating_assemblies/#inspection-of-unaligned-contigs","title":"Inspection of unaligned contigs","text":""},{"location":"day2/ex6_initial_binning/","title":"Introduction to binning","text":"<p>Objectives</p> <ul> <li>Remove short contigs from the data set</li> <li>Obtain coverage profiles for assembled contigs via read mapping</li> <li>Optional: Read mapping using a slurm array</li> </ul>"},{"location":"day2/ex6_initial_binning/#remove-short-contigs-from-the-data-set","title":"Remove short contigs from the data set","text":"<p>Ideally, we do not want to be creating bins from all of the assembled contigs, as there is often a long tail of contigs which are only several \\(k\\)-mers long. These have little biological meaning, as they are too short for robust gene annotation, and they can introduce a significant degree of noise in the clustering algorithms used for binning. We therefore identify a suitable threshold for a minimum length of contigs to be considered for binning.</p> <p>We have already done this in the previous exercise so we could either use the existing filtering at 1,000 bp in length, or move to something stricter. Most binning tools have a default cut-off for minimum contig size - <code>MetaBAT</code> uses a default minimum of 2,500 bp, and recommends at least 1,500 bp. By contrast, <code>MaxBin</code> sets the minimum length at 1,000 bp.</p>"},{"location":"day2/ex6_initial_binning/#obtain-coverage-profiles-for-assembled-contigs-via-read-mapping","title":"Obtain coverage profiles for assembled contigs via read mapping","text":"<p>Binning is done using a combination of information encoded in the composition and coverage of the assembled contigs. Composition refers to k-mer (usually tetranucleotide) frequency profiles of the contigs, which are generally conserved within a genome. By contrast, coverage is a reflection of the abundance of the contigs in the assembly. Organisms which are more abundant will contribute more genomic material to the metagenome, and hence their DNA will be, on average, more abundant in the sample. When binning, we can look for pieces of DNA which are not assembled together, but have similar composition and occur at approximately equal abundances in the sample to identify contigs which likely originate in the same genome.</p> <p>The composition of the contigs is calculated by the binning tool at run time, but to obtain coverage information we must map our unassembled reads from each sample against the assembly to generate the differential abundance profiles for each contig. This is achieved using <code>bowtie2</code> to map the reads against the assembly, then <code>samtools</code> to sort and compress the resulting file.</p>"},{"location":"day2/ex6_initial_binning/#creating-a-mapping-index","title":"Creating a mapping index","text":"<p>Before we can map reads, we need to create a <code>bowtie2</code> index file from the assembly, for use in read mapping. Navigate into the <code>5.binning/</code> folder to begin.</p> <pre><code>module load Bowtie2/2.4.5-GCC-11.3.0\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\nbowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades\n</code></pre> <p>If you look inside the <code>spades_assembly/</code> folder you will now see the following:</p> <pre><code>ls spades_assembly/\n# bw_spades.1.bt2  bw_spades.3.bt2  bw_spades.rev.1.bt2  spades_assembly.m1000.fna\n# bw_spades.2.bt2  bw_spades.4.bt2  bw_spades.rev.2.bt2\n</code></pre> <p>These files ending in .bt2 are the index files for <code>bowtie2</code>, and are specific to this tool. If you wish to map using an alternate tool (for example <code>bowtie</code> or <code>BBMap</code>) you will need to create index/database files using these programs.</p> <p>Generally speaking, we don't need to know the names of the index files, as they are simply referred to be the output name we specified (bw_spades) when running <code>bowtie2</code>.</p>"},{"location":"day2/ex6_initial_binning/#mapping-the-reads","title":"Mapping the reads","text":"<p>We will create a slurm script to perform the mapping steps, as these benefit greatly from the multithreaded capacity of NeSI and we will use a for loop to iterate over each set of reads to simplify our script.</p> <p>The full script is provided here, and we will discuss it below.</p> <p>Open a new script using nano:</p> <pre><code>nano spades_mapping.sl\n</code></pre> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_mapping\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Step 1\nfor i in sample1 sample2 sample3 sample4;\ndo\n# Step 2\nbowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n-x spades_assembly/bw_spades \\\n-1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \\\n-S ${i}.sam\n\n# Step 3\nsamtools sort -@ $SLURM_CPUS_PER_TASK -o ${i}.bam ${i}.sam\n\ndone\n</code></pre> <p>Now run the script using <code>sbatch</code></p> <pre><code>sbatch spades_mapping.sl\n</code></pre>"},{"location":"day2/ex6_initial_binning/#step-1-loop-through-the-sample-files","title":"Step 1 - Loop through the sample files","text":"<p>Since we just want to perform the same set of operations on each file, we can use a for loop to repeat each operation on a set of files. The structure of the loop, and use of variables was covered on the first day.</p> <p>For large sets of files, it can be beneficial to use a slurm array to send the jobs out to different nodes and distribute the process across many independent jobs. An example of how we could modify the above script is given at the bottom of this exercise, but is not necessary for the purposes of this workshop.</p>"},{"location":"day2/ex6_initial_binning/#step-2-map-the-reads-using-bowtie2","title":"Step 2 - Map the reads using bowtie2","text":"<p>This is performed using the following parameters</p> Parameter Function --minins Minimum insert size, determines the minimum distance between the start of each read pair --maxins Maximum insert size, determines the maximum distance between the start of each read pair --threads Number of threads to use in read mapping --sensitive Specifies where we want to be positioned in the trade-off between speed and sensitivity. See the manual for more information -x The base name of our assembly index. Should be exactly the same as what was specified when running bowtie2-build -1 / -2 The forward and reverse read pairs to map to the assembly -S Name of the output file, to be written in sam format"},{"location":"day2/ex6_initial_binning/#step-3-sorting-and-compressing-results","title":"Step 3 - Sorting and compressing results","text":"<p>The default output format for most mapping tools is the Sequence Alignment/Map (sam) format. This is a compact text representation of where each short read sits in the contigs. You can view this file using any text viewer, although owing to the file size <code>less</code> is a good idea.</p> <p>Generally I wouldn't bother with this - there is a lot of information in here and unless you are looking to extract specific information from the alignment directly, this is just an intermediate file in our workflow. In order to save disk space, and prepare the file for downstream analysis we now perform two final steps:</p> <ol> <li>Sort the mapping information</li> <li>Compress the sam file into its binary equivalent, bam</li> </ol> <p>Which is achieved with the following parameters</p> Parameter Function sort Subcommand for <code>samtools</code> to invoke the <code>sort</code> operation -@ Number of threads to use for sorting and compressing -o Output file name. When we specify the bam extension <code>samtools</code> automatically compresses the output <p>Compressing the file to the bam format is an important step as when working with real data sam files can be massive and our storage capacity on NeSI is limited. It is also helpful to sort the mapping information so that reads mapped to a contig are listed in order of their start position. For example</p> <pre><code># Unsorted reads\nRef: REFERENCECONTIG\nMap: --------ECONT--\nMap: REFE-----------\nMap: --FERENCECO----\nMap: -----------NTIG\n\n# Sorted reads\nRef: REFERENCECONTIG\nMap: REFE-----------\nMap: --FERENCECO----\nMap: --------ECONT--\nMap: -----------NTIG\n</code></pre> <p>Reads will initially be mapped in an unsorted order, as they are added to the sam file in more or less the same order as they are encountered in the original fastQ files.</p> <p>Sorting the mapping information is an important prerequisite for performing certain downstream processes. Not every tool we use requires reads to be sorted, but it can be frustrating having to debug the instances where read sorting matters, so we typically just get it done as soon as possible and then we don't have to worry about it again.</p> <p>In newer versions of <code>samtools</code> we can perform the sorting and compressing in a single operation (as shown in the script above). For older versions of <code>samtools</code>, you may need to use a command of the following form.</p> <pre><code>samtools view -bS sample1.sam | samtools sort -o sample1.bam\n</code></pre>"},{"location":"day2/ex6_initial_binning/#optional-read-mapping-using-an-array","title":"Optional: Read mapping using an array","text":"<p>If you have a large number of files to process, it might be worth using a slurm array to distribute your individual mapping jobs across many separate nodes. An example script for how to perform this is given below. We do not need to use an array for read mapping in this workshop, but we will revisit array jobs in further lessons.</p> <p>Open a new script using nano:</p> <pre><code>nano spades_mapping_array.sl\n</code></pre> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_mapping_array\n#SBATCH --time          00:20:00\n#SBATCH --mem           20GB\n#SBATCH --array         0-3\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Load modules\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Load the sample names into a bash array\nsamples=(sample1 sample2 sample3 sample4)\n# Run Bowtie2 and SAMTools, using the SLURM_ARRAY_TASK_ID variable to\n# identify which position in the `samples` array to use\nbowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n-x spades_assembly/bw_spades \\\n-1 ../3.assembly/${samples[ $SLURM_ARRAY_TASK_ID ]}_R1.fastq.gz \\\n-2 ../3.assembly/${samples[ $SLURM_ARRAY_TASK_ID ]}_R2.fastq.gz \\\n-S ${samples[ $SLURM_ARRAY_TASK_ID ]}_a.sam\n\nsamtools sort -@ $SLURM_CPUS_PER_TASK \\\n-o ${samples[ $SLURM_ARRAY_TASK_ID ]}_a.bam \\\n${samples[ $SLURM_ARRAY_TASK_ID ]}_a.sam\n</code></pre> <p>Submit the job to slurm</p> <pre><code>sbatch spades_mapping_array.sl\n</code></pre>"},{"location":"day2/ex7_initial_binning/","title":"Binning (continued)","text":"<p>Objectives</p> <ul> <li>Overview</li> <li>Create initial bins using <code>MetaBAT</code></li> <li>Create initial bins using <code>MaxBin</code></li> </ul>"},{"location":"day2/ex7_initial_binning/#overview","title":"Overview","text":"<p>With the mapping information computed in the last exercise, we can now perform binning. There are a multitude of good binning tools currently published, and each have their strengths and weaknesses. As there is no best tool for binning, the current strategy for binning is to use a number of different tools on your data, then use the tool <code>DAS_Tool</code> to evaluate all potential outcomes and define the best set of bins across all tools used.</p> <p>In our own workflow, we use the tools <code>MetaBAT</code>, <code>MaxBin</code>, and <code>CONCOCT</code> for binning, but there are many alternatives that are equally viable. In the interests of time, we are only going to demonstrate the first two tools. However, we recommend that you experiement with some of the following tools when conducting your own research.</p> <ol> <li>GroopM</li> <li>Tetra-ESOM</li> <li>VAMB</li> </ol>"},{"location":"day2/ex7_initial_binning/#metabat","title":"MetaBAT","text":"<p><code>MetaBAT</code> binning occurs in two steps. First, the bam files from the last exercise are parsed into a tab-delimited table of the average coverage depth and variance per sample mapped. Binning is then performed using this table.</p> <p>The .bam files can be passed in via either a user-defined order, or using wildcards. <pre><code>module purge\nmodule load MetaBAT/2.15-GCC-11.3.0\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Manual specification of files\njgi_summarize_bam_contig_depths --outputDepth metabat.txt sample1.bam sample2.bam sample3.bam sample4.bam\n\n# Wildcard\njgi_summarize_bam_contig_depths --outputDepth metabat.txt sample*.bam\n</code></pre></p> <p>Both give the same result, although the sample order may vary.</p> <p>We can then pass the table <code>metabat.txt</code> into the <code>MetaBAT</code> binning tool.</p> <p>Before we proceed, note that when you run <code>MetaBAT</code> on NeSI you will see the text <code>vGIT-NOTFOUND</code> appear in your command line. This has no impact on the performance of the tool.</p> <pre><code>metabat2 -t 2 -m 1500 \\\n-i spades_assembly/spades_assembly.m1000.fna \\\n-a metabat.txt \\\n-o metabat/metabat\n</code></pre> <p>Note here that we are specifying a minimum contig size of 1,500 bp, which is the lower limit allowed by the authors of <code>MetaBAT</code>. This is larger than the minimum threshold of 1,000 bp when we filtered the assembly, which means there are some assembled contigs which cannot be binned. Consider the choice of this parameter and your initial contig filtering carefully when binning your own data.</p> <p>When specifying the output file, notice that we pass both a folder path (metabat/) and file name (metabat). The reason I do this is that <code>MetaBAT</code> writes its output files using the pattern</p> <p><code>[USER VALUE].[BIN NUMBER].fa</code></p> <p>If we only provided the path, without a file name prefix, <code>MetaBAT</code> would create output like the following:</p> <pre><code>metabat/.1.fa\nmetabat/.2.fa\nmetabat/.3.fa\n</code></pre> <p>The problem with this is that on Linux systems, prefixing a file or folder name with a '.' character means the the file is hidden. This can lead to a lot of confusion when your binning job completes successfully but no files are visible!</p>"},{"location":"day2/ex7_initial_binning/#maxbin","title":"MaxBin","text":"<p>Like <code>MetaBAT</code>, <code>MaxBin</code> requires a text representation of the coverage information for binning. Luckily, we can be sneaky here and just reformat the <code>metabat.txt</code> file into the format expected by <code>MaxBin</code>. We use <code>cut</code> to select only the columns of interest, which are the contigName and coverage columns, but not the contigLen, totalAvgDepth, or variance columns.</p> <p>We can inspect the <code>metabat.txt</code> file with <code>head</code> or <code>less</code> to identify the correct column indices for <code>cut</code>.</p> <pre><code>less metabat.txt\n\ncut -f1,4,6,8,10 metabat.txt &gt; maxbin.txt\n</code></pre> <p>Generally speaking, this pattern of first, fourth, then [n + 2]th should work for any number of mapping files, although we always recommend that you check and confirm before you continue.</p> <p>This table is then passed to <code>MaxBin</code>. Unlike the case with <code>MetaBAT</code>, if we want to direct the output files into a folder, we must create that folder in advance.</p> <p>Create a new script to submit as a slurm job</p> <pre><code>nano maxbin_clustering.sl\n</code></pre> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      maxbin_clustering\n#SBATCH --time          00:05:00\n#SBATCH --mem           10GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load MaxBin/2.2.7-GCC-11.3.0-Perl-5.34.1\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Output directory\nmkdir -p maxbin/\n\n# Run MaxBin\nrun_MaxBin.pl -thread $SLURM_CPUS_PER_TASK -min_contig_length 1500 \\\n-contig spades_assembly/spades_assembly.m1000.fna \\\n-abund maxbin.txt \\\n-out maxbin/maxbin\n</code></pre> <p>Submit the script as a slurm job:</p> <pre><code>sbatch maxbin_clustering.sl\n</code></pre> <p>Note</p> <p>This will take a bit longer to complete, as <code>MaxBin</code> uses gene prediction tools to identify the ideal contigs to use as the start of each bin.</p>"},{"location":"day2/ex8_bin_dereplication/","title":"Bin dereplication","text":"<p>Objectives</p> <ul> <li>Bin dereplication<ul> <li>Objectives</li> <li>Bin dereplication using <code>DAS_Tool</code> - Creating input tables</li> <li>Creating contig/bin tables - MetaBAT</li> <li>Creating contig/bin tables - MaxBin<ul> <li>Warning for unbinned contigs</li> </ul> </li> <li>Bin dereplication using DAS_Tool - Running the tool</li> <li>Evaluating bins using CheckM<ul> <li>Highly conserved, single copy markers</li> <li>Genes are considered as co-located clusters</li> <li>Lineage-specific duplications and losses can be identified</li> </ul> </li> <li>Discussion: dereplication across multiple assemblies</li> </ul> </li> </ul>"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool-creating-input-tables","title":"Bin dereplication using <code>DAS_Tool</code> - Creating input tables","text":"<p>As we discussed in the previous exercise, we have now generated two sets of bins from the same single assembly. With this mock data set we can see that <code>MetaBAT</code> recovered 12 bins, while <code>MaxBin</code> recovered 10. Note that we are aiming to recover prokaryote genomes using these binning tools (we will use other tools to investigate viral genomes in later exercises), and 10 bacterial and archaeal genomes were used in the creation of this mock community. If our mock community only contained these 10 prokaryote genomes and omitted the viral genomes, we shouldn't expect to see more than 10 bins total. In our case here, these tools have likely recovered 10 bins of the same genomes. The additional two bins identified by <code>MetaBAT</code> may be the result of noise introduced into the binning process by the viral contigs included in the data. Furthermore, it is not clear which tool has done a better job of recruiting contigs to each bin - we very rarely expect to see the complete genome recovered from these kinds of data, so while it is probably the case that while an equivalent bin is present in the <code>MetaBAT</code> and <code>MaxBin</code> outputs, they will likely be of differing quality.</p> <p><code>DAS_Tool</code> is a program designed to analyse the bins in each of our binning sets and determine where these equivalent pairs (or triplets if we use three binners) exist and return the 'best' one. <code>DAS_Tool</code> does not use the actual bins, but a set of text files that link contigs to their corresponding bins in each of the bin sets. We can produce these files using <code>bash</code>.</p> <p>For this exercise, we will continue working in the <code>5.binning/</code> directory.</p>"},{"location":"day2/ex8_bin_dereplication/#creating-contigbin-tables-metabat","title":"Creating contig/bin tables - MetaBAT","text":"<p>For each of our binning tools, we need to extract the contigs assigned to each bin and create a single file that reports these as</p> <pre><code>Contig[tab]Bin\n</code></pre> <p>This can be done with a bit of <code>bash</code> scripting. There's quite a bit going on here, so we'll provide the full command, and then a step-by-step explanation of what's happening.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\nfor bin_path in metabat/*.fa; do\nbin_name=$(basename ${bin_path} .fa)\ngrep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; metabat_associations.txt\ndone\n</code></pre> <p>You can check the contents of this file using <code>less</code> or <code>head</code>, and you should be something like:</p> <pre><code>head -n 5 metabat_associations.txt\n# NODE_44_length_186282_cov_0.255381      metabat.10\n# NODE_51_length_159085_cov_0.252240      metabat.10\n# NODE_68_length_132434_cov_0.251275      metabat.10\n# NODE_71_length_129650_cov_0.256954      metabat.10\n# NODE_73_length_121437_cov_0.254006      metabat.10\n</code></pre> <p>We will now walk through the content of the command above, breaking apart each individual step.</p> <pre><code>for bin_path in metabat/*.fa; do\n</code></pre> <p>This is the initial loop, returning each file in the <code>metabat/</code> folder that ends with the file extension .fa.</p> <pre><code>    bin_name=$(basename ${bin_path} .fa)\n</code></pre> <p>As we have previously seen, the <code>basename</code> command removes the path information from the input variable <code>bin_path</code> (i.e. - <code>metabat/metabat.1.fa</code> becomes <code>metabat.1.fa</code>) and assigns it to a new variable, <code>bin_name</code>. As an optional additional parameter, we can pass extra pieces of text to be removed from the variable, in this case the .fa extension.</p> <pre><code>    grep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; metabat_associations.txt\n    |                      |              |                         |\n|                      |              |                         Command 4\n|                      |              Command 3\n|                      Command 2\nCommand 1\n</code></pre> <p>This next step uses piping between several commands to achieve the desired output.</p> <p>Commands</p> <p>Command 1</p> <p>The <code>grep</code> command searches the input file <code>bin_path</code> for lines containing the <code>&gt;</code> character, which is the fastA demarcation for a sequence name.</p> <p>Command 2</p> <p>The <code>sed</code> command replaces the <code>&gt;</code> character with the empty character <code>''</code>, as we do not need this character in the final file.</p> <p>Command 3</p> <p>We now use <code>sed</code> again, this time to replace the <code>$</code> character. In many command line tools and software environments (including <code>R</code> and <code>python</code>) the characters <code>^</code> and <code>$</code> are used as shortcuts for the beginning and ending of a line, respectively. By using the character in this way, we are telling <code>sed</code> to replace the end-of-line with the text <code>\\t${bin_name}</code>. <code>sed</code> will parse this text to mean the tab character followed by the content of the <code>bin_name</code> variable. The nature of <code>sed</code> is that it will automatically insert a new end-of-line.</p> <p>Command 4</p> <p>This is similar to the stdout redirection we have previously used, but the double use of the <code>&gt;</code> character means that we are appending our text to the end of the file <code>metabat_associations.txt</code>. Because we are looping through several files in this exercise, if we were to use the single <code>&gt;</code> character then on each new fastA file read, the content of <code>metabat_associations.txt</code> would be replaced.</p> <p>It is important to note that because we are appending to the file, not replacing the contents, if you make a mistake in the command and need to re-run it, you will need to explicitly delete the <code>metabat_associations.txt</code> file using <code>rm</code>, otherwise your new (correct) output will be pasted to the end of your old (incorrect) output.</p>"},{"location":"day2/ex8_bin_dereplication/#creating-contigbin-tables-maxbin","title":"Creating contig/bin tables - MaxBin","text":"<p>The process for creating the <code>MaxBin</code> table is basically the same, we just need to change the file extension, as <code>MaxBin</code> writes outputs using the .fasta suffix rather than the .fa one.</p> <pre><code>for bin_path in maxbin/*.fasta;\ndo\nbin_name=$(basename ${bin_path} .fasta)\ngrep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; maxbin_associations.txt\ndone\n</code></pre>"},{"location":"day2/ex8_bin_dereplication/#warning-for-unbinned-contigs","title":"Warning for unbinned contigs","text":"<p>Both <code>MetaBAT</code> and <code>MaxBin</code> have the option to output unbinned contigs after binning completes. We have not used that parameter here, but if you do choose to enable it you will end up with another fastA file in your output directory which you will need to avoid in the loops for creating <code>DAS_Tool</code> tables.</p>"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool-running-the-tool","title":"Bin dereplication using DAS_Tool - Running the tool","text":"<p>We are now ready to run <code>DAS_Tool</code>. This can be done from the command line, as it does not take a particularly long time to run for this data set. Start by loading <code>DAS_Tool</code>.</p> <pre><code>module load DAS_Tool/1.1.5-gimkl-2022a-R-4.2.1\n</code></pre> <p>Depending on whether or not your session has been continued from previous exercises, you may encounter an error performing this module load. This is because some of the tools we have used in previous exercises have dependencies which conflict with the dependencies in <code>DAS_Tool</code>. If this is the case for you, you can unload all previous module loads with the following:</p> <pre><code>module purge\n\nmodule load DAS_Tool/1.1.5-gimkl-2022a-R-4.2.1\nmodule load DIAMOND/2.0.15-GCC-11.3.0\nmodule load USEARCH/11.0.667-i86linux32\n</code></pre> <p><code>DAS_Tool</code> should now load without issue. With 2 threads, <code>DAS_Tool</code> should take 10 - 15 minutes to complete.</p> <pre><code># Create DAS_Tool output directory\nmkdir -p dastool_out/\n\n# Run DAS_Tool\nDAS_Tool -i metabat_associations.txt,maxbin_associations.txt \\\n-l MetaBAT,MaxBin \\\n-t 2 --write_bins --search_engine blastp \\\n-c spades_assembly/spades_assembly.m1000.fna \\\n-o dastool_out/\n</code></pre> <p>As usual, we will break down the parameters:</p> Parameter Function -i ... A comma-separated list of the contig/bin files we wish to process -l ... A comma-separated list of the binning tools used -t ... Number of threads to use --write_bins An argument telling <code>DAS_Tool</code> whether or not to write out a new set of binsThis is recommended, because <code>DAS_Tool</code> can create slices of old bins based on marker composition (see the paper for details) --search_engine blastp Specify whether to use <code>usearch</code>, <code>diamond</code>, or <code>BLAST</code> as the alignment tool for comparing gene sequences (see note below) -c ... Path to the assembly used in binning -o .. Output directory for all files <p>This is not a problem - <code>DAS_Tool</code> can use either <code>BLAST</code>, <code>diamond</code>, or <code>usearch</code> for performing its alignment operations. Regardless of which one you specify, it will search to see which ones are available. In this case, it is telling us that <code>diamond</code> and <code>usearch</code> cannot be found, which doesn't really matter because we have specified <code>BLAST</code> as our search engine.</p> <p>When <code>DAS_Tool</code> has completed, we will have a final set of bins located in the folder path <code>dastool_out/_DASTool_bins</code>. Have a look at the output and see which bins made it to the final selection. Did a single binning tool pick the best bins, or are the results a split between <code>MetaBAT</code> and <code>MaxBin</code>?</p>"},{"location":"day2/ex8_bin_dereplication/#evaluating-bins-using-checkm","title":"Evaluating bins using CheckM","text":"<p>Now that we have our dereplicated set of bins, it is a good idea to determine estimates of their completeness (how much of the genome was recovered) and contamination (how many contigs we believe have been incorrectly assigned to the bin). For organisms that lack a reference genome there is not definitive way to do this, but the tool <code>CheckM</code> provides a robust estimate for these statistics by searching each of your bins for a number of highly conserved, single copy genes. The number of markers depends on whether or not you are working with bacterial (120 markers) or archaeal (122 markers) genomes, but <code>CheckM</code> is able to determine which set is more appropriate for each of your bins as it runs.</p> <p>There are several characteristics of the <code>CheckM</code> marker set worth noting:</p>"},{"location":"day2/ex8_bin_dereplication/#highly-conserved-single-copy-markers","title":"Highly conserved, single copy markers","text":"<p>The marker sets used in <code>CheckM</code> were chosen because they are present in at least 95% of bacterial/archaeal genomes, and are single copy in \u226597% genomes tested. This means that if a gene is missing from a genome, it is likely due to incompleteness in either the original assembly or the binning approach. Similarly, if a marker is observed more than once in a bin it is likely the result of over-clustering of the data.</p>"},{"location":"day2/ex8_bin_dereplication/#genes-are-considered-as-co-located-clusters","title":"Genes are considered as co-located clusters","text":"<p>Rather than test the raw presence/absence of genes in the marker sets, the genes are organised into operon-like structures where genes known to be co-located are placed together. This is advantageous for two reasons</p> <ol> <li>These co-located groups are distributed around the prokaryotic genome, so estimates are not biased by lucky/unlucky recovery of a gene hotspot</li> <li><code>CheckM</code> can account for how complete each individual gene cluster is, rather than just whether or not genes are present</li> </ol>"},{"location":"day2/ex8_bin_dereplication/#lineage-specific-duplications-and-losses-can-be-identified","title":"Lineage-specific duplications and losses can be identified","text":"<p>As part of determining the correct marker set to use for each bin (bacterial or archaeal), <code>CheckM</code> uses a set of 43 conserved prokaryotic markers to insert each bin into a guide tree to estimate the phylogeny of the bin. There are several lineages which are known to have lost particular markers, or to have acquired a additional copies, and if <code>CheckM</code> places a bin into one of these lineages it can adjust its completeness/contamination estimates accordingly.</p> <p>This process isn't perfect, however, and we will discuss some times when you might need to create your own marker set in the next session.</p> <p>We will need to run <code>CheckM</code> under a slurm script. This is because the tree placement process requires a large amount of memory to perform, independently of the size of your data set. A basic script for submitting a <code>CheckM</code> job would be as follows:</p> <p>Create a new script</p> <pre><code>nano checkm.sl\n</code></pre> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      CheckM\n#SBATCH --time          00:20:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load CheckM/1.2.1-gimkl-2022a-Python-3.10.5\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Run CheckM\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK --pplacer_threads $SLURM_CPUS_PER_TASK \\\n-x fa --tab_table -f checkm.txt \\\ndastool_out/_DASTool_bins/ checkm_out/\n</code></pre> <p>Submit the script as a slurm job</p> <pre><code>sbatch checkm.sl\n</code></pre> <p>The breakdown of parameters is as follows</p> Parameter Function lineage_wf Specifies with mode of <code>CheckM</code> to run. This is the most common to use, but several others exist -t ... Number of threads to use for the initial marker gene detection and clustering --pplacer_threads ... Number of threads to use when inserting bins into the guide treeNote: Increasing this parameter results in a linear increase in memory requirement - seting it to 10 means that <code>CheckM</code> will need about 10 times more memory than with a single thread -x ... The fastA file extension to look for in the input folder. Default is .fna --tab_table If this parameter is present, a summary table will be written for the <code>CheckM</code> run -f ... The name of the file for the summary dastool/_DASTool_bins/ The location of the bins to test checkm_out/ The location to write intermediate and output files <p>When your job completes, we will download the summary file and examine it.</p>"},{"location":"day2/ex8_bin_dereplication/#discussion-dereplication-across-multiple-assemblies","title":"Discussion: dereplication across multiple assemblies","text":"<p>In this workshop, we have generated a set of putative MAGs by binning scaffolds taken from a single co-assembly. Alternatively, we may have chosen to generate multiple assemblies (for example, mini-co-assemblies for each sample group, or individual assemblies for each sample). In this case, it would be necessary to work through the binning process for each assembly, and then conduct an additional dereplication step across the multiple assemblies to generate a single set of dereplicated bins for all assemblies. </p> <p>This is beyond the scope of this workshop (and unnecessary here, since we are working with a single co-assembly). For future reference for your own work, further information about how to dereplicate bins and viral contigs across multiple assemblies via <code>dRep</code> and <code>dedupe</code> has been provided as an appendix here.</p>"},{"location":"day2/ex9_refining_bins/","title":"Manually refining bins","text":"<p>Objectives</p> <ul> <li>Prepare input files for <code>VizBin</code></li> <li>Project a t-SNE using <code>VizBin</code> and examine bin clusters</li> <li>Refine bins by identifying incorrectly assigned contigs</li> <li>Optional: Refine and filter problematic contigs from bins</li> <li>Optional: Comparing pre- and post-filtered bins via CheckM</li> <li>Optional: Creating new <code>VizBin</code> profiles with different fragment lengths</li> <li>Optional: Scripts for processing data with <code>ESOMana</code></li> </ul>"},{"location":"day2/ex9_refining_bins/#prepare-input-files-for-vizbin","title":"Prepare input files for VizBin","text":"<p>VizBin is a handy, GUI-based tool for creating ordinations of our binning data using the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on YouTube that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS.</p> <p>On its own, <code>VizBin</code> takes a set of contigs and performs the t-SNE projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage <code>VizBin</code> only allows a single coverage value per contig, which is not ideal. This is because <code>VizBin</code> only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own t-SNE projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing <code>VizBin</code> with coverage values for sample1 only. </p> <p>The only required input file for <code>VizBin</code> is a single <code>.fna</code> file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement.</p> <p>NOTE: When running VizBin, it is often preferable to split long contigs into smaller pieces in order to increase the density of clustering in the t-SNE. The data we are working with today are based on our bins output by <code>DAS_Tool</code> in the last binning exercise, but have been further processed using the <code>cut_up_fasta.py</code> script that comes with the binning tool <code>CONCOCT</code> to cut long contigs into 20k fragments. When reviewing our <code>VizBin</code> plots and outputs, it is important to remember that here we are looking at the fragmented sub-contigs, rather than the full complete contigs (the importance of this will be clear when we are reviewing our <code>vb_count_table.txt</code> later in this exercise).</p> <p>In the interests of time today, the input files have been generated and are provided in the <code>6.bin_refinement/</code> folder: </p> <ul> <li><code>all_bins.fna</code> is a concatenation of the bins of fragmented sub-contigs (fragmented to 20k)</li> <li><code>all_bins.sample1.vizbin.ann</code> is the annotation file containing per-subcontig coverage, label (bin ID), and length values.</li> </ul> <p>For future reference, and for working with your own data, a step-by-step process for generating these files from the curated bins generated by <code>DAS_Tool</code> has been provided as an Appendix.</p> <p>Let's first have a quick look at the annotation file. </p> <pre><code>head -n 5 all_bins.sample1.vizbin.ann\n\n# coverage,label,length\n# 16.5255,bin_0.chopped,20000\n# 17.8777,bin_0.chopped,20000\n# 17.6983,bin_0.chopped,20000\n# 16.7296,bin_0.chopped,20000\n</code></pre> <p>This file is a comma-delimited table (csv file) that presents the information in the way that VizBin expects it. The order of rows in this file corresponds to the order of contigs in the concatenated fastA file of our fragmented bins, <code>all_bins.fna</code>.</p> <p>Create a few variations of the .ann file with various columns removed, in order to examine the different outputs they can generate.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Make a few different versions of the .ann file with various columns removed\n## annotation with bin only (to colour by bin)\ncut -f2 -d ',' all_bins.sample1.vizbin.ann &gt; all_bins.sample1.vizbin.bin_only.ann\n## no length, but coverage and bin included\ncut -f1,2 -d ',' all_bins.sample1.vizbin.ann &gt; all_bins.sample1.vizbin.no_length.ann\n</code></pre>"},{"location":"day2/ex9_refining_bins/#project-a-t-sne-and-examine-bin-clusters","title":"Project a t-SNE and examine bin clusters","text":"<p>We can now use these files in <code>VizBin</code> to curate the contigs in our bins. We will load and view the data in a few different steps.</p> <p>For this exercise, we will be using the Virtual Desktop on NeSI which allows us to use programmes with graphical user interfaces (GUI) within the NeSI computing environment.</p>"},{"location":"day2/ex9_refining_bins/#initiate-the-virtual-desktop","title":"Initiate the Virtual Desktop","text":"<ol> <li>Click on File on the top left corner of the Jupyter Hub, and then select New Launcher. You can also click on the small + on the tabbed bar next to your terminal tab.</li> <li>Look for the Virtual Desktop icon and then click on centre of the icon.      </li> <li>A new browser tab named 'TurboVNC:...' should appear and the Virtual Desktop should load.</li> <li>A successful instance of the Virtual Desktop should look like a desktop environment.</li> </ol> <p>Running VizBin: local vs remote</p> <p>Running <code>VizBin</code> remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like <code>VizBin</code>) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing <code>VizBin</code> on your local machine and downloading the relevant input files (e.g. via <code>scp ...</code>) to run locally.</p>"},{"location":"day2/ex9_refining_bins/#initiate-vizbin-within-the-virtual-desktop-environment","title":"Initiate VizBin within the Virtual Desktop environment","text":"<ol> <li>In the Virtual Desktop, click on the terminal icon.      </li> <li>VizBin is a Java programme, therefore we will need to load the Java module for it to work. In the terminal, type the following to load the Java module     <pre><code>module load Java/17\n</code></pre></li> <li> <p>In the terminal, navigate to your directory where the Java file resides</p> <p>Replace path</p> <p>Remember to replace <code>YOUR FOLDER</code> with your user name.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\nls\n</code></pre> <pre><code>all_bins.fna  all_bins.sample1.vizbin.ann  vizbin_count_table_2022.sh  VizBin-dist.jar  vizbin_example_exports\n</code></pre> </li> <li> <p>Type the following into your Virtual Desktop terminal to initiate VizBin.    <pre><code>java -jar VizBin-dist.jar\n</code></pre></p> </li> <li>A successful launch of VizBin will look like the following:     </li> </ol>"},{"location":"day2/ex9_refining_bins/#loading-the-input-files","title":"Loading the input files","text":"<p>Once <code>VizBin</code> is open, to get started, simply click the 'Choose...' button then navigate to the fastA file.</p> <p></p> <p>Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your 'bin only' .ann file into the 'Annotation file (optional)' field.</p> <p></p>"},{"location":"day2/ex9_refining_bins/#executing-the-t-sne","title":"Executing the t-SNE","text":"<p>For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following:</p>"},{"location":"day2/ex9_refining_bins/#contigs-coloured-by-bin","title":"Contigs coloured by bin","text":"Additional annotations by length and coverage <p>If you input <code>all_bins.sample1.vizbin.ann</code> as your annotation file, you can see that the visualisation takes contig length (represented by point size) and coverage (represented by opacity) into account.</p> <p> </p> <p>Similar to other projection techniques, we interpret the closeness of points as a proxy for how similar they are, and because of our .ann file we can see which contigs belong to the same bin.</p>"},{"location":"day2/ex9_refining_bins/#picking-refined-bins","title":"Picking refined bins","text":"<p>We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands:</p> <ol> <li>Left-click and drag: Highlight an area of the ordination to zoom into</li> <li>Right-click, 'Zoom Out', 'Both Axes': Rest of the view</li> <li>Left-click several points: Create a selection of contigs to extract from the data</li> <li>Right-click, 'Selection', 'Export': Save the selected contigs into a new file</li> <li>Right-click, 'Selection', 'Clear selection': Clear the current selection</li> </ol> <p>How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed.</p> <p>Today, we will run through an example of selecting potentially problematic (sub)contigs, and then deciding whether or not we want to filter these contigs out of our refined bins. We can use a combination of <code>VizBin</code> and <code>seqmagick</code> to remove contigs from bins where we do not trust the placement of the contig. We are aiming to reduce each bin to a trusted set of contigs.</p>"},{"location":"day2/ex9_refining_bins/#1-export-vizbin-clusters","title":"1. Export VizBin clusters","text":"<p>First, for each <code>VizBin</code> cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as <code>cluster_1.fna</code>. </p> <p>Try this for one or two clusters. In practice, we would do this for each <code>VizBin</code> cluster, saving each as a new <code>cluster_n.fna</code> file.</p>"},{"location":"day2/ex9_refining_bins/#highlight-a-cluster-to-zoom-into","title":"Highlight a cluster to zoom into","text":""},{"location":"day2/ex9_refining_bins/#select-the-cluster-to-export","title":"Select the cluster to export","text":"<p>Left-click several points around the cluster</p> <p></p>"},{"location":"day2/ex9_refining_bins/#export-the-cluster","title":"Export the cluster","text":"<p>Right-click, 'Selection', 'Export'. Save the output as <code>cluster_1.fna</code>. </p> <p></p>"},{"location":"day2/ex9_refining_bins/#2-export-potentially-problematic-contigs","title":"2. Export potentially problematic contigs","text":""},{"location":"day2/ex9_refining_bins/#select-problematic-contigs-to-examine","title":"Select problematic contigs to examine","text":"<p>Zoom in,  make a selection of potentially problematic contigs, and export as above.</p> <p> </p> <p>Try this for one or two problematic contigs (or subsets of contigs). In practice, you could repeat this for all potentially problemtic contigs, saving each export as a new <code>contigs_n.fna</code> file.</p> <p>Note</p> <p>For the subsequent step using <code>vizbin_count_table_2022.sh</code>, all exported cluster files must share a common prefix (e.g. <code>cluster...fna</code>), and all files of problematic contigs must also share a common prefix (e.g. <code>contigs...fna</code>).*</p>"},{"location":"day2/ex9_refining_bins/#optional-refine-and-filter-problematic-contigs-from-bins","title":"Optional: Refine and filter problematic contigs from bins","text":""},{"location":"day2/ex9_refining_bins/#create-a-count-table-of-counts-of-our-problematic-contigs-across-each-bin","title":"Create a count table of counts of our problematic contigs across each bin","text":"<p>You'll recall that, prior running <code>VizBin</code>, the contigs in our bins were first cut into fragments to improve the density of the clusters in the t-SNE projection. As such, the problematic contigs we have exported from <code>VizBin</code> are sub-contig fragments, rather than full contigs from our bins. It is entirely possible that different fragments of the original contigs have been placed in different clusters during our <code>VizBin</code> analysis - including cases where most sub-contigs have clustered with the bin we expect, and a small number have been identified as \"problematic\" (i.e. clustered with other bins). Based on the information from these extracted problematic sub-contigs, we now have to carefully consider whether or not we want to remove the full contig from our bin data.</p> <p>To do this, we will generate a table containing each exported \"problematic\" sub-contig, and counts of how many of its sister sub-contigs (each of the other sub-contig fragments derived from the same original parent contig) fall into each <code>VizBin</code> cluster.</p> <p>For this exercise, a folder of the exported files from <code>VizBin</code> for all clusters (<code>cluster_[1-n].fna</code>) and problematic sub-contigs (<code>contigs_[1-n].fna</code>) has been provided at <code>vizbin_example_exports/</code></p> <p>We will input these files to the shell script <code>vizbin_count_table_2022.sh</code> to generate a count table of the exported subcontigs across each <code>VizBin</code> cluster (<code>vb_count_table.txt</code>), as well as a working list of contigs to potentially remove from our final bin data (<code>vb_omit_contigs_tmp.txt</code>).</p> <p>For future reference, a copy of this script is available for download here.</p> <pre><code>./vizbin_count_table_2022.sh -i vizbin_example_exports/\n</code></pre> <p>The only required input to <code>vizbin_count_table_2022.sh</code> is the path to the cluster and contigs files exported from <code>VizBin</code>. By default, the script looks for the prefix <code>cluster...</code> for the cluster file names, <code>contig...</code> for the files of problematic sub-contigs, and the file extension <code>.fna</code> for each. The arguments <code>-s &lt;contig_file_prefix&gt; -c &lt;cluster_file_prefix&gt; -e &lt;fasta_file_extension&gt;</code> can optionally be provided if your file name formats differ from the default.</p> <p>View the output count table:</p> <pre><code>less vb_count_table.txt\n</code></pre> <p>Example excerpt:</p> Subcontig_ID Subcontig_vb_cluster cluster_1_count cluster_2_count cluster_3_count cluster_4_count cluster_5_count Total_count &gt;bin_3_NODE_81_length_109410_cov_1.136244.concoct_part_1 cluster_5 0 0 0 4 1 5 &gt;bin_3_NODE_289_length_18049_cov_1.596107.concoct_part_0 cluster_5 0 0 0 0 1 1 &gt;bin_3_NODE_349_length_12681_cov_1.204936.concoct_part_0 cluster_5 0 0 0 0 1 1 <p>Note that in the case of the first contig from the excerpt above, the 'problematic' contig is only one of 5 sub-contigs, and all other 4 sub-contigs are in the expected cluster. In this case, we likely do not want to remove this contig from the bin.</p>"},{"location":"day2/ex9_refining_bins/#generate-a-list-of-contigs-to-exclude-from-filtering","title":"Generate a list of contigs to exclude from filtering","text":"<p>Create a list of contigs identified from <code>vb_count_table.txt</code> that are not to be filtered out by seqmagick in the next step. For example, those contigs that have sub-contigs split across multiple vizbin clusters, and for which it's reasonable to actually keep the contig (such as when a flagged selected sub-contig exported from vizbin is in one unexpected cluster, but all other sub-contigs from that parent contig are in the expected cluster; in this case, you likely don't want to filter out the parent contig from the data set moving forward). </p> <p>Below is an example. Simply replace the contig IDs between the quotes for as many lines as necessary for your data. </p> <p>NOTES:</p> <ol> <li>The first line below must always have only one <code>&gt;</code> character, while all subsequent lines must have two (i.e. <code>&gt;&gt;</code>) to append correctly to the list.</li> <li>We want the original contig ID here, *not the sub-contig, so make sure to remove the <code>.concoct_part_n</code> fragment number at the end if there is one.*</li> </ol> <pre><code>echo \"bin_3_NODE_81_length_109410_cov_1.136244\" &gt; vb_keep_contigs.txt\n</code></pre>"},{"location":"day2/ex9_refining_bins/#create-final-vb_omit_contigs_filteredtxt-list-of-contigs-to-filter-from-bins","title":"Create final vb_omit_contigs_filtered.txt list of contigs to filter from bins","text":"<p>Using <code>grep</code>, filter contigs we wish to keep (after assessing <code>vb_count_table.txt</code>) out of the working <code>vb_omit_contigs_tmp.txt</code> list. </p> <p>This creates <code>vb_omit_contigs_filtered.txt</code>, which we will then pass to <code>seqmagick</code> to filter these contigs out of our actual bin fasta files.</p> <pre><code>grep -v -f vb_keep_contigs.txt vb_omit_contigs_tmp.txt &gt; vb_omit_contigs_filtered.txt\n</code></pre>"},{"location":"day2/ex9_refining_bins/#filter-suspect-contigs-based-on-vizbin-analysis-from-the-bin-data","title":"Filter suspect contigs (based on VizBin analysis) from the bin data","text":"<p>Use <code>seqmagick --exclude-from-file ...</code> to filter problematic contigs (those contigs listed in <code>vb_omit_contigs_filtered.txt</code>) out of the initial unchopped bin fasta files, generating final bins for downstream processing.</p> <pre><code>mkdir filtered_bins/\n\n# Load seqmagick\nmodule purge\nmodule load seqmagick/0.8.4-gimkl-2020a-Python-3.8.2\n\n# filter problematic contigs out of original bin files\nfor bin_file in example_data_unchopped/*.fna; do\nbin_name=$(basename ${bin_file} .fna)\nseqmagick convert --exclude-from-file vb_omit_contigs_filtered.txt ${bin_file} filtered_bins/${bin_name}.filtered.fna\ndone\n</code></pre> <p>Our filtered bins for downstream use are now in <code>filtered_bins/</code></p>"},{"location":"day2/ex9_refining_bins/#optional-comparing-pre-and-post-filtered-bins-via-checkm","title":"Optional: Comparing pre- and post-filtered bins via CheckM","text":"<p>The end goal of this process is the generation of a final set of refined bins. Following this, the <code>CheckM</code> procedure should be re-run, this time on the refined <code>filtered_bins/</code>. This provides <code>CheckM</code> metrics for the final actual (filtered) bin set, and also an opportunity to compare between pre- and post-filtering to see if the <code>VizBin</code> bin refinement steps have, for example, improved the degree of contamination in the bins.</p> <p>For this exercise, a copy of the output from running <code>CheckM</code> on the <code>filtered_bins/</code> is available at <code>6.bin_refinement/filtered_bins_checkm.txt</code>. View the previous <code>CheckM</code> output and the filtered bins output to compare via <code>cat</code>.</p> <pre><code>cat filtered_bins_checkm.txt </code></pre> content of filtered_bins_checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity bin_0.filtered k__Bacteria (UID3060) 138 338 246 1 327 9 1 0 0 99.59 3.79 0.00 bin_1.filtered k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 bin_2.filtered g__Staphylococcus (UID301) 45 940 178 20 918 2 0 0 0 98.32 0.11 0.00 bin_3.filtered c__Betaproteobacteria (UID3959) 235 414 211 1 408 5 0 0 0 99.97 0.90 0.00 bin_4.filtered c__Deltaproteobacteria (UID3218) 61 284 169 10 274 0 0 0 0 94.08 0.00 0.00 bin_5.filtered o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 bin_6.filtered k__Bacteria (UID2565) 2921 149 91 11 136 2 0 0 0 90.66 0.61 0.00 bin_7.filtered p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 bin_8.filtered f__Bradyrhizobiaceae (UID3695) 47 693 296 3 690 0 0 0 0 99.47 0.00 0.00 bin_9.filtered g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 <pre><code>cat ../5.binning/checkm.txt </code></pre> content of checkm.txt Bin Id Marker lineage # genomes # markers # marker sets 0 1 2 3 4 5+ Completeness Contamination Strain heterogeneity maxbin.001.fasta k__Bacteria (UID3060) 138 338 246 1 327 9 1 0 0 99.59 3.79 0.00 maxbin.002.fasta k__Bacteria (UID3060) 138 338 246 1 336 1 0 0 0 99.59 0.41 0.00 metabat.10_sub g__Staphylococcus (UID301) 45 940 178 20 918 2 0 0 0 98.32 0.11 0.00 metabat.11 c__Betaproteobacteria (UID3959) 235 414 211 1 408 5 0 0 0 99.97 0.90 0.00 metabat.12 c__Deltaproteobacteria (UID3218) 61 284 169 10 274 0 0 0 0 94.08 0.00 0.00 metabat.2 o__Pseudomonadales (UID4488) 185 813 308 25 787 1 0 0 0 96.87 0.11 0.00 metabat.3_sub k__Bacteria (UID2565) 2921 149 91 11 136 2 0 0 0 90.66 0.61 0.00 metabat.4 p__Cyanobacteria (UID2143) 129 471 367 0 470 1 0 0 0 100.00 0.14 0.00 metabat.5 f__Bradyrhizobiaceae (UID3695) 47 693 296 3 690 0 0 0 0 99.47 0.00 0.00 metabat.7 g__Vibrio (UID4878) 67 1130 369 4 1125 1 0 0 0 99.46 0.03 0.00 <p>An example of an updated slurm script to run <code>CheckM</code> on the <code>filtered_bins/</code> is as follows:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account nesi02659\n#SBATCH --job-name checkm_refined_bins\n#SBATCH --time 00:20:00\n#SBATCH --mem 50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error %x_%j.err\n#SBATCH --output %x_%j.out\nmodule purge\nmodule load CheckM/1.2.1-gimkl-2022a-Python-3.10.5\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK --pplacer_threads $SLURM_CPUS_PER_TASK -x fna \\\n--tab_table -f filtered_bins_checkm.txt \\\nfiltered_bins/ filtered_bins_checkm_out/\n</code></pre> <p>The data you have been working with was created using the <code>cut_up_fasta.py</code> script that comes with the binning tool <code>CONCOCT</code>. It was run to cut contigs into 20k fragments, to better add density to the cluster. If you would like to visualise the data using different contig fragment sizes, you can create these using the following commands (replace <code>YOUR_CONTIG_SIZE</code> with the size of interest, e.g. <code>10000</code>):</p> <pre><code>module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16\n\nmkdir custom_chop/\n\n# Fragment contigs within each bin, outputting to custom_chop/\nfor bin_file in example_data_unchopped/*;\ndo\nbin_name=$(basename ${bin_file} .fna)\ncut_up_fasta.py -c YOUR_CONTIG_SIZE -o 0 --merge_last ${bin_file} &gt; custom_chop/${bin_name}.chopped.fna\ndone\n# Concatenate the chopped bins into single .fna\ncat custom_chop/*.fna &gt; all_bins_custom_chop.fna\n</code></pre> <p>You can open <code>all_bins_custom_chop.fna</code> in VizBin to view the clustering with this new fragmentation threshold. </p> <p>If you wish to also provide an annotation file to colour by bin, this can be generated with the following:</p> <pre><code># Set up annotation file headers\necho \"label\" &gt; custom_chop.vizbin.ann\n\n# loop through custom_chop .fna files\nfor bin_file in custom_chop/*.fna; do\n# extract bin ID\nbinID=$(basename ${bin_file} .fna)\n# loop through each sequence header in bin_file, adding binID to custom_chop.vizbin.ann for each header present\nfor header in `grep \"&gt;\" ${bin_file}`; do\n# Add binID to vizbin.ann for each header present\necho \"${binID}\" &gt;&gt; custom_chop.vizbin.ann\n    done\ndone\n</code></pre> <p>If you wish to generate the full annotation file, including coverage and length values, you will need to go through the process outlined in the Appendix for this exercise.  </p>"},{"location":"day2/ex9_refining_bins/#optional-scripts-for-processing-data-with-esomana","title":"Optional: Scripts for processing data with ESOMana","text":"<p>A suite of tools for creating input files for <code>ESOMana</code> can be found on github here.</p> <p>The tool <code>ESOMana</code> can be downloaded from SourceForge.</p>"},{"location":"day3/ex10_viruses/","title":"Identifying viral contigs in metagenomic data","text":"<p>Objectives</p> <ul> <li>Identifying viral contigs using VirSorter2</li> <li>Check quality and estimate completeness of the viral contigs via CheckV</li> <li>Introduction to vContact2 for predicting taxonomy of viral contigs</li> <li>OPTIONAL: Visualise the vcontact2 gene-sharing network in Cytoscape</li> </ul>"},{"location":"day3/ex10_viruses/#identifying-viral-contigs","title":"Identifying viral contigs","text":"<p>Viral metagenomics is a rapidly progressing field, and new software are constantly being developed and released each year that aim to better identify and characterise viral genomic sequences from assembled metagenomic sequence reads. </p> <p>Currently, the most commonly used methods are VirSorter2, VIBRANT, and VirFinder (or the machine learning implementation of this, DeepVirFinder). A number of recent studies use one of these tools or a combination of several at once.</p> VirSorter2VIBRANTDeepVirFinder <p>Uses a predicted protein homology reference database-based approach, together with searching for a number of pre-defined metrics based on known viral genomic features. <code>VirSorter2</code> includes dsDNAphage, ssDNA, and RNA viruses, and the viral groups Nucleocytoviricota and lavidaviridae.* </p> <p>More info </p> <ul> <li>VirSorter2 GitHub</li> <li>Guo et al. (2021) VirSorter2: a multi-classifier, expert-guided approach to detect diverse DNA and RNA viruses</li> </ul> <p>Uses a machine learning approach based on protein similarity (non-reference-based similarity searches with multiple HMM sets), and is in principle applicable to bacterial and archaeal DNA and RNA viruses, integrated proviruses (which are excised from contigs by <code>VIBRANT</code>), and eukaryotic viruses. </p> <p>More info </p> <ul> <li>VIBRANT GitHub</li> <li>Kieft, Zhou, and Anantharaman (2020) VIBRANT: automated recovery, annotation and curation of microbial viruses, and evaluation of viral community function from genomic sequences</li> </ul> <p>Uses a machine learning based approach based on k-mer frequencies. Having developed a database of the differences in k-mer frequencies between prokaryote and viral genomes, VirFinder examines assembled contigs and identifies whether their k-mer frequencies are comparable to known viruses in the database, using this to predict viral genomic sequence. This method has some limitations based on the viruses that were included when building the database (bacterial DNA viruses, but very few archaeal viruses, and, at least in some versions of the software, no eukaryotic viruses). However, tools are also provided to build your own database should you wish to develop an expanded one. Due to its distinctive k-mer frequency-based approach, VirFinder may also have the capability of identifying some novel viruses overlooked by tools such as VIBRANT or VirSorter.</p> <p>More info </p> <ul> <li>DeepVirFinder GitHub</li> </ul>"},{"location":"day3/ex10_viruses/#identifying-viral-contigs-using-virsorter2","title":"Identifying viral contigs using VirSorter2","text":"<p>For this exercise, we will use VirSorter2 to identify viral contigs from our assembled contigs. We can also use VirSorter2 to prepare files for later use with the gene annotation tool DRAM-v, which we'll run later in the day.</p>"},{"location":"day3/ex10_viruses/#check-quality-and-estimate-completeness-of-the-viral-contigs-via-checkv","title":"Check quality and estimate completeness of the viral contigs via CheckV","text":"<p>CheckV was developed as an analogue to CheckM. CheckV first performs a 'contaminating sequence' trim, removing any retained (prokaryote) host sequence on the end of contigs with integrated prophage, and then assesses the quality and completeness of the assembled viral contigs. The quality of the contigs are also categoriesed based on the recently developed Minimum Information about an Unclutivated Virus Genome (MIUViG) standards for reporting sequences of unclutivated virus geneomes (such as those recovered from metagenomic sequencing data). The MIUViG were developed as an extension of the Minimum Information about any (x) Sequence (MIxS) standards, which include, among others, standards for Metagenome-Assembled Genomes (MIMAG).</p>"},{"location":"day3/ex10_viruses/#run-virsorter2-and-checkv","title":"Run VirSorter2 and CheckV","text":"<p>These exercises will take place in the <code>7.viruses/</code> folder.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n</code></pre> <p>For VirSorter2, we will input the assembled contigs from the SPAdes assembly we performed earlier. These assembly files have been copied to <code>7.viruses/spades_assembly/</code> for this exercise.</p> <p>We will then run CheckV in the same script, providing the fasta file of viral contigs output by VirSorter2 as input (<code>final-viral-combined.fa</code>).</p> <p>NOTE: For the <code>VirSorter/2.2.3-gimkl-2020a-Python-3.8.2</code> NeSI module to work properly, we must also include <code>module unload XALT</code> in the script below</p> <p>NOTE: The key parameters you may want to consider altering for your own work are <code>--min-score</code> and <code>--include-groups</code>. For today's excersice we will include all available groups (<code>--include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae</code>), and will set the min-score to 0.7. You can expiriment with this value for your own data (see the Virsorter2 github page for more information).</p> <p>NOTE: The required databases for *VirSorter2 are not loaded with the NeSI module. For your own work, you will need to first download these databases and provide the path to the <code>-d</code> flag below. For today's workshop this is already set up.*</p> <p>Create a new script</p> <pre><code>nano VirSorter2_and_checkv.sl\n</code></pre> <p>Warning</p> <p>Paste in the following (updating <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      VirSorter2_and_checkv\n#SBATCH --time          05:00:00\n#SBATCH --mem           2GB\n#SBATCH --cpus-per-task 28\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n\n# VirSorter2\n## Load modules\nmodule purge\nmodule unload XALT\nmodule load VirSorter/2.2.3-gimkl-2020a-Python-3.8.2\n\n## Output directory\nmkdir -p VirSorter2\n\n## Run VirSorter2\nvirsorter run -j $SLURM_CPUS_PER_TASK \\\n-i spades_assembly/spades_assembly.m1000.fna \\\n-d /nesi/project/nesi02659/MGSS_2023/resources/virsorter2_20210909/ \\\n--min-score 0.7 --include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae \\\n--prep-for-dramv \\\n-l mgss -w VirSorter2 \\\n--tmpdir ${SLURM_JOB_ID}.tmp --rm-tmpdir \\\nall \\\n--config LOCAL_SCRATCH=${TMPDIR:-/tmp}\n# CheckV\n## Load module\nmodule purge\nmodule load CheckV/1.0.1-gimkl-2022a-Python-3.10.5\n\n## Output directory\nmkdir -p checkv_out\n\n## Run CheckV\ncheckv end_to_end VirSorter2/mgss-final-viral-combined.fa checkv_out/ -t $SLURM_CPUS_PER_TASK\n</code></pre> <p>Submit the script as a slurm job</p> <pre><code>sbatch VirSorter2_and_checkv.sl\n</code></pre>"},{"location":"day3/ex10_viruses/#outputs-of-virsorter2-and-checkv","title":"Outputs of VirSorter2 and CheckV","text":"<p>Key outputs from VirSorter2 include:</p> <ul> <li><code>mgss-final-viral-combined.fa</code>: FASTA file of identified viral sequences</li> <li><code>mgss-final-viral-score.tsv</code>: table with score of each viral sequences across groups and a few more key features, which can also be used for further filtering</li> <li><code>mgss-for-dramv/</code>: files to be used as input to DRAM-v for gene prediction and annotation (we will be running DRAM-v later today during the gene annotation session)</li> </ul> <p>CheckV provides summary outputs for contamination, completeness, repeats, and an overall quality summary. Later today we will have a brief look at some examples of the information you can draw from these CheckV outputs. </p>"},{"location":"day3/ex10_viruses/#exercise-examine-viral-output-files-from-virsorter2-and-checkv","title":"Exercise: Examine viral output files from VirSorter2 and CheckV","text":"<p>VirSorter2 and CheckV provide several of different output files that are important for identifying and understanding the viruses present in your data. Explore through the following files: </p> <ul> <li><code>7.viruses/VirSorter2/mgss-final-viral-score.tsv</code></li> <li><code>7.viruses/checkv_out/quality_summary.tsv</code></li> </ul> <p>When viewing these files, see if you can find the following information:</p> <ul> <li>How many viral contigs did VirSorter2 identify?</li> <li>How many viral contigs meet the \"High-quality\" (MIUViG) standard?</li> <li>How many might we consider \"complete\" genomes based on CheckV's completeness estimation?</li> <li>Are any of the identified viral contigs complete cirular genomes (based on identifying direct terminal repeat regions on both ends of the genome)? If not, think about why this might be the case for this dataset (hint: the workshop materials are a manufactured \"metagenome\" data set based on compiling several individual genomes)</li> <li>Are there any suspicious contigs that you might want to flag for closer examination (and/or careful consideration in downstream analyses)? (Note that standard practice would be to use these CheckV results as one basis for filtering to remove potential false positives)</li> </ul>"},{"location":"day3/ex10_viruses/#introduction-to-vcontact2-for-predicting-taxonomy-of-viral-contigs","title":"Introduction to vContact2 for predicting taxonomy of viral contigs","text":"<p>Even more so than prokaryote taxonomy, establishing a coherent system for viral taxonomy is complex and continues to evolve. In 2020, the International Committee on Taxonomy of Viruses (ICTV) overhauled the classification code into 15 hierarchical ranks. Furthermore, the knowledge gap in databases of known and taxonomically assigned viruses remains substantial, and so identifying the putative taxonomy of viral contigs from environmental metagenomics data remains challenging.</p> <p>There are a number of approaches that can be used to attempt to predict the taxonomy of the set of putative viral contigs output by programs such as VIBRANT, VirSorter, and VirFinder. vContact2 is one such method that uses 'guilt-by-contig-association' to predict the potential taxonomy of viral genomic sequence data based on relatedness to known viruses within a reference database (such as viral RefSeq). The principle is that, to the extent that the 'unknown' viral contigs cluster closely with known viral genomes, we can then expect that they are closely related enough to be able to predict a shared taxonomic rank. </p> <p>Note</p> <p>Anecdotally, however, in my own experience with this processes I have unfortunately been unable to directly predict the taxonomy of the vast majority of the viral contigs ouput by VIBRANT, VirSorter, or VirFinder from an environmental metagenomic data set (due to not clustering closely enough with known viruses in the reference database). You can, however, visualise the gene-sharing network generated to infer the likely taxonomy of each of your viruses at higher taxonomic ranks due to the relatedness to known reference viral genomes.</p> <p>Running vContact2 can require a considerable amount of computational resources, and so we won't be running this in the workshop today. The required process is outlined for reference in an Appendix for this exercise, should you wish to experiment with this on your own data in the future. </p> <p>For today, we have provided some of the output files from this process when applied to our mock metagenome data. A selection of these can be viewed in the folder <code>7.viruses/vConTACT2_Results/</code> via <code>head</code> or <code>less</code>.</p> <pre><code>less vConTACT2_Results/genome_by_genome_overview.csv\n</code></pre> <pre><code>less vConTACT2_Results/tax_predict_table.txt\n</code></pre> <p>A few notes to consider: </p> <ul> <li>You will see that the <code>genome_by_genome_overview.csv</code> file contains entries for the full reference database used as well as the input viral contigs (contigs starting with <code>NODE</code>). </li> <li> <p>You can use a command such as <code>grep \"NODE\" vConTACT2_Results/genome_by_genome_overview.csv | less</code> to view only the lines for the input contigs of interest. </p> <ul> <li>Note also that these lines however will not contain taxonomy information. </li> <li>See the notes in the Appendix for further information about why this might be.</li> </ul> </li> <li> <p>As per the notes in the Appendix, the <code>tax_predict_table.txt</code> file contains predictions of potential taxonomy (and or taxonomies) of the input viral contigs for order, family, and genus, based on whether they clustered with any viruses in the reference database.</p> <ul> <li>Note that these may be lists of multiple potential taxonomies, in the cases where viral contigs clustered with multiple reference viruses representing more than one taxonomy at the given rank.</li> </ul> <p>The taxonomies are deliberately enclosed in square brackets (<code>[ ]</code>) to highlight the fact that these are predictions, rather than definitive taxonomy assignments.</p> </li> </ul>"},{"location":"day3/ex10_viruses/#optional-visualise-the-vcontact2-gene-sharing-network-in-cytoscape","title":"OPTIONAL: Visualise the vContact2 gene-sharing network in cytoscape","text":"<p>We can visualise the gene-sharing network generated by vContact2 (<code>c1.ntw</code>) using the software Cytoscape. Cytoscape runs as a GUI (graphical user interface), so we will need to either download and install this software or open Cytoscape using NeSI's Virtual Desktop. To open in the Virtual Desktop, click New Laucher in Jupyterhub and lauch Virtual Desktop. This will open a new tab. In the new desktop, open a terminal, and then load and run the Cytoscape module as per below.</p> <p>Copy/paste in the Virtual Desktop</p> <p>You may not be able to copy/paste into the Virtual Desktop, in which case you will need to manually type these commands.</p> <pre><code># Load the module\nmodule purge\nmodule load Cytoscape/3.9.1\n# Run cytoscape\nCytoscape\n</code></pre> <p>Do not update Cytoscape!</p> <p>A dialog box will appear telling you about a new version of Cytoscape. Click \"discard\", as we will not be installing any new versions today!</p> <p>In Cytoscape, we can load the gene-sharing network by clicking <code>File/Import/Network from file</code>, and then opening the <code>c1.ntw</code> file (You may need to click the <code>Home</code> button and then navigate to the relevant directory when <code>c1.ntw</code> is located (i.e. in <code>/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/vConTACT2_Results/</code>)). </p> <p>In the \"Import Network From Table\" pop-up box: </p> <ul> <li> <p>Click Advanced Options:</p> <ol> <li>Select SPACE as the delimiter</li> <li>Uncheck the Use first line as column names box</li> <li>Click OK</li> </ol> </li> <li> <p>In the dropdown menu for \"Column 1\": </p> <ol> <li>Select the green dot (source node) </li> <li>For Column 2 select the red target (Target node)</li> <li>Click OK</li> </ol> </li> </ul> <p>It will now ask if you want to create a view for your large networks now. Click OK. This may take a minute to generate the network visualisation. </p> <p>There are many ways to modify and inspect this visualisation. One basic addition that will help us interpret the results here is to colour code the viral genomes based on reference (RefSeq) genomes and the viral contigs recovered from our dataset. We can do this by loading the <code>genome_by_genome_overview.csv</code> file. </p> <p><code>Dataset</code> column</p> <p>For the purposes of this workshop, we have added the additional column <code>Dataset</code> to the <code>genome_by_genome_overview.csv</code> file stating whether each viral sequence originated from either the reference database (<code>RefSeq</code>) or our own data (<code>Our_data</code>). This column is not generated by vConTACT2, but you can open the file in Excel to add any additional columns you would like to colour code the nodes by.</p> <p>Click <code>File/Import/Table from file</code> and select the <code>genome_by_genome_overview.csv</code> file to open. In the pop-up box, leave the settings as default and click OK. This will add a bunch of metadata to your network table. We can now use this to colour code the nodes in the network.</p> <p>To colour code the nodes by genome source (<code>Refseq</code> or <code>Our_data</code>): </p> <ol> <li>Click the <code>Style</code> tab on the far left</li> <li>Select the dropdown arrow by <code>Fill Color</code></li> <li>Next to <code>Column</code> click on --select value-- and select <code>Dataset</code> </li> <li>For <code>Mapping Type</code>, select <code>Discrete Mapping</code>. </li> <li>Then for each of <code>Refseq</code> and <code>Our_data</code>, click the three dots in the empty box to the right of the label and select a colour for that dataset.</li> </ol> <p>You can now zoom in on the network and select viral contigs from your own dataset, and examine which reference viruses may be the most closely related. This can give an indication of the possible taxonomy of your own viruses based on their gene-sharing relatedness to the known reference genomes.</p>"},{"location":"day3/ex11_coverage_and_taxonomy/","title":"Assigning taxonomy to refined prokaryotic bins","text":"<p>Objectives</p> <ul> <li>Assign taxonomy to the refined bins</li> </ul>"},{"location":"day3/ex11_coverage_and_taxonomy/#assign-taxonomy-to-the-refined-bins","title":"Assign taxonomy to the refined bins","text":"<p>It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us:</p> <ol> <li>Extract 16S rRNA gene sequences from the MAGs and classify them</li> <li>Annotate each gene in the MAG and take the consensus taxonomy</li> <li>Use a profiling tool like <code>Kraken</code>, which matches pieces of DNA to a reference database using k-mer searches</li> <li>Identify a core set of genes in the MAG, and use these to compute a species phylogeny</li> </ol> <p>For this exercise, we will use the last option in the list, making use of the <code>GTDB-TK</code> software (available on github) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, <code>GTDB-TK</code> will perform the following steps on a set of bins.</p> <ol> <li>Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG.</li> <li>Based on the recovered numbers, identify which domain is a more likely assignment for each MAG</li> <li>Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions</li> <li>Filter the alignment down to approximately 5,000 informative sites</li> <li>Insert each MAG into a reference tree create from type material and published MAGs</li> <li>Scale the branch lengths of the resulting tree, as described in Parks et al., to identify an appropriate rank to each branch event in the tree</li> <li>Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree</li> <li>Report the resulting taxonomic assignment, and gene alignment</li> </ol> <p>This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process.</p> <p>For the following exercises, we will be working in <code>8.prokaryotic_taxonomy/</code>.</p> <p>Create a new script</p> <pre><code>nano gtdbtk.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (replacing <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      gtdbtk\n#SBATCH --time          01:00:00\n#SBATCH --mem           140GB\n#SBATCH --cpus-per-task 24\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load GTDB-Tk/2.1.0-gimkl-2020a-Python-3.9.9\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/8.prokaryotic_taxonomy\n\n# Run GTDB-Tk\ngtdbtk classify_wf -x fna --cpus $SLURM_CPUS_PER_TASK \\\n--keep_intermediates \\\n--genome_dir filtered_bins/ \\\n--out_dir gtdbtk_out/\n</code></pre> <p>Submit the script</p> <pre><code>sbatch gtdbtk.sl\n</code></pre> <p>As usual, lets look at the parameters here</p> Parameter Function classify_wf Specifies the sub-workflow from <code>GTDB-TK</code> that we wish to use -x ... Specify the file extension for MAGs within our input directory.Default is .fna, but it's always good practice to specify it anyway --cpus ... Number of threads/CPUs to use when finding marker genes, and performing tree insertion operations --keep_intermediates Keep intermediate outputs --genome_dir ... Input directory containing MAGs as individual fastA files --out_dir ... Output directory to write the final set of files <p>Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw <code>DAS_Tool</code> outputs in the <code>../6.bin_refinement/dastool_out/_DASTool_bins/</code> folder, the renamed set of bins in the <code>../6.bin_refinement/example_data_unchopped/</code> folder, the set of curated bins in the <code>filtered_bins/</code> folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here.</p> <p>When the task completes, you will have a number of output files provided. The main ones to look for are <code>gtdbtk.bac120.summary.tsv</code> and <code>gtdbtk.arch122.summary.tsv</code> which report the taoxnomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the <code>gtdbtk.arch122.summary.tsv</code> file.</p> <p>If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the <code>gtdbtk.bac120.msa.fasta</code> and <code>gtdbtk.arch122.msa.fasta</code> files.</p> <p>Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop.</p>"},{"location":"day3/ex12_gene_prediction/","title":"Gene prediction","text":"<p>Objectives</p> <ul> <li>Overview/refresher of <code>prodigal</code></li> <li>Predicting protein coding sequences in metagenome-assembled genomes</li> <li>Predicting RNA features and non-coding regions</li> </ul>"},{"location":"day3/ex12_gene_prediction/#overviewrefresher-of-prodigal","title":"Overview/refresher of prodigal","text":"<p>At this stage we have recovered a number of high quality genomes or population genomes. While there are interesting biological questions we can ask of the genomes at the DNA/organisational level, it is more likely that we are interested in the genes present in the organism.</p> <p>How we predict genes in the metagenomic data varies depending on what features we are trying to detect. Most often, we are interested in putatively protein coding regions and open reading frames. For features that are functional but not not translated, such as ribosomal RNA and tRNA sequences we need to use alternative tools. When considering protein coding sequences, we avoid the use of the term 'open reading frame' (ORF). The nature of a fragmented assembly is that you may encounter a partial gene on the start or end of a contig that is a function gene, but lacks the start or stop codon due to issues with assembly or sequencing depth.</p> <p>There are many software tools to predict gene sequences and in this workshop we will start with the tool <code>prodigal</code> (PROkaryotic\u202fDynamic Programming Genefinding\u202fALgorithm). <code>prodigal</code> has gone on to become one of the most popular microbial gene prediction algorithms as in incorporates modeling algorithms to profile the coding sequences within your genome and better identify the more cryptic (or partial) genes.</p> <p><code>prodigal</code> is execellent for the following use cases:</p> <ol> <li>Predicting protein-coding genes in draft genomes and metagenomes  </li> <li>Quick and unsupervised execution, with minimal resource requirements</li> <li>Ability to handle gaps, scaffolds, and partial genes </li> <li>Identification of translation initiation sites </li> <li>Multiple output formats, including either straight fastA files or the DNA sequence and protein translation for genes, as well as detailed summary statistics for each gene (e.g. contig length, gene length, GC content, GC skew, RBS motifs used, and start and stop codon usage)</li> </ol> <p><code>prodigal</code> is not the best tool to use for the following cases:</p> <ol> <li>Predicting RNA genes </li> <li>Handling genes with introns </li> <li>Deal with frame shifts</li> </ol> <p>It is also not advised to use <code>prodigal</code> when making predictions through your unassembled reads. If you are working with unassembled data, <code>FragGeneScan</code> is a better tool, as it is more sensitive for partial genes and does not assume every piece of DNA in the input fastA file must be coding.</p>"},{"location":"day3/ex12_gene_prediction/#predicting-protein-coding-sequences-in-mags","title":"Predicting protein coding sequences in MAGs","text":"<p>To get started, move into the exercise directory.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction/\n</code></pre>"},{"location":"day3/ex12_gene_prediction/#examining-the-prodigal-parameters","title":"Examining the prodigal parameters","text":"<p>Before we start runnning <code>prodigal</code>, we will take a quick look at the parameters.</p> <pre><code>module purge\nmodule load prodigal/2.6.3-GCC-11.3.0\n\nprodigal -h\n\n# Usage:  prodigal [-a trans_file] [-c] [-d nuc_file] [-f output_type]\n#                  [-g tr_table] [-h] [-i input_file] [-m] [-n] [-o output_file]\n#                  [-p mode] [-q] [-s start_file] [-t training_file] [-v]\n#          -a:  Write protein translations to the selected file.\n#          -c:  Closed ends.  Do not allow genes to run off edges.\n#          -d:  Write nucleotide sequences of genes to the selected file.\n#          -f:  Select output format (gbk, gff, or sco).  Default is gbk.\n#          -g:  Specify a translation table to use (default 11).\n#          -h:  Print help menu and exit.\n#          -i:  Specify FASTA/Genbank input file (default reads from stdin).\n#          -m:  Treat runs of N as masked sequence; don't build genes across them.\n#          -n:  Bypass Shine-Dalgarno trainer and force a full motif scan.\n#          -o:  Specify output file (default writes to stdout).\n#          -p:  Select procedure (single or meta).  Default is single.\n#          -q:  Run quietly (suppress normal stderr output).\n#          -s:  Write all potential genes (with scores) to the selected file.\n#          -t:  Write a training file (if none exists); otherwise, read and use\n#               the specified training file.\n#          -v:  Print version number and exit.\n</code></pre> <p>There are a few parameters that are worth considering in advance.</p>"},{"location":"day3/ex12_gene_prediction/#output-files","title":"Output files","text":"<p>When running <code>prodigal</code> the default behaviour is to create a gbk file (this is a Genbank-like feature table, see here for more information) from your genome and write it to the stdout of your interface. This can either be captured using a redirect, or the output can instead be placed into a file using the <code>-o</code> flag. You can also change the format of the file using the <code>-f</code> flag.</p> <p>Since we often want to go straight from gene prediction to annotation, <code>prodigal</code> also has the option to create fastA files of the gene prediction (<code>-d</code>) and protein translation (<code>-a</code>) at the same time. This is an extremely helpful feature, and it is worth running all three outputs at the same time. Generally speaking, you will probably find that the amino acid sequence for your genes is all you need for most practical purposes, but having the corresponding nucleotide sequence can sometimes be useful if we want to mine other data sets.</p>"},{"location":"day3/ex12_gene_prediction/#modes-of-gene-prediction","title":"Modes of gene prediction","text":"<p>As mentioned in the introduction to this exercise, <code>prodigal</code> uses the profiles of genes it detects in your data set to better tune its prediction models and improve coding sequence recovery. It has three algorithms for how the training is performed which you must determine in advance:</p> Parameter Mode Description Normal mode -p single Take the sequence(s) you provide and profiles the sequence(s) properties. Gene predictions are then made based upon those properties.Normal mode\u202fshould be used on finished genomes, reasonable quality draft genomes, and big viruses. Anonymous mode -p meta Apply pre-calculated training files to the provided input sequences.Anonymous mode\u202fshould be used on metagenomes, low quality draft genomes, small viruses, and small plasmids. Training mode -p train Works like normal mode, but <code>prodigal</code> saves a training file for future use. <p>Anecdotally, when applied to a MAG or genome, anonymous mode (<code>-p meta</code>) will identify slightly fewer genes than normal mode (<code>-p single</code>). However, single mode can miss laterally transfered elements. There is not necesarily a best choice for which version to use and this is at the users discretion.</p>"},{"location":"day3/ex12_gene_prediction/#executing-prodigal","title":"Executing prodigal","text":"<p>We will now run <code>prodigal</code> over the 10 bins in anonymous mode using an array.</p> <p>Create a new script</p> <pre><code>nano prodigal.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (modifying <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      prodigal\n#SBATCH --time          00:10:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 1\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Load modules\nmodule purge\nmodule load prodigal/2.6.3-GCCcore-7.4.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction\n\n# Output directory\nmkdir -p predictions/\n\n# Variables\nbin_file=filtered_bins/bin_${SLURM_ARRAY_TASK_ID}.filtered.fna\npred_file=$(basename ${bin_file} .fna)\n# Run prodigal\nprodigal -i ${bin_file} -p meta \\\n-d predictions/${pred_file}.genes.fna \\\n-a predictions/${pred_file}.genes.faa \\\n-o predictions/${pred_file}.genes.gbk\n</code></pre> <p>Submit the script</p> <pre><code>sbatch prodigal.sl\n</code></pre> <p>Once <code>prodigal</code> has completed, let's check one of the output files:</p> <pre><code>head -n 5 predictions/bin_0.filtered.genes.faa\n\n# &gt;bin_0_NODE_6_length_607162_cov_1.000759_1 # 2 # 667 # -1 # ID=1_1;partial=10;start_type=ATG;rbs_motif=AGGAG;rbs_spacer=5-10bp;gc_cont=0.321\n# MIKSNGILFTGKKFVMGAVVSALLATSGIAADYTLKFSHVVSPNTPKGKAADFFAKRLEE\n# LSGGKIDVQVYPSSQLYNDSAVLKALRLDSVQMAAPSFSKFGKIVPQLALFNLPFLFKDI\n# DQLHRVQDGPVGEKLKSLVTAKGFVALNFWDNGFKQLSSSKEPLLMPKDAEGQKFRIMSS\n# KVLEAQFKAVGANPQMMPFSEVYSGLQQGVIDAAENPFSNIY\n</code></pre> <p>There are a few thing to unpack here. First lets look at the first line of the fastA file:</p> <pre><code>&gt;bin_0_NODE_6_length_607162_cov_1.000759_1 # 2 # 667 # -1 # ID=1_1;partial=10;start_type=ATG;rbs_motif=AGGAG;rbs_spacer=5-10bp;gc_cont=0.321\n|                                       |   |   |      |   |      |\n|                                       |   |   |      |   |      Are the gene boundaries complete or not\n |                                       |   |   |      |   Unique gene ID\n |                                       |   |   |      Orientation\n |                                       |   |   Stop position\n |                                       |   Start position\n |                                       Gene suffix\n Contig name  </code></pre> <p>Here are the first few pieces of information in the fastA header identified by what they mean. <code>prodigal</code> names genes using the contig name followed by an underscore then the number of the gene along the contig. The next two pieces of information are the start and stop coordinates of the gene. Next, a 1 is report for a gene that is in the forward orientation (relative to the start of the contig) and a -1 for genes that are in reverse orientation.</p> <p>There is also a unique gene ID provided although this may not be necessary. As long as your contig names are unique, then all gene names generated from them will also be unique.</p> <p>The last option that is important to check is the partial parameter. This is reported as two digits which correspond to the start and end of the gene and report whether or not the gene has the expected amino acids for the start (M) and end of a gene (* in the protein file). A 0 indicates a complete gene edge, and 1 means partial. In this case, we have '10' which indicates the gene runs off the left edge of the contig. Alternate outcomes for this field are '00', '01', or '11'.</p>"},{"location":"day3/ex12_gene_prediction/#stripping-metadata","title":"Stripping metadata","text":"<p>While this header information can be very informative, its presence in the fastA files can lead to some downstream issues. The fastA file format specifies that the sequence name for each entry runs from the '&gt;' character to the first space, and everything after the space is metadata. Some bioinformatic programs are aware of this convention and will strip the metadata when producing their outputs, but some tools do not do this. It's really easy to end up in situtations where your gene names are failing to match between analyses because of this inconsistency, so we recommend creating new fastA files with the metadata removed to preempt this problem.</p> <pre><code>for pred_file in predictions/*.fna;\ndo\nfile_base=$(basename ${pred_file} .fna)\ncut -f1 -d ' ' predictions/${file_base}.fna &gt; predictions/${file_base}.no_metadata.fna\n    cut -f1 -d ' ' predictions/${file_base}.faa &gt; predictions/${file_base}.no_metadata.faa\ndone\n</code></pre> <p><code>cut</code> flags</p> <ul> <li><code>-f</code> argument is how we specify which columns to keep. It can be used to specify a range as well</li> <li><code>-d-</code> delimiter : <code>cut</code> uses tab as a default field delimiter but can also work with other delimiter by using <code>-d</code> option</li> </ul> <pre><code>head -n 5 predictions/bin_0.filtered.genes.no_metadata.faa\n\n# &gt;bin_0_NODE_6_length_607162_cov_1.000759_1\n# MIKSNGILFTGKKFVMGAVVSALLATSGIAADYTLKFSHVVSPNTPKGKAADFFAKRLEE\n# LSGGKIDVQVYPSSQLYNDSAVLKALRLDSVQMAAPSFSKFGKIVPQLALFNLPFLFKDI\n# DQLHRVQDGPVGEKLKSLVTAKGFVALNFWDNGFKQLSSSKEPLLMPKDAEGQKFRIMSS\n# KVLEAQFKAVGANPQMMPFSEVYSGLQQGVIDAAENPFSNIY\n</code></pre>"},{"location":"day3/ex12_gene_prediction/#predicting-rna-features-and-non-coding-regions","title":"Predicting RNA features and non-coding regions","text":""},{"location":"day3/ex12_gene_prediction/#predicting-rrna-sequences","title":"Predicting rRNA sequences","text":"<p>While they will not be covered in great detail here, there are a few other prediction tools that are useful when working with metagenomic data. The first of these is <code>MeTaxa2</code>, which can be used to predict ribosomal RNA sequences in a genome. Detection of these is a handy way to link your MAGs to the scientific literature and taxonomy, although recovery of ribosomal sequences like the 16S rRNA subunit is often not successful.</p> <p>To attempt to find the small (16S, SSU) and large (28S, LSU) ribosomal subunits in our data, use the following commands.</p> <p>Create a new script</p> <pre><code>nano metaxa2.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (replacing <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      metaxa2\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 4\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Load modules\nmodule purge\nmodule load Metaxa2/2.2.3-gimkl-2022a\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction\n\n# Output directory\nmkdir -p ribosomes/\n\n# Variables\nbin_file=filtered_bins/bin_${SLURM_ARRAY_TASK_ID}.filtered.fna\npred_file=$(basename ${bin_file} .fna)\n# Run Metaxa2\nfor ribosome_type in ssu lsu; do\nmetaxa2 --cpu $SLURM_CPUS_PER_TASK -g ${ribosome_type} --mode genome \\\n-i ${bin_file} -o ribosomes/${pred_file}.${ribosome_type}\ndone\n</code></pre> <p>Submit the script</p> <pre><code>sbatch metaxa2.sl\n</code></pre> <p>The parameters here are fairly self-explanatory, so we won't discuss them in detail. Briefly, <code>--cpu</code> tells the program how many CPUs to use in sequence prediction, and the <code>-g</code> flag determines whether we are using the training data set for SSU or LSU regions. the <code>-i</code> and <code>-o</code> flags denote the input file and output prefix.</p> <p>The only other parameter that can be helpful is the <code>-t</code> flag to indicate taxa type (<code>b</code>, bacteria, <code>a</code>, archaea, <code>e</code>, eukaryota, <code>m</code>, mitochondrial, <code>c</code>, chloroplast, <code>A</code>, all, <code>o</code>, other). By default, <code>MeTaxa2</code> will search your genome for the following ribosomal signatures:</p> <ol> <li>Bacteria</li> <li>Archaea</li> <li>Chloroplast</li> <li>Mitochondira</li> <li>Eukaryote</li> </ol> <p>It is usually worth letting it search for all options as detecting multiple rRNAs from different lineages can be a good sign of binning contamination. However, if you want to restrict the search or provide a custom training set this can be set with the <code>-t</code> flag.</p>"},{"location":"day3/ex12_gene_prediction/#predicting-trna-and-tmrna-sequences","title":"Predicting tRNA and tmRNA sequences","text":"<p>The MIMAG standard specifies that in order to reach particular quality criteria, a MAG must contain a certain number or tRNA sequences. We can search a MAG or genome for these using <code>Aragorn</code> (link here).</p> <p> </p>"},{"location":"day3/ex13_gene_annotation_part1/","title":"Gene annotation (part 1)","text":"<p>Objectives</p> <ul> <li>Gene annotation (part 1)<ul> <li>Objectives</li> <li>BLAST-like gene annotations and domain annotations</li> <li>BLAST-like annotation</li> <li>HMM-profiling of domains</li> <li>Annotating MAGs against the UniProt database with diamond</li> <li>Annotating MAGs against the Pfam database with hmmer</li> <li>Evaluating the quality of gene assignment</li> <li>Differences in taxonomies</li> <li>Examples of various genome-level and protein taxonomies</li> </ul> </li> </ul>"},{"location":"day3/ex13_gene_annotation_part1/#blast-like-gene-annotations-and-domain-annotations","title":"BLAST-like gene annotations and domain annotations","text":"<p>Broadly speaking, there are two ways we perform gene annotations with protein sequences. Both compare our sequences of interest against a curated set of protein sequences for which function is known, or is strongly suspected. In each case, there are particular strenths to the approach and for particular research questions, one option may be favoured over another.</p>"},{"location":"day3/ex13_gene_annotation_part1/#blast-like-annotation","title":"BLAST-like annotation","text":"<p>The first of these is the <code>BLAST</code> algorithm for sequence alignment. This approach performs pairwise alignment between the gene of interest (query sequence) and the sequences in the database (target sequence). <code>BLAST</code> searches each potential target sequence for k-mers identified in the query sequence. Where these k-mers are found in targets, the ends are extended out to try to create longer regions of highly similar sequence spans. Across this span, the tool identifies the longest span of characters (nucleotide or amino acid) that match within a scoring framework to return the length of the region (coverage) and the sequence identity over the span (identity).</p> <p>The original tool for performing this kind of analysis was the <code>BLAST</code> tool. While <code>BLAST</code> and its variants are still excellent tools for performing this kind of sequence annotation, they suffer from a slow runtime speed due to the need to test each query sequence against every target sequence in the database. For this reason, several tools have been published which take the basic approach of <code>BLAST</code>, but augment it with methods to reduce the number of pairwise comparisons needed to identify targets with high sequence similarity to the query. Two popular pieces of software are the tools <code>usearch</code> here and <code>diamond</code> here.</p>"},{"location":"day3/ex13_gene_annotation_part1/#hmm-profiling-of-domains","title":"HMM-profiling of domains","text":"<p>An alternate method for attributing function to query sequences is to consider them as a collection of independently functioning protein folding domains. This is the approach used in the HMMer software, and the Pfam, TIGRfam, and PANTHER databases. In these analyses, the database consists not of individual sequences, but of Hidden Markov models built from a collection of proteins that share a common domain. These profiles build out a statistical map of the amino acid transitions (from position to position), variations (differences at a position), and insertions/deletions between positions in the domain across the different observations in the training database and apply these maps to the query data.</p> <p>These exercises will take place in the <code>10.gene_annotation_and_coverage/</code> folder. </p>"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-uniprot-database-with-diamond","title":"Annotating MAGs against the UniProt database with diamond","text":"<p>For this exercise we are going to use <code>diamond</code> for performing our annotation. We have chosen to use this tool because it is faster than BLAST, and <code>usearch</code> comes with licensing restrictions that make it hard to work with in a shared computing environment like NeSI.</p> <p>For this exercise we have created a diamond-compatible database from the 2018 release of the UniProt database.</p> <p>For input files, the <code>predictions/</code> results from the previous gene prediction exercise have been copied over to <code>10.gene_annotation_and_coverage/predictions/</code>.</p> <p>In general, <code>diamond</code> takes a pair of input files - the protein coding sequences we wish to annotate and the database we will use for this purpose. There are a few parameters that need to be tweaked for obtaining a useful output file, however.</p> <pre><code>module purge\nmodule load DIAMOND/2.0.15-GCC-11.3.0\n\ndiamond help\n# diamond v2.0.15.153 (C) Max Planck Society for the Advancement of Science\n# Documentation, support and updates available at http://www.diamondsearch.org\n# Please cite: http://dx.doi.org/10.1038/s41592-021-01101-x Nature Methods (2021)\n# Syntax: diamond COMMAND [OPTIONS]\n# Commands:\n# ...\n# blastp  Align amino acid query sequences against a protein reference database\n# ...\n# General options:\n# --threads (-p)         number of CPU threads\n# --db (-d)              database file\n# --out (-o)             output file\n# --outfmt (-f)          output format\n# ...\n</code></pre> <p>There are two output formats we can chose from which are useful for our analysis. We will obtain our output in the BLAST tabular format, which provides the annotation information in a simple-to-parse text file that can be viewed in any text or spreadsheet viewing tool. This will allow us to investigate and evaluate the quality of our annotations. </p> <p>Awkwardly, <code>diamond</code> does not provide the headers for what the columns in the output table mean. This table is a handy reference for how to interpret the output.</p> <p>From here we can view important stastics for each query/target pairing such as the number of identical residues between sequences and the aligned length between query and target.</p> <p>Before we begin, we need to create an directory for outputs.</p> <pre><code>mkdir -p gene_annotations\n</code></pre> <p>Now, lets set up a slurm job to annotate each of our MAGs. </p> <p>Create a new script</p> <pre><code>nano annotate_uniprot.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (update <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_uniprot\n#SBATCH --time          01:00:00\n#SBATCH --mem           20GB\n#SBATCH --cpus-per-task 20\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Load modules\nmodule purge\nmodule load DIAMOND/2.0.15-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Variables\nprot_file=predictions/bin_${SLURM_ARRAY_TASK_ID}.filtered.genes.no_metadata.faa\nout_file=$(basename ${prot_file} .faa)\ndb=/nesi/nobackup/nesi02659/MGSS_resources_2022/databases/uniprot.20181026.dmnd\n\n# Run DIAMOND\ndiamond blastp --threads $SLURM_CPUS_PER_TASK --max-target-seqs 5 --evalue 0.001 \\\n--db $db --query ${prot_file} --outfmt 6 \\\n--out gene_annotations/${out_file}.uniprot.txt\n</code></pre> <p>Submit the script</p> <pre><code>sbatch annotate_uniprot.sl\n</code></pre>"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-pfam-database-with-hmmer","title":"Annotating MAGs against the Pfam database with hmmer","text":"<p>The standard software for performing HMM-profiling annotation is hmmer. Compared to <code>BLAST</code>, <code>FASTA</code>, and other sequence alignment and database search tools based on older scoring methodology, <code>HMMER</code> aims to be significantly more accurate and more able to detect remote homologs because of the strength of its underlying mathematical models. In the past, this strength came at significant computational expense, but in the new <code>HMMER3</code> project, <code>HMMER</code> is now essentially as fast as <code>BLAST</code>. </p> <p><code>HMMER</code> will search one or more profiles against a sequence database for sequence hommologs, and for making sequence alignments, implementing profile hidden Markov models. In this exercise, we will perform a search using <code>hmmsearch</code>. For each profile in hmmfile, <code>HMMER</code> uses that query profile to search the target database of sequences indicated in seqdb, and output ranked lists of the sequences with the most significant matches to the profile. <code>hmmsearch</code> accepts any fastA file as target database input. It also accepts EMBL/UniProtKB text format, and Genbank format. It will automatically determine what format your file is in so you don\u2019t have to specify it. </p> <p>As we did with <code>diamond</code>, we will also have to modify some parameters to get the desired ouotput. </p> <pre><code>module load HMMER/3.3.2-GCC-11.3.0\n</code></pre> <code>hmmsearch -h</code> <pre><code># hmmsearch :: search profile(s) against a sequence database\n# HMMER 3.3.2 (Nov 2020); http://hmmer.org/\n# Copyright (C) 2020 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n# Usage: hmmsearch [options] &lt;hmmfile&gt; &lt;seqdb&gt;\n# Basic options:\n#  -h : show brief help on version and usage\n# Options directing output:\n# ...\n# --tblout &lt;f&gt;     : save parseable table of per-sequence hits to file &lt;f&gt;\n# ....\n# Options controlling reporting thresholds:\n# ...\n# -E &lt;x&gt;     : report sequences &lt;= this E-value threshold in output  [10.0]  (x&gt;0)\n# ...\n# Other expert options:\n# ...\n# --cpu &lt;n&gt;     : number of parallel CPU workers to use for multithreads\n# ...\n</code></pre> <p>We are now going to submit another slurm job to annotate our MAGs using the Pfam database. Pfam used to have a standalone website, but it has recently been integrated into InterPro maintained by the European Bioinformatics Institute (see announcement). Matching sequences to a <code>Pfam</code> entry allows us to transfer the functional information from an experimentally characterised sequence to uncharacterised sequences in the same entry. <code>Pfam</code> then provides comprehensive annotation for each entry.</p> <p>Create a new script</p> <pre><code>nano annotate_pfam.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (update <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_pfam\n#SBATCH --time          00:20:00\n#SBATCH --mem           5GB\n#SBATCH --cpus-per-task 10\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n# Load modules\nmodule purge\nmodule load HMMER/3.3.2-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Variables\nprot_file=predictions/bin_${SLURM_ARRAY_TASK_ID}.filtered.genes.no_metadata.faa\nout_file=$(basename ${prot_file} .faa)\ndb=/nesi/nobackup/nesi02659/MGSS_resources_2022/databases/Pfam-A.hmm\n\n# Run HMMER\nhmmsearch --tblout gene_annotations/${out_file}.pfam.txt -E 0.001 \\\n--cpu $SLURM_CPUS_PER_TASK \\\n${db} ${prot_file}\n</code></pre> <p>Submit the script</p> <pre><code>sbatch annotate_pfam.sl\n</code></pre>"},{"location":"day3/ex13_gene_annotation_part1/#evaluating-the-quality-of-gene-assignment","title":"Evaluating the quality of gene assignment","text":"<p>Determining how trustworthy a gene annotation is can be a very tricky process. How similar do protein sequences need to be to perform the same function? The answer is surprisingly low. A bioinformatic analysis performed in 1999 identified that proteins with as little as 20 - 35% sequence identity can still share the same function (Rost, 1999), but this is not a universal occurrence. When evaluating annotations, consider the following questions:</p> <ol> <li>What is the amino acid identity along the aligned region?</li> <li>What is the amino acid similarity between the aligned region?</li> <li>What is the coverage as a percentage of the query and target genes?</li> <li>If we infer a phylogeny of this query gene with references from the target family, is a stable tree resolved?</li> <li>Does the inclusion of this gene function make sense in the context of the organism's taxonomy?</li> <li>Does the gene sit on a long contig that is core to the MAG, or is it a short contig carrying only a single gene?</li> <li>If we are uncertain of a particular annotation, does the predicted gene occur in an operon? If so, are the other genes present in the annotation?</li> </ol> <p>We must also remain aware of the potential for incorrectly annotated genes in the annotation database and that proteins can perform multiple functions (and may therefore be attributed multiple, inconsistent annotations). Furthermore, it is also important to consider exactly which part of the target gene the alignment is happening across. There are several catalytic centers of enzymes, such as the Fe-S cofactor, which are shared across many different proteins, and if your annotation is only spanning one of these regions then it may simply be the case that you are identifying a generic electron accepting or donating domain.</p>"},{"location":"day3/ex13_gene_annotation_part1/#differences-in-taxonomies","title":"Differences in taxonomies","text":"<p>Another way to determine if an annotation 'belongs' in the MAG of interest is to consider the predicted taxonomy of the query gene with that of the MAG itself. For example, if you detect a Desulfovibrio-like dsrA seuqence in a bin that has been classified as belonging to the genus Desulfovibrio then it is probably a safe bet that the annotation is correct.</p> <p>However, when comparing taxonomic assignments, it is important to be aware of the differing taxonomic schemas that are circulating in the microbiological and bioinformatic literature and to know how to reconcile their differences. Similar to how the 16S rRNA gene taxonomies provided by SILVA, Greengenes, and RDP taxonomies all differ in some aspects, there are multiple competing taxonomies in protein databases.</p>"},{"location":"day3/ex13_gene_annotation_part1/#examples-of-various-genome-level-and-protein-taxonomies","title":"Examples of various genome-level and protein taxonomies","text":"<ul> <li>NCBI</li> <li>Genome Taxonomy Database</li> </ul> <p>This problem exists despite the existance of a formal Code for the naming of bacteria and archaea, because </p> <ol> <li>There are no rules governing how we define the grouping of these names together, other than for type species</li> <li>Defunct synonyms and basonyms are not correctly purged from taxonomy lists (this is quite noticable with the NCBI taxonomy)</li> <li>Valid names cannot be assigned for uncultivate organisms, meaning there are many informal placeholder names in the literature. For example, clades like WPS-2, SAR324, and SAUL are widely cited in the literature despite having no official standing</li> </ol> <p>It is therefore important to periodically sanity check your taxonomic annotations in order to avoid splitting taxa based on spelling differences or the use of historic names that have since been reclassified.</p>"},{"location":"day3/ex14_gene_annotation_part2/","title":"Gene annotation (part 2) and coverage calculation","text":"<p>Objectives</p> <ul> <li>Gene prediction and annotation with <code>DRAM</code></li> <li>Annotation of the MAGs with <code>DRAM</code></li> <li>Annotation of the viral contigs with <code>DRAM-v</code></li> <li>Calculate per-sample coverage stats for prokaryotic bins</li> <li>Calculate per-sample coverage stats for viral contigs</li> <li>Select initial goal</li> </ul>"},{"location":"day3/ex14_gene_annotation_part2/#gene-prediction-and-annotation-with-dram-distilled-and-refined-annotation-of-metabolism","title":"Gene prediction and annotation with DRAM (Distilled and Refined Annotation of Metabolism)","text":"<p>DRAM is a tool designed to profile microbial (meta)genomes for metabolisms known to impact ecosystem functions across biomes. <code>DRAM</code> annotates MAGs and viral contigs using KEGG (if provided by user), UniRef90, PFAM, CAZy, dbCAN, RefSeq viral, VOGDB (Virus Orthologous Groups), and the MEROPS peptidase database. It is also highly customizable to other custom user databases.</p> <p><code>DRAM</code> only uses assembly-derived fastA files input by the user. These input files may come from unbinned data (metagenome contig or scaffold files) or genome-resolved data from one or many organisms (isolate genomes, single-amplified genome (SAGs), MAGs).</p> <p><code>DRAM</code> is run in two stages: annotation and distillation.</p> <p></p>"},{"location":"day3/ex14_gene_annotation_part2/#annotation","title":"Annotation","text":"<p>The first step in <code>DRAM</code> is to annotate genes by assigning database identifiers to genes. Short contigs (default &lt; 2,500 bp) are initially removed. Then, <code>Prodigal</code> is used to detect open reading frames (ORFs) and to predict their amino acid sequences. Next, <code>DRAM</code> searches all amino acid sequences against multiple databases, providing a single Raw output. When gene annotation is complete, all results are merged in a single tab-delimited annotation table, including the best hit for each database for user comparison.</p>"},{"location":"day3/ex14_gene_annotation_part2/#distillation","title":"Distillation","text":"<p>After genome annotation, a distill step follows with the aim to curate these annotations into useful functional categories, creating genome statistics and metabolism summary files, which are stored in the Distillate output. The genome statistics provides most genome quality information required for MIMAG standards, including <code>GTDB-tk</code> and <code>checkM</code> information if provided by the user. The summarised metabolism table includes the number of genes with specific metabolic function identifiers (KO, CAZY ID, etc) for each genome, with information obtained from multiple databases. The Distillate output is then further distilled into the Product, an html file displaying a heatmap, as well as the corresponding data table. We will investigate all these files later on.  </p>"},{"location":"day3/ex14_gene_annotation_part2/#annotation-of-the-mags-with-dram","title":"Annotation of the MAGs with DRAM","text":"<p>Beyond annotation, <code>DRAM</code> aims to be a data compiler. For that reason, output files from both <code>CheckM</code> and <code>GTDB_tk</code> steps can be input to <code>DRAM</code> to provide both taxonomy and genome quality information of the MAGs. </p>"},{"location":"day3/ex14_gene_annotation_part2/#dram-input-files","title":"DRAM input files","text":"<p>For these exercises, we have copied the relevant input files into the folder <code>10.gene_annotation_and_coverage/DRAM_input_files/</code>. <code>gtdbtk.bac120.summary.tsv</code> was taken from the earlier <code>8.prokaryotic_taxonomy/gtdbtk_out/</code> outputs, and <code>filtered_bins_checkm.txt</code> from the result of re-running <code>CheckM</code> on the final refined filtered bins in <code>6.bin_refinement/filtered_bins</code>.</p> <p>Navigate to the <code>10.gene_annotation_and_coverage/</code> folder</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n</code></pre> <p>Along with our filtered bins, the <code>CheckM</code> output file (<code>checkm.txt</code>) and <code>GTDB-Tk</code> summary output <code>gtdbtk.bac120.summary.tsv</code> are used as inputs as is.</p>"},{"location":"day3/ex14_gene_annotation_part2/#dram-annotation","title":"DRAM annotation","text":"<p>In default annotation mode, <code>DRAM</code> only requires as input the directory containing all the bins we would like to annotate in fastA format (either .fa or .fna). There are few parameters that can be modified if not using the default mode. Once the annotation step is complete, the mode <code>distill</code> is used to summarise the obtained results.</p> <p>NOTE: due to the increased memory requirements, UniRef90 database is not default and the flag <code>\u2013use_uniref</code> should be specified in order to search amino acid sequences against UniRef90. In this exercise, due to memory and time constraints, we won't be using the UniRef90 database.</p> <p>We will start by glancing at some of the options for <code>DRAM</code>.</p> <pre><code>module purge\nmodule load DRAM/1.3.5-Miniconda3\n\nDRAM.py --help\n\n# usage: DRAM.py [-h] {annotate,annotate_genes,distill,strainer,neighborhoods,merge_annotations} ...\n# \n# positional arguments:\n#   {annotate,annotate_genes,distill,strainer,neighborhoods,merge_annotations}\n#     annotate            Annotate genomes/contigs/bins/MAGs\n#     annotate_genes      Annotate already called genes, limited functionality compared to annotate\n#     distill             Summarize metabolic content of annotated genomes\n#     strainer            Strain annotations down to genes of interest\n#     neighborhoods       Find neighborhoods around genes of interest\n#     merge_annotations   Merge multiple annotations to one larger set\n# \n# options:\n#   -h, --help            show this help message and exit\n</code></pre> <p>To look at some of the arguments in each command, type the following:</p> <pre><code># DRAM.py &lt;command&gt; --help\n# For example:\nDRAM.py annotate --help\n</code></pre>"},{"location":"day3/ex14_gene_annotation_part2/#submitting-dram-annotation-as-a-slurm-job","title":"Submitting DRAM annotation as a slurm job","text":"<p>To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. </p> <p>Create a new script</p> <pre><code>nano annotate_dram.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (update all of the cases of <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_DRAM\n#SBATCH --time          5:00:00\n#SBATCH --mem           30Gb\n#SBATCH --cpus-per-task 24\n#SBATCH --error         %x_%A.err\n#SBATCH --output        %x_%A.out\n# Load modules\nmodule purge\nmodule load DRAM/1.3.5-Miniconda3\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run DRAM\nDRAM.py annotate -i 'filtered_bins/*.filtered.fna' \\\n--checkm_quality DRAM_input_files/filtered_bins_checkm.txt \\\n--gtdb_taxonomy DRAM_input_files/gtdbtk.bac120.summary.tsv \\\n-o dram_annotations --threads $SLURM_CPUS_PER_TASK\n</code></pre> <p>Submit the job</p> <pre><code>sbatch annotate_dram.sl\n</code></pre> <p>The program will take 4-4.5 hours to run, so we will submit the jobs and inspect the results tomorrow morning.</p>"},{"location":"day3/ex14_gene_annotation_part2/#annotation-of-the-viral-contigs-with-dram-v","title":"Annotation of the viral contigs with DRAM-v","text":"<p>DRAM also has an equivalent program (DRAM-v) developed for predicting and annotating genes of viral genomes. A number of the options are similar to the standard DRAM run above, although the selection of databases differs slightly, and the subseqent distill step is focussed on identifying and classifying auxilliary metabolic genes (AMGs) rather than the metabolic pathways output by DRAM's standard distill step.</p> <p>To see more details on options for DRAM-v we can run the same <code>--help</code> command as above:</p> <pre><code># DRAM-v.py &lt;command&gt; --help\n# For example:\nDRAM-v.py annotate --help\nDRAM-v.py distill --help\n</code></pre>"},{"location":"day3/ex14_gene_annotation_part2/#submitting-dram-v-annotation-as-a-slurm-job","title":"Submitting DRAM-v annotation as a slurm job","text":"<p>To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. </p> <p>Note</p> <p>DRAM-v requires the <code>mgss-for-dramv/</code> files <code>final-viral-combined-for-dramv.fa</code> and <code>viral-affi-contigs-for-dramv.tab</code> that were generated by VirSorter2. These have been copied into <code>10.gene_annotation_and_coverage/</code> for this exercise.</p> <p>Create a new script</p> <pre><code>nano annotate_dramv.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (update all of the cases of <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_DRAMv\n#SBATCH --time          02:00:00\n#SBATCH --mem           4Gb\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%A.err\n#SBATCH --output        %x_%A.out\n# Load modules\nmodule purge\nmodule load DRAM/1.3.5-Miniconda3\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run DRAM-v\nDRAM-v.py annotate --threads ${SLURM_CPUS_PER_TASK} \\\n--min_contig_size 1000 \\\n-i mgss-for-dramv/final-viral-combined-for-dramv.fa \\\n-v mgss-for-dramv/viral-affi-contigs-for-dramv.tab \\\n-o dramv_annotations\n</code></pre> <p>Submit the job</p> <pre><code>sbatch annotate_dramv.sl\n</code></pre> <p>We will submit this job now and inspect the results tomorrow morning.</p>"},{"location":"day3/ex14_gene_annotation_part2/#calculate-per-sample-coverage-stats-of-the-filtered-prokaryote-bins","title":"Calculate per-sample coverage stats of the filtered prokaryote bins","text":"<p>One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? With bins of metagenome-assembled genome (MAG) data, we can investigate this by mapping the quality-filtered unassembled reads back to the refined bins to then generate coverage profiles. Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. </p> <p>As per the preparation step at the start of the binning process, we can do this using read mapping tools such as <code>Bowtie</code>, <code>Bowtie2</code>, and <code>BBMap</code>. Here we will follow the same steps as before using <code>Bowtie2</code>, <code>samtools</code>, and <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>, but this time inputting our refined filtered bins. </p> <p>These exercises will take place in the <code>10.gene_annotation_and_coverage/</code> folder. Our final filtered refined bins from the previous bin refinement exercise have been copied to the <code>10.gene_annotation_and_coverage/filtered_bins/</code> folder.</p> <p>First, concatenate the bin data into a single file to then use to generate an index for the read mapper.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n\ncat filtered_bins/*.fna &gt; filtered_bins.fna\n</code></pre> <p>Now build the index for <code>Bowtie2</code> using the concatenated bin data. We will also make a new directory <code>bin_coverage/</code> to store the index and read mapping output into.</p> <pre><code>mkdir -p bin_coverage/\n\n# Load Bowtie2\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0\n\n# Build Bowtie2 index\nbowtie2-build filtered_bins.fna bin_coverage/bw_bins\n</code></pre> <p>Map the quality-filtered reads (from <code>../3.assembly/</code>) to the index using <code>Bowtie2</code>, and sort and convert to <code>.bam</code> format via <code>samtools</code>.</p> <p>Create a new script</p> <pre><code>nano mapping_filtered_bins.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (replacing <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      mapping_filtered_bins\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run Bowtie2\nfor i in {1..4}; do\nbowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n-x bin_coverage/bw_bins \\\n-1 ../3.assembly/sample${i}_R1.fastq.gz \\\n-2 ../3.assembly/sample${i}_R2.fastq.gz \\\n-S bin_coverage/sample${i}.sam\n    samtools sort -@ $SLURM_CPUS_PER_TASK -o bin_coverage/sample${i}.bam bin_coverage/sample${i}.sam\ndone\n</code></pre> <p>Submit the script</p> <pre><code>sbatch mapping_filtered_bins.sl\n</code></pre> <p>Finally, generate the per-sample coverage table for each contig in each bin via <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>.</p> <pre><code># Load MetaBAT\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth bins_cov_table.txt bin_coverage/sample*.bam\n</code></pre> <p>The coverage table will be generated as <code>bins_cov_table.txt</code>. As before, the key columns of interest are the <code>contigName</code>, and each <code>sample[1-n].bam</code> column.</p> <p>Note</p> <p>Here we are generating a per-sample table of coverage values for each contig within each bin. To get per-sample coverage of each bin as a whole, we will need to generate average coverage values based on all contigs contained within each bin. We will do this in <code>R</code> during our data visualisation exercises on day 4 of the workshop, leveraging the fact that we added bin IDs to the sequence headers.*</p>"},{"location":"day3/ex14_gene_annotation_part2/#calculate-per-sample-coverage-stats-of-viral-contigs","title":"Calculate per-sample coverage stats of viral contigs","text":"<p>Here we can follow the same steps as outlined above for the bin data, but with a concatenated fastA file of viral contigs. </p> <p>To quickly recap: </p> <ul> <li>In previous exercises, we first used <code>VirSorter2</code> to identify viral contigs from the assembled reads, generating a new fasta file of viral contigs: <code>final-viral-combined.fa</code> </li> <li>We then processed this file using <code>CheckV</code> to generate quality information for each contig, and to further trim any retained (prokaryote) sequence on the ends of prophage contigs. </li> </ul> <p>The resultant fasta files generated by <code>CheckV</code> (<code>proviruses.fna</code> and <code>viruses.fna</code>) have been copied to to the <code>10.gene_annotation_and_coverage/checkv</code> folder for use in this exercise.</p> <p>Note</p> <p>Due to the rapid mutation rates of viruses, with full data sets it will likely be preferable to first further reduce viral contigs down based on a percentage-identity threshold using a tool such as <code>BBMap</code>'s <code>dedupe.sh</code> or Cluster_genomes_5.1.pl from Simon Roux's group. This would be a necessary step in cases where you had opted for generating multiple individual assemblies or mini-co-assemblies (and would be comparable to the use of a tool like <code>dRep</code> for prokaryote data), but may still be useful even in the case of single co-assemblies incorporating all samples.*</p> <p>We will first need to concatenate these files together.</p> <pre><code>cat checkv/proviruses.fna checkv/viruses.fna &gt; checkv_combined.fna\n</code></pre> <p>Now build the index for <code>Bowtie2</code> using the concatenated viral contig data. We will also make a new directory <code>viruses_coverage/</code> to store the index and read mapping output into.</p> <pre><code>mkdir -p viruses_coverage/\n\n# Load Bowtie2\nmodule load Bowtie2/2.4.5-GCC-11.3.0\n\n# Build Bowtie2 index\nbowtie2-build checkv_combined.fna viruses_coverage/bw_viruses\n</code></pre> <p>Map the quality-filtered reads (from <code>../3.assembly/</code>) to the index using <code>Bowtie2</code>, and sort and convert to <code>.bam</code> format via <code>samtools</code>.</p> <p>Create a new script</p> <pre><code>nano mapping_viruses.sl\n</code></pre> <p>Warning</p> <p>Paste in the script (replacing <code>&lt;YOUR FOLDER&gt;</code>)</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      mapping_viruses\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load modules\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run Bowtie2\nfor i in {1..4}; do\nbowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n-x viruses_coverage/bw_viruses \\\n-1 ../3.assembly/sample${i}_R1.fastq.gz \\\n-2 ../3.assembly/sample${i}_R2.fastq.gz \\\n-S viruses_coverage/sample${i}.sam\n  samtools sort -@ $SLURM_CPUS_PER_TASK -o viruses_coverage/sample${i}.bam viruses_coverage/sample${i}.sam\ndone\n</code></pre> <p>Run the script</p> <pre><code>sbatch mapping_viruses.sl\n</code></pre> <p>Finally, generate the per-sample coverage table for each viral contig via <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>.</p> <pre><code># Load MetaBAT\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth viruses_cov_table.txt viruses_coverage/sample*.bam\n</code></pre> <p>The coverage table will be generated as <code>viruses_cov_table.txt</code>. As before, the key columns of interest are the <code>contigName</code>, and each <code>sample[1-n].bam</code> column.</p> <p>Note</p> <p>Unlike the prokaryote data, we have not used a binning process on the viral contigs (since many of the binning tools use hallmark characteristics of prokaryotes in the binning process). Here, <code>viruses_cov_table.txt</code> is the final coverage table. This can be combined with <code>CheckV</code> quality and completeness metrics to, for example, examine the coverage profiles of only those viral contigs considered to be \"High-quality\" or \"Complete\".* </p>"},{"location":"day3/ex14_gene_annotation_part2/#normalising-coverage-values","title":"Normalising coverage values","text":"<p>Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. </p> <p>For an example of one way in which the <code>cov_table.txt</code> output generated by <code>jgi_summarize_bam_contig_depths</code> above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix.</p>"},{"location":"day3/ex14_gene_annotation_part2/#select-initial-goal","title":"Select initial goal","text":"<p>It is now time to select the goals to investigate the genomes you have been working with. We ask you to select one of the following goals:</p> <ol> <li>Denitrification (Nitrate or nitrite to nitrogen)</li> <li>Ammonia oxidation (Ammonia to nitrite or nitrate)</li> <li>Anammox (Ammonia and nitrite to nitrogen)</li> <li>Sulfur oxidation (SOX pathway, thiosulfate to sulfate)</li> <li>Sulfur reduction (DSR pathway, sulfate to sulfide)</li> <li>Photosynthetic carbon fixation</li> <li>Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl)</li> <li>Non-polar flagella expression due to a chromosomal deletion</li> <li>Plasmid-encoded antibiotic resistance</li> <li>Aerobic (versus anaerobic) metabolism</li> </ol> <p>Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them.</p> <p>To conduct this exersise, you should use the information generated with <code>DRAM</code> as well as the annotation files we created previously that will be available in the directory <code>10.gene_annotation_and_coverage/gene_annotations</code>. </p> <p>Please note that we have also provided further annotation files within the directory <code>10.gene_annotation_and_coverage/example_annotation_tables</code> that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here. These files were created by using an in-house python script designed to aggregate different annotations and as part of the environmental metagenomics worflow followed in Handley's lab. Information about using this script as well as the script is available here </p>"},{"location":"day4/ex15_gene_annotation_part3/","title":"Gene annotation (part 3)","text":"<p>Objectives</p> <ul> <li>Overview of <code>DRAM.py annotate</code> and <code>DRAM-v.py annotate</code> output</li> <li>DRAM and DRAM-v distillation step and visualization of results</li> <li>Tie findings to your initial goal</li> </ul>"},{"location":"day4/ex15_gene_annotation_part3/#overview-of-drampy-annotate-output","title":"Overview of DRAM.py annotate output","text":"<p>The submitted jobs from the previous session should now be completed. If we examine the output directory <code>10.gene_annotation_and_coverage/dram_annotations/</code> we will see the following files:</p> File name Description genes.faa and genes.fna Fasta files with all the genes called by <code>Prodigal</code>, with additional header information gained from the annotation as nucleotide and amino acid records, respectively genes.gff GFF3 file with the same annotation information as well as gene locations scaffolds.fna A collection of all scaffolds/contigs given as input to <code>DRAM.py annotate</code> with added bin information annotations.tsv This file includes all annotation information about every gene from all MAGs trnas.tsv Summary of the tRNAs found in each MAG rrnas.tsv Summary of the rRNAs found in each MAG <p>If we inspect the head of the annotation file we will see the following</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n\nhead -n 5 dram_annotations/annotations.tsv #    fasta   scaffold        gene_position   start_position  end_position    strandedness    rank    ko_id   kegg_hit        peptidase_id    peptidase_family        peptidase_hit    peptidase_RBH   peptidase_identity      peptidase_bitScore      peptidase_eVal  pfam_hits       cazy_id cazy_hits       heme_regulatory_motif_count     bin_taxonomy    bin_completeness bin_contamination\n# bin_0.filtered_bin_0_NODE_11_length_361162_cov_0.994424_1       bin_0.filtered  bin_0_NODE_11_length_361162_cov_0.994424        1       2       277     -1      D               Fumarate hydratase (Fumerase) [PF05681.17]                       0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis      99.59   3.79\n# bin_0.filtered_bin_0_NODE_11_length_361162_cov_0.994424_2       bin_0.filtered  bin_0_NODE_11_length_361162_cov_0.994424        2       474     764     1       E               0d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis     99.59   3.79\n# bin_0.filtered_bin_0_NODE_11_length_361162_cov_0.994424_3       bin_0.filtered  bin_0_NODE_11_length_361162_cov_0.994424        3       775     1452    1       D               Response regulator receiver domain [PF00072.27]; Transcriptional regulatory protein, C terminal [PF00486.31]                     0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis      99.59   3.79\n# bin_0.filtered_bin_0_NODE_11_length_361162_cov_0.994424_4       bin_0.filtered  bin_0_NODE_11_length_361162_cov_0.994424        4       1449    2696    1       D               GHKL domain [PF14501.9]                  0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis      99.59   3.79\n</code></pre> <p>For each gene annotated, <code>DRAM</code> provides a summary rank (from A to E), representing the confidence of the annotation based on reciprocal best hits (RBH). The following figure briefly explains how this summary rank is calculated:</p> <p></p>"},{"location":"day4/ex15_gene_annotation_part3/#overview-of-dram-vpy-annotate-output","title":"Overview of DRAM-v.py annotate output","text":"<p>DRAM-v generates the same output files as DRAM, but this time for the viral contigs. These files can be viewed in the output directory <code>10.gene_annotation_and_coverage/dramv_annotations/</code>. In this case, <code>annotations.tsv</code> also includes some viral-specific columns, including viral gene database matches (<code>vogdb</code>), and categories that are used by DRAM-v.py distill to identify putative auxiliary metabolic genes (AMGs) (<code>virsorter_category</code>, <code>auxiliary_score</code>,  <code>is_transposon</code>, <code>amg_flags</code>)</p>"},{"location":"day4/ex15_gene_annotation_part3/#dram-and-dram-v-distillation-of-the-results","title":"DRAM and DRAM-v distillation of the results","text":"<p>After the annotation is finished, we will summarise and visualise these annotations with the so-called distillation step. We do so by running the following commands directly in the terminal. This will generate the distillate and liquor files for each dataset.</p> <p>For the viral annotations, we will also include the parameters <code>--remove_transposons</code> (\"Do not consider genes on scaffolds with transposons as potential AMGs\") and <code>--remove_fs</code> (\"Do not consider genes near ends of scaffolds as potential AMGs\") to filter out some potential false positives for auxilliary metabolic gene identification.</p> <p>code</p> <pre><code>module purge\nmodule load DRAM/1.3.5-Miniconda3\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n\n# Prokaryote annotations\nDRAM.py distill -i dram_annotations/annotations.tsv -o dram_distillation --trna_path dram_annotations/trnas.tsv --rrna_path dram_annotations/rrnas.tsv\n\n# viral annotations\nDRAM-v.py distill --remove_transposons --remove_fs \\\n-i dramv_annotations/annotations.tsv \\\n-o dramv_distillation\n</code></pre>"},{"location":"day4/ex15_gene_annotation_part3/#drampy-distill-output-files","title":"DRAM.py distill output files","text":"<p>The DRAM distillation step generates the following files that can be found within the <code>dram_distillation</code> directory :</p> File name Description genome_stats.tsv Genome quality information required for MIMAG metabolism_summary.xlsx Summarised metabolism table containing number of genes with specific metabolic function identifiers product.html HTML file displaying a heatmap summarising pathway coverage, electron transport chain component completion, and presence/absence of specific functions product.tsv Data table visualised in product.html <p>First, let's have a look at the <code>genome_stats.tsv</code> file to check the assembly quality of our bins by double-clicking the file within the <code>Jupyter</code> environment, viewing from the terminal via <code>less</code> or <code>cat</code>, or downloading the files from here and opening locally (e.g. via excel).</p> Content of <code>genome_stats.tsv</code> genome number of scaffolds taxonomy completeness score contamination score 5S rRNA 16S rRNA 23S rRNA tRNA count assembly quality bin_0 1 d__Bacteria;p__Cyanobacteria;c__Cyanobacteriia;o__Synechococcales;f__Cyanobiaceae;g__Prochlorococcus_C;s__ 100 0.14 - 2 present 2 present 47 med bin_1 4 d__Bacteria;p__Firmicutes;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__ 99.51 0.08 6 present 5 present 5 present 60 med bin_2 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__ 96.45 0.11 3 present - - 43 med bin_3 3 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__ 99.73 0.03 9 present 8 present 8 present 98 med bin_4 1 d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__ 99.97 0.74 bin_4, (74043, 74150) bin_4, (69143, 70676) bin_4, (71085, 73967) 42 high bin_5 1 d__Bacteria;p__Proteobacteria;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__ 99.8 0 bin_5, (643507, 643615) bin_5, (638306, 639791) bin_5, (640621, 643431) 49 high bin_6 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__;s__ 99.59 0.41 4 present 4 present 4 present 49 med bin_7 1 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__ 99.59 2.98 4 present 4 present 4 present 54 med bin_8 19 d__Bacteria;p__Desulfobacterota_A;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__ 99.41 0 2 present bin_8, (3744, 5289) bin_8, (379, 3300) 57 med bin_9 1 d__Bacteria;p__Planctomycetota;c__Brocadiae;o__Brocadiales;f__Brocadiaceae;g__;s__ 97.8 1.65 bin_9, (1066028, 1066130) bin_9, (1069811, 1071397) bin_9, (1066309, 1069302) 46 high <p>To finish, we visualize the Product, an .HTML file produced in the distillation step, by double-clicking on it in our Jupyter lab notebook or downloading from here. The Product has three primary parts:</p> <ol> <li>Modules. Central metabolism pathways coverage. Completion of pathways is based on the structure of KEGG modules, with the pathway coverage calculated as the percent of steps with at least one gene present.</li> </ol> <p> </p> <ol> <li>ETC Complexes. Electron Transport Chain component completion</li> </ol> <p> </p> <ol> <li>Presence of specific functions, including CAZy, Nitrogen metabolism, Sulfur metabolism and Photosynthesis. Note that the taxonomic classification of each of the bins is also shown in the first figure</li> </ol> <p> </p>"},{"location":"day4/ex15_gene_annotation_part3/#dram-vpy-distill-output-files","title":"DRAM-v.py distill output files","text":"<p>The DRAM-v distillation step for the viral contigs generates the following files that can be found within the <code>dramv_distillation/</code> directory :</p> File name Description vMAG_stats.tsv \"Genome\" (in this case viral contigs of varying completeness) information including: total gene counts, viral vs host gene counts, and counts of genes related to viral replication, structure, and those with presumed viral or host benefits amg_summary.tsv Genes identified as putative auxiliary metabolic genes (AMGs) and various columns for metabolic characterisation of each gene product.html HTML file displaying a heatmap summarising AMG counts and presence/absence for different broad metabolic categories for each viral contig <p>When viewing these files, see if you can find the following information:</p> <ul> <li>What are some annotations of interest within the output annotations file? </li> <li>NOTE: the *VirSorter2 annotations file includes multiple columns for both prokaryote and viral protein predictions. Be careful as to which column you are looking at (as well as its associated confidence score) when assessing viral annotations vs. AMGs*.</li> <li>Among these annotations, how many were flagged as AMGs by DRAM-v?</li> <li>What broad metabolic categories did the AMGs fall into? </li> <li>Discussion point: How might we investigate whether identified putative AMGs are actually within the viral genomes, rather than residual contaminating host genomic sequence attached to the end of integrated prophage (but incompletely trimmed off in the excision process)?</li> </ul>"},{"location":"day4/ex15_gene_annotation_part3/#tie-findings-to-your-initial-goal","title":"Tie findings to your initial goal","text":"<p>It is now time to explore the genomes and try to address your original goal!</p> <p>You were tasked with identifying one of the following.</p> <ol> <li>Denitrification (Nitrate or nitrite to nitrogen)</li> <li>Ammonia oxidation (Ammonia to nitrite or nitrate)</li> <li>Anammox (Ammonia and nitrite to nitrogen)</li> <li>Sulfur oxidation (SOX pathway, thiosulfate to sulfate)</li> <li>Sulfur reduction (DSR pathway, sulfate to sulfide)</li> <li>Photosynthetic carbon fixation</li> <li>Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl)</li> <li>Non-polar flagella expression due to a chromosomal deletion</li> <li>Plasmid-encoded antibiotic resistance</li> <li>Aerobic (versus anaerobic) metabolism</li> </ol> <p>Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them.</p> <p>To conduct this exersise, you should use the information generated with <code>DRAM</code> as well as the annotation files we created yesterday and that are available in the directory <code>10.gene_annotation_and_coverage/gene_annotations</code>. </p> <p>Please note that we have also provided further annotation files within the directory <code>10.gene_annotation_and_coverage/example_annotation_tables</code> that contain information obtained after annotating the MAGs against additional databases (UniProt, UniRef100, KEGG, PFAM and TIGRfam). These example files can also be downloaded from here. </p>"},{"location":"day4/ex16a_data_presentation_Intro/","title":"Presentation of data: Introduction","text":"<p>Objectives</p> <ul> <li>Overview, <code>RStudio</code>, and using <code>R</code> in the <code>Jupyter</code> environment</li> <li>Data visualisation and accessibility</li> <li>Logging in to the NeSI <code>Jupyter hub</code></li> </ul>"},{"location":"day4/ex16a_data_presentation_Intro/#overview-rrstudio-and-using-r-in-the-jupyter-environment","title":"Overview, R/RStudio, and using R in the Jupyter environment","text":"<p>There are a number of powerful packages within the <code>R</code> software environment which can be used to create high quality images of genomic and metagenomic data. While each of these packages comes with its own documentation, these documents and tutorials usually assume that your data is already in some already-prepared format. Our data will almost never be in this format, though, so these exercises show a few brief examples of how we can scrape data from our existing files to create useful figures. As such, these examples are more complicated than what you would get reading the tutorials and manuals of the plotting tools, but will be transferable to your own work.</p> <p>In your own work, it may be preferable to download the relevant files from NeSI (e.g. via <code>scp ...</code>, download from <code>Jupyter</code> file explorer pane) and work with them on a locally installed version of <code>RStudio</code> on your own machine. That way you have control over what packages you can install. For today, to be able to run these <code>R</code> exercises in a stable environment within the NeSI platform, we will be running an <code>RStudio</code> from within a Jupyter environment.</p> <p>By now you should be very familiar with running the terminal window from within the NeSI Jupyter hub. In addition to the terminal, NeSI has recently integrated <code>RStudio</code> into the <code>Jupyter</code> hub. You can also opt to run R code on <code>Jupyter Notebooks</code> which also provides an interactive space that allows for mixing multiple languages within a single document, including Markdown, <code>Python</code>, and <code>R</code> (by default, <code>Markdown</code> and one coding language such as <code>R</code> can be used within one document, but there are add-ons available to expand this, should you wish to). <code>Jupyter Notebooks</code> can be extremely useful as a workspace that is the equivalent of an electronic \"lab book\".</p> <p>These exercises will take place with files in the <code>11.data_presentation/</code> folder.</p>"},{"location":"day4/ex16a_data_presentation_Intro/#data-visualisation-and-accessibility","title":"Data visualisation and accessibility","text":"<p>In this section, we will work through a number of example exercises for visualising various aspects of metagenomic data.</p> <p>As the fundamental point of data visualisation is communication, when building figures it is important to be mindful of aspects of your figures that might affect the accessibility of what you're trying to communicate (i.e. to maximise the number of people you will be communicating effectively with). A considerable number of your intended audience will be affected by one of the forms of colour vision deficiency (colour blindness). There are a number of useful resources available online for both selecting and testing the appropriateness of your colour selection. Some include:</p> <ul> <li>ColorBrewer2 (select 'colorblindsafe')</li> <li>chroma.js</li> <li>Selecting and checking your colour choice using Viz Palette</li> <li>Blog post by Brian Connelly.</li> <li>Several useful colour palettes designed by Martin Krzywinski are available here</li> <li>Stack Overflow community suggestions</li> </ul> <p>We have been mindful to make appropriate colour selections throughout these examples, but please do let us know if you spot any we might have overlooked.</p>"},{"location":"day4/ex16a_data_presentation_Intro/#getting-started-logging-in-to-the-nesi-jupyter-hub","title":"Getting started: logging in to the NeSI Jupyter hub","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub.</p> <p>Within the <code>Jupyter</code> launcher, click on the <code>RStudio</code> button to start a session.</p> <p>All of the required packages for these exercises are already installed. </p> Local RStudio <p>If you are running this on your local <code>R</code> or <code>RStudio</code>, you will need to run the following:</p> <p>code</p> <pre><code>install.packages('ade4')\ninstall.packages('genoPlotR')\ninstall.packages('pheatmap')\ninstall.packages('gplots')\ninstall.packages('vegan')\n# Install 'tidyverse' (includes: ggplot2, tibble, tidyr, readr, purrr, dplyr, string4, forcats)\ninstall.packages('tidyverse')\n# Install 'pathview' package (part of Bioconductor)\nif (!require(BiocManager)) {\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"pathview\", update = FALSE)\n}\n# Install 'KEGGREST' package (part of Bioconductor)\nif (!require(BiocManager)) {\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"KEGGREST\", update = FALSE)\n}\n</code></pre> <p>If you are new to <code>RStudio</code>, spend a few minutes familiarising yourself with the environment (also known as the \"workspace\"). There is a pane for the <code>R</code> console on the left that prints information on the <code>R</code> version you are using on start-up. On the top right pane, there are 2 important tabs to note: <code>Environment</code> (where you can explore your data) and <code>History</code> (all code that you have run). The bottom right pane has tabs for <code>Files</code> (where you can navigate the directory structure), <code>Plots</code> (where your plots appear), <code>Packages</code> (available packages), <code>Help</code> (all help pages and manual are shown here), and <code>Viewer</code> (some packages output interactive content and it will show up here). At the very top, there are two toolbars: the first leads to other settings and options (you can explore this on your own time), the second one has icons for (starting from the far left and hover mouse pointer to see description):</p> <ul> <li>Open new file</li> <li>Create an <code>R</code> project</li> <li>Open existing file</li> <li>Save current document</li> <li>Save all open documents</li> <li>Print current file</li> <li>Search bar to navigate open documents</li> <li>Manage workspace panes</li> <li>Options for additional plugins</li> </ul> <p>To start, open a new file to start writing code in.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/","title":"Presentation of data: Per-sample coverage heatmaps","text":"<p>Objectives</p> <ul> <li>Building a heatmap of MAG coverage per sample</li> <li>Building a heatmap of viral contigs per sample</li> </ul>"},{"location":"day4/ex16b_data_presentation_Coverage/#build-a-heatmap-of-average-coverage-per-sample-using-r","title":"Build a heatmap of average coverage per sample using R","text":"<p>One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? In the previous coverage calculation exercises we generated per-sample coverage tables by mapping the quality-filtered unassembled reads back to the refined bins and the viral contigs to then generate coverage profiles for each. </p> <p>As a reminder:</p> <p>Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample.</p> <p>A simple way to present this information is via a heatmap. In this exercise we will build a clustered heatmap of these coverage profiles in <code>R</code>. Since we also have tables of taxonomy assignments (via <code>gtdb-tk</code> for MAGs) and/or predictions (via <code>vContact2</code> for viral contigs), we will also use these to add taxonomy information to the plot.</p> <p>The coverage and taxonomy tables generated in earlier exercises have been copied to <code>11.data_presentation/coverage/</code> for use in these exercises.</p> <p>In addition to this, a simple mapping file has also been created (<code>11.data_presentation/coverage/mapping_file.txt</code>). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column (Group_A, Group_B, and Group_C). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination, or correlation analyses) you could also add new columns to this file and load them at the same time.</p> <p>Note</p> <p>As discussed in the coverage and taxonomy exercises, it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the normalised coverage tables into the steps outlined below.*</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#part-1-building-a-heatmap-of-mag-coverage-per-sample","title":"Part 1 - Building a heatmap of MAG coverage per sample","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub and open a <code>Notebook</code> running the <code>R 4.0.1</code> module as the kernel (or, outside the context of this workshop, open <code>RStudio</code> with the required packages installed (see the data presentation intro docs for more information)).</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#11-prepare-environment","title":"1.1 Prepare environment","text":"<p>First, set the working directory and load the required libraries.</p> <p>code</p> <pre><code># Set working directory ----\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/coverage')\n# Load libraries ----\n# Tidyverse packages \nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(tibble)\n# Visualisation package\nlibrary(gplots)\n# Ecological analyses\nlibrary(vegan)\n</code></pre> <p>NOTE: after copying this code into the empty script file in <code>Rstudio</code>, remember that, to run the code, press <code>&lt;shift&gt; + &lt;enter&gt;</code> with the code block selected.</p> <p>Import all relevant data as follows:</p> <p>code</p> <pre><code># Read files ----\ncontig_cov &lt;- read_tsv(\"bins_cov_table.txt\") # Bin contig coverage table\ngtdbtk_out &lt;- read_tsv(\"gtdbtk.bac120.summary.tsv\") # Prokaryotic taxonomy from GTDB-Tk\nvirus_cov &lt;- read_tsv(\"viruses_cov_table.txt\") # Viral contig coverage table\ncheckv_summary &lt;- read_tsv(\"checkv_quality_summary.tsv\") # Viral contig quality from CheckV\nvContact2_out &lt;- read_tsv(\"tax_predict_table.txt\") # Viral taxonomy from vContact2\nmetadata &lt;- read_tsv(\"mapping_file.txt\") # Metadata/mapping file of environmental parameters\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#12-wrangle-data","title":"1.2 Wrangle data","text":"<p>After importing the data tables, we need to subset the tables to only relevant columns. As noted during the coverage exercises, it is important to remember that we currently have a table of coverage values for all contigs contained within each MAG. Since we're aiming to present coverage for each MAG, we need to reduce these contig coverages into a single mean coverage value per MAG per sample.</p> <p>In the following code, we first <code>select()</code> columns of interest (i.e. the contig ID and sample coverages). We then remove the <code>.bam</code> suffix using the <code>rename_with()</code> function. Given that we require mean coverages per MAG, we create a column of MAG/bin IDs (via <code>mutate</code>) by extracting the relevant text from the contig ID using <code>str_replace()</code>. Finally, we group the data by the bin ID (via <code>group_by()</code>) then use a combination of <code>summarise()</code> and <code>across()</code> to obtain a mean of coverage values per bin per sample. Here, <code>summarise()</code> calculates the <code>mean</code> based on grouping variables set by <code>group_by()</code>, and this is done <code>across()</code> columns that have the column header/name which <code>contains(\"sample\")</code>.</p> <p>code</p> <pre><code>## 1. Obtain average coverage per MAG per sample ----\nMAG_cov &lt;- contig_cov %&gt;%\n# Pick relevant columns\nselect(contigName, ends_with(\".bam\")) %&gt;%\n# Remove \".bam\" from sample columns\nrename_with(\n.fn = function(sample_name) str_remove(sample_name, \".bam\"),\n.cols = ends_with(\".bam\")\n) %&gt;%\n# Add bin ID to data\nmutate(\nbinID = str_replace(contigName, \"(.*)_NODE_.*\", \"\\\\1\")\n) %&gt;%\n# Designate \"binID\" as a grouping factor\ngroup_by(binID) %&gt;%\n# Calculate average coverage per sample per bin\nsummarise(\nacross(\ncontains(\"sample\"), mean\n)\n)\n</code></pre> <p>Next, we would also like to append the lowest identified taxonomy to each MAG. But before that, we will need to tidy up the taxonomy table to make it more readable.</p> <p>In the following code, we first <code>select()</code> the <code>user_genome</code> (which is equivalent to <code>binID</code> above) and the relevant taxonomic <code>classification</code> columns. We then <code>separate()</code> the taxonomic <code>classification</code> column from a long semicolon-separated (<code>;</code>) string to 7 columns describing the MAG's <code>domain</code>, <code>phylum</code>, <code>class</code>, <code>order</code>, <code>family</code>, <code>genus</code>, and <code>species</code>. Finally, we polish the taxonomy annotations. We remove the hierarchical prefixes by retaining everything after the double underscores (<code>__</code>) via <code>str_replace</code> across all columns except <code>user_genome</code> (via inverse selection).</p> <p>code</p> <pre><code>## 2. Clean up taxonomy table ----\nbin_taxonomy &lt;- gtdbtk_out %&gt;%\n# Select relevant columns\nselect(user_genome, classification) %&gt;%\n# Split taxonomic classifications into individual columns\nseparate(\ncol = classification,\ninto = c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"),\nsep = ';'\n) %&gt;%\nmutate(\n# Remove \".filtered\" from the bin ID\nuser_genome = str_remove(user_genome, \".filtered\"),\n# Remove \"d__\" from all taxonomy columns\nacross(\n# Negative selection: change all columns except \"user_genome\"\n-user_genome,\n.fns = function(taxa) str_replace(taxa, \"[a-z]__(.*)\", \"\\\\1\")\n)\n)\n</code></pre> <p>After obtaining a tidy taxonomy table, we append the species annotations to the bin IDs as follows:</p> <ol> <li><code>left_join()</code> a subset of the columns (<code>user_genome</code>, <code>species</code>) from <code>bin_taxonomy</code> generated above based on bin IDs. Bin IDs is <code>binID</code> in <code>MAG_cov</code> and in <code>user_genome</code> in <code>bin_taxonomy</code>, so we must specifically tell <code>left_join()</code> which columns to join <code>by</code>.</li> <li>We <code>unite()</code> the columns <code>binID</code> and <code>species</code> as 1 column and place an underscore <code>_</code> betwee the strings.</li> <li>We then convert the <code>binID</code> column into <code>rownames</code> for the <code>data.frame</code> in preparation for the other steps.</li> </ol> <p>code</p> <pre><code>## 3. Add species to bin ID ----\nMAG_cov &lt;- MAG_cov %&gt;%\nleft_join(\nselect(bin_taxonomy, user_genome, species),\nby = c(\"binID\" = \"user_genome\")\n) %&gt;%\nunite(col = \"binID\", c(binID, species), sep = \"_\") %&gt;%\ncolumn_to_rownames(\"binID\")\n</code></pre> Too much data? <p>In your own work, if you have too many MAGs, it can be difficult to present them concisely in a heatmap. Thus, you may want to only retain MAGs that fulfill some criteria. </p> <p>For example, you may only be interested in the top 10 MAGs based on coverage:</p> <pre><code># Create a vector of top 10 MAGs\ntop10 &lt;- sort(rowSums(MAG_cov), decreasing = TRUE)[1:10]\n# Retain only top 10 MAGs\nMAG_cov_top10 &lt;- MAG_cov[rownames(MAG_cov) %in% names(top10), ]\n</code></pre> <p>Perhaps you want to filter MAGs based on their prevalence across your sampling regime? Assuming you would like to retain MAGs that are present in at least 50% of your samples:</p> <pre><code># Create a presence/absence matrix\nMAG_prevalence &lt;- ifelse(MAG_cov &gt; 0, 1, 0)\n# Create a logical vector that fulfill criteria\ntop_prev &lt;- rowSums(MAG_prevalence) &gt; (0.5 * ncol(MAG_prevalence))\n# Retain only MAGs in coverage table that fulfill criteria\nMAG_cov_top_prev &lt;- MAG_cov[top_prev, ]\n</code></pre> <p>Very often, coverage data generated using metagenomic methods can be sparse and/or have values that differ by large orders of magnitude. This can hamper effective visualisation of the data. To remedy this, we can perform numeric transformations that can enhance visualisation of relative differences between values. Here, we will use a logarithm (base 2) transform with 1 added as a pseudocount (because \\(\\log 0\\) is undefined).</p> <p>On data transformation</p> <p>Transformations applied to enhance data visualisation are not necessarily suitable for downstream statistical analyses. Depending on the attributes of your data (shape, sparseness, potential biases, etc.) and choice of analyses, it is recommended that you become familiar with field and tool-specifc assumptions, norms and requirements for data transformation.</p> <pre><code>## 4. Transform coverage data ----\nMAG_cov_log2 &lt;- log2(MAG_cov + 1)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#13-order-the-heatmap","title":"1.3 Order the heatmap","text":"<p>We then need to prepare some form of ordering for our heatmap. This is usually presented in the form of a dendrogram based on results from hierarchical clustering. To do this, we first need to generate two dissimilarity/distance matrices based on transformed coverage values:</p> <ul> <li>Sample-wise dissimilarities</li> <li>MAG-wise dissimilarities</li> </ul> <p>Here, we calculate Bray-Curtis dissimilarity using <code>vegdist()</code> to generate dissimilarities between samples and MAGs. Then, the matrices are used as input to perform hierarchical clustering and plotted.</p> <p>code</p> <pre><code># Perform hierarchical clustering ----\n## Dissimilarities between samples\nsample_dist &lt;- vegdist(t(MAG_cov_log2), method = \"bray\")\nsample_hclust &lt;- hclust(sample_dist, \"average\")\nplot(\nsample_hclust,\nmain = \"Bray-Curtis dissimilarities between samples\\n(MAG)\",\nxlab = \"Sample\",\nylab = \"Height\",\nsub = \"Method: average linkage\",\nhang = -1\n)\n## Dissimilarities between MAGs\nMAG_dist &lt;- vegdist(MAG_cov_log2)\nMAG_hclust &lt;- hclust(MAG_dist, \"average\")\nplot(\nMAG_hclust,\nmain = \"Bray-Curtis dissimilarities between MAGs\",\nxlab = \"MAGs\",\nylab = \"Height\",\nsub = \"Method: average linkage\",\nhang = -1,\ncex = 0.75\n)\n</code></pre> <p> </p> <p>Exporting figures</p> <p>To export figures you have made, navigate your mouse pointer to the bottom right pane, then click on 'Export' on the second toolbar of the pane. You can export your figure as an image (e.g. TIFF, JPEG, PNG, BMP), a vector image (i.e. PDF, SVG) or PostScript (EPS). </p> <p>If you would like to do this via code, you can wrap the plot code in between functions for graphic devices (i.e. <code>png()</code>, <code>jpeg()</code>, <code>tiff()</code>, <code>bmp()</code>, <code>pdf()</code>, and <code>postscript()</code>) and <code>dev.off()</code>. The following is an example:</p> <pre><code>png(filename = \"file.png\", ...)\nplot(...)\ndev.off()\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#14-set-colour-palettes","title":"1.4 Set colour palettes","text":"<p>The penultimate step before building our heatmap is to set the colours that will be used to represent annotations and the cell/tile values. In this case, annotations are sample groups as designated in the metadata (a.k.a. mapping) file and MAG taxonomy. In the code below, we will set 3 palettes:</p> <ul> <li>Sample groups</li> <li>MAG phylum</li> <li>Cell/tile values</li> </ul> <p>Pre-installed colour palettes</p> <p>Base <code>R</code> has many colour palettes that come pre-installed. Older versions of R had colour palettes (also known as palette <code>R3</code>) that were not ideal for people with colour vision deficiencies. However, newer versions (4.0+) now carry better palettes that are more colour-blind friendly (see here).</p> <p>You can also quickly check what these colour palettes are:</p> <p><pre><code># What colour palettes come pre-installed?\npalette.pals()\n# Plotting the \"Okabe-Ito\" palette\n# (Requires the \"scales\" package)\nscales::show_col(\npalette.colors(palette = \"Okabe-Ito\")\n)\n</code></pre> </p> <p>code</p> <pre><code># Set colour palette ----\n## Prepare sample groups and colour\nmetadata &lt;- metadata %&gt;%\nmutate(\nGroup = as.factor(Group)\n)\ngroup_colour &lt;- data.frame(\nGroup = levels(metadata$Group),\ncolour = palette.colors(\nn = length(levels(metadata$Group)),\npalette = \"Okabe-Ito\")\n)\nsample_colour &lt;- left_join(metadata, group_colour)\n## Prepare MAG colours based on taxonomic class\n### Remember that the new labels (bin ID) are 'binID_species'\nbin_phylum &lt;- bin_taxonomy %&gt;%\nselect(user_genome, species, phylum) %&gt;%\nunite(col = \"binID\", user_genome, species, sep = \"_\") %&gt;%\nmutate(\nphylum = as.factor(phylum)\n)\nphylum_colour &lt;- data.frame(\nphylum = levels(bin_phylum$phylum),\ncolour = palette.colors(\nn = length(levels(bin_phylum$phylum)),\npalette = \"R4\")\n)\nMAG_colour &lt;- left_join(bin_phylum, phylum_colour)\n## Set a grayscale for cell colours\ncell_colour &lt;- colorRampPalette(c(\"white\", \"black\"), space = \"Lab\")(2^8)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#15-build-heatmap","title":"1.5 Build heatmap","text":"<p>code</p> <pre><code>heatmap.2(\nx = as.matrix(MAG_cov_log2), # Log2 transformed coverage table\nColv = as.dendrogram(sample_hclust), # Arrange columns based on sample-wise dissimilarities\nRowv = as.dendrogram(MAG_hclust), # Arrange rows based on MAG-wise dissimilarities\nmargins = c(30, 20), # Numbers represent margins for bottom &amp; right side\nRowSideColors = MAG_colour$colour, # Colours on the row represent MAG phylum\nColSideColors = sample_colour$colour, # Colours on the column represent sample groups\nscale = \"none\", # Do not scale values provided\ncol = cell_colour, # Colour shading based on values\ntrace = \"none\", density.info = \"none\"\n)\n## Legend for MAG phyla\nlegend(\nx = \"bottomleft\", # Legend position\nlegend = phylum_colour$phylum, # Legend text\ncol = phylum_colour$colour, # Legend colour\nlty = 1, # Line type\nlwd = 6, # Line width\nncol = 1, # Number of columns for legend,\nbty = \"n\", # Border type (set to no border)\ntitle = \"Y-axis key: MAG taxonomy\"\n)\n## Legend for sample grouping\nlegend(\nx = \"bottomright\",\nlegend = group_colour$Group,\ncol = group_colour$colour,\nlty = 1,\nlwd = 6,\nncol = 1,\nbty = \"n\",\ntitle = \"X-axis key: Sample group\"\n)\n</code></pre> <p> </p> <p>Results at a glance</p> <p>The heatmap shows us that:</p> <ul> <li>4 MAGs belong to the phylum Proteobacteria</li> <li>Sample 4 belonging to Group C is somewhat of an outlier, with only 2 MAGs recovered from this sample</li> <li><code>bin_8</code>, a Nitrobacter, is only found in sample 4</li> <li>Presumably, sample 4 has a high abundance of nitrogen cycling MAGs</li> <li>Samples from Groups A and B recovered the same bins</li> </ul> <p>You can and should experiment with different paramters and arguments within the <code>heatmap.2</code> function. A good starting point is <code>cex = ...</code> which controls \"character expansion\" (i.e. text size). Here, we use the <code>...SideColors</code> argument to annotate MAG phylum and sample grouping. We also use <code>legend</code> to provide additional information on what those colours represent. Read the help page for <code>heatmap.2()</code> by typing <code>?heatmap.2</code> into the console. The help manual will appear on the bottom right pane.</p> <p>Here, we arrange our columns based Bray-Curtis dissimilarities between samples. However, you may want to enforce or preserve some form of sample order (samples from the same transect, chronological/topological order, etc.). You can do this by setting <code>Colv = FALSE</code>.</p> <p>Arranging figure elements</p> <p>As observed in the above plot, the positioning and arrangement of the colour key and legends relative to the main heatmap may not be ideal. Often times, it is desirable to manually arrange or resize elements of a figure (legends, keys, text). This is usually done by exporting the figure in a format capable of preseving vectors (e.g. SVG or PDF) and then post-processing them in vector graphics software such as Adobe Illustrator or Inkscape.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#part-2-building-a-heatmap-of-viral-contigs-per-sample","title":"Part 2 - Building a heatmap of viral contigs per sample","text":"<p>We can run through the same process for the viral contigs. Many of the steps are as outlined above, so we will work through these a bit quicker and with less commentary along the way. However, we will highlight a handful of differences compared to the commands for the MAGs above, for example:  steps for selecting and/or formatting the taxonomy; importing the quality output from <code>CheckV</code>; and the (optional) addition of filtering out low quality contigs.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#21-prepare-environment","title":"2.1 Prepare environment","text":"<p>This has already been done above. We will immediately begin wrangling the data.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#22-wrangle-data","title":"2.2 Wrangle data","text":"<p>To start, we need to identify viral contigs of at least medium quality. THis will be used as a filtering vector.</p> <p>code</p> <pre><code>## 1. Clean up viral taxonomy annotation quality ----\nvirus_quality &lt;- checkv_summary %&gt;%\nmutate(\ncheckv_quality = factor(\ncheckv_quality,\nlevels = c(\n\"Not-determined\",\n\"Low-quality\",\n\"Medium-quality\",\n\"High-quality\",\n\"Complete\"\n))\n) %&gt;%\nselect(contig_id, checkv_quality)\n## Subset good quality predictions\nvirus_hq &lt;- virus_quality %&gt;%\nfilter(!(checkv_quality %in% c(\"Not-determined\", \"Low-quality\"))) %&gt;%\npull(contig_id)\nvirus_taxonomy_hq &lt;- vContact2_out %&gt;%\nfilter(Genome %in% virus_hq) %&gt;%\nmutate(\nOrder_VC_predicted = as.factor(Order_VC_predicted)\n)\n</code></pre> <p>We then obtain the coverage matrix and transform the values to enhance visualisation.</p> <p>code</p> <pre><code>## 2. Obtain viral contig coverage matrix ----\n## Unlike MAGs, we do not need to average coverage values.\n## However, we do need to filter out annotations of lesser quality.\n## We find 15 contigs that needs to be kept\nvirus_cov_matrix &lt;- virus_cov %&gt;%\nselect(contigName, ends_with(\".bam\")) %&gt;%\nrename_with(\n.fn = function(name) str_remove(name, \".bam\"),\n.cols = everything()\n) %&gt;%\nfilter(contigName %in% virus_hq) %&gt;%\ncolumn_to_rownames(\"contigName\") %&gt;%\nas.matrix()\n## 3. Log2 transform coverage matrix ----\nvirus_cov_matrix_log2 &lt;- log2(virus_cov_matrix + 1)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#23-order-the-heatmap","title":"2.3 Order the heatmap","text":"<p>code</p> <pre><code># Perform hierarchical clustering (viral contigs) ----\n## Dissimilarities between samples\nvirus_sample_dist &lt;- vegdist(t(virus_cov_matrix_log2))\nvirus_sample_hclust &lt;- hclust(virus_sample_dist, \"average\")\nplot(\nsample_hclust,\nmain = \"Bray-Curtis dissimilarities between samples\\n(virus)\",\nxlab = \"Sample\",\nylab = \"Height\",\nsub = \"Method: average linkage\",\nhang = -1\n)\n## Dissimilarities between viruses\nvirus_dist &lt;- vegdist(virus_cov_matrix_log2)\nvirus_hclust &lt;- hclust(virus_dist, \"average\")\nplot(\nvirus_hclust,\nmain = \"Bray-Curtis dissimilarities between viruses\",\nxlab = \"MAGs\",\nylab = \"Height\",\nsub = \"Method: average linkage\",\nhang = -1,\ncex = 0.75\n)\n</code></pre> <p> </p>"},{"location":"day4/ex16b_data_presentation_Coverage/#24-set-colour-palettes","title":"2.4 Set colour palettes","text":"<p>For colours based on sample group, we will retain the colours set as above. Here, we will only set a new palette (Tableau 10) for viruses based on the predicted taxonomy.</p> <pre><code># Prepare grouping variables and colour palettes ----\n# Check palette colours\nscales::show_col(\npalette.colors(\npalette = \"Tableau 10\"\n)\n)\n</code></pre> <p></p> <p>code</p> <pre><code>## Set colours for viral orders\nvirus_order_colour &lt;- data.frame(\n\"virus_order\" = levels(virus_taxonomy_hq$Order_VC_predicted),\n\"colour\" = palette.colors(\nn = length(levels(virus_taxonomy_hq$Order_VC_predicted)),\npalette = \"Tableau 10\"\n)\n)\n## Set colours for viruses\nvirus_colour &lt;- virus_taxonomy_hq %&gt;%\nselect(Genome, Order_VC_predicted) %&gt;%\nleft_join(virus_order_colour, by = c(\"Order_VC_predicted\" = \"virus_order\"))\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#25-build-heatmap","title":"2.5 Build heatmap","text":"<p>code</p> <pre><code>heatmap.2(\nx = virus_cov_matrix_log2,\nColv = as.dendrogram(virus_sample_hclust),\nRowv = as.dendrogram(virus_hclust),\nmargins = c(30, 20),\nRowSideColors = virus_colour$colour,\nColSideColors = sample_colour$colour,\nscale = \"none\",\ncol = cell_colour,\ntrace = \"none\",\nkeysize = 1,\ndensity.info = \"none\"\n)\n## Legends\nlegend(\nx = \"bottomleft\",\nlegend = virus_order_colour$virus_order,\ncol = virus_order_colour$colour,\nlty = 1,\nlwd = 6,\nncol = 1,\nbty = \"n\",\ntitle = \"Y-axis key: Virus taxonomy\"\n)\nlegend(\nx = \"bottomright\",\nlegend = group_colour$Group,\ncol = group_colour$colour,\nlty = 1,\nlwd = 6,\nncol = 1,\nbty = \"n\",\ntitle = \"X-axis key: Sample group\"\n)\n</code></pre> <p> </p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/","title":"Presentation of data: Ordinations","text":"<p>Objectives</p> <ul> <li>Import and wrangle data in <code>R</code></li> <li>Calculate weighted and unweighted Bray-Curtis dissimilarity metrics and nMDS analyses</li> <li>Build nMDS plots in <code>R</code> using the <code>ggplot2</code> package</li> <li>Discussion: Follow-up analyses</li> </ul>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#introduction","title":"Introduction","text":"<p>A common method to investigate the relatedness of samples to one another is to calculate ordinations and to visualise this in the form of a principal components analysis (PCA) or non-metric multidimensional scaling (nMDS) plot. In this exercise, we will calculate ordinations based on weighted and unweighted (binary) Bray-Curtis dissimilarity and present these in nMDS plots.</p> <p>The coverage tables generated in earlier exercises have been copied to <code>11.data_presentation/coverage/</code> a for use in these exercises.</p> <p>In addition to this, a simple mapping file has also been created (<code>11.data_presentation/coverage/mapping_file.txt</code>). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column (Group_A, Group_B, and Group_C). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination) you could also add new columns to this file and load them at the same time.</p> <p>Note</p> <p>As discussed in the coverage exercises, it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the normalised coverage tables into the steps outlined below.*</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#1-import-and-wrangle-data-in-r","title":"1. Import and wrangle data in R","text":"<p>To get started, open <code>RStudio</code> and start a new document.</p> <p>Note</p> <p>You will recognise that the first few steps will follow the same process as the previous exercise on generating coverage heatmaps. In practice, these two workflows can be combined to reduce the repetitive aspects.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#11-prepare-environment","title":"1.1 Prepare environment","text":"<p>First, set the working directory and load the required libraries.</p> <p>code</p> <pre><code># Set working directory\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/coverage')\n# Load libraries ----\n# Tidyverse libraries\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(ggplot2)\n# Ecological analyses\nlibrary(vegan)\n</code></pre> <p>Import coverage tables and mapping file.</p> <pre><code># Read files ----\ncontig_cov &lt;- read_tsv(\"bins_cov_table.txt\") # Bin contig coverage table\nvirus_cov &lt;- read_tsv(\"viruses_cov_table.txt\") # Viral contig coverage table\nmetadata &lt;- read_tsv(\"mapping_file.txt\") # Metadata/mapping file of environmental parameters\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#12-wrangle-data","title":"1.2 Wrangle data","text":"<p>As before in coverage exercise, we need to obtain per MAG and sample average coverage values. We begin by selecting relavent columns and renaming them.</p> <pre><code>## Select relevant columns and rename them\ncontig_cov &lt;- contig_cov %&gt;%\nselect(contigName, ends_with(\".bam\")) %&gt;%\nrename_with(\n.fn = function(sample_name) str_remove(sample_name, \".bam\"),\n.cols = everything()\n)\nvirus_cov &lt;- virus_cov %&gt;%\nselect(contigName, ends_with(\".bam\")) %&gt;%\nrename_with(\n.fn = function(sample_name) str_remove(sample_name, \".bam\"),\n.cols = everything()\n)\n## Calculate average bin coverage based on contig coverage\nMAG_cov &lt;- contig_cov %&gt;%\nmutate(\nbinID = str_replace(contigName, \"(.*)_NODE_.*\", \"\\\\1\")\n) %&gt;%\ngroup_by(binID) %&gt;%\nsummarise(\nacross(\n-contigName,\n.fns = function(coverage) mean(coverage)\n)\n)\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#calculate-weighted-and-unweighted-dissimilarities","title":"Calculate weighted and unweighted dissimilarities","text":"<p>It is often useful to examine ordinations based on both weighted and unweighted (binary) dissimilarity (or distance) metrics. Weighted metrics take into account the proportions or abundances of each variable (in our case, the coverage value of each bin or viral contig). This can be particularly useful for visualising broad shifts in overall community structure (while the membership of the community may remain relatively unchanged). Unweighted metrics are based on presence/absence alone, and can be useful for highlighting cases where the actual membership of communities differs (ignoring their relative proportions within the communities).</p> <p>Here we will use the functions <code>vegdist()</code> and <code>metaMDS()</code> from the <code>R</code> package <code>vegan</code> to generate weighted and unweighted Bray-Curtis dissimilarity matrices and nMDS solutions for the microbial bin data and viral contigs data.</p> <p>Note</p> <p>You may also wish to make use of the <code>set.seed()</code> function before each calculation to ensure that you obtain consistent results if the same commands are re-run at a later date.</p> <p>code</p> <pre><code># Calculate dissimilarities ----\n## Unweighted dissimilarities (presence/absence)\nMAG_binary_bray &lt;- MAG_cov %&gt;%\n# Convert \"binID\" column into rownames\n# (vegan prefers to work on numeric matrices)\ncolumn_to_rownames(\"binID\") %&gt;%\n# Transpose the data frame\nt() %&gt;%\n# Calculate dissimilarities\nvegdist(x = ., method = \"bray\", binary = T)\nvirus_binary_bray &lt;- virus_cov %&gt;%\ncolumn_to_rownames(\"contigName\") %&gt;%\nt() %&gt;%\nvegdist(x = ., method = \"bray\", binary = T)\n## Weighted dissimilarities\nMAG_bray &lt;- MAG_cov %&gt;%\ncolumn_to_rownames(\"binID\") %&gt;%\nt() %&gt;%\nvegdist(x = ., method = \"bray\")\nvirus_bray &lt;- virus_cov %&gt;%\ncolumn_to_rownames(\"contigName\") %&gt;%\nt() %&gt;%\nvegdist(x = ., method = \"bray\")\n</code></pre> <p>From here on out, we will process the data using the same functions/commands. We can make our code less redundant by compiling all necessary inputs as a list, then processing them together. This is achieved by using the <code>map(...)</code> family of functions from the <code>purrr</code> package.</p> <pre><code># Collect dissimilarities into a list for collective processing ----\nbray_list &lt;- list(\n\"MAG_binary_bray\" = MAG_binary_bray,\n\"MAG_bray\" = MAG_bray,\n\"virus_binary_bray\" = virus_binary_bray,\n\"virus_bray\" = virus_bray\n)\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#3-generate-ordination","title":"3. Generate ordination","text":"<p>We now generate and visualise all ordinations in 4 panels them using native plotting methods.</p> <pre><code># Perform non-metric multidimensional scaling (nMDS) ----\nnmds_list &lt;- map(bray_list, function(bray) metaMDS(bray, trymax = 999))\n## Check nMDS plot natively\npar(mfrow = c(2, 2)) # Sets panels\nmap2(nmds_list, names(nmds_list), function(nmds, lbl) {\nordiplot(nmds, main = lbl, type = \"t\", display = \"sites\")\n})\n</code></pre> <p></p> <p>Plotting via this method is a quick and easy way to look at what your ordination looks like. However, this method is often tedious to modify to achieve publication quality figures. In the following section, we will use <code>ggplot2</code> to generate a polished and panelled figure.</p> <p>The <code>map(...)</code> function</p> <p>The <code>map(...)</code> function iterates through a list and applies the same set of commands/functions to each of them. For <code>map2(...)</code>, two lists are iterated concurrently and one output is generated based on inputs from both lists. The base <code>R</code> equivalent is the <code>apply(...)</code> family of functions. Here, we need to perform the same analyses on similar data types (dissimilarities of different kinds and organisms). These are powerful functions that can make your workflow more efficient, less repetitive, and more readable. However, this is not a panacea and there will be times where <code>for</code> loops or brute coding is necessary.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#4-extract-data-from-ordination","title":"4. Extract data from ordination","text":"<p>Before proceeding to plotting using <code>ggplot2</code>. We need to extract the X and Y coordinates from the ordination result using <code>scores()</code>. We then also need to \"flatten\" the list to a single data frame as well as extract other relevant statistics (e.g. stress values).</p> <p>code</p> <pre><code># Extract data from nMDS ----\n## Obtain coordinates\nscrs_list &lt;- map(nmds_list, function(nmds) {\nscores(nmds, display = \"sites\") %&gt;%\nas.data.frame() %&gt;%\nrownames_to_column(\"sample\")\n})\n## Collect nMDS scores in a single data frame\nscrs_all &lt;- bind_rows(scrs_list, .id = \"data_type\")\n## Collect nMDS statistics (stress values)\nstress_values &lt;- map(nmds_list, function(nmds) {\ndata.frame(\"label\" = paste(\"Stress =\", nmds$stress))\n}) %&gt;%\nbind_rows(.id = \"data_type\")\n</code></pre> <p>If you click on <code>scrs_all</code> in the 'Environment' pane (top right), you will see that it is a data frame with the columns:</p> <ul> <li><code>data_type</code>: Label for data output (weighted MAG coverage; unweighted MAG coverage, ...)</li> <li><code>sample</code>: Sample names</li> <li><code>NMDS1</code> and <code>NMDS2</code>: X and Y coordinates for each plot</li> </ul>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#5-build-the-ordination-plot","title":"5. Build the ordination plot","text":"<p>After obtaining the coordinates (and associated statistics), we can use it as input to <code>ggplot()</code> (the function is <code>ggplot()</code> from the package called <code>ggplot2</code>). As before, we will set our colour palette first. We will also generate a vector of panel headers.</p> <p>code</p> <pre><code># Plot nMDS using ggplot2 ----\n## Set up colours\nmetadata &lt;- metadata %&gt;%\nmutate(\nGroup = as.factor(Group)\n)\ngroup_colour &lt;- palette.colors(n = length(levels(metadata$Group)),\npalette = \"Okabe-Ito\") %&gt;%\nsetNames(., levels(metadata$Group))\n## Append sample grouping to scores\nscrs_all &lt;- left_join(scrs_all, metadata, by = c(\"sample\" = \"SampleID\"))\n## Create panel headers\npanel_labels &lt;- c(\n\"MAG_binary_bray\" = \"MAG coverage\\n(unweighted Bray-Curtis)\",\n\"MAG_bray\" = \"MAG coverage\\n(weighted Bray-Curtis)\",\n\"virus_binary_bray\" = \"Virus coverage\\n(unweighted Bray-Curtis)\",\n\"virus_bray\" = \"Virus coverage\\n(weighted Bray-Curtis)\"\n)\n## Call ggplot\nggplot(data = scrs_all, aes(x = NMDS1, y = NMDS2, colour = Group)) +\n# Plot scatterplot\ngeom_point(size = 0.5) +\n# Designate sample group colours\nscale_colour_manual(values = group_colour, name = \"Sample group\") +\n# Split plots based on \"data_type\"\nfacet_wrap(~ data_type, labeller = labeller(data_type = panel_labels)) +\n# Add text annotations\ngeom_text(data = stress_values, aes(label = label), inherit.aes = F,\nx = 0.4, y = 0.4, vjust = 0, hjust = 0,\nsize = 1.2) +\n# Set general theme\ntheme_bw() +\ntheme(\npanel.grid = element_blank(), # remove grid lines\nlegend.position = \"bottom\", # set legend position at the bottom\ntext = element_text(size = 5), # All text size should be 5 points\nrect = element_rect(size = 0.25), # All edges of boxes should be 0.25 wide\nline = element_line(size = 0.25) # \n)\n</code></pre> <p>The code above can be summarised to the following:</p> <ol> <li>Call <code>ggplot()</code> which initiates an empty plot. It reads in the data frame <code>scrs_all</code> as the main source of data and sets <code>NMDS1</code> and <code>NMDS1</code> as <code>x</code> and <code>y</code> coordinates, as well as designates colour based on the <code>Group</code> column.</li> <li><code>geom_point</code> specifies that we want to plot points (i.e. a scatterplot), and we also set a size for all points.</li> <li><code>scale_colour_manual</code> specifies our preferred colour palette and names our colour legend.</li> <li><code>facet_wrap</code> specifies that we want to panel our figure based on the <code>data_type</code> column and that the labels/headers (a.k.a. <code>labellers</code>) should be printed according to the <code>panel_label</code> object.</li> <li>We also add text annotations using <code>geom_text</code>, but that it should read data from the <code>stress_values</code> data frame and do not use data arguments (via <code>inherit.aes = F</code>) specified above in <code>ggplot()</code>. Text adjustments were made using <code>size</code>, <code>vjust</code> and <code>hjust</code>.</li> <li><code>theme_bw</code> specifies the application of a general, monochromatic theme.</li> <li>Elements of the monochromatic theme were modified based on arguments within <code>theme()</code></li> </ol> <p>You should try and modify any of the arguments above to see what changes: Change sizes, colours, labels, etc...</p> <p><code>ggplot2</code> implements extremely flexible plotting methods. It uses the concept of layers where each line after <code>+</code> adds another layer that modifies the plot. Each geometric layer (the actual plotted data) can also read different inputs. Nearly all aspects of a figure can be modified provided one knows which layer to modify. To start out, it is recommended that you take a glance at the cheat sheet.</p> <p></p> <p>Note</p> <p>How informative these types of analyses are depends in part on the number of samples you actually have and the degree of variation between the samples. As you can see in the nMDS plots based on unweighted (binary) Bray-Curtis dissimilarities (especially for the MAGs data) there are not enough differences between any of the samples (in this case, in terms of community membership, rather than relative abundances) for this to result in a particularly meaningful or useful plot in these cases.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#follow-up-analyses","title":"Follow-up analyses","text":"<p>It is often valuable to follow these visualisations up with tests for beta-dispersion (whether or not sample groups have a comparable spread to one another, i.e. is one group of communities more heterogeneous than another?) and, provided that beta-dispersion is not significantly different between groups, PERMANOVA tests (the extent to which the variation in the data can be explained by a given variable (such as sample groups or other environmental factors, based on differences between the centroids of each group).</p> <p>Beta-dispersion can be calculated using the  <code>betadisper()</code> function from the <code>vegan</code> package (passing the <code>bray.dist</code> data and the <code>map$Group</code> variable to group by), followed by <code>anova()</code>, <code>permutest()</code>, or <code>TukeyHSD()</code> tests of differences between the groups (by inputting the generated <code>betadisper</code> output). PERMANOVA tests can be conducted via the <code>adonis()</code> function from the <code>vegan</code> package (for example, via: <code>adonis(bray.dist ~ Group, data=map, permutations=999, method=\"bray\")</code>.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/","title":"Presentation of data: KEGG pathway maps","text":"<p>Objectives</p> <ul> <li>Build a KEGG pathway map using <code>R</code></li> <li>Import and wrangle data in <code>R</code></li> <li>Subset the data for KO IDs of interest</li> <li>Reshape the data for input into pathview</li> <li>Creating pathway map of genes related to nitrogen metabolism</li> </ul>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#build-a-kegg-pathway-map-using-r","title":"Build a KEGG pathway map using R","text":"<p>In this exercise, we will generate KEGG a pathways map from genome annotations to visualize potential pathways present in our assembled, binned genome sequences.</p> <p>The key package used here is pathview, available from <code>Bioconductor</code> (for installation on your local version of <code>RStudio</code>, see the previous intro to data presentation section). <code>pathview</code> is mainly used in transcriptomic analyses but can be adapted for any downstream analyses that utilise the KEGG database. For this exercise, we are interested in visualising the prevalence of genes that we have annotated in a pathway of interest.</p> <p>To get started, if you're not already, log back in to NeSI's Jupyter hub and open <code>RStudio</code>.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#1-prepare-environment","title":"1. Prepare environment","text":"<p>Set the working directory, load the required packages, and import data.</p> <p>code</p> <pre><code># Set working directories ----\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/kegg_map')\n# Load libraries ----\n# Tidyverse packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(tibble)\n# Colour package\nlibrary(viridis)\n# KEGG maps\nlibrary(pathview)\nlibrary(KEGGREST)\n</code></pre> <p>Load your files into R. Here, we are loading them into a list. Given that there are ten files, it is easier to load, clean, and analyze the data using list methods available in <code>tidyverse</code>.</p> <pre><code>filenames &lt;- list.files(pattern = \".*.aa\")\nbin_ids &lt;- str_extract(filenames, \"bin_\\\\d+\")\nannotations &lt;- map(filenames, function(file) read_tsv(file)) %&gt;% setNames(., bin_ids)\n</code></pre> <p>Note</p> <p>R will print some messages about column types and renaming columns. However, we do not need all the columns for this analysis and it is not necessary to reformat them.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#2-subset-the-data","title":"2. Subset the data","text":"<p>For this exercise, we really only require the KEGG orthology IDs (KO numbers) for each bin. There are many columns in the table with annotations pulled from many databases. Lets begin by finding out which columns correspond to each predicted gene and relevant KEGG annotations in the unified annotations table. We will use annotations for <code>bin_0</code> as an example.</p> <pre><code># Find out what columns are in the table.\nnames(annotations$bin_0)\n# names(annotations$bin_0)\n#  [1] \"Query Gene\"              \"GC%\"                     \"Contig name\"             \"Start position\"         \n#  [5] \"Stop position\"           \"Orientation\"             \"Query sequence\"          \"Signalling\"             \n#  [9] \"Signal confidence\"       \"Target gene (UniProt)\"   \"Identity...11\"           \"Coverage...12\"          \n# [13] \"E-value...13\"            \"Description...14\"        \"Taxonomy...15\"           \"Target gene (UniRef100)\"\n# [17] \"Identity...17\"           \"Coverage...18\"           \"E-value...19\"            \"Description...20\"       \n# [21] \"Taxonomy...21\"           \"Target gene (KEGG)\"      \"Identity...23\"           \"Coverage...24\"          \n# [25] \"E-value...25\"            \"Description...26\"        \"Taxonomy...27\"           \"Target gene (Pfam)\"     \n# [29] \"E-value...29\"            \"Description...30\"        \"Target gene (TIGRfam)\"   \"E-value...32\"           \n# [33] \"Description...33\"\n</code></pre> <p>We can see that the headers related to KEGG annotations are in columns 22 to 27, with column headers <code>Target gene (KEGG)</code>, <code>Identity...23</code>, <code>Coverage...24</code>, <code>E-value...25</code>, <code>Description...26</code>, and <code>Taxonomy...27</code>. However, lets be surgical and only get columns that match the pattern of a KO number. All KO numbers start with K and are followed by 5 digits. We can use this pattern to find the required column.</p> <pre><code>map(annotations$bin_0, function(column) {\nany(str_detect(column, \"K\\\\d{5}\"))\n}) %&gt;% keep(isTRUE)\n# $Description...26\n# [1] TRUE\n</code></pre> <p>The code above goes through each column in the annotation table of bin 0, then detects if any string in the column contains the KO pattern. We then follow through by keeping only those columns that report <code>TRUE</code> in detecting the requisite pattern.</p> <p>We can move on by only selecting for the gene ID (here, <code>Query Gene</code>) and the column where KO numbers are. The code below also creates a separate column <code>KO</code> that contains only string that matches the KO pattern.</p> <pre><code>KEGG_annotations &lt;- map(annotations, function(data) {\ndata %&gt;% # Select columns for gene ID and KEGG annotations\nselect(`Query Gene`, `Description...26`) %&gt;% # Create a column for KO numbers\nmutate(\nKO = str_extract_all(`Description...26`, \"K\\\\d{5}\")\n)\n})\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#3-summarise-ko-per-bin","title":"3. Summarise KO per bin","text":"<p>Here, we are interested in the available KO in each bin. Thus, we can summarise the data by the bin to generate a list of KO per bin. Note that some annotations do not have KO numbers attached to them. In these cases, we will filter these data out.</p> <p>Multiple KOs per bin</p> <p>Multiple annotations per bin is possible and not entirely rare, even if you did filter by E-value/bitscore. Some genes may just be very difficult to tell apart based on pairwise sequence alignment annotations. In this case, we are looking for overall trends. Our question here is: Does this MAG have this pathway? We can further refine annotations by comparing domains and/or gene trees to known, characterised gene sequences if gene annotations look suspicious.</p> <p>code</p> <pre><code>KO_bins &lt;- map(KEGG_annotations, function(data) {\ndata %&gt;% # Selecting the relevant columns\nselect(`Query Gene`, KO)\n}) %&gt;% # Concatenate data frames into one big data frame\nbind_rows(.id = \"bin_id\") %&gt;% # Separate multiple KO per row into their own row\nunnest(KO) %&gt;% # Tally hits by KO and MAG\ngroup_by(KO, bin_id) %&gt;% tally(name = \"hits\") %&gt;% # Filter tallies that are not based on KO numbers (some KEGG annotations do\n# not have an assigned KO number)\nfilter(str_detect(KO, \"K\\\\d{5}\")) %&gt;% # Arrange columns by MAG/bin ID and KO numbers (aesthetic purposes)\narrange(bin_id, KO)\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#identifying-pathway-maps-of-interest","title":"Identifying pathway maps of interest","text":"<p>Before moving on, we must first identify the pathway map ID of our pathway of interest. Lets say, for this exercise, we are interested in the TCA cycle. Here, we will use <code>KEGGREST</code> to access the KEGG database and query it with a search term. </p> <p><code>KEGGREST</code> can also help you identify other information stored in the KEGG database. For more information, the <code>KEGGREST</code> vignette can be viewed using the <code>vignette</code> function in <code>R</code>: <code>vignette(\"KEGGREST-vignette\")</code></p> <pre><code>keggFind(database = \"pathway\", query = \"TCA cycle\")\n#               path:map00020 \n# \"Citrate cycle (TCA cycle)\"\n# We find the map ID is 00020 and assign it to an object.\ntca_map_id &lt;- \"00020\"\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#reshaping-the-data-for-input-into-pathview","title":"Reshaping the data for input into pathview","text":"<p><code>pathview</code> needs the data as a numeric matrix with IDs as row names and samples/experiments/bins as column names. Here, we will reshape the data into a matrix of counts per KO number in each bin.</p> <pre><code>KO_matrix &lt;- pivot_wider(\nKO_bins,\nnames_from = \"bin_id\", values_from = \"hits\", values_fill = NA\n) %&gt;% column_to_rownames(\"KO\") %&gt;% as.matrix()\n</code></pre> <p>If you click on <code>KO_matrix</code> in the Environment pane, you can see that it is now a matrix of counts per KO per bin. Bins that do not possess a particular KO number is given NA. Do not worry about that as <code>pathview</code> can deal with that.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#creating-pathway-map-of-genes-related-to-tca-cycle","title":"Creating pathway map of genes related to TCA cycle","text":"<p>Now we can generate images of the KEGG pathway maps using the matrix we just made. For this section, we will try to find genes invovled in the TCA cycle.</p> <pre><code>pv_bin_0 &lt;- pathview(\ngene.data = KO_matrix[, \"bin_0\"],\npathway.id = tca_map_id,\nspecies = \"ko\",\nout.suffix = \"pv_bin_0\"\n)\n</code></pre> <p>There is no plot output for this command as it automatically writes the results into the current working directory. By default, it names the file as <code>&lt;species&gt;&lt;mapID&gt;.&lt;out.suffix&gt;.png</code>. If this is the first time this is run, it will also write the pathway map's original image file <code>&lt;species&gt;&lt;mapID&gt;.png</code> and the <code>&lt;species&gt;&lt;mapID&gt;.xml</code> with information about how the pathway is connected.</p> <p>Lets take a look at our first output.</p> <p></p> <p>Boxes highlighted in red means that our MAG has this gene. However, the colour scale is a little strange seeing as there cannot be negative gene annotation hits (its either NA or larger than 0). Also, we know that there are definitely bins with more than 1 of some KO, but the colour highlights do not show that. Lets tweak the code further and perhaps pick better colours. For the latter, we will use the <code>viridis</code> colour package that is good for showing a gradient.</p> <p>code</p> <pre><code># Set colours\npath_colours &lt;- viridis(n = 3, begin = 0.65, end = 1, direction = 1)\n# For more information on the viridis package: \n# vignette(\"intro-to-viridis\")\n# Plot pathway\npv_bin_0.2 &lt;- pathview(\ngene.data = KO_matrix[, \"bin_0\"],\npathway.id = tca_map_id,\nspecies = \"ko\",\n# Lets make an arbitrary assumption that 5 copies is a lot\nlimit = list(\ngene = c(1, 5),\ncpd = c(1, 5)\n),\nbins = list(\ngene = 4,\ncpd = 4\n),\n# We are plotting number of hits, so specify TRUE for this\n# If plotting, say, gene/transcript abundance, set this to FALSE\ndiscrete = list(\ngene = TRUE,\ncpd = TRUE\n),\n# Tally colours\nlow = path_colours[1],\nmid = path_colours[2],\nhigh = path_colours[3],\nout.suffix = \"pv_bin_0.2\"\n)\n</code></pre> <p></p> <p>This plot looks much better. We can see that some genes do have more hits than others. Now, lets propagate this using <code>map(...)</code> based on our bin IDs.</p> <p>code</p> <pre><code>pv_bin_all &lt;- map(bin_ids, function(bin) {\n# Get column with correct bin ID\nbin_data &lt;- KO_matrix[, bin]\n# Prepare output suffix\nout.suffix = paste0(\"TCA.\", bin)\n# Plot\npathview(\ngene.data = bin_data,\npathway.id = tca_map_id,\nspecies = \"ko\",\n# Lets make an arbitrary assumption that 5 copies is a lot\nlimit = list(\ngene = c(1, 5),\ncpd = c(1, 5)\n),\nbins = list(\ngene = 4,\ncpd = 4\n),\n# We are plotting number of hits, so specify TRUE for this\n# If plotting, say, gene/transcript abundance, set this to FALSE\ndiscrete = list(\ngene = TRUE,\ncpd = TRUE\n),\n# Tally colours\nlow = path_colours[1],\nmid = path_colours[2],\nhigh = path_colours[3],\nout.suffix = out.suffix\n)\n})\n</code></pre> <p>Results</p> Bin 0Bin 1Bin 2Bin 3Bin 4Bin 5Bin 6Bin 7Bin 8Bin 9 <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Based on the plots, it seems that not all bins have a complete TCA cycle. Bins 4, 6, and 7 have a substantially truncated cycle.</p> <p>Now that you know how to make pathway maps, try it using different pathways of interest!</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/","title":"Presentation of data: Gene synteny","text":"<p>Objectives</p> <ul> <li>Build a sulfur assimilation gene alignment figure to investigate gene synteny using <code>R</code></li> <li>Parsing files in <code>bash</code></li> <li>Import and wrangle data in <code>R</code></li> <li>Create a comparison table and build the plot in <code>R</code></li> </ul>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#build-a-sulfur-assimilation-gene-alignment-figure-to-investigate-gene-synteny-using-r","title":"Build a sulfur assimilation gene alignment figure to investigate gene synteny using <code>R</code>","text":"<p>When investigating the evolution of genomes, we sometimes want to consider not only the presence/absence of genes in a genome, but also how they are arranged in an operon. For this exercise, we are going to visualise several sulfur assimilation genes from our MAGs and compare their arrangements.</p> <p>This exercise involves the use of commands in <code>bash</code> and <code>R</code>. In the first part, we will need to find and parse gene coordinates in <code>bash</code>. This is followed by data wrangling and plot generation in <code>R</code>.</p> <p>For this exercise, navigate to the directory <code>11.data_presentation/gene_synteny/</code>. You have been provided with a copy of the <code>prodigal</code> gene predictions for each of the bins (<code>.faa</code> files), an annotation output table using multiple databases (<code>.aa</code> files), a small table of the annotation of some key genes of interest (<code>cys.txt</code> files), and tBLASTx output (<code>blast*.txt</code>) comparing the genes of interest from these organisms. The annotation files were created by manually searching the annotations obtained in the previous exercises.</p> <p>Note</p> <p>Refer to this appendix for detailed information on how to generate input data.</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-1-parsing-files-in-bash","title":"Part 1 - Parsing files in bash","text":"<p>We will be performing this exercise in two stages. Firstly, in <code>bash</code>, we will use <code>cut</code> and <code>tail</code> to pull out the genes of interest listed in the <code>*cys.txt</code> files from the <code>prodigal</code> files. The gene names will then be used to create a table of gene coordinates from the <code>prodigal</code> output using <code>grep</code>, <code>cut</code>, and <code>sed</code>.</p> <p>For these <code>bash</code> steps, we will need to return to our logged in NeSI terminal. Switch over to a NeSI <code>Jupyter hub</code> terminal or log in to a fresh session in a new terminal. </p> <p>Navigate to the <code>11.data_presentation/gene_synteny/</code> folder, and then perform the <code>cut</code> and <code>tail</code> steps outlined above.</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/gene_synteny/\n\nfor bin_file in *_cys.txt; do\nbin=$(basename ${bin_file} _cys.txt)\ncut -f1 ${bin_file} | tail -n+2 &gt; ${bin}_cys.genes\ndone\n</code></pre> <p>We now have three new files, ending with the <code>.genes</code> suffix which are simply a list of the genes that we wish to extract from the <code>prodigal</code> files. We can then use each of these files as input for <code>grep</code> to pull out the FASTA entries that correspond to these genes.</p> <p>code</p> <pre><code>for gene_list in *.genes; do\nbin=$(basename ${gene_list} _cys.genes)\ngrep -f ${gene_list} ${bin}.filtered.genes.faa \\\n| sed -e 's/ # /\\t/g' -e 's/&gt;//g' \\\n| cut -f 1-4 &gt; ${bin}_cys.coords\ndone\n</code></pre> <p>As previously, we will quickly go through the steps of this command:</p> <ul> <li><code>grep -f ${gene_list} ${bin}.filtered.genes.faa</code> searches the <code>prodigal</code> FASTA amino acid sequence output for headers that match those in our <code>.genes</code> file</li> <li><code>sed -e 's/ # /\\t/g' -e 's/&gt;//g'</code> replaces the default <code>prodigal</code> header line delimiter \" # \" with a tab, then removes <code>&gt;</code> from the header.</li> <li><code>cut -f 1-4</code> selects the first 4 columns delimited by a tab (default for <code>cut</code>).</li> </ul> <p>Check the content of the <code>.coords</code> files now. You should see something like the following:</p> <p>code</p> <pre><code>cat bin_3_cys.coords\nbin_3_NODE_53_length_158395_cov_1.135272_128    135928  136935  1\nbin_3_NODE_53_length_158395_cov_1.135272_129    136994  137299  1\nbin_3_NODE_53_length_158395_cov_1.135272_130    137411  138322  1\nbin_3_NODE_53_length_158395_cov_1.135272_131    138413  139201  1\nbin_3_NODE_53_length_158395_cov_1.135272_132    139267  140100  1\nbin_3_NODE_53_length_158395_cov_1.135272_133    140110  140988  1\nbin_3_NODE_53_length_158395_cov_1.135272_134    140985  142073  1\n</code></pre> <p>If you recall from the previous exercise on gene prediction, we have taken the first four entries from each line of the <code>prodigal</code> output, which consists of:</p> <ol> <li>The gene name, written as [CONTIG]_[GENE]</li> <li>The start position of the gene</li> <li>The stop position of the gene</li> <li>The orienation of the gene</li> </ol> <p>We will now use these tables, together with the annotation tables to create the gene synteny view in <code>R</code>.</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#part-2-producing-the-figure-in-r","title":"Part 2 - Producing the figure in R","text":"<p>First, move back to the Jupyter hub pane and start an <code>RStudio</code> session</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#21-prepare-environment","title":"2.1 Prepare environment","text":"<p>Similar to previous sessions, we will begin by preparing the environment by setting our working directory and loading required libraries.</p> <p>code</p> <pre><code># Set working directory\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/gene_synteny/')\n# Load libraries\n# Tidyverse packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(tidyr)\n# Gene synteny\nlibrary(genoPlotR)\n</code></pre>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#22-import-files","title":"2.2 Import files","text":"<p>Using genoPlotR requires files with 3 different types of information:</p> <ul> <li>Gene coordinates (from <code>prodigal</code>)</li> <li>Gene annotations (from our annotation workflows)</li> <li>Contig comparisons (from tBLASTx output)</li> </ul> <p>First, we will import the <code>.coords</code> files, and set column names to the files.</p> <p>code</p> <pre><code># Gene coordinates\ncoords_files &lt;- list.files(pattern = \".coords\")\ncoords_header &lt;- c(\"name\", \"start\", \"end\", \"strand\")\ncoords_list &lt;- map(coords_files, function(file) {\nread_tsv(file, col_names = coords_header)\n}) %&gt;%\nsetNames(., str_remove(coords_files, \"cys.\"))\n</code></pre> <p>Take a look at the content of each of these data.frames, by entering their names into the terminal. You should notice that the coordinates occur at quite different positions between the genomes. If we were looking at complete genomes, this would indicate their position relative to the origin of replication, but as these are unclosed genomes obtained from MAGs, they reflect the coordinates upon their particular contig.</p> <p>Lets continue to import the other files</p> <p>code</p> <pre><code># Gene annotations\nannot_files &lt;- list.files(pattern = \"^bin_.*.txt\")\nannot_list &lt;- map(annot_files, function(file) {\nread_tsv(file, col_names = TRUE)\n}) %&gt;%\nsetNames(., str_replace(annot_files, \"cys.txt\", \"annot\"))\n# BLAST outputs\nblast_files &lt;- list.files(pattern = \"^blast_.*.txt\")\nblast_list &lt;- map(blast_files, function(file) {\nread_comparison_from_blast(file)\n}) %&gt;%\nsetNames(., str_remove(blast_files, \".txt\"))\n</code></pre> <p>Notice that the file reading function for BLAST files are different (<code>read_comparison_from_blast()</code> versus <code>read_tsv()</code>). This is a function specific to <code>genoPlotR</code> for parsing BLAST outputs. If you have some custom comparison pipeline, <code>genoPlotR</code> can also read tab-delimited files via <code>read_comparison_from_tab()</code></p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#23-wrangle-data","title":"2.3 Wrangle data","text":"<p>We now need to do the following:</p> <ul> <li>Parse coordinate data into a format genoPlotR can use</li> <li>Tidy up annotation tables so they will show the correct labels</li> </ul> <p>We start by converting our coordinate data into a <code>dna_seg</code> (DNA segment) data class, then check what is in one of the objects in the list.</p> <p>code</p> <pre><code># Transform coordinate tables into \"dna_seg\" objects\nds_list &lt;- map(coords_list, dna_seg)\nds_list$bin_5_coords\n</code></pre> name start end strand col fill lty lwd pch cex gene_type bin_5_NODE_95_length_91726_cov_0.379302_67 71608 72606 1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379302_68 72768 73586 1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379302_69 73597 74466 1 blue blue 1 1 8 1 arrows bin_5_NODE_95_length_91726_cov_0.379302_70 74470 75459 1 blue blue 1 1 8 1 arrows <p>Now we tidy our annotations table and create a joint coordinate-annotation table. But first, lets take a look at our annotation tables to get a feel for what needs to change.</p> <p>code</p> <pre><code>annot_list\n# $bin_3_annot\n# # A tibble: 7 \u00d7 3\n#   `Query gene`                                 KO                                      Annotation                                 \n#   &lt;chr&gt;                                        &lt;chr&gt;                                   &lt;chr&gt;                                      \n# 1 bin_3_NODE_53_length_158395_cov_1.135272_128 K02048                                  sbp1; Prokaryotic sulfate-/thiosulfate-bin\u2026\n# 2 bin_3_NODE_53_length_158395_cov_1.135272_129 possible predicted diverged CheY-domain possible predicted diverged CheY-domain    \n# 3 bin_3_NODE_53_length_158395_cov_1.135272_130 Domain of unknown function 2            Domain of unknown function 2               \n# 4 bin_3_NODE_53_length_158395_cov_1.135272_131 Domain of unknown function 2            Domain of unknown function 2               \n# 5 bin_3_NODE_53_length_158395_cov_1.135272_132 K02046                                  cysU; cysU; sulfate transport ABC transpor\u2026\n# 6 bin_3_NODE_53_length_158395_cov_1.135272_133 K02047                                  cysW; cysW; sulfate transport ABC transpor\u2026\n# 7 bin_3_NODE_53_length_158395_cov_1.135272_134 K02045                                  cysA; cysA; sulfate transport ATP-binding \u2026\n# \n# $bin_5_annot\n# # A tibble: 4 \u00d7 3\n#   `Query gene`                               KO     Annotation                                                        \n#   &lt;chr&gt;                                      &lt;chr&gt;  &lt;chr&gt;                                                             \n# 1 bin_5_NODE_95_length_91726_cov_0.379302_67 K02048 sbp; sulfate-binding protein                                      \n# 2 bin_5_NODE_95_length_91726_cov_0.379302_68 K02046 cysT; sulfate transporter CysT                                    \n# 3 bin_5_NODE_95_length_91726_cov_0.379302_69 K02047 cysW; sulfate transporter CysW                                    \n# 4 bin_5_NODE_95_length_91726_cov_0.379302_70 K02045 cysA; sulfate.thiosulfate ABC transporter ATP-binding protein CysA\n# \n# $bin_8_annot\n# # A tibble: 4 \u00d7 3\n#   `Query gene`                                KO     Annotation                                    \n#   &lt;chr&gt;                                       &lt;chr&gt;  &lt;chr&gt;                                         \n# 1 bin_8_NODE_60_length_149231_cov_0.651774_55 K02048 Thiosulphate-binding protein                  \n# 2 bin_8_NODE_60_length_149231_cov_0.651774_56 K02046 sulfate ABC transporter permease subunit CysT \n# 3 bin_8_NODE_60_length_149231_cov_0.651774_57 K02047 Sulfate ABC transporter, permease protein CysW\n# 4 bin_8_NODE_60_length_149231_cov_0.651774_58 K02045 sulphate transport system permease protein 1  \n# \n# $bin_9_annot\n# # A tibble: 4 \u00d7 3\n#   `Query gene`                                 KO     Annotation                                           \n#   &lt;chr&gt;                                        &lt;chr&gt;  &lt;chr&gt;                                                \n# 1 bin_9_NODE_12_length_355673_cov_0.516990_147 K02048 thiosulfate ABC transporter substrate-binding protein\n# 2 bin_9_NODE_12_length_355673_cov_0.516990_148 K02046 sulfate ABC transporter permease                     \n# 3 bin_9_NODE_12_length_355673_cov_0.516990_149 K02047 sulfate ABC transporter permease                     \n# 4 bin_9_NODE_12_length_355673_cov_0.516990_150 K02045 sulfate ABC transporter ATP-binding protein   \n</code></pre> <p>Immediately, we can see that there are some inconsistencies in the annotations. </p> <ul> <li>In the annotations for bin 3, there are repeat gene names in the same annotation line.</li> <li>Annotations for the same KO number is inconsistent across tables. For instance, K02046 is cysU in bin 3 and cysT in bins 5 and 8.</li> <li>There is a complete lack of gene names in bin 9 annotations. </li> </ul> <p>We need to remedy this by having a consistent label system. First, create a data frame of labels you want to show in your synteny plot.</p> <p>code</p> <pre><code>KO_genes &lt;- data.frame(\n\"KO\" = paste0(\"K0204\", 5:8),\n\"gene_name\" = c(\"cysA\", \"cysU\", \"cysW\", \"sbp1\")\n)\n# Check what we have created\nKO_genes\n#       KO gene_name\n# 1 K02045      cysA\n# 2 K02046      cysU\n# 3 K02047      cysW\n# 4 K02048      sbp1\n</code></pre> <p>Then, we join the KO_genes, annotation, and coordinate tables. We also replace rows that contain <code>NA</code> with <code>\"Unknown domain\"</code>.</p> <p>code</p> <pre><code>ann_coords_list &lt;- map2(annot_list, coords_list, function(ann, coord) {\nleft_join(ann, coord, by = c('Query gene' = 'name')) %&gt;%\nleft_join(KO_genes) %&gt;%\nmutate(\ngene_name = replace_na(gene_name, \"Unknown domain\")\n)\n})\n# Check one of the tables\nView(ann_coords_list$bin_3_annot) # In RStudio, View() will open a data viewing tab on the top left pane!\n</code></pre> <p>Much better! We can use the <code>gene_name</code> column as our annotation on our gene synteny plot.</p> <p>We also need to convert the tidy annotation table into something that <code>genoPlotR</code> can use (i.e. an <code>annotations</code> class object).</p> <p>code</p> <pre><code>annotations_list &lt;- map(ann_coords_list, function(data) {\nannotation(\nx1 = data$start, # Where each gene starts\nx2 = data$end, # Gene end coordinate\ntext = data$gene_name # Labels to show\n)\n})\n# Check one of the annotations_list objects\nannotations_list$bin_5_annot\n#      x1    x2 text color rot\n# 1 71608 72606 sbp1 black   0\n# 2 72768 73586 cysU black   0\n# 3 73597 74466 cysW black   0\n# 4 74470 75459 cysA black   0\n</code></pre>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#24-plot-data","title":"2.4 Plot data","text":"<p>Lets plot our data to see what it looks like and if it need tweaking.</p> <p>code</p> <pre><code>plot_gene_map(\ndna_segs = ds_list,\ncomparisons = blast_list,\nannotations = annotations_list,\ndna_seg_labels = str_remove(names(ds_list), \"_coords\"),\ndna_seg_scale = TRUE,\ngene_type = \"arrows\"\n)\n</code></pre> <p></p> <p>We have a plot! However, it is rather cluttered and could be further improved:</p> <ul> <li>Remove gene names in the middle tracks</li> <li>Concatenate the \"Unknown domain\" portion into a single annotation</li> <li>Showing only the best tBLASTx hits, reducing short alignments</li> </ul> <p>code</p> <pre><code># Filter BLAST results\nfilt_blast_list &lt;- map(blast_list, function(data) {\ndata %&gt;%\ngroup_by(name1, name2) %&gt;%\nfilter(e_value == min(e_value)) %&gt;% ungroup() %&gt;%\nas.comparison()\n})\n# Concatenate \"Unknown domain\" in bin 3\nannotations_bin_3_mod &lt;- annotations_list$bin_3_annot %&gt;%\nfilter(text != \"Unknown domain\") %&gt;%\nadd_row(\nx1 = 136994, x2 = 139201, text = \"Unknown domain\", color = \"black\", rot = 0\n)\n# Remove gene names in middle tracks\nplot_gene_map(\ndna_segs = ds_list,\ncomparisons = filt_blast_list,\nannotations = list(annotations_bin_3_mod, NULL, NULL, annotations_list[[4]]),\ndna_seg_labels = str_remove(names(ds_list), \"_coords\"),\ndna_seg_scale = TRUE,\ngene_type = \"arrows\"\n)\n</code></pre> <p></p> <p>This looks much cleaner!</p> <p>Results at a glance</p> <p>From the last plot, we see that:</p> <ul> <li>The arrangement sulfur assimilation genes are well conserved</li> <li>Gene sequences of sbp1, cysW, and cysA are very conserved between bins 3, 5, and 8</li> <li>Bin 3 has 3 unknown domains between its sulfur binding protein and the other sulfur assimilation genes</li> <li>cysW in bin 9 shows some sequence homology with cysW from bin 8</li> </ul>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/","title":"Presentation of data: CAZy annotations heatmap","text":"<p>Objectives</p> <ul> <li>Build a basic heatmap from <code>BLAST</code> data using <code>R</code></li> <li>Import and wrangle data in <code>R</code></li> <li>Build the plot in <code>R</code></li> </ul>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#build-a-basic-heatmap-from-annotation-data-using-r","title":"Build a basic heatmap from annotation data using R","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub and open <code>RStudio</code>.</p> <p>For this exercise, set <code>11.data_presentation/cazy_heatmap/</code> as the working directory. These annotation files we will be using have been pre-computed by annotating the <code>prodigal</code> gene predictions against the CAZy database using the dbCAN resource. Briefly, each annotation was made by:</p> <ol> <li>Annotating each <code>prodigal</code> output against the dbCAN database using <code>hmmer</code></li> <li>Converting the raw <code>hmmer</code> output into a table using the <code>hmmscan-parser.py</code> script that bundles with dbCAN</li> </ol>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#import-the-data-into-an-r-dataframe","title":"Import the data into an R data.frame","text":"<p>The first thing we need to do is import these annotations into <code>R</code>. We will do this using the following workflow</p> <ol> <li>Import <code>tidyverse</code> libraries for manipulating data tables</li> <li>Create an empty data.frame (master data.frame)</li> <li>Importing each of the text file into an <code>R</code>, then <ul> <li>Append a new column to the text file, consisting of the name of the bin</li> <li>Select only the bin name and gene annotation from the imported table</li> <li>Append the text table to the master data.frame</li> </ul> </li> </ol> <p>First, we import our <code>R</code> libraries with the <code>library()</code> command. For this workflow, we need three libraries from the <code>tidyverse</code> package:</p> <pre><code>setwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/cazy_heatmap/')\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\n</code></pre> <p>We can then import our data using the <code>list.files()</code> function to loop over each text file, and the <code>mutate</code>, <code>select</code>, and pipe (<code>%&gt;%</code>) functions from the <code>dplyr</code> library.</p> <pre><code>cazy_files = list.files('.')\n# For each file, import it, drop unneeded columns, and add a column recording the bin name\ncazy_df = data.frame()\nfor( cazy_file in cazy_files ) {\ndf = read.table(cazy_file, sep='\\t', stringsAsFactors=F, header=F) %&gt;% mutate('Bin' = cazy_file) %&gt;%\nselect(CAZy=V1, Bin)\ncazy_df = rbind(cazy_df, df)\n}\n</code></pre> <p>We can inspect the final data.frame using the <code>head</code> command:</p> <pre><code>head(cazy_df)\n#                      CAZy                 Bin\n# 1                 AA3.hmm bin_0_parsed.domtbl\n# 2                 AA4.hmm bin_0_parsed.domtbl\n# 3 GT2_Glycos_transf_2.hmm bin_0_parsed.domtbl\n# 4                 CE1.hmm bin_0_parsed.domtbl\n# 5                GH23.hmm bin_0_parsed.domtbl\n# 6                 GT8.hmm bin_0_parsed.domtbl\n</code></pre> <p>We can also confirm that we have imported all of the text files by looking at the unique entries in the <code>Bin</code> column:</p> <pre><code>unique(cazy_df$Bin)\n#  [1] \"bin_0_parsed.domtbl\" \"bin_1_parsed.domtbl\" \"bin_2_parsed.domtbl\" \"bin_3_parsed.domtbl\"\n#  [5] \"bin_4_parsed.domtbl\" \"bin_5_parsed.domtbl\" \"bin_6_parsed.domtbl\" \"bin_7_parsed.domtbl\"\n#  [9] \"bin_8_parsed.domtbl\" \"bin_9_parsed.domtbl\"\n</code></pre> <p>We will now perform a summarising step, aggregating instances of multiple genes with the same annotation into a single count for each genome. We do this by</p> <ul> <li>For each bin in the data.frame</li> <li>For each annotation in the bin<ul> <li>Count the number of times the annotation is observed</li> </ul> </li> </ul> <p>For the majority of cases this will probably be one, but there will be a few cases where multiple annotations have been seen.</p> <p>This process is done using the <code>group_by</code> and <code>tally</code> functions from <code>dplyr</code>, again using pipes to pass the data between functions.</p> <pre><code>cazy_counts = cazy_df %&gt;% group_by(Bin, CAZy) %&gt;% tally()\ncazy_counts\n# A tibble: 402 \u00d7 3\n# Groups:   Bin [10]\n#    Bin                 CAZy          n\n#    &lt;chr&gt;               &lt;chr&gt;     &lt;int&gt;\n#  1 bin_0_parsed.domtbl AA3.hmm       1\n#  2 bin_0_parsed.domtbl AA4.hmm       1\n#  3 bin_0_parsed.domtbl AA6.hmm       2\n#  4 bin_0_parsed.domtbl CBM12.hmm     1\n#  5 bin_0_parsed.domtbl CBM50.hmm     2\n#  6 bin_0_parsed.domtbl CBM78.hmm     2\n#  7 bin_0_parsed.domtbl CE1.hmm       3\n#  8 bin_0_parsed.domtbl CE11.hmm      1\n#  9 bin_0_parsed.domtbl CE3.hmm       1\n# 10 bin_0_parsed.domtbl CE4.hmm       1\n# \u2026 with 392 more rows\n</code></pre> <p>We now have a data.frame-like object (a tibble) with three columns. We can convert this into a gene matrix using the <code>pivot_wider</code> function from the <code>tidyr</code> library to create a genome x gene matrix in the following form:</p> Bin CAZy_1 CAZy_2 ... CAZy_n bin_0 N. of genes N. of genes ... N. of genes bin_1 N. of genes ... ... ... ... ... ... ... ... bin_9 N. of genes ... ... ... <pre><code>cazy_matrix = cazy_counts %&gt;% pivot_wider(id_cols=Bin, names_from=CAZy, values_from=n, values_fill=list(n = 0))\n</code></pre>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#build-the-plot-in-r","title":"Build the plot in R","text":"<p>Finally, we create the actual plot by passing this matrix into the <code>pheatmap</code> library. Before doing this, we need to take the text column <code>Bin</code> from the matrix and move it into the rownames, as this is how <code>pheatmap</code> infers the names of our samples. Also, if we left text data in the numeric input for a heatmap, the function would crash. We can quickly transfer the <code>Bin</code> column to the rownames using the <code>column_to_rownames</code> function from the <code>tibble</code> library.</p> <p><pre><code>library(pheatmap)\ncolours &lt;- colorRampPalette(c(\"#fff9e7\",\"#920000\"), space=\"Lab\")(100)\ncazy_matrix %&gt;% column_to_rownames('Bin') %&gt;% as.matrix(.) %&gt;% pheatmap(., col = colours)\n</code></pre> </p> <p>And there we go. This is a pretty basic heatmap, so there are a number of presentation issues with it. If you have time, try to do the following fixes to the heatmap by exploring the manual for <code>pheatmap</code> or other <code>tidyverse</code> and <code>R</code> functions.</p> <ol> <li>Replace the column-wise clustering with alphabetic arrangement of the gene columns</li> <li>Change the colour palette of the heatmap</li> <li>Reduce the font size in the column (gene) labels</li> <li>Remove the <code>.hmm</code> extensions from the gene names, and the <code>.txt</code> extensions from the bin names</li> <li>Add colouring to the row (bins), marking bins belonging to the same phyla</li> </ol>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/","title":"APPENDIX (ex8): Dereplicating data from multiple assemblies","text":"<p>Objectives</p> <ul> <li>Understand the common issues with using <code>dRep</code> and <code>CheckM</code></li> <li>Use <code>CheckM</code> and <code>dRep</code> together to dereplicate a set of genomes</li> <li>De-duplicate viral contigs using <code>BBMap</code>'s <code>dedupe.sh</code></li> </ul>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#using-drep-and-checkm","title":"Using dRep and CheckM","text":"<p>Before we begin to use <code>dRep</code>, it is important to understand the workflow that it applies to a data set. The basic idea of <code>dRep</code> is that genomes or MAGs are processed as follows:</p> <ol> <li>Genomes are scored for completeness and contamination estimates using <code>CheckM</code></li> <li>Genomes are assigned to primary clusters using a quick and rough average nucleotide identidy (ANI) calculation</li> <li>Clusters of genomes sharing greater than 90% ANI are grouped together and ANI is calculated using a more sensitive method</li> <li>Where groups of genomes sharing &gt;99% ANI are found, the best (determined by completeness and contamination statistics) is picked as a representative of the cluster</li> </ol> <p>When run on its own, <code>dRep</code> will automatically try to run <code>CheckM</code> in the background. There are two problems with this approach, namely:</p> <ol> <li><code>dRep</code> is written in version 3.6 of the <code>python</code> language, and the version of <code>CheckM</code> avaiable on NeSI is written in version 2.7. These are not compatible with each other</li> <li>There are two parameters in <code>CheckM</code> which speed up the workflow through multithreading, but <code>dRep</code> only has access to one of them</li> </ol> <p>For these reasons, when working on NeSI we run <code>CheckM</code> on our data set first, and then pass the results directly into <code>dRep</code>, avoiding the need for <code>dRep</code> to try to call <code>CheckM</code>.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#use-checkm-and-drep-together-to-dereplicate-a-set-of-mags","title":"Use CheckM and dRep together to dereplicate a set of MAGs","text":"<p>For this exercise, we will be working with a different set of MAGs to the mock community, as there is not enough strain-level variation in the mock metagenome for <code>dRep</code> to actually remove any MAGs.</p> <p>We will write a single slurm script to run all necessary commands, then analyse the content.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      checkm_drep\n#SBATCH --time          30:00\n#SBATCH --mem           80GB\n#SBATCH --cpus-per-task 16\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/12.drep_example/\n\n# Step 1\nmodule purge\nmodule load CheckM/1.2.1-gimkl-2022a-Python-3.10.5\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK \\\n--pplacer_threads $SLURM_CPUS_PER_TASK \\\n-x fa --tab_table -f checkm.txt \\\ninput_bins/ checkm_out/\n\n# Step 2\necho \"genome,completeness,contamination\" &gt; dRep.genomeInfo\ncut -f1,12,13 checkm.txt \\\n| sed 's/\\t/.fa\\t/' \\\n| sed 's/\\t/,/g' \\\n| tail -n+2 &gt;&gt; dRep.genomeInfo\n\n# Step 3\nmodule purge\nmodule load drep/2.3.2-gimkl-2018b-Python-3.7.3\n\ndRep dereplicate --genomeInfo dRep.genomeInfo \\\n-g input_bins/*.fa \\\n-p $SLURM_CPUS_PER_TASK \\\ndrep_output/\n</code></pre> <p>Walking through this script, step by step, we are performing the following tasks:</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-1","title":"Step 1","text":"<p>This should look familiar to you. Here we are simply loading <code>CheckM</code>, then running it over a set of MAGs to get the completeness and contamination estimates for our data.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-2","title":"Step 2","text":"<p>When running <code>dRep</code>, we have the option to either let <code>dRep</code> execute <code>CheckM</code> in the background, or we can pass a comma-separated file of the MAG name and its statistics. Unfortunately, <code>dRep</code> does not take the <code>CheckM</code> output itself, so we must use some shell commands to reformat the data. To achieve this, we use the following steps:</p> <pre><code>echo \"genome,completeness,contamination\" &gt; dRep.genomeInfo\n</code></pre> <p>This line creates the header row for the <code>dRep</code> file, which we are calling <code>dRep.genomeInfo</code>.</p> <pre><code>cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | tail -n+2 &gt;&gt; dRep.genomeInfo\n</code></pre> <p>This line cuts the columns 1, 12, and 13 from the <code>CheckM</code> output table, which correspond to the MAG name and completeness/contamination estimates. We then redirect these columns using the <code>|</code> character and use them as input in a <code>sed</code> command. Because <code>CheckM</code> reports our MAG names without their fastA file extension, but <code>dRep</code> requires the extension to be present in the MAG name, we add the trailing <code>.fa</code> with our <code>sed</code> command. This also gives us an opportunity to replace the tab-delimiting character from the <code>CheckM</code> output with the comma character that <code>dRep</code> uses for marking columns in the table. We then pass the output into a second <code>sed</code> command to replace the tab between columns 12 and 13 with a comma.</p> <p>We then use another redirect (<code>|</code>) to pass the resulting text stream to the <code>tail</code> command. The way we are calling <code>tail</code> here will return every row in the text stream except for the first, which means that we are getting all the MAG rows but not their column names. We remove these names because <code>`dRep</code> uses a different naming convention for specifying columns.</p> <p>We append the MAG statistics to the end of the <code>dRep.genomeInfo</code> file, which contains the correct column names for <code>dRep</code>.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-3","title":"Step 3","text":"<p>Here we simply load the modules required for <code>dRep</code>, then execute the command. Because of the compatibility issues between the <code>python</code> version required by <code>CheckM</code> and <code>dRep</code>, we use the <code>module purge</code> command to unload all current modules before loading <code>dRep</code>. This removes the <code>CheckM</code> library, and its <code>python2.7</code> dependency, allowing the <code>dRep</code> and <code>python3.6</code> to load correctly.</p> <p>The parameters for <code>dRep</code> are as follows:</p> Parameter Function dereplicate Activate the dereplicate workflow from <code>dRep</code> --genomeInfo Skip quality checking via <code>CheckM</code>, instead use the values in the table provided -g List of MAGs to dereplicate, passed by wildcard -p Number of processors to use drep_output/ Output folder for all outputs <p>When <code>dRep</code> finishes running, there are a few useful outputs to examine:</p> <pre><code>drep_output/dereplicated_genomes/   # The representative set of MAGs\ndrep_output/figures/                # Dendrograms to visualise the clustering of genomes\ndrep_output/data_tables/            # The primary and secondary clustering of the MAGs, and scoring information\n</code></pre>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#de-duplicate-viral-contigs-using-bbmaps-dedupesh","title":"De-duplicate viral contigs using BBMap's dedupe.sh","text":"<p>Part of the process for <code>dRep</code> includes measures specific to prokaryotes. Hence, the above approach will not be appropriate for dereplicating viral contigs derived from different assemblies. <code>dedupe.sh</code> from the <code>BBMap</code> suite of tools is one alternative to achieve a similar process for these data.</p> <p><code>dedupe.sh</code> takes a comma separated list of assembly fastA files as input, and filters out any contigs that are either full duplicates of another contig, or fully contained within another (longer) contig (i.e. matching alignment within another longer contig). <code>minidentity=...</code> sets the minimum identity threshold, and <code>out=...</code> results in a single deduplicated set of contigs as output.</p> <p>An example of how <code>dedupe.sh</code> might be run on multiple fastA files of assembled viral contigs (e.g. those output by tools such as <code>VIBRANT</code> or <code>VirSorter</code>) is as follows:</p> <p>code</p> <pre><code>cd /path/to/viral/contigs/from/multiple/assemblies/\nmkdir -p dedupe\n\n# load BBMap\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Set infiles\ninfiles=\"assembly_viral_1.fna,assembly_viral_2.fna,assembly_viral_3.fna,assembly_viral_4.fna\"\n# Run main analyses \ndedupe.sh threads=1 in=${infiles} \\\nminidentity=98 exact=f sort=length mergenames=t mergedelimiter=___ overwrite=t \\\nout=dedupe/dedupe.fa\n</code></pre> <p>Note</p> <p><code>dedupe.sh</code> will dereplicate contigs that are duplicates or are fully contained by another contig, but unfortunately not those that share a partial overlap (i.e. sharing an overlapping region, but with non-overlapping sections hanging off the ends). <code>dedupe.sh</code> does include the functionality to cluster these contigs together (via <code>c</code> and <code>mo</code>) and output as separate fastA files, but not to then merge these sequences together into a single representative (this appears to have been a \"to do\" item for a number of years). One option in this case could be to develop a method that: outputs all of the clusters, aligns sequences within each cluster, generates a consensus sequence from the alignment (i.e. effectively performing new mini-assemblies on each of the clusters of overlapping contigs), and then adds this back to the deduplicated fasta output from <code>dedupe.sh</code> (n.b. this is unfortunately a less trivial process than it sounds...)*</p>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/","title":"APPENDIX (ex9): Generating input files for VizBin from DAS_Tool curated bins","text":"<p>The final bins that we obtained in the previous step (output from <code>DAS_Tool</code>) have been copied into <code>6.bin_refinement/dastool_out/_DASTool_bins/</code></p> <p>1. Generalise bin naming and add bin IDs to sequence headers</p> <p>We will first modify the names of our bins to be simply numbered 1 to n bins. We will use a loop to do this, using the wildcard ( * ) to loop over all files in the <code>_DASTool_bins</code> folder, copying to the new <code>example_data_unchopped/</code> folder and renaming as <code>bin_[1-n].fna</code>. The <code>sed</code> command then adds the bin ID to the start of sequence headers in each of the new bin files (this will be handy information to have in the sequence headers for downstream processing).</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Make a new directory the renamed bins\nmkdir example_data_unchopped/\n\n# Copy and rename bins into generic &lt;bin_[0-n].fna&gt; filenames\ni=0\nfor file in dastool_out/_DASTool_bins/*;\ndo\n# Copy and rename bin file\ncp ${file} example_data_unchopped/bin_${i}.fna\n    # extract bin ID\nbinID=$(basename example_data_unchopped/bin_${i}.fna .fna)\n# Add bin ID to sequence headers\nsed -i -e \"s/&gt;/&gt;${binID}_/g\" example_data_unchopped/bin_${i}.fna\n    # Increment i\n((i+=1))\ndone\n</code></pre> <p>2. Fragment contigs</p> <p>Using the <code>cut_up_fasta.py</code> script that comes with the binning tool <code>CONCOCT</code>, cut contigs into 20k fragments to add better density to the cluster.</p> <p>code</p> <pre><code># Load CONCONCT module\nmodule load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16\n\n# Make directory to add chopped bin data into\nmkdir example_data_20k/\n\n# loop over .fna files to generate chopped (fragmented) files using CONCONT's cut_up_fasta.py\nfor bin_file in example_data_unchopped/*;\ndo\nbin_name=$(basename ${bin_file} .fna)\ncut_up_fasta.py -c 20000 -o 0 --merge_last ${bin_file} &gt; example_data_20k/${bin_name}.chopped.fna\ndone\n</code></pre> <p>3. Concatenate fragmented bins</p> <p>Concatenate chopped bins into a single fastA file.</p> <p>Concatenate chopped bins into a single <code>all_bins.fna</code> file to use as input for both subcontig read mapping via <code>Bowtie2</code> and visualisation via <code>VizBin</code>.</p> <pre><code>cat example_data_20k/*.fna &gt; all_bins.fna\n</code></pre> <p>4. Read mapping of subcontigs (fragmented contigs based on 20k length)</p> <p>4a. Build mapping index</p> <p>Build <code>Bowtie2</code> mapping index based on the concatenated chopped bins.</p> <pre><code>mkdir -p read_mapping/\n\nmodule load Bowtie2/2.4.5-GCC-11.3.0\n\nbowtie2-build all_bins.fna read_mapping/bw_bins\n</code></pre> <p>4b. Map sample reads to index</p> <p>Map quality filtered reads to the index using <code>Bowtie2</code>.</p> <p>Example slurm script:</p> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      6.bin_refinement_mapping\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         6.bin_refinement_mapping.err\n#SBATCH --output        6.bin_refinement_mapping.out\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Step 1\nfor i in sample1 sample2 sample3 sample4;\ndo\n# Step 2\nbowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\\n-x read_mapping/bw_bins \\\n-1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \\\n-S read_mapping/${i}.sam\n\n# Step 3\nsamtools sort -@ 10 -o read_mapping/${i}.bam read_mapping/${i}.sam\n\ndone\n</code></pre> <p>Note</p> <p>These settings are appropriate for this workshop's mock data. Full data sets will likely require considerably greater memory and time allocations.*</p> <p>5. Generate coverage table of subcontigs (contigs fragmented based on 20k length)</p> <p>Use <code>MetaBAT</code>'s <code>jgi_summarise_bam_contig_depths</code> to generate a coverage table.</p> <pre><code># Load module\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth example_data_20k_cov.txt read_mapping/sample*.bam\n</code></pre> <p>6. Generate annotation table for <code>VizBin</code></p> <p>Using the chopped bin files (<code>example_data_20k/</code>) and the coverage table generated above (<code>example_data_20k_cov.txt</code>), we can use the following script to generate an annotation table in the format that <code>VizBin</code> is expecting. Note that here we are including columns for per-sub-contig coverage based on sample1 (see note at the start of this exercise), label (bin ID), and length values, and storing this in <code>all_bins.sample1.vizbin.ann</code>.</p> <p>What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information for sample1 which is associated with that contig in the <code>example_data_20k_cov.txt</code> file, and adding that as a new column to the file. (If you wished to instead look based on sample2, you would need to modify the <code>cut</code> command in the line <code>sample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4)</code> accordingly (e.g. <code>cut -f6</code>).</p> <p>code</p> <pre><code># Set up annotation file headers\necho \"coverage,label,length\" &gt; all_bins.sample1.vizbin.ann\n\n# loop through bin .fna files\nfor bin_file in example_data_20k/*.fna; do\n# extract bin ID\nbinID=$(basename ${bin_file} .fna)\n# loop through each sequence header in bin_file\nfor header in `grep \"&gt;\" ${bin_file}`; do\ncontigID=$(echo ${header} | sed 's/&gt;//g')\n# identify this line from the coverage table (example_data_20k_cov.txt), and extract contigLen (column 2) and coverage for sample1.bam (column 4)\ncontigLen=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f2)\nsample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4)\n# Add to vizbin.ann file\necho \"${sample1_cov},${binID},${contigLen}\" &gt;&gt; all_bins.sample1.vizbin.ann\n    done\ndone\n</code></pre> <p>We now have the <code>all_bins.fna</code> and <code>all_bins.sample1.vizbin.ann</code> files that were provided at the start of the VizBin exercise.</p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/","title":"APPENDIX (ex11): Normalise per-sample coverage values by average library size (example)","text":"<p>Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by normalising to minimum or average library size alone. </p> <p>In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the <code>cov_table.txt</code> output generated by <code>jgi_summarize_bam_contig_depths</code> above could then be normalised based on average library size. </p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#normalise-to-average-read-depth-via-the-python-script-norm_jgi_cov_tablepy","title":"Normalise to average read depth via the python script <code>norm_jgi_cov_table.py</code>","text":"<p>The script <code>norm_jgi_cov_table.py</code> is available in the folder <code>10.gene_annotation_and_coverage/</code>, and is also available for download for future reference at this link. </p> <p>Note</p> <p>This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected.</p> <p>In brief, this <code>python</code> script leverages the fact that the standard error output from <code>bowtie2</code> includes read counts for each sample. This has been saved in <code>mapping_filtered_bins_&lt;ADD JOB ID&gt;.err</code>, as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that <code>bowtie2</code> processed the samples (based on the loop we provided to <code>bowtie2</code>: <code>for i in sample1 sample2 sample3 sample4</code>), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the <code>*.bam</code> columns) are normalised for each sample based on: <code>(coverage / sample_read_depth) * average_read_depth</code>. Finally, this is saved to the new file with the prefix 'normalised_' (e.g. <code>normalised_bins_cov_table.txt</code>).</p> <p>Note</p> <p>In your own work, if you alternatively chose to use <code>BBMap</code> (and <code>BBMap</code>s <code>covstats</code> output) for the previous coverage calculation step, read counts can similarly be extracted from the <code>scafstats</code> output by searching for the line \"Reads Used: ...\".</p> <p><code>norm_jgi_cov_table.py</code> requires two input arguments, and takes an additional optional output path argument.</p> <ul> <li><code>-c</code>: Coverage table generated by <code>jgi_summarize_bam_contig_depths</code></li> <li><code>-e</code>: Standard error file created by read mapping via <code>bowtie2</code></li> <li><code>-o</code>: (Optional) Path to output directory (must already exist) (default is the current directory).</li> </ul> <p>Run <code>norm_jgi_cov_table.py</code> for microbial bins data and viral contigs, inputting:</p> <ul> <li>A. the <code>bins_cov_table.txt</code> and <code>mapping_filtered_bins_&lt;ADD JOB ID&gt;.err</code> files</li> <li>B. the <code>viruses_cov_table.txt</code> and <code>mapping_viruses_&lt;ADD JOB ID&gt;.err</code> files. </li> </ul> <p>This will generate the outputs <code>normalised_bins_cov_table.txt</code> and <code>normalised_viruses_cov_table.txt</code>. </p> <p>Note</p> <p>If this <code>python</code> script is in the directory you are currently in, you can call it simply by adding <code>./</code> in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path.*</p> <p>code</p> <pre><code>module purge\nmodule load Python/3.8.2-gimkl-2020a\n\n./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins_&lt;ADD JOB ID&gt;.err\n./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses_&lt;ADD JOB ID&gt;.err\n</code></pre>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#what-if-i-only-have-bam-files","title":"What if I only have BAM files?","text":"<p>You can still normalise your data even with BAM files alone. But this will involve an additional step of obtaining library size/read depth information from the BAM files using <code>SAMtools</code>.</p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#1-obtain-library-size-information","title":"1. Obtain library size information","text":"<p>We can extract this based on the output from <code>SAMtools</code> <code>flagstat</code> command.</p> <p>code</p> <pre><code>module purge\nmodule load SAMtools/1.15.1-GCC-11.3.0\n\nfor i in bin_coverage/*.bam; do\nfilename=$(basename $i)\nlibsize=$(($(samtools flagstat $i | head -n 1 | cut -f 1 -d ' ')/2))\nprintf \"%s\\t%d\\n\" $filename $libsize &gt;&gt; libsize.txt\ndone\n</code></pre> <p>Most of the commands in the above code block should be familiar to you. Here, we use a loop to go through all the BAM files (mapping outputs from <code>bowtie2</code>). Something that might be new here is <code>$(($(samtools flagstat sample1.bam | head -n 1 | cut -f 1 -d ' ')/2))</code>. Let's go through the code chunk by chunk, starting inside and moving our way outwards:</p> <ul> <li><code>samtools flagstat $i</code>: This calls the flagstat subcommand which provides some stats about our mapping.If you were to run it on sample1.bam, the results would look like the following (notice in the first line that the first number is double that of read 1 or read 2): <pre><code># 1099984 + 0 in total (QC-passed reads + QC-failed reads)\n# 1099984 + 0 primary\n# 0 + 0 secondary\n# 0 + 0 supplementary\n# 0 + 0 duplicates\n# 0 + 0 primary duplicates\n# 1002053 + 0 mapped (91.10% : N/A)\n# 1002053 + 0 primary mapped (91.10% : N/A)\n# 1099984 + 0 paired in sequencing\n# 549992 + 0 read1\n# 549992 + 0 read2\n# 549784 + 0 properly paired (49.98% : N/A)\n# 998676 + 0 with itself and mate mapped\n# 3377 + 0 singletons (0.31% : N/A)\n# 6466 + 0 with mate mapped to a different chr\n# 6267 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre></li> <li><code>head -n 1</code>: We only want the first line of this output.</li> <li><code>cut -f 1 -d ' '</code>: We only want the first column (first number) in the line that is space delimited.</li> <li><code>$(( ))</code>: Anything encased in these double round brackets will be parsed arithmetically. If you were to do <code>echo $((1+2))</code>, it will print <code>3</code>. Here, we want to perform a division based on the outputs of<code>samtools flagstat ...</code> (notice the <code>/2</code> at the end).</li> </ul> <p>Altogether, we generate, for each BAM file, the output of flagstat, take the first number and divide it by 2. This is our library size (written out to a file as <code>libsize.txt</code>). </p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#2-normalise-and-scale-contig-coverage","title":"2. Normalise and scale contig coverage","text":"<p>We then use the coverage table and library size files as inputs for a custom R script (you can also find it here). We can call this script via the command line like so:</p> <p>code</p> <pre><code>module purge\nmodule load R/4.2.1-gimkl-2022a\n\n./normalise_jgi_cov.r bins_cov_table.txt libsize.txt\n</code></pre> <p>This script will generate an output in the current directory with the <code>normalised_</code> prefix before the coverage table file name. It will also inform you if there are unequal or unmatched sample names.</p>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/","title":"APPENDIX (ex11) : Viral taxonomy prediction via vContact2","text":"<p>1. Predict genes via <code>prodigal</code></p> <pre><code># Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Create output directory\nmkdir -p viral_taxonomy\n</code></pre> <p>Example slurm script:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      prodigal\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 2\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Load dependencies\nmodule purge\nmodule load prodigal/2.6.3-GCC-11.3.0\n\n# Set up working directories\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Run Prodigal to predict genes \nprodigal -p meta -q \\\n-i checkv_combined.fna \\\n-a viral_taxonomy/checkv_combined.faa </code></pre> <p>2. Generate required mapping file for <code>vContact2</code></p> <p>Use <code>vContact2</code>'s <code>vcontact2_gene2genome</code> script to generate the required mapping file from the output of <code>prodigal</code>.</p> <p>code</p> <pre><code># Ensure you are in the correct working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Load modules\nmodule purge\nmodule unload XALT\nmodule load Singularity/3.10.3 DIAMOND/2.0.15-GCC-11.3.0 MCL/14.137-gimkl-2020a\n\n# Bind path to Singularity container\nexport SINGULARITY_BIND=\"$PWD\"\ncontainer=/opt/nesi/containers/vContact2\n\n# Run script\nsingularity run $container/vcontact2.simg \\\nvcontact2_gene2genome --proteins viral_taxonomy/checkv_combined.faa \\\n--output viral_taxonomy/viral_genomes_g2g.csv \\\n-s 'Prodigal-FAA'\n</code></pre> <p>3. Run <code>vContact2</code></p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      vConTACT2\n#SBATCH --time          02:00:00\n#SBATCH --mem           20GB\n#SBATCH --cpus-per-task 16\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Load modules\nmodule purge\nmodule unload XALT\nmodule load Singularity/3.10.3 MCL/14.137-gimkl-2020a DIAMOND/2.0.15-GCC-11.3.0\n\n# Bind paths\nexport SINGULARITY_BIND=\"$PWD\"\ncontainer=/opt/nesi/containers/vContact2\n\n# Run vConTACT2\nsingularity run $container/vcontact2.simg \\\nvcontact2 --raw-proteins viral_taxonomy/checkv_combined.faa \\\n--rel-mode Diamond \\\n--threads $SLURM_CPUS_PER_TASK \\\n--proteins-fp viral_taxonomy/viral_genomes_g2g.csv \\\n--db 'ProkaryoticViralRefSeq201-Merged' \\\n--c1-bin /opt/conda/bin/cluster_one-1.0.jar \\\n--output-dir vConTACT2_Results\n</code></pre> <p>4. Predict taxonomy of viral contigs based on ouput of <code>vContact2</code></p> <p><code>vContact2</code> doesn't actually assign taxonomy to your input viral contigs. It instead provides an output outlining which reference viral genomes your viral contigs clustered with (if they clustered with any at all). Based on how closely they clustered with any reference genome(s), you can then use this to predict the likely taxonomy of the contig. </p> <p>From the <code>vContact2</code> online docs:</p> <p>One important note is that the taxonomic information is not included for user sequences. This means that each user will need to find their genome(s) of interest and check to see if reference genomes are located in the same VC. If the user genome is within the same VC subcluster as a reference genome, then there's a very high probability that the user genome is part of the same genus. If the user genome is in the same VC but not the same subcluster as a reference, then it's highly likely the two genomes are related at roughly genus-subfamily level. If there are no reference genomes in the same VC or VC subcluster, then it's likely that they are not related at the genus level at all.</p> <p>The summary output of <code>vContact2</code> is the file <code>vConTACT2_Results/genome_by_genome_overview.csv</code>. As the comment above notes, one approach would be to search this file for particular contigs of interest, and see if any reference genomes fall into the same viral cluster (VC), using this reference to predict the taxonomy of the contig of interest.</p> <p>The following <code>python</code> script is effectively an automated version of this for all input contigs (Note: this script has not been widely tested, and so should be used with some degree of caution). This script groups together contigs (and reference genomes) that fall into each VC, and then for each, outputs a list of all taxonomies (at the ranks of 'Order', 'Family', and 'Genus', separately) that were found in that cluster. The predictions (i.e. the list of all taxonomies found in the same VC) for each rank and each contig is output to the table <code>tax_predict_table.txt</code>. </p> <p>Note</p> <p>The taxonomies are deliberately enclosed in square brackets (<code>[ ]</code>) to highlight the fact that these are predictions, rather than definitive taxonomy assignments.*</p> <p>For future reference, a copy of this script is available for download here</p> <p>code</p> <pre><code>module purge\nmodule load Python/3.8.2-gimkl-2020a\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n\n./vcontact2_tax_predict.py \\\n-i viral_taxonomy/vConTACT2_Results/genome_by_genome_overview.csv \\\n-o viral_taxonomy/\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/","title":"APPENDIX (ex15): Prepare input for gene synteny visualisation","text":"<p>In order to produce gene synteny plots using <code>genoPlotR</code> as outlined in Presentation of data: Gene synteny, we need to know the annotations and relative nucleotide positions of our genes of interest. We can use our annotations file generated via homology and domain searches (here, we have parsed and aggregated these annotations using an in-house custom script) or DRAM in order to obtain relevant genes and their labels. Gene nucleotide positions relative to assembled contigs/scaffolds can be obtained from <code>prodigal</code> outputs. </p> <p>Additionally, we need <code>BLAST</code> outputs for comparing between genes along their contigs. For this, we rely on outputs from pairwise <code>tBLASTx</code> (translates a nucleotide database then searches it using a translated nucleotide query) to perform sequential comparisons across different bins.</p> <p>For this example, we use our aggregated annotations (provided in <code>10.gene_annotation_and_coverage/example_annotation_tables</code>) and prodigal outputs to generate input files for visualising the synteny of genes that encode the sulfate/thiosulfate ABC transporter involved in assimilating extracellular sulfate. We will begin by subsetting our genes of interest based on KEGG orthology numbers and relevant annotation labels, followed by a pairwise <code>tBLASTx</code> of our contigs.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#1-navigating-the-working-directory","title":"1. Navigating the working directory","text":"<p>We will be working in <code>10.gene_annotation_and_coverage/</code> where you will find the <code>example_annotation_tables/</code>, <code>filtered_bins/</code> and <code>predictions/</code> sub-directories already prepared for you. </p> <p>code</p> <pre><code># Navigate to working directory\ncd 10.gene_annotation_and_coverage\n\n# View relevant sub-directories\nls example_annotation_tables/\n# bin_0.annotation.aa  bin_3.annotation.aa  bin_6.annotation.aa  bin_9.annotation.aa\n# bin_1.annotation.aa  bin_4.annotation.aa  bin_7.annotation.aa\n# bin_2.annotation.aa  bin_5.annotation.aa  bin_8.annotation.aa\nls predictions/\n# bin_0.filtered.genes.faa              bin_5.filtered.genes.faa\n# bin_0.filtered.genes.fna              bin_5.filtered.genes.fna\n# bin_0.filtered.genes.gbk              bin_5.filtered.genes.gbk\n# bin_0.filtered.genes.no_metadata.faa  bin_5.filtered.genes.no_metadata.faa\n# bin_0.filtered.genes.no_metadata.fna  bin_5.filtered.genes.no_metadata.fna\n# bin_1.filtered.genes.faa              bin_6.filtered.genes.faa\n# bin_1.filtered.genes.fna              bin_6.filtered.genes.fna\n# bin_1.filtered.genes.gbk              bin_6.filtered.genes.gbk\n# bin_1.filtered.genes.no_metadata.faa  bin_6.filtered.genes.no_metadata.faa\n# bin_1.filtered.genes.no_metadata.fna  bin_6.filtered.genes.no_metadata.fna\n# bin_2.filtered.genes.faa              bin_7.filtered.genes.faa\n# bin_2.filtered.genes.fna              bin_7.filtered.genes.fna\n# bin_2.filtered.genes.gbk              bin_7.filtered.genes.gbk\n# bin_2.filtered.genes.no_metadata.faa  bin_7.filtered.genes.no_metadata.faa\n# bin_2.filtered.genes.no_metadata.fna  bin_7.filtered.genes.no_metadata.fna\n# bin_3.filtered.genes.faa              bin_8.filtered.genes.faa\n# bin_3.filtered.genes.fna              bin_8.filtered.genes.fna\n# bin_3.filtered.genes.gbk              bin_8.filtered.genes.gbk\n# bin_3.filtered.genes.no_metadata.faa  bin_8.filtered.genes.no_metadata.faa\n# bin_3.filtered.genes.no_metadata.fna  bin_8.filtered.genes.no_metadata.fna\n# bin_4.filtered.genes.faa              bin_9.filtered.genes.faa\n# bin_4.filtered.genes.fna              bin_9.filtered.genes.fna\n# bin_4.filtered.genes.gbk              bin_9.filtered.genes.gbk\n# bin_4.filtered.genes.no_metadata.faa  bin_9.filtered.genes.no_metadata.faa\n# bin_4.filtered.genes.no_metadata.fna  bin_9.filtered.genes.no_metadata.fna\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#2-obtain-annnotations-for-genes-of-interest","title":"2. Obtain annnotations for genes of interest","text":""},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#21-subset-annotations","title":"2.1 Subset annotations","text":"<p>Based on the KEGG module for sulfate-sulfur assimilation, we need annotations that match <code>K02048</code>, <code>K23163</code>, <code>K02046</code>, <code>K02047</code>, and <code>K02045</code>. Out of all the columns, we are only interested in the gene ID and the KEGG annotations. We know that column 1 is the gene ID, so lets find the column index that contains the KEGG annotations.</p> <p>code</p> <pre><code>grep -P \"K0204[5-8]|K23163\" example_annotation_tables/*.aa \\\n| awk -F '\\t' '\n      {\n        for (i = 1; i &lt;= NF; i++) {\n          if (match($i, \"K[0-9]{5}\"))\n            printf(\"Column %d contains KO of interest.\\n\", i)\n        }\n      }\n    ' \\\n| uniq\n\n# Column 26 contains KO of interest.\n</code></pre> <p>Deconstruct the code block above:</p> Code Action <code>grep -P \"K0204[5-8]\\|K23163\" *.aa</code> Finds the rows that contain the pattern for our KO of interest in all annotation files within this directory. <code>awk -F '\\t' '</code> Starts the <code>awk</code> programme to parse the output of <code>grep</code> and tells it that the outputs have tab-delimited columns. <code>for (i = 1; i &lt;= NF; i++) {</code> Starts the C-style <code>for</code> loop within <code>awk</code> by assigning it an initial value of 1 (<code>i = 1</code>), continue the loop as long as the value of <code>i</code> is lower than the total number of rows (<code>i &lt;= NF</code>), and increment by 1 as the loop progresses (<code>i++</code>). <code>if (match($i, \"K[0-9]{5}\"))</code> Starts an <code>if</code> loop and evaluates if a column contains string that matches the pattern <code>K[0-9]{5}</code> (which is the regex for a KO number). This is evaluated per row. <code>printf(\"Column %d contains KO of interest.\\n\", i)</code> If there is a match in the row, print out the resulting column index. <code>uniq</code> Returns only unique results. <p>The code above says that all KO annotations are in column 26. This is true for all annotation files. Lets move on to subset to the KO numbers we want and cut out the columns we need. We will do this per bin.</p> <p>code</p> <pre><code>for bin_number in {0..9}; do\ngrep -P \"K0204[5-8]|K23163\" example_annotation_tables/bin_${bin_number}.annotation.aa \\\n| cut -f 1,26 &gt; bin_${bin_number}.goi.aa\n\nif [ ! -s bin_${bin_number}.goi.aa ]; then\nrm bin_${bin_number}.goi.aa\n  fi\ndone\n</code></pre> Deconstructing the code block <p>The above code block loops through each bin annotation file to subset relevant rows from the main annotation files (<code>grep -P \"K0204[5-8]|K23163\" example_annotation_tables/bin_${bin_number}.annotation.aa</code>) then selects the first and 26th columns (<code>cut -f 1,26</code>). </p> <p>It also evaluates if the output is an empty file (in the case where the KO of interest is not found in the bin annotation) (<code>if [ ! -s bin_${bin_number}.goi.aa ]</code>) and removes it if it is. </p> <p>We have 4 files after running the above code.</p> <p>code</p> <pre><code>ls *.aa\n# bin_3.goi.aa  bin_5.goi.aa  bin_8.goi.aa  bin_9.goi.aa\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#22-check-for-contiguous-and-sequential-outputs","title":"2.2 Check for contiguous and sequential outputs","text":"<p>We need to check that the genes are contiguous (all on the same contig) and sequential (identify gaps between genes and fill them). Remember that prodigal headers (for which were propagated through our annotation workflows) always have <code>contigID_geneID</code> where <code>geneID</code> is always relative to the order it was found on the contig's nucleotide sequence. With that in mind, lets check for gene contiguity first.</p> <p>code</p> <pre><code>for aa_file in *goi.aa; do\nbin=$(basename ${aa_file} .goi.aa)\ncontig=$(cut -f 1 ${aa_file} | sed -E 's/_[0-9]+$//g' | sort -u | wc -l)\nif [ $contig -gt 1 ]; then\nprintf \"%s has %d contig(s).\\n\" $bin $contig\nfi\ndone\n# bin_5 has 2 contig(s).\n</code></pre> Deconstructing the code block <p>The code above loops through all annotation subsets and finds the number of contigs in each file. It does this by using <code>cut -f 1</code> which selects the first columns, then <code>sed -E 's/_[0-9]+$//g'</code> removes the trailing gene numbers of the gene ID, followed by a dereplication of contigs via <code>sort -u</code> and finally counts the number of lines of the results using <code>wc -l</code>.</p> <p>if the bin annotation subset has more than 1 contig (<code>if [ $contig -gt 1 ]; then</code>), then it reports/prints the bin and number of contigs found (<code>printf \"%s has %d contig(s).\\n\" $bin $contig</code>). </p> <p>The code block above shows that we have non-contiguous genes in bin 5. We will need to remove the extra gene(s) originating from another contig. Lets check which contig we need to remove.</p> <p>code</p> <pre><code>cat bin_5.goi.aa\n\n# bin_5_NODE_52_length_158668_cov_0.373555_136    K02048: cysP; sulfate ABC transporter substrate-binding protein\n# bin_5_NODE_95_length_91726_cov_0.379302_67      K02048: sbp; sulfate-binding protein\n# bin_5_NODE_95_length_91726_cov_0.379302_68      K02046: cysT; sulfate transporter CysT\n# bin_5_NODE_95_length_91726_cov_0.379302_69      K02047: cysW; sulfate transporter CysW\n# bin_5_NODE_95_length_91726_cov_0.379302_70      K02045: cysA; sulfate.thiosulfate ABC transporter ATP-binding protein CysA\n</code></pre> <p>We find that contig <code>bin_5_NODE_52_length_158668_cov_0.373555</code> is not contiguous with the set of genes in contig <code>bin_5_NODE_95_length_91726_cov_0.379302</code>. We will make a note of that. Now lets check for continuity of genes.</p> <p>code</p> <pre><code>for aa_file in *goi.aa; do\nbin=$(basename ${aa_file} .goi.aa)\ngene_number=$(cut -f 1 ${aa_file} | sed -E 's/.*_([0-9]+$)/\\1/g' | uniq)\necho \"$bin: $(echo ${gene_number[@]})\"\ndone\n# bin_3: 128 132 133 134\n# bin_5: 136 67 68 69 70\n# bin_8: 55 56 57 58\n# bin_9: 147 148 149 150\n</code></pre> Deconstructing the code block <p>We loop through each of the annotation subset to select the first column (<code>cut -f 1</code>), extract the trailing gene order/number (<code>sed -E 's/.*_([0-9]+$)/\\1/g'</code>) and returns unique entries (<code>uniq</code>). The result is stored as an array named <code>gene_number</code>.</p> <p>It also reports the results of each bin and the array of gene numbers.</p> <p>Results from the above code block shows that bin 3 has a gap between genes. We need to make sure we get the genes in between gene number 128 and 132.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#23-create-subsets-of-annotation-files-with-correct-genes","title":"2.3 Create subsets of annotation files with correct genes","text":"<p>Based on previous checks, we find that we need to: - remove contig <code>bin_5_NODE_52_length_158668_cov_0.373555</code> from bin 5 when we create the final annotation file. - fill in gaps in genes for bin 3.</p> <p>code</p> <pre><code># Remove non-contiguous gene from bin_5.goi.aa\nsed -i '/bin_5_NODE_52_length_158668_cov_0.373555/d' bin_5.goi.aa\n\n# Obtain genes between 129 and 131 in contig bin_3_NODE_53_length_158395_cov_1.135272 and add to annotations\ncat example_annotation_tables/bin_3.annotation.aa \\\n| grep -E \"bin_3_NODE_53_length_158395_cov_1.135272_\" \\ # Search for required contig\n| grep -E \"_129|_130|_131\" \\                            # Search for required gene numbers\n| cut -f 1,26 &gt;&gt; bin_3.goi.aa                           # Select columns 1 and 26\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#24-clean-up-annotation-tables","title":"2.4 Clean up annotation tables","text":"<p>Now we have gene annotations that are contiguous and continuous. Lets clean up the tables by:</p> <ol> <li>Sorting entries by gene order</li> <li>Separating KO from other annotation information</li> <li>Adding headers</li> <li>Replace anything that doesn't have KO numbers with an annotation so we do not get empty fields</li> </ol> <p>code</p> <pre><code>for aa_file in *.aa; do\nbin=$(basename ${aa_file} .goi.aa)\nsort -u -k 1 ${aa_file} \\\n| sed -e 's/: /\\t/g' \\\n| sed '1i\\Query gene\\tKO\\tAnnotation' \\\n| awk -F '\\t' -v OFS='\\t' '\n        {\n          if (!match($2, \"K\")) {\n            $3=$2\n          }\n          print\n        }\n      ' &gt; ${bin}_cys.txt\ndone\n</code></pre> Deconstructing the code block <p>We loop through each annotation subset, then:</p> Code Action <code>sort -u -k 1</code> Dereplicate and sort entries based on the first column <code>sed '1i\\Query gene\\tKO\\tAnnotation'</code> Add header <code>awk -F '\\t' -v OFS='\\t'</code> Initiate the <code>awk</code> programme by setting the input and output field separator as tabs <code>if (!match($2, \"K\")) {$3=$2}</code> If entries in the second column that do not have a \"K\", use value in the second column as values for the third column <code>print</code> Return/print out all lines"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#3-create-comparison-tables","title":"3. Create comparison tables","text":"<p>After obtaining relevant annotations for our genes of interest, we need to align the sequences against each other in order to identify sequence homology and directionality across our genes of interest. This can be achieved via pairwise sequence alignment using BLAST (or BLAST-like algorithms). Here, we will use <code>tBLASTx</code> to obtain requisite comparison tables. Here, we will do this using the command line. However, you can also use online BLAST as outlined here. This is a 2 step process:</p> <ol> <li>Obtain nucleotide sequences for relevant contigs in each bin</li> <li>Run (sequentially) a pairwise <code>tBLASTx</code> between each bin</li> </ol> Which BLAST algorithms to use? <p>For visualising gene synteny using <code>genoPlotR</code>, one can use outputs of <code>BLASTn</code> or <code>tBLASTx</code>. The determination of which algorithm to use depends on the phylogenetic relationship between MAGs. For closely related genomes, you can use <code>BLASTn</code> (or its variants) given that the genes will likely be conserved at the nucleotide sequence level. However, if your genomes are phylogentically distant, you will need to use <code>tBLASTx</code>. This allows us to compare gene homology at the amino acid sequence level and retain nucleotide position information.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#31-subset-contigs-per-bin","title":"3.1 Subset contigs per bin","text":"<p>We begin by getting contig headers from our annotation files then using that to search and subset binned contigs. To subset sequences in FASTA format, we will be using <code>seqtk</code></p> <p>code</p> <pre><code>module purge\nmodule load seqtk/1.3-gimkl-2018b\n\nfor gene_file in *_cys.txt; do\nbin=$(basename ${gene_file} _cys.txt)\ncut -f 1 ${gene_file} | tail -n+2 | sed -e 's/_[0-9]*$//g' | uniq &gt; ${bin}_cys.contigID\n  seqtk subseq filtered_bins/${bin}.filtered.fna ${bin}_cys.contigID &gt; ${bin}_cys_contig.fna\ndone\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#33-run-tblastx","title":"3.3 Run <code>tBLASTx</code>","text":"<p>We do not need to perform pairwise comparisons for all bin combinations (you can, if you want to). We will run tBLASTx in the order which we want to observe the genes in: <code>bin_3</code>, <code>bin_5</code>, <code>bin_8</code>, then <code>bin_9</code>. To do this, we set an array for bin IDs then run them through sequentially.</p> <p>code</p> <pre><code># Set array\nbin_array=(bin_3 bin_5 bin_8 bin_9)\n# Initiate index\ni=0\n# Set maximum array index\nmax_i=$(( ${#bin_array[@]}-1 ))\n# Run tBLASTx, comparing bins sequentially in the order of the array\nwhile [ $i -lt $max_i ]; do\nqbin=${bin_array[$i]}\nsbin=${bin_array[$(($i+1))]}\nprintf \"Comparing %s against %s\\n\" $qbin $sbin\ntblastx -query ${qbin}_cys_contig.fna -subject ${sbin}_cys_contig.fna \\\n-out blast_${qbin}_${sbin}.txt -outfmt \"7 std ppos frames\"\n((i++)) # Increment i after each comparison\ndone\n</code></pre> <p>The above workflow should generate the required files for the annotation subset (<code>&lt;binID&gt;_cys.txt</code>) and pairwise <code>BLAST</code> comparisons (<code>blast_&lt;query binID&gt;_&lt;subject binID&gt;.txt</code>). Along with these outputs, copy the <code>prodigal</code> preditions (either <code>*.faa</code> or <code>*.fna</code>; do not use <code>no_metadata</code> files) from relevant bins into your working directory and follow the steps outlined in Presentation of data: gene synteny to generate synteny plots.</p>"},{"location":"resources/5_APPENDIX_ex15_gene_synteny_Generate_blast_files/","title":"APPENDIX (ex15): Generating pairwise contig comparisons using online BLAST","text":"<p>There are several ways of getting the blast files. <code>genoplotR</code> can read tabular files: either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by genoPlotR, the <code>-m</code> 8 or 9 option should be used in blastall, or <code>-outfmt</code> 6 or 7 with the BLAST+ suite.</p> <p>In this exercise, we are using <code>tblastx</code> on the NCBI website. Alternatively, you can use the command line version of <code>tblastx</code> in BLAST suite to get the same output (but remember to create the database first).</p> <p>Firstly, we will need to get the input <code>.fna</code> files for blast. Navigate to the <code>11.data_presentation/gene_synteny/</code> folder, then we can grab the node of interest and load <code>seqtk</code> on <code>jupyter</code> to grab the fastA sequence.</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/gene_synteny/\n\n#grab node name\nfor i in *cys.txt ;do grep 'bin_' $i | sed 's/.*bin/bin/g;s/cov_\\(.*\\)_.*/cov_\\1/g' | uniq &gt; node_$i;\ndone\n#grab sequence using seqtk\nexport dir=/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction/filtered_bins/\nmodule load seqtk\n\nfor i in {4,5,7};do seqtk subseq ${dir}/bin_${i}.filtered.fna node_bin_${i}_cys.txt &gt; bin_${i}_cys.fna;\ndone\n</code></pre> <p>Download the <code>*cys.fna</code> files to your local computer and then upload them to the NCBI website for blasting between bin 4 and bin 5, and then again between bin 5 and bin 7. </p> <p>  You will get the output that looks like this. Click <code>download all</code>, and select <code>Hit Table (text)</code>.</p> <p> </p> <p>That's it! Now you will have downloaded two files (one comparing between bin 4 and bin 5, and another between bin 5 and bin 7).</p>"},{"location":"resources/7_command_line_shortcuts/","title":"Useful command line shortcuts","text":""},{"location":"resources/7_command_line_shortcuts/#overview","title":"Overview","text":"<ul> <li>Bash</li> <li>Nano</li> <li>SLURM commands</li> <li>SBATCH flags</li> </ul>"},{"location":"resources/7_command_line_shortcuts/#bash-commands","title":"Bash commands","text":"Command Function Example <code>cat</code> Read and print contents of a file <code>cat my_file.txt</code> <code>grep</code> Search a file for lines containing text of interest <code>grep \"words\" my_file.txt</code> <code>sed</code> Find and replace text in a file <code>sed 's/words/pictures/' my_file.txt</code> <code>head</code> Print the first n lines from a file <code>head -n 10 my_file.txt</code> <code>tail</code> Print the last n lines from a file <code>tail -n 10 my_file.tx</code>t <code>less</code> Partially open a text file for viewing,reading lines as the user scrolls <code>less my_file.txt</code> <code>cut</code> Retrieve particular columns of a file.Uses tab as the delimiting character,can change with <code>-d</code> parameter <code>cut -f1,2,5 -d \",\" my_file.txt</code> <code>paste</code> Opposite of <code>cut</code>, stitch together files in a row-wise manner <code>paste my_file.txt my_other_file.txt</code>"},{"location":"resources/7_command_line_shortcuts/#nano-shortcuts","title":"Nano shortcuts","text":"Command Function Ctrl + X Close the file Ctrl + O Save the file Ctrl + W Search for a string in the file Ctrl + Space Go forward one word in a line Alt + Space Go backwards one word in a line Ctrl + A Go to the beginning of a line Ctrl + E Go to the end of a line Ctrl + W, Ctrl + V Go to the last line of the file Ctrl + K Cut the current line and save it to the clipboard Ctrl + U Paste (**U**n-cut) the text in the clipboard Ctrl + Shift + 6 Select text"},{"location":"resources/7_command_line_shortcuts/#slurm-commands","title":"SLURM commands","text":"Command Function Example <code>sbatch</code> Submits a SLURM script to the queue manager <code>sbatch job.sl</code> <code>squeue</code> Display jobs in the queueDisplay jobs belonging to user usr9999Display jobs on the long partition <code>squeue</code><code>squeue -u usr9999</code><code>squeue -p long</code> <code>sacct</code> Displays all the jobs run by you that dayDisplay job 123456789 <code>sacct</code><code>sacct -j 123456789</code> <code>scancel</code> Cancel a queued or running jobCancel all jobs belonging to usr9999 <code>scancel 123456789</code><code>scancel -u usr9999</code> <code>sshare</code> Shows the Fair Share scores for all projects of which usr9999 is a member <code>sshare -U usr9999</code> <code>sinfo</code> Shows the current state of the SLURM partitions <code>sinfo</code> <code>sview</code> Launches a GUI for monitoring SLURM jobs <code>sview</code>"},{"location":"resources/7_command_line_shortcuts/#sbatch-flags","title":"SBATCH flags","text":"<p>Most of these commands have a single character shortcut that you can use instead. These can be found by running</p> <pre><code>man sbatch\n</code></pre> <p>While logged into NeSI.</p> Command Description Example <code>--job-name</code> The name that will appear when using <code>squeue</code> or <code>sacct</code> <code>#SBATCH --job-name=MyJob</code> <code>--account</code> The account to which your core hours will be 'charged' <code>#SBATCH --account=nesi9999</code> <code>--time</code> Job max walltime. After this duration,the job will be terminated <code>#SBATCH --time=DD-HH:MM:SS</code> <code>--mem</code> Memory required by the job. If exceededthe job will be termintated <code>#SBATCH --mem=30GB</code> <code>--cpu</code> Number of CPUs to use for multithreading <code>#SBATCH --cpu=20</code> <code>--partition</code> NeSI partition for job to run (default is large) <code>#SBATCH --partition=long</code> <code>--output</code> Path and name of standard output file <code>#SBATCH --output=output.out</code> <code>--error</code> Path and name of standard error file <code>#SBATCH --output=output.err</code> <code>--mail-user</code> Address to send mail notifications <code>#SBATCH --mail-user=bob123@gmail.com</code> <code>--mail-type</code> When to send mail notificationsWill send a mail notification at BEGIN/END/FAILWill send message at 80% walltime <code>#SBATCH --mail-type=ALL</code><code>#SBATCH --mail-type=TIME_LIMIT_80</code> <code>--ntasks</code> Number of MPI tasks to useIt is very rare to use this option in bioinformatics"},{"location":"supplementary/supplementary_1/","title":"NeSI Setup","text":"S.1.1 : NeSI Mahuika Jupyter login <ol> <li><p>Follow https://jupyter.nesi.org.nz/hub/login</p></li> <li><p>Enter NeSI username, HPC password and 6 digit second factor token </p></li> <li> <p><p>Choose server options as below</p></p> <ul> <li>make sure to choose the correct project code <code>nesi02659</code>, number of CPUs <code>CPUs=4</code>, memory <code>8 GB</code> prior to pressing  button.</li> </ul> <p></p> </li> <li> <p><p>Once logged in, click the Terminal tile/icon to make sure a terminal session can be launched without any issues</p> <p></p> </p></li> </ol> S.1.2 : Set NeSI HPC Password <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and set your NeSI HPC password as below</li> </ol> <p></p> S.1.3 : Set NeSI HPC Second Factor <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and set your NeSI HPC Second factor as below</li> </ol> <p></p> S.1.4 : Reset NeSI HPC Password <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and Reset your NeSI HPC Second factor as below</li> </ol> <p></p> S.1.5 : Reset NeSI HPC Second Factor <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and Reset your NeSI HPC Second factor as below</li> </ol> <p></p>"},{"location":"supplementary/supplementary_2/","title":"NeSI File system, Working directory and Symlinks","text":"<p>The part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called \u2018folders\u2019), which hold files or other directories.</p> <p>Directories are like places \u2014 at any time while we are using the shell, we are in exactly one place called our current working directory. Commands mostly read and write files in the current working directory, i.e. \u2018here\u2019, so knowing where you are before running a command is important.</p> <p>NeSI Filesystem (For Researchers)</p> <p>All HPC platforms have custom File Systems for the purpose of general use and admin. NeSI Filesystem looks like above </p> <p> </p> <p>This may look a bit obscure but thing of them as different labels for some familiar names such as Desktop, Downloads, Documents</p> <ul> <li><code>/home/username</code> is for user-specific files such as configuration files, environment setup, source code, etc. This will be the default landing file system during a login</li> <li><code>/nesi/project/projectcode</code> is for persistent project-related data, project-related software, etc</li> <li><code>/nesi/nobackup/projectode</code> is a 'scratch space', for data you don't need to keep long term. Old data is periodically deleted from nobackup</li> </ul> <p><code>projectode</code> for this event is <code>nesi02659</code>. If you are to open a NeSI project for your own research, it will have a unique project code with a prefix to represent your affiliated institute and a five digit number (randomly generated). </p> <p>Therefore, full path to persistent and nobackup/scratch file systems will be in the format of </p> <ul> <li><code>/nesi/project/nesi02659</code></li> <li><code>/nesi/nobackup/nesi02659</code></li> </ul>"},{"location":"supplementary/supplementary_2/#symlinks-shortcuts","title":"Symlinks (shortcuts ?)","text":"<p>All of the SummerSchool material will be hosted on <code>/nesi/nobackup/nesi02659</code> file system as it is the largest and fastest filesystem. Also, each one of the attendee has an individual working space in <code>/nesi/nobackup/nesi02659/MGSS_U/</code>. Although this is great in everyway, having to type the full path to access this particular path (or having to remember it) from the default login site (<code>/home</code> filesystem) can be a tedious task. Therefore, we recommend creating a Symbolic link to your individual working directory from <code>/home</code> Think of it as a shortcut from your Desktop \ud83d\ude42</p> <p>Creating a symlink from <code>/home/$USER</code> to nobackup(scratch)</p> <ul> <li>Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material and open a terminal session</li> <li>Let's call the symlink (shortcut) <code>mgss</code></li> <li> <p>Following command will create the <code>mgss</code> symlink from your <code>/home</code> directory to individual working directory in <code>/nesi/nobackup/nesi02659/MGSS_U/</code> <pre><code>ln -s /nesi/nobackup/nesi02659/MGSS_U/$USER ~/mgss\n</code></pre></p> </li> <li> <p>Now, you can access your working directory with    <pre><code>cd ~/mgss\n</code></pre></p> </li> <li>Run <code>pwd</code> to check the current working directory</li> <li><code>pwd</code> commands on symlinks will print the \"relative path\" (location from where we are, rather than from the root of the file system) with respect to \"absolute path\" (entire path from the root directory)</li> <li>Run the command <code>realpath /path/you/want/to/know</code>  to show the absolute path of the symlink. i..e <pre><code>realpath ~/mgss\n</code></pre> OR if you are already at the symlinked path <pre><code>cd ~/mgss\n</code></pre> <pre><code>realpath .\n</code></pre></li> <li>Summary</li> </ul> <p></p>"},{"location":"supplementary/supplementary_2/#jupyter-file-explorer","title":"Jupyter File explorer","text":"<p>Guide  File Explorer to correct working directory</p> <p>Jupyter terminal and file explorer (on left) operate independently of each other. Therefore, changing the directory via terminal to your individual directory will not change the default working directory in explorer. Changing it to your individual directories can be done will couple of click </p> <p></p>"}]}