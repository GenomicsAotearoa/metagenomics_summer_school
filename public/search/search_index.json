{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"day1/ex1_bash_scripting/","text":"Introduction to the command line \u00b6 Objectives \u00b6 Navigating your file system Copying, Moving, Renaming and Removing files Examining file contents Redirection, manipulation and extraction Text and file manipulation Loops Shell scripting Moving files between your laptop and NeSI Introduction to slurm Navigating your file system \u00b6 Log in to NeSI and find the current location by running the command pwd which stands for print working directory . At any given time, current working directory is current default directory. pwd # /home/UserName/ We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\". ls Navigating to the MGSS_Intro/ directory can be done with the cd command which stands for change directory . cd MGSS_Intro/ Run the ls command to list the contents of the current directory. Check whether there are two .fastq files. The mkdir command ( make directory ) is used to make a directory. Enter mkdir followed by a space, then the directory name you want to create mkdir backup/ Copying, Moving, Renaming and Removing files \u00b6 Make a second copy of SRR097977.fastq and rename it as Test_1_backup.fastq . Then move that file to backup/ directory. cp SRR097977.fastq Test_1_backup.fastq mv Test_1_backup.fastq backup Navigate to backup/ directory and use mv command to rename and move Test_1_backup.fastq as Test_1_copy.fastq to the directory immediately above. cd backup/ mv Test_1_backup.fastq ../Test_1_copy.fastq Return to the directory immediately above, check whether the Test_1_copy.fastq was moved and renamed as instructed and remove it by using the rm command. cd .. rm Test_1_copy.fastq See whether you can remove the backup/ directory by using the rm command as well. rm backup/ # rm : can not remove 'backup/': Is a directory By default, rm will not delete directories. This can be done by using -r (recursive) option. rm -r backup Examining file contents \u00b6 There are a number of ways to examine the content of a file. cat and less are two commonly used programs for a quick look. Check the content of SRR097977.fastq by using these commands. Take a note of the differences. cat SRR097977.fastq # less SRR097977.fastq A few useful shortcuts for navigating in less There are ways to take a look at parts of a file. For example, the head and tail commands will scan the beginning and end of a file, respectively. head SRR097977.fastq tail SRR097977.fastq Adding -n option to either of these commands will print the first or last n lines of a file. head -n 1 SRR097977.fastq # @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36 Redirection, manipulation and extraction \u00b6 Although using cat and less commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is grep . Let's use this command to search for the string NNNNNNNNNN in SRR098026.fastq file. grep NNNNNNNNNN SRR098026.fastq Retrieve and discuss the output you get when grep was executed with the -B1 and -A1 flags. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is > . Redirecting the string of bad reads that was searched using the grep command to a file named bad_reads.txt can be done with grep -B1 -A2 NNNNNNNNNN SRR098026.fastq > bad_reads.txt Use the wc command to count the number of words, lines and characters in the bad_reads.txt file. wc bad_reads.txt Add -l flag to wc command and compare the number with the above output wc -l bad_reads.txt In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of >> (append redirect) command which will append the new output to the end of the file, rather than overwriting it. grep -B1 -A2 NNNNNNNNNN SRR097977.fastq >> bad_reads.txt Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards , which are symbols or special characters that represent other characters. For an example. Using * wildcard, we can run the previous grep command on both files at the same time. grep -B1 -A2 NNNNNNNNNN *.fastq >> bad_reads.txt wc -l bad_reads.txt The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The | (pipe) command is a commonly used method to apply an operation for an ouput without creating intermediate files. It takes the output generated by one command and uses it as the input to another command. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l Text and file manipulation \u00b6 There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below. Cut \u00b6 The cut command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that cut uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the -d parameter. See what results you get from the file names.txt . cat names.txt cut -d \" \" -f 1 names.txt cut -d \" \" -f 1 -3 names.txt cut -d \" \" -f 1 ,3 names.txt basename \u00b6 basename is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use basename to remove the .fastq extension from the files that we've been working with. basename SRR097977.fastq .fastq sed \u00b6 sed is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the sed command in UNIX is for substitution or for find and replace. By using sed you can edit files even without opening them, which is extremely important when working with large files. View the contents of the animals.txt file using cat . cat animals.txt We will now use sed to make some replacements to the text in this file. Example - Replacing/substituting a string \u00b6 sed is mostly used to replace the text in a file. In the following example, sed replaces the word 'dogs' with 'cats' in the file. sed 's/dogs/cats/' animals.txt Here the s specifies the substitution operation. The / characters are delimiters. The dogs is the search pattern and the cats is the replacement string. By default, the sed command replaces the first occurrence of the pattern in each line and it won't replace additional occurrences in the line. Example - Replacing all occurrences \u00b6 The substitute flag g (global replacement) can be added to the command to replace all the occurrences of the string in the line. sed 's/dogs/cats/g' animals.txt Example - Deleting a line \u00b6 To delete a particular line, we can specify the line number followed by the d character. For example sed '1d' animals.txt # Delete the first line sed '2d' animals.txt # Delete the second line sed '$d' animals.txt # Delete the last line Loops \u00b6 Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in bash scripting, for - iterates over a list of items and performs the given set of commands for item in [LIST] do [COMMANDS] done while - Performs a given set of commands an unknown number of times as long as the given condition evaluates is true while [CONDITION] do [COMMANDS] done until - Execute a given set of commands as longs as the given condition evaluates to false For most of our uses, a for loop is sufficient for our needs, so that is what we will be focusing on for this exercise. Shell identifies the for command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the do and done keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the $ character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example i = \"DAVE WAS HERE\" echo i # i echo $i # DAVE WAS HERE echo ${ i } # DAVE WAS HERE This prevents the shell interpreter from treating i as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files: for filename in *.fastq do head -n 2 ${filename} done Another useful command to be used with for loops is basename which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed). basename SRR097977.fastq .fastq basename is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things for filename in *.fastq do name=$(basename ${filename} .fastq) echo ${name} done Scripts \u00b6 Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal. The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as nano , emacs , and vi provide the platforms to write scripts. For this workshop we will use nano to create a file named ForLoop.sh . nano ForLoop.sh Add the following for-loop to the script (note the header #!/bin/bash ). #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } done Because nano is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing Ctrl + O , then exit nano using Ctrl + X . If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word . Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. bash ForLoop.sh Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the ls -l ForLoop.sh command and evaluate the first part of the output ls -l ForLoop.sh # -rw-rw-r-- 1 user user 88 Dec 6 19:52 ForLoop.sh There are three file permission flags that a file we create on NeSI can possess. Two of these, the read ( r ) and write ( w ) are marked for the ForLoop.sh file .The third flag, executable ( x ) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using chmod command. Add the executable permissions ( +x ) to ForLoop.sh and run ls again to see what has changed. chmod +x ForLoop.sh ls -l ForLoop.sh # -rwxrwxr-x 1 user user 88 Dec 6 19:52 ForLoop.sh Re-open the file in nano and append the output to TwoLines.txt , save and exit #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } >> TwoLines.txt done Execute the file ForLoop.sh . We'll need to put ./ at the beginning so the computer knows to look here in this directory for the program. ./ForLoop.sh Moving Files between your laptop and remote cluster/machine \u00b6 There are multiple commands and tools to move files between your laptop and remote clusters/machines. scp and rsync are some of the commands, and Globus and Cyberduck are some of these tools. scp is commonly used, as this is a simple to use tool that makes use of the ssh configuration we have already established for connecting to NeSI. In order it use it error free, we need to pay attention to whether the file is moving FROM or TO the remote cluster/machine, absolute paths, relative paths, Local vs Remote, etc. The format of the scp command is: scp path/to/copy/from path/to/copy/to If you wish to copy whole directories, include the -r flag: scp -r path/to/copy/from path/to/copy/to Finally, include mahuika: or ga-vl01: at the start of the remote machine file path (in this case, NeSI) to identify that this path is located within the remote cluster/machine. NOTE: the following templates are written in a way where the commands are to be **executed from local* ; i.e. a terminal not logged in to NeSI. On some systems, it may be preferable to open one terminal, log in to NeSI in that terminal, and then open a new terminal (that is not logged into NeSI) to perform the scp command. This avoids having to provide your NeSI log in details each time you wish to copy something.* FROM local TO remote \u00b6 scp /path/from/local/ ga-vl01:/path/to/remote/ FROM remote TO local \u00b6 scp ga-vl01:/path/from/remote/ /path/to/local/ If the ~/.ssh/config is not set with aliases you will need to replace the shortcut ga-vl01 with the full address of the remote: scp -oProxyCommand=\"ssh -W %h:%p user@lander.nesi.org.nz\" /path/in/local/ user@ga-vl01.mahuika.nesi.org.nz:/path/to/remote/ NOTE: If you are already comfortable with using scp , you are welcome to use this. However, for the purposes of this workshop, the relevant example files are available for download from the workshop main page . Alternatively, if you are working within the NeSI Jupyter hub , you can download individual files by right clicking on the file in the navigation pane on the left and clicking download . Introduction to slurm \u00b6 Jobs running on NeSI are submitted in the form of a batch script containing the code you want to run and a header of information needed by a job scheduler. All NeSI systems use the slurm batch scheduler for the submission, control and management of user jobs. Slurm provides a rich set of features for organising your workload and an extensive array of tools for managing your resource usage. In most cases, you need to know the commands: Copy the contents of the BLAST/ folder to your current directory, using the following command cp -r /nesi/nobackup/nesi02659/SLURM/BLAST ./ We will then navigate into this directory with the cd command, and inspect the text of the file blast-test.sh using less or nano . cd BLAST/ less blast-test.sh Evaluate the contents of the blast-test.sh script. Take a note of the basic slurm variables, path variables, etc. We will revisit these in the afternoon, when you create your own slurm scripts. Submit the script to the job queue as below. sbatch blast-test.sh","title":"Introduction to the command line"},{"location":"day1/ex1_bash_scripting/#introduction-to-the-command-line","text":"","title":"Introduction to the command line"},{"location":"day1/ex1_bash_scripting/#objectives","text":"Navigating your file system Copying, Moving, Renaming and Removing files Examining file contents Redirection, manipulation and extraction Text and file manipulation Loops Shell scripting Moving files between your laptop and NeSI Introduction to slurm","title":"Objectives"},{"location":"day1/ex1_bash_scripting/#navigating-your-file-system","text":"Log in to NeSI and find the current location by running the command pwd which stands for print working directory . At any given time, current working directory is current default directory. pwd # /home/UserName/ We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\". ls Navigating to the MGSS_Intro/ directory can be done with the cd command which stands for change directory . cd MGSS_Intro/ Run the ls command to list the contents of the current directory. Check whether there are two .fastq files. The mkdir command ( make directory ) is used to make a directory. Enter mkdir followed by a space, then the directory name you want to create mkdir backup/","title":"Navigating your file system"},{"location":"day1/ex1_bash_scripting/#copying-moving-renaming-and-removing-files","text":"Make a second copy of SRR097977.fastq and rename it as Test_1_backup.fastq . Then move that file to backup/ directory. cp SRR097977.fastq Test_1_backup.fastq mv Test_1_backup.fastq backup Navigate to backup/ directory and use mv command to rename and move Test_1_backup.fastq as Test_1_copy.fastq to the directory immediately above. cd backup/ mv Test_1_backup.fastq ../Test_1_copy.fastq Return to the directory immediately above, check whether the Test_1_copy.fastq was moved and renamed as instructed and remove it by using the rm command. cd .. rm Test_1_copy.fastq See whether you can remove the backup/ directory by using the rm command as well. rm backup/ # rm : can not remove 'backup/': Is a directory By default, rm will not delete directories. This can be done by using -r (recursive) option. rm -r backup","title":"Copying, Moving, Renaming and Removing files"},{"location":"day1/ex1_bash_scripting/#examining-file-contents","text":"There are a number of ways to examine the content of a file. cat and less are two commonly used programs for a quick look. Check the content of SRR097977.fastq by using these commands. Take a note of the differences. cat SRR097977.fastq # less SRR097977.fastq A few useful shortcuts for navigating in less There are ways to take a look at parts of a file. For example, the head and tail commands will scan the beginning and end of a file, respectively. head SRR097977.fastq tail SRR097977.fastq Adding -n option to either of these commands will print the first or last n lines of a file. head -n 1 SRR097977.fastq # @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36","title":"Examining file contents"},{"location":"day1/ex1_bash_scripting/#redirection-manipulation-and-extraction","text":"Although using cat and less commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is grep . Let's use this command to search for the string NNNNNNNNNN in SRR098026.fastq file. grep NNNNNNNNNN SRR098026.fastq Retrieve and discuss the output you get when grep was executed with the -B1 and -A1 flags. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is > . Redirecting the string of bad reads that was searched using the grep command to a file named bad_reads.txt can be done with grep -B1 -A2 NNNNNNNNNN SRR098026.fastq > bad_reads.txt Use the wc command to count the number of words, lines and characters in the bad_reads.txt file. wc bad_reads.txt Add -l flag to wc command and compare the number with the above output wc -l bad_reads.txt In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of >> (append redirect) command which will append the new output to the end of the file, rather than overwriting it. grep -B1 -A2 NNNNNNNNNN SRR097977.fastq >> bad_reads.txt Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards , which are symbols or special characters that represent other characters. For an example. Using * wildcard, we can run the previous grep command on both files at the same time. grep -B1 -A2 NNNNNNNNNN *.fastq >> bad_reads.txt wc -l bad_reads.txt The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The | (pipe) command is a commonly used method to apply an operation for an ouput without creating intermediate files. It takes the output generated by one command and uses it as the input to another command. grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l","title":"Redirection, manipulation and extraction"},{"location":"day1/ex1_bash_scripting/#text-and-file-manipulation","text":"There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below.","title":"Text and file manipulation"},{"location":"day1/ex1_bash_scripting/#cut","text":"The cut command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that cut uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the -d parameter. See what results you get from the file names.txt . cat names.txt cut -d \" \" -f 1 names.txt cut -d \" \" -f 1 -3 names.txt cut -d \" \" -f 1 ,3 names.txt","title":"Cut"},{"location":"day1/ex1_bash_scripting/#basename","text":"basename is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use basename to remove the .fastq extension from the files that we've been working with. basename SRR097977.fastq .fastq","title":"basename"},{"location":"day1/ex1_bash_scripting/#sed","text":"sed is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the sed command in UNIX is for substitution or for find and replace. By using sed you can edit files even without opening them, which is extremely important when working with large files. View the contents of the animals.txt file using cat . cat animals.txt We will now use sed to make some replacements to the text in this file.","title":"sed"},{"location":"day1/ex1_bash_scripting/#example-replacingsubstituting-a-string","text":"sed is mostly used to replace the text in a file. In the following example, sed replaces the word 'dogs' with 'cats' in the file. sed 's/dogs/cats/' animals.txt Here the s specifies the substitution operation. The / characters are delimiters. The dogs is the search pattern and the cats is the replacement string. By default, the sed command replaces the first occurrence of the pattern in each line and it won't replace additional occurrences in the line.","title":"Example - Replacing/substituting a string"},{"location":"day1/ex1_bash_scripting/#example-replacing-all-occurrences","text":"The substitute flag g (global replacement) can be added to the command to replace all the occurrences of the string in the line. sed 's/dogs/cats/g' animals.txt","title":"Example - Replacing all occurrences"},{"location":"day1/ex1_bash_scripting/#example-deleting-a-line","text":"To delete a particular line, we can specify the line number followed by the d character. For example sed '1d' animals.txt # Delete the first line sed '2d' animals.txt # Delete the second line sed '$d' animals.txt # Delete the last line","title":"Example - Deleting a line"},{"location":"day1/ex1_bash_scripting/#loops","text":"Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in bash scripting, for - iterates over a list of items and performs the given set of commands for item in [LIST] do [COMMANDS] done while - Performs a given set of commands an unknown number of times as long as the given condition evaluates is true while [CONDITION] do [COMMANDS] done until - Execute a given set of commands as longs as the given condition evaluates to false For most of our uses, a for loop is sufficient for our needs, so that is what we will be focusing on for this exercise. Shell identifies the for command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the do and done keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the $ character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example i = \"DAVE WAS HERE\" echo i # i echo $i # DAVE WAS HERE echo ${ i } # DAVE WAS HERE This prevents the shell interpreter from treating i as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files: for filename in *.fastq do head -n 2 ${filename} done Another useful command to be used with for loops is basename which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed). basename SRR097977.fastq .fastq basename is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things for filename in *.fastq do name=$(basename ${filename} .fastq) echo ${name} done","title":"Loops"},{"location":"day1/ex1_bash_scripting/#scripts","text":"Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal. The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as nano , emacs , and vi provide the platforms to write scripts. For this workshop we will use nano to create a file named ForLoop.sh . nano ForLoop.sh Add the following for-loop to the script (note the header #!/bin/bash ). #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } done Because nano is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing Ctrl + O , then exit nano using Ctrl + X . If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word . Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. bash ForLoop.sh Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the ls -l ForLoop.sh command and evaluate the first part of the output ls -l ForLoop.sh # -rw-rw-r-- 1 user user 88 Dec 6 19:52 ForLoop.sh There are three file permission flags that a file we create on NeSI can possess. Two of these, the read ( r ) and write ( w ) are marked for the ForLoop.sh file .The third flag, executable ( x ) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using chmod command. Add the executable permissions ( +x ) to ForLoop.sh and run ls again to see what has changed. chmod +x ForLoop.sh ls -l ForLoop.sh # -rwxrwxr-x 1 user user 88 Dec 6 19:52 ForLoop.sh Re-open the file in nano and append the output to TwoLines.txt , save and exit #!/bin/bash for filename in *.fastq do head -n 2 ${ filename } >> TwoLines.txt done Execute the file ForLoop.sh . We'll need to put ./ at the beginning so the computer knows to look here in this directory for the program. ./ForLoop.sh","title":"Scripts"},{"location":"day1/ex1_bash_scripting/#moving-files-between-your-laptop-and-remote-clustermachine","text":"There are multiple commands and tools to move files between your laptop and remote clusters/machines. scp and rsync are some of the commands, and Globus and Cyberduck are some of these tools. scp is commonly used, as this is a simple to use tool that makes use of the ssh configuration we have already established for connecting to NeSI. In order it use it error free, we need to pay attention to whether the file is moving FROM or TO the remote cluster/machine, absolute paths, relative paths, Local vs Remote, etc. The format of the scp command is: scp path/to/copy/from path/to/copy/to If you wish to copy whole directories, include the -r flag: scp -r path/to/copy/from path/to/copy/to Finally, include mahuika: or ga-vl01: at the start of the remote machine file path (in this case, NeSI) to identify that this path is located within the remote cluster/machine. NOTE: the following templates are written in a way where the commands are to be **executed from local* ; i.e. a terminal not logged in to NeSI. On some systems, it may be preferable to open one terminal, log in to NeSI in that terminal, and then open a new terminal (that is not logged into NeSI) to perform the scp command. This avoids having to provide your NeSI log in details each time you wish to copy something.*","title":"Moving Files between your laptop and remote cluster/machine"},{"location":"day1/ex1_bash_scripting/#from-local-to-remote","text":"scp /path/from/local/ ga-vl01:/path/to/remote/","title":"FROM local TO remote"},{"location":"day1/ex1_bash_scripting/#from-remote-to-local","text":"scp ga-vl01:/path/from/remote/ /path/to/local/ If the ~/.ssh/config is not set with aliases you will need to replace the shortcut ga-vl01 with the full address of the remote: scp -oProxyCommand=\"ssh -W %h:%p user@lander.nesi.org.nz\" /path/in/local/ user@ga-vl01.mahuika.nesi.org.nz:/path/to/remote/ NOTE: If you are already comfortable with using scp , you are welcome to use this. However, for the purposes of this workshop, the relevant example files are available for download from the workshop main page . Alternatively, if you are working within the NeSI Jupyter hub , you can download individual files by right clicking on the file in the navigation pane on the left and clicking download .","title":"FROM remote TO local"},{"location":"day1/ex1_bash_scripting/#introduction-to-slurm","text":"Jobs running on NeSI are submitted in the form of a batch script containing the code you want to run and a header of information needed by a job scheduler. All NeSI systems use the slurm batch scheduler for the submission, control and management of user jobs. Slurm provides a rich set of features for organising your workload and an extensive array of tools for managing your resource usage. In most cases, you need to know the commands: Copy the contents of the BLAST/ folder to your current directory, using the following command cp -r /nesi/nobackup/nesi02659/SLURM/BLAST ./ We will then navigate into this directory with the cd command, and inspect the text of the file blast-test.sh using less or nano . cd BLAST/ less blast-test.sh Evaluate the contents of the blast-test.sh script. Take a note of the basic slurm variables, path variables, etc. We will revisit these in the afternoon, when you create your own slurm scripts. Submit the script to the job queue as below. sbatch blast-test.sh","title":"Introduction to slurm"},{"location":"day1/ex2_quality_filtering/","text":"Quality filtering raw reads \u00b6 Objectives \u00b6 Visualising raw reads with FastQC on NeSI Read trimming and adapter removal with trimmomatic Diagnosing poor libraries Understand common issues and best practices Optional : Filtering out host DNA with BBMap Visualising raw reads \u00b6 FastQC is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues: Adapter/barcode sequences Low quality regions of sequence Quality drop-off towards the end of read-pair sequence Loading FastQC \u00b6 These exercises will take place in the 2.fastqc/ folder. First, navigate to this folder. Copy the command below into your terminal (logged in to NeSI), replacing <YOUR FOLDER> , and then running the command. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ To activate FastQC on NeSI, you need to first load the module using the command module purge module load FastQC/0.11.7 Running FastQC \u00b6 We will run FastQC from the command line as follows: fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz Viewing the outputs from FastQC \u00b6 FastQC generates output reports in .html files that can be viewed in a standard web browser. Unfortunately, these can not be viewed from within the standard NeSI terminal environment. In day-to-day usage, it will be necessary to download the resulting files using scp and view them on your computer. Fortunately, if you're currently using the terminal within Jupyter hub for today's session, we can open the .html files directly from here: Click on the folder icon in the top left to open the folder navigator pane (if not already open). The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). Click through MGSS_U , into your folder, and then into the 2.fastqc/ folder. Double click on the output ...fastqc.html files to open them in the a new tab within the Jupyter hub . Examples of the output files are also available for download here . Note that FastQC does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. At a first glance, we can see the follow statistics: The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming. There are 100,000 reads in the file The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing. Have a quick look through the left hand columns. As you can see, the data has passed most of the basic parameters. Per-base sequence quality Per base sequence content Adapter Content Per sequence GC content The only aspect of the data that FastQC is flagging as potentially problematic is the GC% content of the data set. This is a common observation, as we are dealing with a mixed community and organisms and it is therefore unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of FastQC , but completely consistent with the biology of the system. FastQC outputs for libraries with errors \u00b6 Lets take a look at a library with significant errors. Process the sequence file mock_R1.adapter_decay.fastq with FastQC . fastqc mock_R1.adapter_decay.fastq.gz Compare the results with the mock_R1.good.fastq.gz file. Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not? Per-base sequence quality Read trimming and adapter removal with trimmomatic \u00b6 There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use trimmomatic , but this should in no way be interpreted as an endorsement of trimmomatic over equivalent tools like BBMap , sickle , cutadapt or any other. For a first run with trimmomatic , type the following commands into your console: module load Trimmomatic/0.39-Java-1.8.0_144 trimmomatic PE -threads 10 -phred33 \\ mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\ mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\ HEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80 There is a lot going on in this command, so here is a breakdown of the parameters in the command above Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 10 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used mock_R1.adapter_decay.fastq.gz / mock_R2.adapter_decay.fastq.gz positional The paired forward and reverse reads to trim mock_R1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner also passed mock_s1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) mock_R2.qc.fastq.gz / mock_s2.qc.fastq.gz positional The reverse-sequence equivalent of above HEADCROP:10 positional Adapter trimming command. Remove the first 10 positions in the sequence SLIDINGWINDOW:4:30 positional Quality filtering command. Analyses each sequence in a 4 base pair sliding window, truncating if the average quality drops below Q30 MINLEN:80 positional Length filtering command, discard sequences that are shorter than 80 base pairs after trimming Running the trimmed files back through FastQC , we can see that this signifcantly improves the output. Quality of reads \u00b6 Nucleotide distribution \u00b6 Considerations when working with trimmomatic \u00b6 Order of operations The basic format for a trimmomatic command is trimmomatic PE <keyword flags> <sequence input> <sequence output> <trimming parameters> The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users. For example, consider these two scenarios: trimmomatic PE <keyword flags> <sequence input> <sequence output> SLIDINGWINDOW:4:30 MINLEN:80 trimmomatic PE <keyword flags> <sequence input> <sequence output> MINLEN:80 SLIDINGWINDOW:4:30 In the first run we would not expect any sequence shorter than 80 base pairs to exist in the output files, but we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming the MINLEN filtering does not execute a second time. In the first instance, we are excluding performing trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the MINLEN run. A more subtle consequence of this behaviour is the interplay between adapter removal and quality filtering. In the command above, we try to identify adapters before quality trimming. Why do you think this is? Optional: Working with the ILLUMINACLIP command \u00b6 Trimmomatic also provides the ILLUMINACLIP command, which can be used to pass a fastA file of adapter and barcode sequences to be found and removed from your data. The format for the ILLUMINACLIP parameter can be quite confusing to work with and so we generally favour a HEADCROP where possitble. If you wish to work with the ILLUMINACLIP command, then you can see its use here in the trimmomatic manual . ILLUMINACLIP:iua.fna:1:25:7 | | | | | | | | | | | Simple clip threshold | | Palindrome clip threshold | Seed mismatches File of expected sequences There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. Where possible, we favour the use of simple positional trimming. Diagnosing poor libraries \u00b6 Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating FastQC : Does the sequencing length match what you ordered from the facility? If the sequences are shorter than expected, is adapter read-through a concern? What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation? Are adapters and/or barcodes removed? (look at the Per base sequence content to diagnose this) Is there unexpected sequence duplication? (this can occur when low-input library preparations are used) Are over-represented k -mers present? (this can be a sign of adapter and barcode contamination) Understand common issues and best practices \u00b6 Do I need to remove (rare) adapters? I don\u2019t know if adapters have been removed or not How do I identify and remove adapter read-through Identifying incomplete barcode/adapter removal Over aggressive trimming GC skew is outside of expected range Optional : Filtering out host DNA \u00b6 Metagenome data derived from microbial communities associated with a host should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species. There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. NOTE: This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via Kraken2 and then filtering out all reads that map to the pylum or kingdom of your host taxa). This exercise provides an example using BBMap to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping the quality-filtered reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that: are presumbed microbial contaminant in the reference; have high homology to microbial genes/genomes (e.g. ribosomes); or those that are of low complexity. This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed. NOTE: The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using bbmask. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread . Downloading the masked human reference genome \u00b6 The masked reference genome is available via a google drive link. We can use gdown to download this file from google drive via the command line. To install gdown , we can use pip . # Install gdown (for downloading from google drive) module purge module load Python/3.8.2-gimkl-2020a pip install --user gdown Nest, download the reference. It will also be necessary to first add your local bin location to the PATH variable via the export PATH=... command, as this is where gdown is located (modify <your_username> before running the code below). mkdir BBMask_human_reference/ cd BBMask_human_reference/ export PATH = \"/home/<your_username>/.local/bin: $PATH \" gdown https://drive.google.com/uc?id = 0B3llHR93L14wd0pSSnFULUlhcUk Indexing the reference genome and read mapping with BBMap \u00b6 We will cover more about read mapping in later exercises . For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a BBMap index, and then use BBMap to map the quality-filtered reads to that index, ultimately retaining only those reads that do not map to the index. Build index reference via BBMap . We will do this by submitting the job via slurm. NOTE: See Preparing an assembly job for slurm for more information about how to submit a job via slurm. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J host_filt_bbmap_index #SBATCH --res SummerSchool #SBATCH -J 2.qc_bbmap_ref #SBATCH --time 00:20:00 #SBATCH --mem 23GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH -e host_filt_bbmap_index.err #SBATCH -o host_filt_bbmap_index.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/BBMask_human_reference/ # Load BBMap module module purge module load BBMap/38.90-gimkl-2020a # Build indexed reference file via BBMap srun bbmap.sh ref = hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz -Xmx23g Finally, map the quality-filtered reads to the reference via BBMap . Here we will submit the job as a slurm array, with one array job per sample. Breaking down this command a little: We pass the path to the ref file (the reference we just built) to path=... . Provide quality-filtered reads as input (i.e. output of the trimmomatic process above). In this case, we will provide the fastq files located in ../3.assembly/ which have been processed via trimmomatic in the same manner as the exercise above. These are four sets of paired reads (representing metagenome data from four 'samples') that the remainder of the workshop exercises will be working with. The flags -Xmx27g and -t=20 set the max memory and threads allocations, and must match the --mem and --cpus_per_task allocations in the slurm headers at the top of the script. The rest of the settings in the BBMap call here are as per the recommendations found within this thread about processing data to remove host reads. Finally, the filtered output fastq files for downstream use are output to the host_filtered_reads/ folder (taken from the outputs outu1= and otu2= , which include only those reads that did not map to the host reference genome). NOTE: Slurm array jobs automatically create a variable SLURM_ARRAY_TASK_ID for that job, which contains the array task number (i.e. between 1 and 4 in the case below). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" ( sample${SLURM_ARRAY_TASK_ID} is read in as sample3 ). #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name host_filt_bbmap_map #SBATCH --res SummerSchool #SBATCH --time 01:00:00 #SBATCH --mem 27GB #SBATCH --ntasks 1 #SBATCH --array=1-4 #SBATCH --cpus-per-task 20 #SBATCH --error host_filt_bbmap_map_%a.err #SBATCH --output host_filt_bbmap_map_%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ mkdir -p host_filtered_reads/ # Load BBMap module module purge module load BBMap/38.95-gimkl-2020a # Run bbmap srun bbmap.sh -Xmx27g -t = 20 \\ minid = 0 .95 maxindel = 3 bwr = 0 .16 bw = 12 quickmatch fast minhits = 2 qtrim = rl trimq = 10 untrim \\ in1 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ in2 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ path = BBMask_human_reference/ \\ outu1 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R1_hostFilt.fastq \\ outu2 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R2_hostFilt.fastq The filtered reads are now available in host_filtered_reads/ for downstream use.","title":"Quality filtering raw reads"},{"location":"day1/ex2_quality_filtering/#quality-filtering-raw-reads","text":"","title":"Quality filtering raw reads"},{"location":"day1/ex2_quality_filtering/#objectives","text":"Visualising raw reads with FastQC on NeSI Read trimming and adapter removal with trimmomatic Diagnosing poor libraries Understand common issues and best practices Optional : Filtering out host DNA with BBMap","title":"Objectives"},{"location":"day1/ex2_quality_filtering/#visualising-raw-reads","text":"FastQC is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues: Adapter/barcode sequences Low quality regions of sequence Quality drop-off towards the end of read-pair sequence","title":"Visualising raw reads"},{"location":"day1/ex2_quality_filtering/#loading-fastqc","text":"These exercises will take place in the 2.fastqc/ folder. First, navigate to this folder. Copy the command below into your terminal (logged in to NeSI), replacing <YOUR FOLDER> , and then running the command. cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ To activate FastQC on NeSI, you need to first load the module using the command module purge module load FastQC/0.11.7","title":"Loading FastQC"},{"location":"day1/ex2_quality_filtering/#running-fastqc","text":"We will run FastQC from the command line as follows: fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz","title":"Running FastQC"},{"location":"day1/ex2_quality_filtering/#viewing-the-outputs-from-fastqc","text":"FastQC generates output reports in .html files that can be viewed in a standard web browser. Unfortunately, these can not be viewed from within the standard NeSI terminal environment. In day-to-day usage, it will be necessary to download the resulting files using scp and view them on your computer. Fortunately, if you're currently using the terminal within Jupyter hub for today's session, we can open the .html files directly from here: Click on the folder icon in the top left to open the folder navigator pane (if not already open). The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). Click through MGSS_U , into your folder, and then into the 2.fastqc/ folder. Double click on the output ...fastqc.html files to open them in the a new tab within the Jupyter hub . Examples of the output files are also available for download here . Note that FastQC does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. At a first glance, we can see the follow statistics: The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming. There are 100,000 reads in the file The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing. Have a quick look through the left hand columns. As you can see, the data has passed most of the basic parameters. Per-base sequence quality Per base sequence content Adapter Content Per sequence GC content The only aspect of the data that FastQC is flagging as potentially problematic is the GC% content of the data set. This is a common observation, as we are dealing with a mixed community and organisms and it is therefore unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of FastQC , but completely consistent with the biology of the system.","title":"Viewing the outputs from FastQC"},{"location":"day1/ex2_quality_filtering/#fastqc-outputs-for-libraries-with-errors","text":"Lets take a look at a library with significant errors. Process the sequence file mock_R1.adapter_decay.fastq with FastQC . fastqc mock_R1.adapter_decay.fastq.gz Compare the results with the mock_R1.good.fastq.gz file. Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not? Per-base sequence quality","title":"FastQC outputs for libraries with errors"},{"location":"day1/ex2_quality_filtering/#read-trimming-and-adapter-removal-with-trimmomatic","text":"There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use trimmomatic , but this should in no way be interpreted as an endorsement of trimmomatic over equivalent tools like BBMap , sickle , cutadapt or any other. For a first run with trimmomatic , type the following commands into your console: module load Trimmomatic/0.39-Java-1.8.0_144 trimmomatic PE -threads 10 -phred33 \\ mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\ mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\ HEADCROP:10 SLIDINGWINDOW:4:30 MINLEN:80 There is a lot going on in this command, so here is a breakdown of the parameters in the command above Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 10 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used mock_R1.adapter_decay.fastq.gz / mock_R2.adapter_decay.fastq.gz positional The paired forward and reverse reads to trim mock_R1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner also passed mock_s1.qc.fastq.gz positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) mock_R2.qc.fastq.gz / mock_s2.qc.fastq.gz positional The reverse-sequence equivalent of above HEADCROP:10 positional Adapter trimming command. Remove the first 10 positions in the sequence SLIDINGWINDOW:4:30 positional Quality filtering command. Analyses each sequence in a 4 base pair sliding window, truncating if the average quality drops below Q30 MINLEN:80 positional Length filtering command, discard sequences that are shorter than 80 base pairs after trimming Running the trimmed files back through FastQC , we can see that this signifcantly improves the output.","title":"Read trimming and adapter removal with trimmomatic"},{"location":"day1/ex2_quality_filtering/#quality-of-reads","text":"","title":"Quality of reads"},{"location":"day1/ex2_quality_filtering/#nucleotide-distribution","text":"","title":"Nucleotide distribution"},{"location":"day1/ex2_quality_filtering/#considerations-when-working-with-trimmomatic","text":"Order of operations The basic format for a trimmomatic command is trimmomatic PE <keyword flags> <sequence input> <sequence output> <trimming parameters> The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users. For example, consider these two scenarios: trimmomatic PE <keyword flags> <sequence input> <sequence output> SLIDINGWINDOW:4:30 MINLEN:80 trimmomatic PE <keyword flags> <sequence input> <sequence output> MINLEN:80 SLIDINGWINDOW:4:30 In the first run we would not expect any sequence shorter than 80 base pairs to exist in the output files, but we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming the MINLEN filtering does not execute a second time. In the first instance, we are excluding performing trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the MINLEN run. A more subtle consequence of this behaviour is the interplay between adapter removal and quality filtering. In the command above, we try to identify adapters before quality trimming. Why do you think this is?","title":"Considerations when working with trimmomatic"},{"location":"day1/ex2_quality_filtering/#optional-working-with-the-illuminaclip-command","text":"Trimmomatic also provides the ILLUMINACLIP command, which can be used to pass a fastA file of adapter and barcode sequences to be found and removed from your data. The format for the ILLUMINACLIP parameter can be quite confusing to work with and so we generally favour a HEADCROP where possitble. If you wish to work with the ILLUMINACLIP command, then you can see its use here in the trimmomatic manual . ILLUMINACLIP:iua.fna:1:25:7 | | | | | | | | | | | Simple clip threshold | | Palindrome clip threshold | Seed mismatches File of expected sequences There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. Where possible, we favour the use of simple positional trimming.","title":"Optional: Working with the ILLUMINACLIP command"},{"location":"day1/ex2_quality_filtering/#diagnosing-poor-libraries","text":"Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating FastQC : Does the sequencing length match what you ordered from the facility? If the sequences are shorter than expected, is adapter read-through a concern? What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation? Are adapters and/or barcodes removed? (look at the Per base sequence content to diagnose this) Is there unexpected sequence duplication? (this can occur when low-input library preparations are used) Are over-represented k -mers present? (this can be a sign of adapter and barcode contamination)","title":"Diagnosing poor libraries"},{"location":"day1/ex2_quality_filtering/#understand-common-issues-and-best-practices","text":"Do I need to remove (rare) adapters? I don\u2019t know if adapters have been removed or not How do I identify and remove adapter read-through Identifying incomplete barcode/adapter removal Over aggressive trimming GC skew is outside of expected range","title":"Understand common issues and best practices"},{"location":"day1/ex2_quality_filtering/#optional-filtering-out-host-dna","text":"Metagenome data derived from microbial communities associated with a host should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species. There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. NOTE: This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via Kraken2 and then filtering out all reads that map to the pylum or kingdom of your host taxa). This exercise provides an example using BBMap to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping the quality-filtered reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that: are presumbed microbial contaminant in the reference; have high homology to microbial genes/genomes (e.g. ribosomes); or those that are of low complexity. This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed. NOTE: The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using bbmask. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread .","title":"Optional: Filtering out host DNA"},{"location":"day1/ex2_quality_filtering/#downloading-the-masked-human-reference-genome","text":"The masked reference genome is available via a google drive link. We can use gdown to download this file from google drive via the command line. To install gdown , we can use pip . # Install gdown (for downloading from google drive) module purge module load Python/3.8.2-gimkl-2020a pip install --user gdown Nest, download the reference. It will also be necessary to first add your local bin location to the PATH variable via the export PATH=... command, as this is where gdown is located (modify <your_username> before running the code below). mkdir BBMask_human_reference/ cd BBMask_human_reference/ export PATH = \"/home/<your_username>/.local/bin: $PATH \" gdown https://drive.google.com/uc?id = 0B3llHR93L14wd0pSSnFULUlhcUk","title":"Downloading the masked human reference genome"},{"location":"day1/ex2_quality_filtering/#indexing-the-reference-genome-and-read-mapping-with-bbmap","text":"We will cover more about read mapping in later exercises . For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a BBMap index, and then use BBMap to map the quality-filtered reads to that index, ultimately retaining only those reads that do not map to the index. Build index reference via BBMap . We will do this by submitting the job via slurm. NOTE: See Preparing an assembly job for slurm for more information about how to submit a job via slurm. #!/bin/bash -e #SBATCH -A nesi02659 #SBATCH -J host_filt_bbmap_index #SBATCH --res SummerSchool #SBATCH -J 2.qc_bbmap_ref #SBATCH --time 00:20:00 #SBATCH --mem 23GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH -e host_filt_bbmap_index.err #SBATCH -o host_filt_bbmap_index.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/BBMask_human_reference/ # Load BBMap module module purge module load BBMap/38.90-gimkl-2020a # Build indexed reference file via BBMap srun bbmap.sh ref = hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz -Xmx23g Finally, map the quality-filtered reads to the reference via BBMap . Here we will submit the job as a slurm array, with one array job per sample. Breaking down this command a little: We pass the path to the ref file (the reference we just built) to path=... . Provide quality-filtered reads as input (i.e. output of the trimmomatic process above). In this case, we will provide the fastq files located in ../3.assembly/ which have been processed via trimmomatic in the same manner as the exercise above. These are four sets of paired reads (representing metagenome data from four 'samples') that the remainder of the workshop exercises will be working with. The flags -Xmx27g and -t=20 set the max memory and threads allocations, and must match the --mem and --cpus_per_task allocations in the slurm headers at the top of the script. The rest of the settings in the BBMap call here are as per the recommendations found within this thread about processing data to remove host reads. Finally, the filtered output fastq files for downstream use are output to the host_filtered_reads/ folder (taken from the outputs outu1= and otu2= , which include only those reads that did not map to the host reference genome). NOTE: Slurm array jobs automatically create a variable SLURM_ARRAY_TASK_ID for that job, which contains the array task number (i.e. between 1 and 4 in the case below). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" ( sample${SLURM_ARRAY_TASK_ID} is read in as sample3 ). #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name host_filt_bbmap_map #SBATCH --res SummerSchool #SBATCH --time 01:00:00 #SBATCH --mem 27GB #SBATCH --ntasks 1 #SBATCH --array=1-4 #SBATCH --cpus-per-task 20 #SBATCH --error host_filt_bbmap_map_%a.err #SBATCH --output host_filt_bbmap_map_%a.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL # Set up working directories cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/2.fastqc/ mkdir -p host_filtered_reads/ # Load BBMap module module purge module load BBMap/38.95-gimkl-2020a # Run bbmap srun bbmap.sh -Xmx27g -t = 20 \\ minid = 0 .95 maxindel = 3 bwr = 0 .16 bw = 12 quickmatch fast minhits = 2 qtrim = rl trimq = 10 untrim \\ in1 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ in2 = ../3.assembly/sample ${ SLURM_ARRAY_TASK_ID } _R1.fastq.gz \\ path = BBMask_human_reference/ \\ outu1 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R1_hostFilt.fastq \\ outu2 = host_filtered_reads/sample ${ SLURM_ARRAY_TASK_ID } _R2_hostFilt.fastq The filtered reads are now available in host_filtered_reads/ for downstream use.","title":"Indexing the reference genome and read mapping with BBMap"},{"location":"day1/ex3_assembly/","text":"Assembly \u00b6 Objectives \u00b6 Become familiar with the standard input files for SPAdes and IDBA-UD Understand the basic parameters that should be modified when using these assemblers Prepare an assembly job to run under slurm All work for this exercise will occur in the 3.assembly/ directory. The standard input files for SPAdes and IDBA-UD \u00b6 Although they both make use of the same types of data, both SPAdes and IDBA-UD have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by SPAdes : module load SPAdes/3.13.1-gimkl-2018b spades.py -h # ... #Input data: #-1 <filename> file with forward paired-end reads #-2 <filename> file with reverse paired-end reads #-s <filename> file with unpaired reads #--mp<#>-1 <filename> file with forward reads for mate-pair library number <#> (<#> = 1,2,..,9) #--mp<#>-2 <filename> file with reverse reads for mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-1 <filename> file with forward reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-2 <filename> file with reverse reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--nxmate<#>-1 <filename> file with forward reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--nxmate<#>-2 <filename> file with reverse reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--sanger <filename> file with Sanger reads #--pacbio <filename> file with PacBio reads #--nanopore <filename> file with Nanopore reads #--tslr <filename> file with TSLR-contigs #--trusted-contigs <filename> file with trusted contigs #--untrusted-contigs <filename> file with untrusted contigs At a glance, you could provide any of the following data types to SPAdes and have it perform an assembly: Illumina paired-end sequencing data, either as standard library or Mate Pairs Sanger sequences PacBio reads Oxford Nanopore reads Pre-assembled scaffolds for guiding the assembly Awkwardly, while SPAdes accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the -meta flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing. cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz > for_spades_R1.fq.gz cat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz > for_spades_R2.fq.gz Note that these fastQ files are compressed, yet we can concatenate them together with the cat command regardless. This is a nice feature of .gz files that is handy to remember. By contrast, what does IDBA-UD accept? module load IDBA/1.1.3-gimkl-2017a idba_ud # ... # -o, --out arg (=out) output directory # -r, --read arg fasta read file (<=128) # ... # -l, --long_read arg fasta long read file (>128) 'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using IDBA-UD we will need to pool the paired-end data into a single, interleaved fastA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example # Paired-end file, forward >read1_1 ... >read2_1 ... # Paired-end file, forward >read1_2 ... >read2_2 ... # Interleaved file >read1_1 ... >read1_2 ... >read2_1 ... >read2_2 ... Fortunately, the IDBA set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first. module load pigz/2.4-GCCcore-7.4.0 for i in sample1 sample2 sample3 sample4 ; do pigz --keep --decompress ${ i } _R1.fastq.gz ${ i } _R2.fastq.gz fq2fa --merge ${ i } _R1.fastq ${ i } _R2.fastq ${ i } .fna done cat sample1.fna sample2.fna sample3.fna sample4.fna > for_idba.fna Basic assembly parameters \u00b6 For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be. Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include: k -mer sizes to be assembled over, and step size if using a range Number of threads to use during assembly Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space Setting the k -mer size \u00b6 Depending on which assembler you are using, the commands for chosing the k -mer sizes for the assembly vary slightly, but they are recognisable between programs. In SPAdes , you can set the k -mer size using either spades.py -k 21 ,33,55,77,99,121 ... spades.py -k auto ... The first command lets us specify the k -mers ourselves, or we are letting SPAdes automatically pick the most appropriate size. For IDBA-UD , we can select the k -mer size using idba_ud --mink 21 --maxk 121 --step 22 Unlike SPAdes , we do not have fine-scale control over the k -mer sizes used in the assembly. We instead provide IDBA-UD with the first and last k -mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k -mer of uneven (odd) length in order to avoid the creation of palindromic k -mers. Specifying the number of threads \u00b6 This is simple in either assembler: spades.py -t 20 ... idba_ud --num_threads 20 ... The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, SPAdes will assemble with 16 threads. IDBA-UD will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm. Setting a memory limit \u00b6 By far, the worst feature of SPAdes is the high memory requirement for performing an assembly. In the absence of monitoring, SPAdes will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing SPAdes : spades.py -m 400GB ... No such parameter exists in IDBA-UD , but it requires far less RAM than SPAdes , so you are less likely to need it. Preparing an assembly job for slurm \u00b6 NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to ( lander02 ) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm . The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs. Submitting a SPAdes job to NeSI using slurm \u00b6 To begin, we need to open a text file using the nano text editor. cd 3 .assembly/ nano assembly_spades.sl Into this file, either write or copy/paste the following commands: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_assembly #SBATCH --res SummerSchool #SBATCH --time 00:50:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 12 #SBATCH --error spades_assembly.err #SBATCH --output spades_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load SPAdes/3.15.3-gimkl-2020a spades.py --meta -k 33 ,55,77,99,121 -t 12 -1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz -o spades_assembly/ To save your file, use Ctrl + O to save the file, then Ctrl + X to exit nano . Going through those lines one by one; Slurm parameter Function #!/bin/bash -e Header for the file, letting NeSI know how to interpret the following commands. The -e flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) --account ... The name of the project account to run the run under. You are provided with this when you create a project on NeSI --job-name ... The name of the job, to display when using the squeue command **--res ...* This is a parameter that we need due to a system reservation for this workshop. For your own work, ignore this flag --time ... Maximum run time for the job before it is killed --mem ... Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated --ntasks ... Number of tasks to distribute the job across. For all tools we use today, this will remain as 1 --cpus-per-task ... number of processing cores to assign to the job. This should match with the number used by your assembler --error ... File to log the standard error stream of the program. This is typically used to prove error reports, or to just inform the user of job progress --output ... File to log the standard output stream of the program. This is typically used to inform the user of job progress and supply messages The module load command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that There is no chance of having the job fail immediately because it cannot find the relevant files When looking back through your slurm logs, you know where the data is meant to be When executing the SPAdes command, there are a few parameters to note here: Parameter Function --meta Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions -k ... k -mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly -1 .. Forward reads, matched to their reverse partners -2 ... Reverse reads, matched to their forward partners -o ... Output directory for all files Note that we also prefix the command ( spades.py ) with the srun command. This is a command specific to slurm and allows NeSI to track the resource usage of the SPAdes job. We don't explicitly set memory or thread counts for this job, simply for the sake of keeping the command uncluttered. The default memory limit of SPAdes (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and SPAdes will terminate the assembly. We have also left the number of threads at the default value of 16, which matches the number specified in the slurm header. It is a good idea to match your number of threads request in the slurm script with what you intend to use with SPAdes because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e. request 10 in slurm, set thread count to 30 in SPAdes ) will result in reduced performance, as your SPAdes job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post . Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command sbatch assembly_spades.sl You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise. Monitoring job progress \u00b6 You can view the status of your current jobs using the command squeue -u <login name> JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly PD Priority 2019 -11-29T16:37:43 0 :00 00 :30:00 1 16 We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At the stage the START_TIME is an estimation of when the resources are expected to become available. When they do, the output will change to JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly R None 2019 -11-29T16:40:00 1 :20 00 :28:40 1 16 Which allows us to track how far into our run we are, and see the remaining time for the job. The START_TIME column now reports the time the job actually began. Submitting an IDBA-UD job to NeSI using slurm \u00b6 To run an equivalent assembly with IDBA-UD , create a new slurm script as follows nano idbaud_assembly.sl Paste or type in the following: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name idbaud_assembly #SBATCH --res SummerSchool #SBATCH --time 00:35:00 #SBATCH --mem 4GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 8 #SBATCH --error idbaud_assembly.err #SBATCH --output idbaud_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load IDBA/1.1.3-gimkl-2017a idba_ud --num_threads 8 --mink 33 --maxk 99 --step 22 -r for_idba.fna -o idbaud_assembly/ And submit the script as a slurm job: sbatch idbaud_assembly.sl Remember to record your job identification number.","title":"Assembly"},{"location":"day1/ex3_assembly/#assembly","text":"","title":"Assembly"},{"location":"day1/ex3_assembly/#objectives","text":"Become familiar with the standard input files for SPAdes and IDBA-UD Understand the basic parameters that should be modified when using these assemblers Prepare an assembly job to run under slurm All work for this exercise will occur in the 3.assembly/ directory.","title":"Objectives"},{"location":"day1/ex3_assembly/#the-standard-input-files-for-spades-and-idba-ud","text":"Although they both make use of the same types of data, both SPAdes and IDBA-UD have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by SPAdes : module load SPAdes/3.13.1-gimkl-2018b spades.py -h # ... #Input data: #-1 <filename> file with forward paired-end reads #-2 <filename> file with reverse paired-end reads #-s <filename> file with unpaired reads #--mp<#>-1 <filename> file with forward reads for mate-pair library number <#> (<#> = 1,2,..,9) #--mp<#>-2 <filename> file with reverse reads for mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-1 <filename> file with forward reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--hqmp<#>-2 <filename> file with reverse reads for high-quality mate-pair library number <#> (<#> = 1,2,..,9) #--nxmate<#>-1 <filename> file with forward reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--nxmate<#>-2 <filename> file with reverse reads for Lucigen NxMate library number <#> (<#> = 1,2,..,9) #--sanger <filename> file with Sanger reads #--pacbio <filename> file with PacBio reads #--nanopore <filename> file with Nanopore reads #--tslr <filename> file with TSLR-contigs #--trusted-contigs <filename> file with trusted contigs #--untrusted-contigs <filename> file with untrusted contigs At a glance, you could provide any of the following data types to SPAdes and have it perform an assembly: Illumina paired-end sequencing data, either as standard library or Mate Pairs Sanger sequences PacBio reads Oxford Nanopore reads Pre-assembled scaffolds for guiding the assembly Awkwardly, while SPAdes accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the -meta flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing. cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz > for_spades_R1.fq.gz cat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz > for_spades_R2.fq.gz Note that these fastQ files are compressed, yet we can concatenate them together with the cat command regardless. This is a nice feature of .gz files that is handy to remember. By contrast, what does IDBA-UD accept? module load IDBA/1.1.3-gimkl-2017a idba_ud # ... # -o, --out arg (=out) output directory # -r, --read arg fasta read file (<=128) # ... # -l, --long_read arg fasta long read file (>128) 'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using IDBA-UD we will need to pool the paired-end data into a single, interleaved fastA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example # Paired-end file, forward >read1_1 ... >read2_1 ... # Paired-end file, forward >read1_2 ... >read2_2 ... # Interleaved file >read1_1 ... >read1_2 ... >read2_1 ... >read2_2 ... Fortunately, the IDBA set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first. module load pigz/2.4-GCCcore-7.4.0 for i in sample1 sample2 sample3 sample4 ; do pigz --keep --decompress ${ i } _R1.fastq.gz ${ i } _R2.fastq.gz fq2fa --merge ${ i } _R1.fastq ${ i } _R2.fastq ${ i } .fna done cat sample1.fna sample2.fna sample3.fna sample4.fna > for_idba.fna","title":"The standard input files for SPAdes and IDBA-UD"},{"location":"day1/ex3_assembly/#basic-assembly-parameters","text":"For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be. Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include: k -mer sizes to be assembled over, and step size if using a range Number of threads to use during assembly Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space","title":"Basic assembly parameters"},{"location":"day1/ex3_assembly/#setting-the-k-mer-size","text":"Depending on which assembler you are using, the commands for chosing the k -mer sizes for the assembly vary slightly, but they are recognisable between programs. In SPAdes , you can set the k -mer size using either spades.py -k 21 ,33,55,77,99,121 ... spades.py -k auto ... The first command lets us specify the k -mers ourselves, or we are letting SPAdes automatically pick the most appropriate size. For IDBA-UD , we can select the k -mer size using idba_ud --mink 21 --maxk 121 --step 22 Unlike SPAdes , we do not have fine-scale control over the k -mer sizes used in the assembly. We instead provide IDBA-UD with the first and last k -mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k -mer of uneven (odd) length in order to avoid the creation of palindromic k -mers.","title":"Setting the k-mer size"},{"location":"day1/ex3_assembly/#specifying-the-number-of-threads","text":"This is simple in either assembler: spades.py -t 20 ... idba_ud --num_threads 20 ... The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, SPAdes will assemble with 16 threads. IDBA-UD will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm.","title":"Specifying the number of threads"},{"location":"day1/ex3_assembly/#setting-a-memory-limit","text":"By far, the worst feature of SPAdes is the high memory requirement for performing an assembly. In the absence of monitoring, SPAdes will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing SPAdes : spades.py -m 400GB ... No such parameter exists in IDBA-UD , but it requires far less RAM than SPAdes , so you are less likely to need it.","title":"Setting a memory limit"},{"location":"day1/ex3_assembly/#preparing-an-assembly-job-for-slurm","text":"NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to ( lander02 ) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm . The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs.","title":"Preparing an assembly job for slurm"},{"location":"day1/ex3_assembly/#submitting-a-spades-job-to-nesi-using-slurm","text":"To begin, we need to open a text file using the nano text editor. cd 3 .assembly/ nano assembly_spades.sl Into this file, either write or copy/paste the following commands: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name spades_assembly #SBATCH --res SummerSchool #SBATCH --time 00:50:00 #SBATCH --mem 10GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 12 #SBATCH --error spades_assembly.err #SBATCH --output spades_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load SPAdes/3.15.3-gimkl-2020a spades.py --meta -k 33 ,55,77,99,121 -t 12 -1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz -o spades_assembly/ To save your file, use Ctrl + O to save the file, then Ctrl + X to exit nano . Going through those lines one by one; Slurm parameter Function #!/bin/bash -e Header for the file, letting NeSI know how to interpret the following commands. The -e flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) --account ... The name of the project account to run the run under. You are provided with this when you create a project on NeSI --job-name ... The name of the job, to display when using the squeue command **--res ...* This is a parameter that we need due to a system reservation for this workshop. For your own work, ignore this flag --time ... Maximum run time for the job before it is killed --mem ... Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated --ntasks ... Number of tasks to distribute the job across. For all tools we use today, this will remain as 1 --cpus-per-task ... number of processing cores to assign to the job. This should match with the number used by your assembler --error ... File to log the standard error stream of the program. This is typically used to prove error reports, or to just inform the user of job progress --output ... File to log the standard output stream of the program. This is typically used to inform the user of job progress and supply messages The module load command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that There is no chance of having the job fail immediately because it cannot find the relevant files When looking back through your slurm logs, you know where the data is meant to be When executing the SPAdes command, there are a few parameters to note here: Parameter Function --meta Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions -k ... k -mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly -1 .. Forward reads, matched to their reverse partners -2 ... Reverse reads, matched to their forward partners -o ... Output directory for all files Note that we also prefix the command ( spades.py ) with the srun command. This is a command specific to slurm and allows NeSI to track the resource usage of the SPAdes job. We don't explicitly set memory or thread counts for this job, simply for the sake of keeping the command uncluttered. The default memory limit of SPAdes (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and SPAdes will terminate the assembly. We have also left the number of threads at the default value of 16, which matches the number specified in the slurm header. It is a good idea to match your number of threads request in the slurm script with what you intend to use with SPAdes because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e. request 10 in slurm, set thread count to 30 in SPAdes ) will result in reduced performance, as your SPAdes job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post . Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command sbatch assembly_spades.sl You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise.","title":"Submitting a SPAdes job to NeSI using slurm"},{"location":"day1/ex3_assembly/#monitoring-job-progress","text":"You can view the status of your current jobs using the command squeue -u <login name> JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly PD Priority 2019 -11-29T16:37:43 0 :00 00 :30:00 1 16 We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At the stage the START_TIME is an estimation of when the resources are expected to become available. When they do, the output will change to JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 8744675 dwai012 ga02676 spades_assembly R None 2019 -11-29T16:40:00 1 :20 00 :28:40 1 16 Which allows us to track how far into our run we are, and see the remaining time for the job. The START_TIME column now reports the time the job actually began.","title":"Monitoring job progress"},{"location":"day1/ex3_assembly/#submitting-an-idba-ud-job-to-nesi-using-slurm","text":"To run an equivalent assembly with IDBA-UD , create a new slurm script as follows nano idbaud_assembly.sl Paste or type in the following: #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name idbaud_assembly #SBATCH --res SummerSchool #SBATCH --time 00:35:00 #SBATCH --mem 4GB #SBATCH --ntasks 1 #SBATCH --cpus-per-task 8 #SBATCH --error idbaud_assembly.err #SBATCH --output idbaud_assembly.out #SBATCH --export NONE export SLURM_EXPORT_ENV = ALL module purge module load IDBA/1.1.3-gimkl-2017a idba_ud --num_threads 8 --mink 33 --maxk 99 --step 22 -r for_idba.fna -o idbaud_assembly/ And submit the script as a slurm job: sbatch idbaud_assembly.sl Remember to record your job identification number.","title":"Submitting an IDBA-UD job to NeSI using slurm"},{"location":"day1/ex4_assembly/","text":"Assembly (part 2) \u00b6 Objectives \u00b6 Examine the effect of changing parameters for assembly All work for this exercise will occur in the 3.assembly/ directory. Examine the effect of changing assembly parameters \u00b6 For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the asembly parameters. You will have a chance to compare the effects of these changes tomorrow. SPAdes parameters \u00b6 Make a few copies of your SPAdes slurm script like so; cp assembly_spades.sl assembly_spades_var1.sl Change a few of the parameters for run time. Some potential options include Change the k -mer sizes to either a different specification, or change to the auto option Disable error correction Assemble without the --meta flag Employ a coverage cutoff for assembling IDBA-UD parameters \u00b6 Make variants of your IDBA-UD assembly script and change some parameters. Some potential options include Change the minimum/maximum k -mer sizes, or the k -mer step size Change the alignment similarity parameter Adjust the prefix length for the k -mer sub-table Submit two or three jobs per variation.","title":"Assembly (part 2)"},{"location":"day1/ex4_assembly/#assembly-part-2","text":"","title":"Assembly (part 2)"},{"location":"day1/ex4_assembly/#objectives","text":"Examine the effect of changing parameters for assembly All work for this exercise will occur in the 3.assembly/ directory.","title":"Objectives"},{"location":"day1/ex4_assembly/#examine-the-effect-of-changing-assembly-parameters","text":"For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the asembly parameters. You will have a chance to compare the effects of these changes tomorrow.","title":"Examine the effect of changing assembly parameters"},{"location":"day1/ex4_assembly/#spades-parameters","text":"Make a few copies of your SPAdes slurm script like so; cp assembly_spades.sl assembly_spades_var1.sl Change a few of the parameters for run time. Some potential options include Change the k -mer sizes to either a different specification, or change to the auto option Disable error correction Assemble without the --meta flag Employ a coverage cutoff for assembling","title":"SPAdes parameters"},{"location":"day1/ex4_assembly/#idba-ud-parameters","text":"Make variants of your IDBA-UD assembly script and change some parameters. Some potential options include Change the minimum/maximum k -mer sizes, or the k -mer step size Change the alignment similarity parameter Adjust the prefix length for the k -mer sub-table Submit two or three jobs per variation.","title":"IDBA-UD parameters"},{"location":"day1/ex5_evaluating_assemblies/","text":"Evaluating the assemblies \u00b6 Objectives \u00b6 Evaluate the resource consumption of various assemblies Evaluate the assemblies Future considerations Evaluate the resource consumption of various assemblies \u00b6 Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the squeue command. squeue -u <user name> # JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS If there are no jobs listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the sacct command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying. sacct -S 2020 -11-16 # JobID JobName Elapsed TotalCPU Alloc MaxRSS State #-------------- --------------- ----------- ------------ ----- -------- ---------- #8744675 spades_assembly 00:15:16 01:41:43 10 COMPLETED #8744675.batch batch 00:15:16 00:00.547 10 3908K COMPLETED #8744675.extern extern 00:15:16 00:00:00 10 0 COMPLETED #8744675.0 spades.py 00:15:15 01:41:42 10 6260072K COMPLETED #8744677 idbaud_assembly 00:11:06 01:27:42 10 COMPLETED #8744677.batch batch 00:11:06 00:00.477 10 3760K COMPLETED #8744677.extern extern 00:11:06 00:00:00 10 0 COMPLETED #8744677.0 idba_ud 00:11:05 01:27:42 10 2541868K COMPLETED Each job has been broken up into several lines, but the main ones to keep an eye on are the base JobID values, and the values suffixed with .0 . The first of these references the complete job. The later (and any subsequent suffixes like .1 , .2 ) are the individual steps in the script that were called with the srun command. We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the seff command seff 8744675 #Job ID: 8744675 #Cluster: mahuika #User/Group: dwai012/dwai012 #State: COMPLETED (exit code 0) #Nodes: 1 #Cores per node: 10 #CPU Utilized: 01:41:44 #CPU Efficiency: 66.64% of 02:32:40 core-walltime #Job Wall-clock time: 00:15:16 #Memory Utilized: 5.97 GB #Memory Efficiency: 29.85% of 20.00 GB Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was not particularly efficient. (Note that for this particular job, 1 hr and 20 GB RAM were requested.) In hindsight I could have request a lot less time and RAM and still had the job run to completion. CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like bowtie and BBMap can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in SPAdes go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down. Evaluate the assemblies \u00b6 Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true. A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use seqmagick for performing the length filtering, and then just count sequence numbers using grep . These steps will take place in the 4.evaluation/ folder, which contains copies of our SPAdes and IDBA-UD assemblies. module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/4.evaluation/ seqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\ spades_assembly/spades_assembly.m1000.fna grep -c '>' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna # spades_assembly/spades_assembly.fna:1343 # spades_assembly/spades_assembly.m1000.fna:945 seqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\ idbaud_assembly/idbaud_assembly.m1000.fna grep -c '>' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna # idbaud_assembly/idbaud_assembly.fna:5057 # idbaud_assembly/idbaud_assembly.m1000.fna:1996 If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands. seqmagick convert --min-length 1000 ../3.assembly/spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna seqmagick convert --min-length 1000 ../3.assembly/idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna Note: The tool seqtk is also available on NeSI and performs many of the same functions as seqmagick . My choice of seqmagick is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either seqtk or seqmagick as both perform a lot of common *fastA and fastQ file manipulations.* As we can see here, the SPAdes assembly has completed with fewer contigs assembled than the IDBA-UD , both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has SPAdes managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 of the assembly with BBMap . module load BBMap/38.73-gimkl-2018b stats.sh in = spades_assembly/spades_assembly.m1000.fna This gives quite a verbose output: A C G T N IUPAC Other GC GC_stdev 0 .2537 0 .2465 0 .2462 0 .2536 0 .0018 0 .0000 0 .0000 0 .4928 0 .0956 Main genome scaffold total: 945 Main genome contig total: 2713 Main genome scaffold sequence total: 34 .296 MB Main genome contig sequence total: 34 .233 MB 0 .184% gap Main genome scaffold N/L50: 54 /160.826 KB Main genome contig N/L50: 106 /72.909 KB Main genome scaffold N/L90: 306 /15.236 KB Main genome contig N/L90: 817 /4.63 KB Max scaffold length: 1 .044 MB Max contig length: 1 .044 MB Number of scaffolds > 50 KB: 152 % main genome in scaffolds > 50 KB: 76 .69% Minimum Number Number Total Total Scaffold Scaffold of of Scaffold Contig Contig Length Scaffolds Contigs Length Length Coverage -------- -------------- -------------- -------------- -------------- -------- All 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 1 KB 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 2 .5 KB 750 2 ,453 33 ,961,961 33 ,903,372 99 .83% 5 KB 586 2 ,136 33 ,371,126 33 ,318,361 99 .84% 10 KB 396 1 ,590 32 ,000,574 31 ,962,900 99 .88% 25 KB 237 929 29 ,506,913 29 ,487,807 99 .94% 50 KB 152 587 26 ,301,390 26 ,289,386 99 .95% 100 KB 92 408 22 ,108,408 22 ,099,623 99 .96% 250 KB 30 138 12 ,250,722 12 ,247,681 99 .98% 500 KB 6 28 4 ,735,549 4 ,735,329 100 .00% 1 MB 1 1 1 ,043,932 1 ,043,932 100 .00% But what we can highlight here is that the statistics for the SPAdes assembly, with short contigs removed, yielded an N50 of 106 kbp at the contig level. We will now compute those same statistics from the other assembly options stats.sh in = spades_assembly/spades_assembly.fna stats.sh in = idbaud_assembly/idbaud_assembly.m1000.fna stats.sh in = idbaud_assembly/idbaud_assembly.fna Assembly N50 (contig) L50 (contig) SPAdes (filtered) 106 kbp 73 SPAdes (unfiltered) 107 kbp 72 IDBA-UD (filtered) 82 kbp 104 IDBA-UD (unfiltered) 88 kbp 97 Optional: Evaluating assemblies using MetaQUAST \u00b6 For more genome-informed evaluation of the assembly, we can use the MetaQUAST tool to view our assembled metagenome. This is something of an optional step because, like QUAST , MetaQUAST aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a MetaQUAST evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection: Kraken2 (DNA based, k -mer classification) CLARK (DNA based. k -mer classification) Kaiju (Protein based, BLAST classification) Centrifuge (DNA based, sequence alignment classification) MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification) MetaPhlAn2 (DNA based, clade-specific marker gene classification) A good summary and comparison of these tools (and more) was recently published by Ye et al. . However, since we do know the composition of the original communities used to build this mock metagenome, MetaQUAST will work very well for us today. In your 4.evaluation/ directory you will find a file called ref_genomes.txt . This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for MetaQUAST . module load QUAST/5.0.2-gimkl-2018b metaquast.py spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna \\ idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna \\ --references-list ref_genomes.txt --max-ref-number 21 -t 10 By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the --max-ref-number flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 names in the file ref_genomes.txt (10 prokaryote species and 11 viruses), MetaQUAST will download one of each. If we increase the number we will start to get multiple references per name provided which is usually desirable. We will now look at a few interesting assembly comparisons. When working from a standard terminal logged into NeSI, you can copy the entire folder 4.evaluation/quast_results/latest/ to your local environment using the scp -r mahuika:/nesi/path/to/quast_results/latest local/path/to/copy/to command to then open the report. Note that the browser requires JavaScript enabled to render the report: If you are working from the NeSI Jupyter hub environment today, the html viewer within the NeSI Jupyter hub does not currently support this (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the 4.evaluation/quast_results/latest/ folder, right-click report.html/ and select download. The downloaded file will then open within a new tab in the browser. ( NOTE: rendering the full report requires the other folders from within latest/ to also be downloaded and available in the same directory as report.html . Unfortunately, the Jupyter hub environment does not appear to currently support downloading entire folders using this method. ) An example of the MetaQUAST output files are also available for download here . Brief summary of assemblies \u00b6 Comparison of NGA50 between assemblies \u00b6 Comparison of aligned contigs \u00b6 Inspection of unaligned contigs \u00b6","title":"Evaluating the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#evaluating-the-assemblies","text":"","title":"Evaluating the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#objectives","text":"Evaluate the resource consumption of various assemblies Evaluate the assemblies Future considerations","title":"Objectives"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-resource-consumption-of-various-assemblies","text":"Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the squeue command. squeue -u <user name> # JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS If there are no jobs listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the sacct command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying. sacct -S 2020 -11-16 # JobID JobName Elapsed TotalCPU Alloc MaxRSS State #-------------- --------------- ----------- ------------ ----- -------- ---------- #8744675 spades_assembly 00:15:16 01:41:43 10 COMPLETED #8744675.batch batch 00:15:16 00:00.547 10 3908K COMPLETED #8744675.extern extern 00:15:16 00:00:00 10 0 COMPLETED #8744675.0 spades.py 00:15:15 01:41:42 10 6260072K COMPLETED #8744677 idbaud_assembly 00:11:06 01:27:42 10 COMPLETED #8744677.batch batch 00:11:06 00:00.477 10 3760K COMPLETED #8744677.extern extern 00:11:06 00:00:00 10 0 COMPLETED #8744677.0 idba_ud 00:11:05 01:27:42 10 2541868K COMPLETED Each job has been broken up into several lines, but the main ones to keep an eye on are the base JobID values, and the values suffixed with .0 . The first of these references the complete job. The later (and any subsequent suffixes like .1 , .2 ) are the individual steps in the script that were called with the srun command. We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the seff command seff 8744675 #Job ID: 8744675 #Cluster: mahuika #User/Group: dwai012/dwai012 #State: COMPLETED (exit code 0) #Nodes: 1 #Cores per node: 10 #CPU Utilized: 01:41:44 #CPU Efficiency: 66.64% of 02:32:40 core-walltime #Job Wall-clock time: 00:15:16 #Memory Utilized: 5.97 GB #Memory Efficiency: 29.85% of 20.00 GB Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was not particularly efficient. (Note that for this particular job, 1 hr and 20 GB RAM were requested.) In hindsight I could have request a lot less time and RAM and still had the job run to completion. CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like bowtie and BBMap can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in SPAdes go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down.","title":"Evaluate the resource consumption of various assemblies"},{"location":"day1/ex5_evaluating_assemblies/#evaluate-the-assemblies","text":"Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true. A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use seqmagick for performing the length filtering, and then just count sequence numbers using grep . These steps will take place in the 4.evaluation/ folder, which contains copies of our SPAdes and IDBA-UD assemblies. module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3 cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/4.evaluation/ seqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\ spades_assembly/spades_assembly.m1000.fna grep -c '>' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna # spades_assembly/spades_assembly.fna:1343 # spades_assembly/spades_assembly.m1000.fna:945 seqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\ idbaud_assembly/idbaud_assembly.m1000.fna grep -c '>' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna # idbaud_assembly/idbaud_assembly.fna:5057 # idbaud_assembly/idbaud_assembly.m1000.fna:1996 If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands. seqmagick convert --min-length 1000 ../3.assembly/spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna seqmagick convert --min-length 1000 ../3.assembly/idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna Note: The tool seqtk is also available on NeSI and performs many of the same functions as seqmagick . My choice of seqmagick is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either seqtk or seqmagick as both perform a lot of common *fastA and fastQ file manipulations.* As we can see here, the SPAdes assembly has completed with fewer contigs assembled than the IDBA-UD , both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has SPAdes managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 of the assembly with BBMap . module load BBMap/38.73-gimkl-2018b stats.sh in = spades_assembly/spades_assembly.m1000.fna This gives quite a verbose output: A C G T N IUPAC Other GC GC_stdev 0 .2537 0 .2465 0 .2462 0 .2536 0 .0018 0 .0000 0 .0000 0 .4928 0 .0956 Main genome scaffold total: 945 Main genome contig total: 2713 Main genome scaffold sequence total: 34 .296 MB Main genome contig sequence total: 34 .233 MB 0 .184% gap Main genome scaffold N/L50: 54 /160.826 KB Main genome contig N/L50: 106 /72.909 KB Main genome scaffold N/L90: 306 /15.236 KB Main genome contig N/L90: 817 /4.63 KB Max scaffold length: 1 .044 MB Max contig length: 1 .044 MB Number of scaffolds > 50 KB: 152 % main genome in scaffolds > 50 KB: 76 .69% Minimum Number Number Total Total Scaffold Scaffold of of Scaffold Contig Contig Length Scaffolds Contigs Length Length Coverage -------- -------------- -------------- -------------- -------------- -------- All 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 1 KB 945 2 ,713 34 ,296,303 34 ,233,142 99 .82% 2 .5 KB 750 2 ,453 33 ,961,961 33 ,903,372 99 .83% 5 KB 586 2 ,136 33 ,371,126 33 ,318,361 99 .84% 10 KB 396 1 ,590 32 ,000,574 31 ,962,900 99 .88% 25 KB 237 929 29 ,506,913 29 ,487,807 99 .94% 50 KB 152 587 26 ,301,390 26 ,289,386 99 .95% 100 KB 92 408 22 ,108,408 22 ,099,623 99 .96% 250 KB 30 138 12 ,250,722 12 ,247,681 99 .98% 500 KB 6 28 4 ,735,549 4 ,735,329 100 .00% 1 MB 1 1 1 ,043,932 1 ,043,932 100 .00% But what we can highlight here is that the statistics for the SPAdes assembly, with short contigs removed, yielded an N50 of 106 kbp at the contig level. We will now compute those same statistics from the other assembly options stats.sh in = spades_assembly/spades_assembly.fna stats.sh in = idbaud_assembly/idbaud_assembly.m1000.fna stats.sh in = idbaud_assembly/idbaud_assembly.fna Assembly N50 (contig) L50 (contig) SPAdes (filtered) 106 kbp 73 SPAdes (unfiltered) 107 kbp 72 IDBA-UD (filtered) 82 kbp 104 IDBA-UD (unfiltered) 88 kbp 97","title":"Evaluate the assemblies"},{"location":"day1/ex5_evaluating_assemblies/#optional-evaluating-assemblies-using-metaquast","text":"For more genome-informed evaluation of the assembly, we can use the MetaQUAST tool to view our assembled metagenome. This is something of an optional step because, like QUAST , MetaQUAST aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a MetaQUAST evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection: Kraken2 (DNA based, k -mer classification) CLARK (DNA based. k -mer classification) Kaiju (Protein based, BLAST classification) Centrifuge (DNA based, sequence alignment classification) MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification) MetaPhlAn2 (DNA based, clade-specific marker gene classification) A good summary and comparison of these tools (and more) was recently published by Ye et al. . However, since we do know the composition of the original communities used to build this mock metagenome, MetaQUAST will work very well for us today. In your 4.evaluation/ directory you will find a file called ref_genomes.txt . This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for MetaQUAST . module load QUAST/5.0.2-gimkl-2018b metaquast.py spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna \\ idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna \\ --references-list ref_genomes.txt --max-ref-number 21 -t 10 By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the --max-ref-number flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 names in the file ref_genomes.txt (10 prokaryote species and 11 viruses), MetaQUAST will download one of each. If we increase the number we will start to get multiple references per name provided which is usually desirable. We will now look at a few interesting assembly comparisons. When working from a standard terminal logged into NeSI, you can copy the entire folder 4.evaluation/quast_results/latest/ to your local environment using the scp -r mahuika:/nesi/path/to/quast_results/latest local/path/to/copy/to command to then open the report. Note that the browser requires JavaScript enabled to render the report: If you are working from the NeSI Jupyter hub environment today, the html viewer within the NeSI Jupyter hub does not currently support this (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the 4.evaluation/quast_results/latest/ folder, right-click report.html/ and select download. The downloaded file will then open within a new tab in the browser. ( NOTE: rendering the full report requires the other folders from within latest/ to also be downloaded and available in the same directory as report.html . Unfortunately, the Jupyter hub environment does not appear to currently support downloading entire folders using this method. ) An example of the MetaQUAST output files are also available for download here .","title":"Optional: Evaluating assemblies using MetaQUAST"},{"location":"day1/ex5_evaluating_assemblies/#brief-summary-of-assemblies","text":"","title":"Brief summary of assemblies"},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-nga50-between-assemblies","text":"","title":"Comparison of NGA50 between assemblies"},{"location":"day1/ex5_evaluating_assemblies/#comparison-of-aligned-contigs","text":"","title":"Comparison of aligned contigs"},{"location":"day1/ex5_evaluating_assemblies/#inspection-of-unaligned-contigs","text":"","title":"Inspection of unaligned contigs"}]}