{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#metagenomics-summer-school","title":"Metagenomics Summer School","text":"Day Lesson topic Day 1: Assembly 1. (Pre-Summer School) Introduction I: Shell2. (Pre-Summer School) Introduction II: HPC and job scheduler3. Filter raw reads by quality4. Assembly I: Assembling contigs5. Assembly II: Variable parameters6. Assembly evaluation Day 2: Binning 1. Introduction to binning2. Binning with multiple tools3. Bin dereplication4. Manual bin refinement3. Identifying viral contigs in metagenomic data Day 3: Annotation 1. Assigning taxonomy to refined prokaryotic bins2. Phylogenomics3. Virus taxonomy4. Gene prediction5. Gene annotation I: BLAST-like and HMM6. Gene annotation II: DRAM and coverage calculation Day 4: Visualisation 1. Gene annotation III: DRAM distillation2. Introduction to data presentation3. Coverage heatmaps4. Ordinations5. KEGG pathway maps6. Gene synteny7. CAZy heatmaps"},{"location":"#timetable-2024","title":"Timetable 2024","text":"Day 1: Tuesday, 3<sup>rd</sup> SepDay 2: Wednesday, 4<sup>th</sup> SepDay 3: Thursday, 5<sup>th</sup> SepDay 4: Friday, 6<sup>th</sup> Sep Time Event Session leader 09:00 \u2013 09:25 IntroductionsWelcomeOverview Login to NeSI via Jupyter Hub and test script Jian Sheng Boey 09:25 \u2013 09:55 TALK Metagenomics decision tree DISCUSSION What are your research questions? Kim Handley 09:55 \u2013 10:30 Read QC TALK Quality filtering raw reads TASK Visualisation with FastQC  and MultiQC Annie West 10:30 \u2013 10:50 Morning tea 10:50 \u2013 11:25 TASK Read trimming &amp; adapter removal TASK Diagnosing poor libraries TALK Common issues &amp; best practices TASK Filter out host DNA Annie West 11:25 \u2013 12:30 Assembly TALK Choice of assemblers, parameter considerations, and when to stop! TASK Prepare data for assembly TASK Exploring assembler options TASK Submit assembly jobs to NeSI via Slurm TASK Submit variant assembly jobs Kim Handley 12:30 \u2013 13:30 Lunch 13:30 \u2013 14:15 TALK Other considerations: assembly strategies TALK Assembly evaluation TASK Evaluate your assemblies TASK Short contig removal Mike Hoggard 14:15 \u2013 15:00 TALK Other considerations: rRNA reconstruction TASK Ribosomal RNA reconstruction using PhyloFlash TALK Other considerations: read classification Annie WestJian Sheng Boey 15:00 \u2013 15:20 Afternoon tea 15:20 \u2013 15:50 TASK Sequence classification using Kraken2 and Bracken Annie WestJian Sheng Boey 15:50 \u2013 16:00 TALK Project introduction and description Kim Handley Time Event Session leader 09:00 \u2013 09:10 IntroductionsRecap of day 1Overview of the day Annie West 09:10 \u2013 10:30 TALK Overview of binning history, key parameters TASK Read mapping TASK Multi-binning strategy with MetaBat and MaxBin Kim Handley 10:30 \u2013 10:50 Morning tea 10:50 \u2013 11:45 TALK Binning strategies TASK Bin dereplication using DAS_Tool Kim Handley 11:45 \u2013 12:10 PRESENTATION Alternative approaches to binning Amali Thrimawithana 12:10 \u2013 13:00 Lunch 13:00 \u2013 15:00 TASK Bin evaluation using CheckM and CheckM2 TALK Bin dereplication across assemblies, working with minimal or non-prokaryotic genomes, and bin refinement strategies TASK Visual bin refinement using VizBin Kim HandleyMike Hoggard 15:00 \u2013 15:20 Afternoon tea 15:20 \u2013 15:35 TASK Checking VizBin results Annie West 15:35 \u2013 16:05 TALK Viruses in metagenomic data TASK Identify viral contigs using VirSorter2 and CheckV Mike Hoggard 16:05 \u2013 16:30 DISCUSSION How will you approach your research question? Time Event Session leader 09:00 \u2013 09:10 IntroductionsRecap of day 2Recap of binning outputsOverview of the day Jian Sheng Boey 09:10 \u2013 10:20 TALK Taxonomic classification: Bin and species determination TASK Taxonomic classification using GTDB-Tk Annie West 10:20 \u2013 10:40 Morning tea 10:40 \u2013 11:15 TALK Primer to phylogenetic analyses TASK Build a phylogenetic tree using FastTree and visualise tree using iTOL Jian Sheng Boey 11:15 \u2013 12:05 TASK Exploring results from VirSorter2 and CheckV TALK Predicting viral taxonomy using vConTACT2 TASK (Optional) Visualise vConTACT2 viral gene sharing network in Cytoscape Mike Hoggard 12:05 \u2013 12:50 Lunch 12:50 \u2013 13:20 TALK Tools for predicting genes: Prodigal, RNAmer, Aragorn, etc. TASK Predict ORFs and protein sequences using Prodigal Jian Sheng Boey 13:20 \u2013 14:20 TALK Methods for gene annotation TASK Annotate predicted genes using DIAMOND and HMMER3 Jian Sheng Boey 14:20 \u2013 14:50 TALK Using online resources (KEGG, BioCyc, MetaCyc, InterPro, PSORT) TASK View KEGG annotations online TASK View phylogenetic trait distribution using ANNOTREE Jian Sheng Boey 14:50 \u2013 15:05 Afternoon tea 15:05 \u2013 15:30 PRESENTATION Metagenomic analyses sans binning Jess Wallbank 15:30 \u2013 15:50 TASK MAG annotation with DRAM TASK Coverage calculation Annie West 15:50 \u2013 16:00 TASK Group formation and project topic selection Kim Handley Time Event Session leader 09:00 \u2013 09:10 IntroductionsRecap of day 3Overview of the day Jian Sheng Boey 09:10 \u2013 09:55 TALK Overview of DRAM results TASK Explore DRAM results Annie West 09:55 \u2013 10:25 TALK Visualising environmental distribution TASK Coverage heatmap and nMDS ordination Mike Hoggard 10:25 \u2013 10:45 Morning tea 10:45 \u2013 10:55 TASK Workshop survey 10:55 \u2013 12:05 TALK Visualising genomic and metabolic features TASK KEGG metabolic maps TASK Gene synteny Jian Sheng BoeyAnnie West 12:05 \u2013 12:55 Lunch 12:45 \u2013 14:40 Group work TASK Analyse data and prepare presentations Kim HandleyMike HoggardAnnie WestJian Sheng Boey 14:40 \u2013 14:55 PRESENTATION Group presentations Kim HandleyMike HoggardAnnie WestJian Sheng Boey 14:55 \u2013 15:10 Afternoon tea 15:10 \u2013 15:55 PRESENTATION Group presentations Kim HandleyMike HoggardAnnie WestJian Sheng Boey 15:55 \u2013 16:00 Wrap up and final discussions Jian Sheng Boey Appendices Appendix ID Appendix 1 Dereplicating data from multiple assemblies Appendix 2 Generating input files for \"VizBin\" from \"DAS_Tool\" curated bins Appendix 3 Normalise per-sample coverage values by average library size Appendix 4 Viral taxonomy prediction via vContact2 <p>Post-workshop survey</p> <p>Thank you for attending Metagenomics Summer School 2024! We would like your feedback on how we have done and what we can improve on. You can provide feedback here.</p> <p></p> <p>License</p> <p>Genomics Aotearoa / New Zealand eScience Infrastructure / University of Auckland Metagenomics Summer School material is licensed under the GNU General Public License v3.0, 29 June 2007. (Follow this link for more information)</p> <p></p> <p>Slides for workshop</p> <p>You can find a copy of the slides presented during the workshop, with published figures removed, in the slides/ folder.</p> <p></p> <p>Snapshots of results to download</p> <p>If you are having trouble downloading files using <code>scp</code>, we are providing exemplar output files, which you can download through your browser, here, or via the following links: </p> <ul> <li>FastQC results</li> <li>Quast results and required references</li> <li>Input files for VizBin</li> <li>Gene annotation tables</li> <li>DRAM output files</li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/","title":"Introduction I: Shell","text":"<p>This lesson will be covered/referred during pre-Summer School sessions. We will start Day 1 with Introduction to HPC &amp; HPC job scheduler</p> <p>Objectives</p> <ul> <li>Navigating your file system</li> <li>Copying, Moving, Renaming and Removing files</li> <li>Examining file contents</li> <li>Redirection, manipulation and extraction</li> <li>Text and file manipulation</li> <li>Loops</li> <li>Shell scripting</li> <li>Introduction to HPC and the SLURM scheduler</li> </ul> <p>Quick recap</p> <ul> <li>Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material</li> <li>Then open a Jupyter Terminal Session </li> <li>We do recommend referring to NeSI File system,..Symlinks Supplementary material first </li> <li>This lesson is a quick recap on basic/essential linux commands and will be covered during the pre-summer school sessions. </li> <li>If you would like to follow through and learn a bit more on this topic, refer to  Intermediate Shell for Bioinformatics material (or welcome to attend that workshop which will be offered two or three times per year)</li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#navigating-your-file-system","title":"Navigating your file system","text":"<ul> <li> <p>Check the current working directory.(terminal session  will be land on <code>/home</code> directory) <pre><code>pwd\n# /home/UserName/\n</code></pre></p> </li> <li> <p>Switch to individual working directory on nobackup ( below ) <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/$USER\n</code></pre></p> </li> <li> <p>OR you can navigate to above by using the symlink created as per instructions on  Supplementary material with just <code>cd ~/mgss</code></p> </li> <li> <p>Change the directory to <code>MGSS_Intro</code> <pre><code>cd MGSS_Intro/\n</code></pre></p> </li> <li> <p>Run the <code>ls</code> command to list the contents of the current directory. Check whether there are two .fastq files.</p> </li> <li> <p>The <code>mkdir</code> command (make directory) is used to make a directory. Enter <code>mkdir</code> followed by a space, then the directory name you want to create <pre><code>mkdir backup/\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#copying-moving-renaming-and-removing-files","title":"Copying, Moving, Renaming and Removing files","text":"<ul> <li> <p>Make a second copy of <code>SRR097977.fastq</code> and rename it as <code>Test_1_backup.fastq</code>. <pre><code>cp SRR097977.fastq Test_1_backup.fastq\n</code></pre></p> </li> <li> <p>Then move that file to <code>backup/</code> directory. <pre><code>mv Test_1_backup.fastq backup\n</code></pre></p> </li> <li> <p>Navigate to <code>backup/</code> directory and use <code>mv</code> command to rename and move <code>Test_1_backup.fastq</code> as <code>Test_1_copy.fastq</code> to the directory immediately above. <pre><code>cd backup/\n</code></pre> <pre><code>mv Test_1_backup.fastq ../Test_1_copy.fastq\n</code></pre></p> </li> <li> <p>Return to the directory immediately above, check whether the <code>Test_1_copy.fastq</code> was moved and renamed as instructed and remove it by using the <code>rm</code> command. <pre><code>cd ..\n\nrm Test_1_copy.fastq\n</code></pre></p> </li> <li> <p>See whether you can remove the <code>backup/</code> directory by using the <code>rm</code> command as well.  <pre><code>rm backup/\n# rm : can not remove 'backup/': Is a directory\n</code></pre></p> </li> <li> <p>By default, <code>rm</code> will not delete directories. This can be done by using <code>-r</code> (recursive) option. <pre><code>rm -r backup\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#examining-file-contents","title":"Examining file contents","text":"<ul> <li> <p>There are a number of ways to examine the content of a file. <code>cat</code> and <code>less</code> are two commonly used programs for a quick look. Check the content of <code>SRR097977.fastq</code> by using these commands. Take a note of the differences.  <pre><code>cat SRR097977.fastq\n# less SRR097977.fastq\n</code></pre></p> </li> <li> <p>A few useful shortcuts for navigating in <code>less</code></p> </li> </ul> <p></p> <ul> <li> <p>There are ways to take a look at parts of a file. For example, the <code>head</code> and <code>tail</code> commands will scan the beginning and end of a file, respectively.  <pre><code>head SRR097977.fastq\n\ntail SRR097977.fastq\n</code></pre></p> </li> <li> <p>Adding <code>-n</code> option to either of these commands will print the first or last n lines of a file. <pre><code>head -n 1 SRR097977.fastq\n# @SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#redirection-and-extraction","title":"Redirection and extraction","text":"<ul> <li> <p>Although using <code>cat</code> and <code>less</code> commands will allow us to view the content of the whole file, most of the time we are in search of particular characters (strings) of interest, rather than the full content of the file. One of the most commonly used command-line utilities to search for strings is <code>grep</code>. Let's use this command to search for the string <code>NNNNNNNNNN</code> in <code>SRR098026.fastq</code> file. <pre><code>grep NNNNNNNNNN SRR098026.fastq\n</code></pre></p> </li> <li> <p>Retrieve and discuss the output you get when <code>grep</code> was executed with the <code>-B1</code> and <code>-A1</code> flags. <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq\n</code></pre></p> </li> <li> <p>In both occasions, outputs were printed to the terminal where they can not be reproduced without the execution of the same command. In order for \"string\" of interest to be used for other operations, this has to be \"redirected\" (captured and written into a file). The command for redirecting output to a file is <code>&gt;</code>. Redirecting the string of bad reads that was searched using the <code>grep</code> command to a file named <code>bad_reads.txt</code> can be done with <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt\n</code></pre></p> </li> <li> <p>Use the <code>wc</code> command to count the number of words, lines and characters in the <code>bad_reads.txt</code> file. <pre><code>wc bad_reads.txt\n</code></pre></p> </li> <li> <p>Add <code>-l</code> flag to <code>wc</code> command and compare the number with the above output <pre><code>wc -l bad_reads.txt\n</code></pre></p> </li> <li> <p>In an instance where the same operation has to be applied for multiple input files and the outputs are to be redirected to the same output file, it is important to make sure that the new output is not over-writing the previous output. This can be avoided with the use of <code>&gt;&gt;</code> (append redirect) command which will append the new output to the end of the file, rather than overwriting it.  <pre><code>grep -B1 -A2 NNNNNNNNNN SRR097977.fastq &gt;&gt; bad_reads.txt\n</code></pre></p> </li> <li> <p>Executing the same operation on multiple files with the same file extension (or different) can be done with wildcards, which are symbols or special characters that represent other characters. For an example. Using <code>*</code> wildcard, we can run the previous <code>grep</code> command on both files at the same time. <pre><code>grep -B1 -A2 NNNNNNNNNN *.fastq &gt;&gt; bad_reads.txt\nwc -l bad_reads.txt\n</code></pre></p> </li> <li> <p>The objective of the redirection example above is to search for a string in a set of files, write the output to a file, and then count the number of lines in that file. Generating output files for short routine tasks like this will end up generating an excessive number of files with little value. The <code>|</code> (pipe) command is a commonly used method to apply an operation for an output without creating intermediate files. It takes the output generated by one command and uses it as the input to another command.  <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l\n</code></pre></p> </li> </ul>"},{"location":"day1/ex1_bash_and_scheduler/#text-and-file-manipulation","title":"Text and file manipulation","text":"<p>There are a number of handy command line tools for working with text files and performing operations like selecting columns from a table or modifying text in a file stream. A few examples of these are below.</p>"},{"location":"day1/ex1_bash_and_scheduler/#cut","title":"Cut","text":"<p>The <code>cut</code> command prints selected parts of lines from each file to standard output. It is basically a tool for selecting columns of text, delimited by a particular character. The tab character is the default delimiter that <code>cut</code> uses to determine what constitutes a field. If the columns in your file are delimited by another character, you can specify this using the <code>-d</code> parameter.</p> <p>See what results you get from the file <code>names.txt</code>.</p> <pre><code>cat names.txt\n\ncut -d \" \" -f 1 names.txt\ncut -d \" \" -f 1-3 names.txt\ncut -d \" \" -f 1,3 names.txt\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#basename","title":"basename","text":"<p><code>basename</code> is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use <code>basename</code> to remove the .fastq extension from the files that we've been working with.</p> <pre><code>basename SRR097977.fastq .fastq\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#sed","title":"sed","text":"<p><code>sed</code> is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file, or input from a pipeline) like, searching, find and replace, insertion or deletion. The most common use of the <code>sed</code> command in UNIX is for substitution or for find and replace. By using <code>sed</code> you can edit files even without opening them, which is extremely important when working with large files.</p> <ul> <li>Some find and replace examples</li> </ul> <p>Find and replace all <code>chr</code> to <code>chromosome</code> in the example.bed file and append the the edit to a new file named example_chromosome.bed</p> <p><pre><code>sed 's/chr/chromosome/g' example.bed &gt; example_chromosome.bed\n</code></pre> Find and replace <code>chr</code> to <code>chromosome</code>, only if you also find 40 in the line</p> <p><pre><code>sed '/40/s/chr/chromosome/g' example.bed &gt; example_40.bed\n</code></pre> Find and replace directly on the input, but save an old version too</p> <pre><code>sed -i.old 's/chr/chromosome/g' example.bed\n</code></pre> <p><code>-i</code> to edit files in-place instead of printing to standard output</p> <ul> <li>Print specific lines of the file</li> </ul> <p>To print a specific line you can use the address function. Note that by default, <code>sed</code> will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option <code>-n</code> </p> <p><code>-n</code>, <code>--quiet</code>, <code>--silent</code> = suppress automatic printing of pattern space</p> <p>print 5<sup>th</sup> line of example.bed</p> <pre><code>sed -n '5p' example.bed\n</code></pre> <p>We can provide any number of additional lines to print using <code>-e</code> option. Let's print line 2 and 5, </p> <pre><code>sed -n -e '2p' -e '5p' example.bed\n</code></pre> <p>It also accepts range, using <code>,</code>. Let's print line 2-6,</p> <pre><code>sed -n '2,6p' example.bed\n</code></pre>"},{"location":"day1/ex1_bash_and_scheduler/#loops","title":"Loops","text":"<p>Loops are a common concept in most programming languages which allow us to execute commands repeatedly with ease. There are three basic loop constructs in <code>bash</code> scripting,</p> Types of Loops <code>for</code> <code>while</code><code>until</code> <p>iterates over a list of items and performs the given set of commands <pre><code>for item in [LIST]\ndo\n    [COMMANDS]\ndone\n</code></pre> For most of our uses, a <code>for loop</code> is sufficient for our needs, so that is what we will be focusing on for this exercise.</p> <p>Shell identifies the <code>for</code> command and repeats a block of commands once for each item in a list. The for loop will take each item in the list (in order, one after the other), assign that item as the value of a variable, execute the commands between the <code>do</code> and <code>done</code> keywords, then proceed to the next item in the list and repeat over. The value of a variable is accessed by placing the <code>$</code> character in front of the variable name. This will tell the interpreter to access the data stored within the variable, rather than the variable name. For example</p> <pre><code>i=\"MG Summer School\"\n\necho i\n# i\necho $i\n# MG Summer School\necho ${i}\n# MG Summer School\n</code></pre> <p>This prevents the shell interpreter from treating <code>i</code> as a string or a command. The process is known as expanding the variable. We will now write a for loop to print the first two lines of our fastQ files:</p> <p><pre><code>for filename in *.fastq\ndo\n    head -n 2 ${filename}\ndone\n</code></pre> Another useful command to be used with <code>for</code> loops is <code>basename</code> which strips directory information and suffixes from file names (i.e. prints the filename name with any leading directory components removed).</p> <pre><code>basename SRR097977.fastq .fastq\n</code></pre> <p><code>basename</code> is rather a powerful tool when used in a for loop. It enables the user to access just the file prefix which can be use to name things</p> <pre><code>for filename in *.fastq\ndo\n    name=$(basename ${filename} .fastq)\n    echo ${name}\ndone\n</code></pre> <p>Performs a given set of commands an unknown number of times as long as the given condition evaluates is true <pre><code>while [CONDITION]\ndo\n    [COMMANDS]\ndone\n</code></pre></p> <p>Execute a given set of commands as longs as the given condition evaluates to false</p>"},{"location":"day1/ex1_bash_and_scheduler/#scripts","title":"Scripts","text":"<p>Executing operations that contain multiple lines/tasks or steps such as for loops via command line is rather inconvenient. For an example, imagine fixing a simple spelling mistake made somewhere in the middle of a for loop that was directly executed on the terminal.</p> <p>The solution for this is the use of shell scripts, which are essentially a set of commands that you write into a text file and then run as a single command. In UNIX-like operating systems, inbuilt text editors such as <code>nano</code>, <code>emacs</code>, and <code>vi</code> provide the platforms to write scripts. For this workshop we will use <code>nano</code> to create a file named <code>ForLoop.sh</code>.</p> <pre><code>nano ForLoop.sh\n</code></pre> <p>Add the following for-loop to the script (note the header <code>#!/bin/bash</code>).</p> <pre><code>#!/bin/bash\n\nfor filename in *.fastq\ndo\n    head -n 2 ${filename}\ndone\n</code></pre> <p>Because <code>nano</code> is designed to work without a mouse for input, all commands you pass into the editor are done via keyboard shortcuts. You can save your changes by pressing <code>Ctrl + O</code>, then exit <code>nano</code> using <code>Ctrl + X</code>. If you try to exit without saving changes, you will get a prompt confirming whether or not you want to save before exiting, just like you would if you were working in Notepad or Word.</p> <p>Now that you have saved your file, see if you can run the file by just typing the name of it (as you would for any command run off the terminal). You will notice the command written in the file will not be executed. The solution for this is to tell the machine what program to use to run the script. </p> <pre><code>bash ForLoop.sh\n</code></pre> <p>Although the file contains enough information to be considered as a program itself, the operating system can not recognise it as a program. This is due to it's lacking \"executable\" permissions to be executed without the assistance of a third party. Run the <code>ls -l ForLoop.sh</code> command and evaluate the first part of the output</p> <pre><code>ls -l ForLoop.sh \n# -rw-rw-r-- 1 user user 88 Dec  6 19:52 ForLoop.sh\n</code></pre> <p>There are three file permission flags that a file we create on NeSI can possess. Two of these, the read (<code>r</code>) and write (<code>w</code>) are marked for the <code>ForLoop.sh</code> file .The third flag, executable (<code>x</code>) is not set. We want to change these permissions so that the file can be executed as a program. This can be done by using <code>chmod</code> command. Add the executable permissions (<code>+x</code>) to <code>ForLoop.sh</code> and run <code>ls</code> again to see what has changed.</p> <pre><code>chmod +x ForLoop.sh\nls -l ForLoop.sh \n# -rwxrwxr-x 1 user user 88 Dec  6 19:52 ForLoop.sh\n</code></pre> <p>Re-open the file in <code>nano</code> and append the output to TwoLines.txt, save and exit</p> <pre><code>#!/bin/bash\n\nfor filename in *.fastq\ndo\n    head -n 2 ${filename} &gt;&gt; TwoLines.txt\ndone\n</code></pre> <p>Execute the file <code>ForLoop.sh</code>. We'll need to put <code>./</code> at the beginning so the computer knows to look here in this directory for the program.</p> <pre><code>./ForLoop.sh\n</code></pre> <p>Cheat sheet</p> Key commands for navigating around the filesystem are:Other useful commands: <ul> <li><code>ls</code> - list the contents of the current directory</li> <li><code>ls -l</code> - list the contents of the current directory in more detail</li> <li><code>pwd</code> - show the location of the current directory</li> <li><code>cd DIR</code> - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type <code>ls</code> OR you need to specify either a full or relative path to DIR)</li> <li><code>cd -</code> - change back to the last directory you were in</li> <li><code>cd</code> (also <code>cd ~/</code>) - change to your home directory</li> <li><code>cd ..</code> - change to the directory one level above</li> </ul> <ul> <li><code>mv</code> - move files or directories</li> <li><code>cp</code> - copy files or directories</li> <li><code>rm</code> - delete files or directories</li> <li><code>mkdir</code> - create a new directory</li> <li><code>cat</code> - concatenate and print text files to screen</li> <li><code>more</code> - show contents of text files on screen</li> <li><code>less</code> - cooler version of <code>more</code>. Allows searching (use <code>/</code>)</li> <li><code>tree</code> - tree view of directory structure</li> <li><code>head</code> - view lines from the start of a file</li> <li><code>tail</code> - view lines from the end of a file</li> <li><code>grep</code> - find patterns within files</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/","title":"Introduction II: HPC and job scheduler","text":""},{"location":"day1/ex2_1_intro_to_scheduler/#defining-high-performance-computing","title":"Defining high-performance computing","text":"<p>The simplest way of defining high-performance computing is by saying that it is the using of high-performance computers (HPC). However, this leads to our next question what is a HPC .</p> <p>HPC</p> <p>A high-performance computer is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer.</p> <p></p> <p>While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time.</p> <p>One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster.</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#hpc-architectures","title":"HPC architectures","text":"<p>Most HPC systems follow the ideas described above of taking many computers and linking them via network switches.  described above is:</p> <p></p> <p>What distinguishes a high-performance computer from the computer clusters</p> <ul> <li>The number of computers/nodes </li> <li>The strength of each individual computer/node </li> <li>The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit.</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#nesi-mahuika-cluster-architecture","title":"NeSI Mahuika Cluster architecture","text":"<p>NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> </ul> Overview of HPC ArchitectureComposition of a nodeIn reality <p> </p> <p> </p> <p> </p> <p>Jupyter Terminal</p> <ul> <li>Jupyter terminal should be treated as a login node. .i.e. Just like what we have done so far; use it to develop, test and debug scripts but do not to deploy the production level workflow interactively.</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#from-hardware-to-software","title":"From Hardware to Software","text":"<p>Over 90% HPCs &amp; supercomputers employ Linux as their operating system.  Linux has four essential properties which make it an excellent operating system for the HPCs &amp;  science community:</p> PerformanceFunctionalityFlexibilityPortability <p>Performance of the operating system can be optimized for specific tasks such as running small portable devices or large supercomputers.</p> <p>A number of community-driven scientific applications and libraries have been developed under Linux such as molecular dynamics, linear algebra, and fast-Fourier transforms.</p> <p>The system is flexible enough to allow users to build applications with a wide array of support tools such as compilers, scientific libraries, debuggers, and network monitors.</p> <p>The operating system, utilities, and libraries have been ported to a wide variety of devices including desktops, clusters, supercomputers, mainframes, embedded systems, and smart phones.</p> Supplementary : The Linux operating system is made up of three parts; the kernel, the shell and the software <p>Kernel \u2212 The kernel is the heart of the operating system. It interacts with the hardware and most of the tasks like memory management, task scheduling and file management.</p> <p>Shell \u2212 The shell is the utility that processes your requests (acts as an interface between the user and the kernel). When you type in a command at your terminal, the shell interprets (operating as in interpreter) the command and calls the program that you want. The shell uses standard syntax for all commands. The shell recognizes a limited set of commands, and you must give commands to the shell in a way that it understands: Each shell command consists of a command name, followed by command options (if any are desired) and command arguments (if any are desired). The command name, options, and arguments, are separated by blank space. </p> <ul> <li>An interpreter operates in a simple loop: It accepts a command, interprets the command, executes the command, and then waits for another command. The shell displays a \"prompt,\" to notify you that it is ready to accept your command.  </li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#accessing-software-via-modules","title":"Accessing software via modules","text":"<p>On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run.</p> <p>Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are:</p> <ul> <li>software incompatibilities</li> <li>versioning</li> <li>dependencies</li> </ul> <p>One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</p> <p>There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is <code>Lmod</code> where the <code>module</code> command is used to interact with environment modules.</p> <p>Viewing, Accessing and Deploying software with <code>module</code> command</p> <ul> <li>View available modules</li> </ul> <pre><code>#View all modules\n$ module avail\n\n# View all modules which match the keyword in their name\n$ module avail KEYWORD\n\n# View all modules which match the keyword in their name or description\n$ module spider KEYWORD\n</code></pre> <ul> <li> <p>Load a specific program</p> <p>All module names on NeSI Software stack have a version and toolchain/environment suffixes. If none is specified, then the default version of the software is loaded. The default version can be seen with the <code>module avail modulename</code> command (corresponding module name will have <code>(D)</code> suffix)</p> </li> </ul> <pre><code>$ module load Module_Name\n</code></pre> <ul> <li>Unload all current modules</li> </ul> <pre><code>$ module purge\n</code></pre> <p>Please do not use <code>module --force purge</code></p> <ul> <li>Swap a currently loaded module for a different one</li> </ul> <pre><code>$ module switch CURRENT_MODULE DESIRED_MODULE\n</code></pre> <p>Demo  - Use of login node and modules</p> <ul> <li>Navigate to  hpc-and-slurm directory  <pre><code>cd ~/mgss/hpc-and-slurm\n</code></pre></li> <li>Run <code>ls -F</code> command to list the file/directories in this directory. There are four files and a directory  <pre><code>blast.slurm*  Exercise/  mm-first.faa  mm-protein.faa  mm-second.faa\n</code></pre></li> <li><code>mm-protein.faa</code> is a mouse RefSeq protein data set with 64,599 sequences.. <code>mm-first.faa</code> and <code>mm-second.faa</code> are subsets of <code>mm-protein.faa</code><ul> <li><code>mm-first.faa</code> contains the first two sequences whereas <code>mm-second.faa</code> has the first 96 sequences. </li> <li>Let's BLAST these two sequences against the <code>swissprot</code> databae ( technically, we should be using <code>refseq_protein</code> database for this but it is substantially larger than <code>swissprot</code> .i.e. Not suitable for a quick demo) load modules<pre><code>module purge\nmodule load BLASTDB/2023-07\nmodule load BLAST/2.13.0-GCC-11.3.0\n</code></pre> This will take  ~ 10 seconds<pre><code>blastp -query mm-first.faa -db swissprot\n</code></pre> This will take ~3.5 minutes<pre><code>blastp -query mm-second.faa -db swissprot\n</code></pre></li> </ul> </li> </ul> <ul> <li>Runtime for <code>mm-second.faa</code> is starting to demonstrate the limitation of running this query interactively on \"login node\". If we want to query <code>mm-protein.faa</code> which has 54503 more sequences than <code>mm-second.faa</code>, we will need access to more compute resources (Compute nodes) and ideally a non-interacive method where we don't have to leave the desktop/laptop on. In other words, a \"Remote\" mechanism</li> <li>Both of these problems can be solved with the help of a HPC \"Scheduler\"</li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#working-with-job-scheduler","title":"Working with job scheduler","text":"<p>An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a </p> <ul> <li>Mechanism to control access by many users to shared computing resources</li> <li>Queuing / scheduling system for users\u2019 jobs</li> <li>Manages the reservation of resources and job execution on these resources </li> <li>Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d)</li> </ul> <p>Why do we need a scheduler ?</p> <ul> <li>To ensure the machine is utilised as fully as possible</li> <li>To ensure all users get a fair chance to use compute resources (demand usually exceeds supply)</li> <li>To track usage - for accounting and budget control</li> <li>To mediate access to other resources e.g. software licenses</li> </ul> <p>Commonly used schedulers</p> <ul> <li>Slurm</li> <li>PBS , Torque</li> <li>Grid Engine</li> </ul> <p></p> <p>Researchers can not communicate directly to  Compute nodes from the login node. Only way to establish a connection OR send scripts to compute nodes is to use scheduler as the carrier/manager</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#life-cycle-of-a-slurm-job","title":"Life cycle of a slurm job","text":"Commonly used Slurm commands Command Function <code>sbatch</code> Submit non-interactive (batch) jobs to the scheduler <code>squeue</code> List jobs in the queue <code>scancel</code> Cancel a job <code>sacct</code> Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database <code>srun</code> Slurm directive for parallel computing <code>sinfo</code> Query the current state of nodes <code>salloc</code> Submit interactive jobs to the scheduler <p>About</p>"},{"location":"day1/ex2_1_intro_to_scheduler/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0","text":"<p>As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command (<code>sbatch</code> in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to <code>sbatch</code> should be prepended with <code>#SBATCH</code>.</p> <p> </p> <p>Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The <code>#SBATCH</code> parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with <code>#</code>. This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the <code>#!</code>, then uses the program at the subsequent path to interpret the script, in our case <code>/bin/bash</code> (the program <code>bash</code> found in the /bin directory</p> Slurm variables header use description --job-name <code>#SBATCH --job-name=MyJob</code> The name that will appear when using <code>squeue</code> or <code>sacct</code>. --account <code>#SBATCH --account=nesi12345</code> The account your core hours will be 'charged' to. --time <code>#SBATCH --time=DD-HH:MM:SS</code> Job max walltime. --mem <code>#SBATCH --mem=512MB</code> Memory required per node. --cpus-per-task <code>#SBATCH --cpus-per-task=10</code> Will request 10 logical CPUs per task. --output <code>#SBATCH --output=%j_output.out</code> Path and name of standard output file. <code>%j</code> will be replaced by the job ID. --mail-user <code>#SBATCH --mail-user=me23@gmail.com</code> address to send mail notifications. --mail-type <code>#SBATCH --mail-type=ALL</code> Will send a mail notification at BEGIN END FAIL. <code>#SBATCH --mail-type=TIME_LIMIT_80</code> Will send message at 80% walltime. <p></p> <p>Assigning values to Slurm variables</p> <p></p> <p></p> <p></p> Demo <ul> <li>Make sure you are in your <code>hpc-and-slurm</code> directory .i.e. <code>cd ~/mgss/hpc-and-slurm/</code></li> <li>Open <code>blast.slurm</code> script with <code>nano</code> OR direct the the File explorer to nobackup_nesi02659 &gt; MGSS_U &gt; YOUR_USERNAME &gt; hpc-and-slurm ( refer to Jupyter File explorer on Supplementary) and double click the file <p>Review the script and submit it to cluster as below</p> <pre><code>sbatch blast.slurm\n</code></pre> <p></p> Exercise <code>bowtie-test.slurm</code> Fill in the blanks and submit <ul> <li>Navigate to Exercise directory  <pre><code>cd ~/mgss/hpc-and-slurm/Exercise/\n</code></pre></li> <li><code>bowtie-test.slurm</code> was compiled to execute a variant calling workflow for a set of reads belong to a lambda virus </li> <li>Review the content of <code>bowtie-test.slurm</code>, fill in the missing values for Slurm variables and then the module commands <ul> <li>Name of the job/process can be anything. ( highly recommend giving it a \"representative\" name such as variant-calling OR lambda-variants,etc)</li> <li>2 CPUs and ~2G of memory is sufficient </li> <li>Runtime will be about 35 seconds but we can assign ~2 minutes for it.  ( counting for the overhead and unexpected slowdowns)</li> <li>We can ask Slurm to send an email notification when the job gets started and finished (regardless of the outcome)</li> <li>modules are listed but the command to load them are missing </li> </ul> </li> </ul> <p>Submit the script</p> <pre><code>sbatch bowtie-test.slurm\n</code></pre> Optional : Exercise-2 <ul> <li>Time permitting, we will discuss Exercise-2 and GPUs \ud83d\ude0a (not needed for Summer School material) <pre><code>cd /mgss/hpc-and-slurm/Exercise-2/\n</code></pre></li> </ul>"},{"location":"day1/ex2_1_intro_to_scheduler/#day-1-starts-here-submitting-a-slurm-job","title":"Day 1 starts here: Submitting a slurm job","text":"<p>As the first task of the workshop, we would like you to submit a slurm script to ensure that things will move smoothly throughout the workshop days.</p> <p>Nagivate to the working directory:</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/1.Slurm_test/\n</code></pre> <p>In the directory, you should see 2 objects:</p> <ul> <li><code>bowtie-test.slurm</code></li> <li><code>example/</code></li> </ul> <p>We will need to modify the slurm script. We will use <code>nano</code> for this:</p> <p>code</p> <pre><code>nano bowtie-test.slurm\n</code></pre> <p>Using your keyboard arrow keys, navigate to the line that starts with <code>#SBATCH --mail-user</code> and enter your email address</p> <p>code</p> <pre><code>#SBATCH --mail-user myaddress@email.com\n</code></pre> <p>Save your edits by pressing the keys CTRL + O. When asked for <code>File Name to Write: bowtie-test.slurm</code>, press Enter. Exit <code>nano</code> by pressing CTRL + X</p> <p>Submit your job</p> <p>code</p> <pre><code>sbatch bowtie-test.slurm\n</code></pre> <p>The job will send email notifications on its progress to your specified address.</p> <p>If you receive email notifications about its status and have output files in the directory, that means you are good to go!</p>"},{"location":"day1/ex2_quality_filtering/","title":"Filter raw reads by quality","text":"<p>Objectives</p> <ul> <li>Visualising raw reads with <code>FastQC</code></li> <li>Read trimming and adapter removal with <code>trimmomatic</code></li> <li>Diagnosing poor libraries</li> <li>Understand common issues and best practices</li> <li>Filtering out host DNA with <code>BBMap</code></li> </ul> <p> </p>"},{"location":"day1/ex2_quality_filtering/#visualising-raw-reads","title":"Visualising raw reads","text":"<p><code>FastQC</code> is an extremely popular tool for checking your sequencing libraries, as the visual interface makes it easy to identify the following issues:</p> <ol> <li>Adapter/barcode sequences</li> <li>Low quality regions of sequence</li> <li>Quality drop-off towards the end of read-pair sequence</li> </ol>"},{"location":"day1/ex2_quality_filtering/#loading-fastqc","title":"Loading <code>FastQC</code>","text":"<p>These exercises will take place in the <code>2.fastqc/</code> directory. First, navigate to this directory. Copy the command below into your terminal (logged in to NeSI), replacing <code>&lt;YOUR FOLDER&gt;</code>, and then running the command.</p> <p>Navigate to our working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/\n</code></pre> <p>To activate <code>FastQC</code> on NeSI, you need to first load the module using the command</p> <pre><code>module purge\nmodule load FastQC/0.12.1\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#running-fastqc","title":"Running <code>FastQC</code>","text":"<p>We will run <code>FastQC</code> from the command line as follows:</p> <pre><code>fastqc mock_R1.good.fastq.gz mock_R2.good.fastq.gz\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#viewing-the-outputs-from-fastqc","title":"Viewing the outputs from <code>FastQC</code>","text":"<p><code>FastQC</code> generates output reports in <code>.html</code> files that can be viewed in a standard web browser. </p> <p>Fortunately, if you're currently using the terminal within <code>Jupyter hub</code> for today's session, we can open the <code>.html</code> files directly from here:</p> <ul> <li>Click on the directory icon in the top left to open the directory navigator pane (if not already open).</li> <li>The default viewed location will be the overall project that you have logged in to (in this case, the 'Genomics Aotearoa Virtual Lab Training Access (nesi02659)' project). </li> <li>Click through <code>MGSS_U</code>, into your directory, and then into the <code>2.fastqc/</code> directory. </li> <li>Double click on the output <code>...fastqc.html</code> files to open them in the a new tab within the <code>Jupyter hub</code>.</li> </ul> <p>Examples of the output files are also available for download here.</p> <p>Note that <code>FastQC</code> does not load the forward and reverse pairs of a library in the same window, so you need to be mindful of how your samples relate to each other. </p> <p></p> <p>At a first glance, we can see the follow statistics:</p> <ol> <li>The data is stored in Sanger / Illumina 1.9 encoding. This will be important to remember when read trimming.</li> <li>There are 100,000 reads in the file</li> <li>The maximum sequence length is 251 base pairs. This is good to check, since the data were generated using Illumina 2x250 bp sequencing.</li> </ol> <p>Have a quick look through the menu sidebar on the left. As you can see, the data has passed most of the basic parameters.</p> Per-base sequence qualityPer sequence GC contentAdapter Content <p></p> <p></p> <p></p> <p>The only aspect of the data that <code>FastQC</code> is flagging as potentially problematic is the GC% content of the data set. This observation is expected as we deal with a mixed community and organisms. Therefore, it is unlikely that there will be a perfect normal distribution around an average value. For example, a community comprised of low- and high-GC organisms would manifest a bimodal distribution of peaks which would be a problematic outcome in terms of the expectations of <code>FastQC</code>, but completely consistent with the biology of the system.</p>"},{"location":"day1/ex2_quality_filtering/#fastqc-outputs-for-libraries-with-errors","title":"<code>FastQC</code> outputs for libraries with errors","text":"<p>Lets take a look at a library with significant errors. Process the sequence file <code>mock_R1.adapter_decay.fastq</code> with <code>FastQC</code>.</p> <p>code</p> <pre><code>fastqc mock_R1.adapter_decay.fastq.gz\n</code></pre> <p>Compare the results with the <code>mock_R1.good.fastq.gz</code> file.</p> <p>Which of the previous fields we examined are now flagged as problematic? How does this compare with your expectation? Are there any which should be flagged which are not?</p> Per-base sequence qualityAdapter content <p></p> <p></p>"},{"location":"day1/ex2_quality_filtering/#combining-fastqc-outputs-using-multiqc","title":"Combining <code>FastQC</code> outputs using <code>MultiQC</code>","text":"<p>In this workshop, there are only few <code>fastq</code> files to work with. However, for an actual metagenomics experiment/survey, there might be 100s of files to QC. Here, we can use <code>MultiQC</code> to aggregate all of the reports generated by <code>FastQC</code> into one HTML file. </p> <p>code</p> <pre><code>module load MultiQC/1.13-gimkl-2022a-Python-3.10.5\n\nmultiqc ./\n</code></pre> <p>In the command above, MultiQC is directed to look for relevant files in the current directory and produce a browser-friendly HTML report there.</p>"},{"location":"day1/ex2_quality_filtering/#read-trimming-and-adapter-removal-with-trimmomatic","title":"Read trimming and adapter removal with trimmomatic","text":"<p>There are a multitude of programs which can be used to quality trim sequence data and remove adapter sequence. For this exercise we are going to use <code>trimmomatic</code>, but this should in no way be interpreted as an endorsement of <code>trimmomatic</code> over equivalent tools like <code>BBMap</code>, <code>sickle</code>, <code>cutadapt</code> or any other.</p> <p>For a first run with <code>trimmomatic</code>, type the following commands into your console:</p> <pre><code>module load Trimmomatic/0.39-Java-1.8.0_144\n\n# Run Trimmomatic\ntrimmomatic PE -threads 4 -phred33 \\\n               mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz \\\n               mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz \\\n               ILLUMINACLIP:NexteraPE-PE.fa:1:25:7 SLIDINGWINDOW:4:30 MINLEN:80\n</code></pre> <p>There is a lot going on in this command, so here is a breakdown of the parameters in the command above</p> Parameter Type Description <code>PE</code> positional Specifies whether we are analysing single- or paired-end reads <code>-threads 2</code> keyword Specifies the number of threads to use when processing <code>-phred33</code> keyword Specifies the fastq encoding used <code>mock_R1.adapter_decay.fastq.gz</code> / <code>mock_R2.adapter_decay.fastq.gz</code> positional The paired forward and reverse reads to trim <code>mock_R1.qc.fastq.gz</code> positional The file to write forward reads which passed quality trimming, if their reverse partner also passed <code>mock_s1.qc.fastq.gz</code> positional The file to write forward reads which passed quality trimming, if their reverse partner failed (orphan reads) <code>mock_R2.qc.fastq.gz</code> / <code>mock_s2.qc.fastq.gz</code> positional The reverse-sequence equivalent of above <code>ILLUMINACLIP:NexteraPE-PE.fa:1:25:7</code> positional Adapter trimming allowing for 1 seed mismatch, palindrome clip score threshold of 25, and simple clip score threshold of 7 <code>SLIDINGWINDOW:4:30</code> positional Quality filtering command. Analyse each sequence in a 4 base pair sliding window and then truncate if the average quality drops below Q30 <code>MINLEN:80</code> positional Length filtering command. Discard sequences that are shorter than 80 base pairs after trimming <p>Terminal output</p> <pre><code>TrimmomaticPE: Started with arguments:\n -threads 4 -phred33 mock_R1.adapter_decay.fastq.gz mock_R2.adapter_decay.fastq.gz mock_R1.qc.fastq.gz mock_s1.qc.fastq.gz mock_R2.qc.fastq.gz mock_s2.qc.fastq.gz ILLUMINACLIP:NexteraPE-PE.fa:1:25:7 SLIDINGWINDOW:4:30 MINLEN:80\nUsing PrefixPair: 'AGATGTGTATAAGAGACAG' and 'AGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'CTGTCTCTTATACACATCTCCGAGCCCACGAGAC'\nUsing Long Clipping Sequence: 'CTGTCTCTTATACACATCTGACGCTGCCGACGA'\nILLUMINACLIP: Using 1 prefix pairs, 4 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nInput Read Pairs: 2000000 Both Surviving: 1160013 (58.00%) Forward Only Surviving: 340638 (17.03%) Reverse Only Surviving: 124184 (6.21%) Dropped: 375165 (18.76%)\nTrimmomaticPE: Completed successfully\n</code></pre> <p>After Trimmomatic has completed, run FastQC again to check how well your reads have done.</p>"},{"location":"day1/ex2_quality_filtering/#working-with-the-illuminaclip-command","title":"Working with the <code>ILLUMINACLIP</code> command","text":"<p>Adapter trimming in <code>Trimmomatic</code> is called via the <code>ILLUMINACLIP</code> command, which can be used to pass a FASTA file of adapter and barcode sequences to be found and removed from your data. For further information about the <code>ILLUMINACLIP</code> command, please refer to the <code>trimmomatic</code> manual for a detailed description.</p> <p>There is always some subjectivity in how sensitive you want your adapter (and barcode) searching to be. If the settings are too strict you might end up discarding real sequence data that only partially overlaps with the Illumina adapters. If your settings are not strict enough then you might leave partial adapters in the sequence. For short inserts (DNA fragments), the sequencing process can read through into the adapter on the other side (3'), thus adapters need to be removed prior to quality-based trimming as the chance of hitting adapters (thus, removing them) is highest when the sequence is longer.</p> <p>Running the trimmed files back through <code>FastQC</code>, we can see that this significantly improves the output.</p> Quality of reads for trimmed R1Adapter content for trimmed R1 <p></p> <p></p>"},{"location":"day1/ex2_quality_filtering/#considerations-when-working-with-trimmomatic","title":"Considerations when working with <code>trimmomatic</code>","text":"<p>Order of operations</p> <p>The basic format for a <code>trimmomatic</code> command is</p> <p>code</p> <pre><code>trimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; &lt;trimming parameters&gt;\n</code></pre> <p>The trimming parameters are processed in the order you specify them. This is a deliberate behaviour, but can have some unexpected consequences for new users.</p> <p>For example, consider these two scenarios:</p> <p>code</p> <pre><code># Command 1\ntrimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; \\\n                ILLUMINACLIP:${adapter}:1:25:7 SLIDINGWINDOW:4:30 MINLEN:80\n\n# Command 2\ntrimmomatic PE &lt;keyword flags&gt; &lt;sequence input&gt; &lt;sequence output&gt; \\\n                ILLUMINACLIP:${adapter}:1:25:7 MINLEN:80 SLIDINGWINDOW:4:30\n</code></pre> <p>In the first run, we would not expect any sequence shorter than 80 base pairs to exist in the output files. However, we might encounter them in the second command. This is because in the second command we remove sequences shorter than 80 base pairs, then perform quality trimming. If a sequence is trimmed to a length shorter than 80 base pairs after trimming, the <code>MINLEN</code> filtering does not execute a second time. In the first instance, we do not perform trimming before size selection, so any reads that start longer than 80 base pairs, but are trimmed to under 80 base pairs during quality trimming will be caught in the <code>MINLEN</code> run.</p>"},{"location":"day1/ex2_quality_filtering/#diagnosing-poor-libraries","title":"Diagnosing poor libraries","text":"<p>Whether a library is 'poor' quality or not can be a bit subjective. These are some aspects of the library that you should be looking for when evaluating <code>FastQC</code>:</p> <ol> <li>Does the sequencing length match what you ordered from the facility?</li> <li>If the sequences are shorter than expected, is adapter read-through a concern?</li> <li>What does the sequence quality look like for the whole length of the run? Are there any expected/unexpected regions of quality degradation?</li> <li>Are adapters and/or barcodes removed? <ul> <li>Look at the Per base sequence content to diagnose this.</li> </ul> </li> <li>Is there unexpected sequence duplication? <ul> <li>This can occur when low-input library preparations are used.</li> </ul> </li> <li>Are over-represented k-mers present? <ul> <li>This can be a sign of adapter and barcode contamination.</li> </ul> </li> </ol>"},{"location":"day1/ex2_quality_filtering/#understanding-common-issues-and-best-practices","title":"Understanding common issues and best practices","text":"<ol> <li>Do I need to remove (rare) adapters?</li> <li>I don\u2019t know if adapters have been removed or not</li> <li>How do I identify and remove adapter read-through?</li> <li>Identifying incomplete barcode/adapter removal</li> <li>Over aggressive trimming</li> <li>GC skew is outside of expected range</li> </ol>"},{"location":"day1/ex2_quality_filtering/#filtering-out-host-dna","title":"Filtering out host DNA","text":"<p>Metagenome data derived from host-associated microbial communities should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing (since we will no longer be processing a bunch of data that we are likely not interested in), and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available). This is especially important for any studies involving human subjects or those involving samples derived from Taonga species.</p> <p>There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset. </p> <p>Note</p> <p>This process may be more complicated if a reference genome for your host taxa is not readily available. In this case an alternative method would need to be employed (for example: predicting taxonomy via <code>Kraken2</code> and then filtering out all reads that map to the phylum or kingdom of your host taxa).</p> <p>This exercise provides an example using <code>BBMap</code> to map against a masked human reference genome and retain only those reads that do not map to the reference. Here we are mapping some mock human microbiome reads against a pre-prepared human genome that has been processed to mask sections of the genome, including those that:</p> <ul> <li>are presumed microbial contaminant in the reference</li> <li>have high homology to microbial genes/genomes (e.g. ribosomes)</li> <li>those that are of low complexity </li> </ul> <p>This ensures that reads that would normally map to these sections of the human genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the human genome are removed.</p> <p>Note</p> <p>The same process can be used to remove DNA matching other hosts (e.g. mouse), however you would need to search if anyone has prepared (and made available) a masked version of the reference genome, or create a masked version using <code>bbmask</code>. The creator of BBMap has made available masked human, mouse, cat, and dog genomes. More information, including links to these references and instructions on how to generate a masked genome for other taxa, can be found within this thread.*</p>"},{"location":"day1/ex2_quality_filtering/#the-masked-human-reference-genome","title":"The masked human reference genome","text":"<p>The masked reference genome is available via Google drive and Zenodo. For this workshop, we have provided you with the file within the <code>2.fastqc/BBMask_human_reference/</code> directory.</p> Downloading your own copy of the masked genome <p>We can use <code>gdown</code> to download this file from Google drive via the command line.</p> <p>To install <code>gdown</code>, we can use <code>pip</code>. </p> <p>Install <code>gdown</code></p> <pre><code># Install gdown (for downloading from google drive)\nmodule purge\nmodule load Python/3.10.5-gimkl-2022a\npip install --user gdown\n</code></pre> <p>Next, download the reference. It will also be necessary to first add your local <code>bin</code> location to the <code>PATH</code> variable via the <code>export PATH=...</code> command, as this is where <code>gdown</code> is located (modify <code>&lt;your_username&gt;</code> before running the code below).</p> <p>Download reference</p> <pre><code>mkdir BBMask_human_reference/\ncd BBMask_human_reference/\n\nexport PATH=\"/home/&lt;your_username&gt;/.local/bin:$PATH\"\n\ngdown https://drive.google.com/uc?id=0B3llHR93L14wd0pSSnFULUlhcUk\n</code></pre>"},{"location":"day1/ex2_quality_filtering/#indexing-the-reference-genome-and-read-mapping-with-bbmap","title":"Indexing the reference genome and read mapping with <code>BBMap</code>","text":"<p>We will cover more about read mapping in later exercises. For now, it is important to know that it is first necessary to build an index of the reference using the read mapping tool of choice. Here, we will first build a <code>BBMap</code> index, and then use <code>BBMap</code> to map the reads to that index, ultimately retaining only those reads that do not map to the index.</p> <p>Build index reference via <code>BBMap</code>. We will do this by submitting the job via slurm. </p> <p>Create a new script named <code>host_filt_bbmap_index.sl</code> using <code>nano</code>:</p> <p>code</p> <pre><code>nano host_filt_bbmap_index.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      host_filt_bbmap_index\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           32GB\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/BBMask_human_reference/\n\n# Load BBMap module\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Build indexed reference file via BBMap\nbbmap.sh ref=hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz\n</code></pre> <p>Save your script by pressing Ctrl + o , then exit the editor by pressing Ctrl + x .</p> <p>Submit your newly created script to the scheduler as follows:</p> <p>code</p> <pre><code>sbatch host_filt_bbmap_index.sl\n</code></pre> <p>Finally, map the reads to the reference via <code>BBMap</code>. Here we will submit the job as a slurm array, with one array job per sample. </p> <p>Again, we will create a script using <code>nano</code>: </p> <p>code</p> <pre><code>nano host_filt_bbmap_map.sl\n</code></pre> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      host_filt_bbmap_map\n#SBATCH --partition \u00a0 \u00a0 milan\n#SBATCH --time          01:00:00\n#SBATCH --mem           27GB\n#SBATCH --cpus-per-task 20\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Set up working directories\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/2.fastqc/\nmkdir -p host_filtered_reads/\n\n# Load BBMap module\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Run bbmap\nbbmap.sh -Xmx27g -t=12 usejni=t \\\n  minid=0.95 maxindel=3 bwr=0.16 bw=12 quickmatch fast minhits=2 qtrim=rl trimq=10 untrim \\\n  in1=human_microb_reads.R1.fastq.gz \\\n  in2=human_microb_reads.R2.fastq.gz \\\n  path=BBMask_human_reference/ \\\n  outu1=host_filt_R1.fastq \\\n  outu2=host_filt_R2.fastq\n</code></pre> <p><code>BBMap</code> parameters</p> Parameter Description <code>-Xmx27g</code> Set maximum memory allocation to match <code>#SBATCH --mem</code>) <code>-t</code> Set number of threads (CPUs) to match <code>#SBATCH --cpus-per-task</code> All flags in line 22 Recommended parameters found within this thread about processing data to remove host reads <code>in1</code> / <code>in2</code> Input paired-end reads 1 and 2 stored in <code>3.assembly/</code>, each of which are \"samples\". These are the same files we will use in further lessons <code>path</code> The parent directory where <code>ref/</code> (our indexed and masked reference) exists <code>outu1</code> / <code>outu2</code> Reads that were not mapped to our masked reference written to <code>host_filtered_reads/</code> <p>We'll submit the mapping script:</p> <p>code</p> <pre><code>sbatch host_filt_bbmap_map.sl\n</code></pre> Monitoring job progress <p>We can monitor our job progress using <code>squeue --me</code> or <code>sacct &lt;job_id&gt;</code>. This will be covered in detail as part of the main content when we evaluate assemblies. </p> <p>Array jobs</p> <p>Slurm array jobs automatically create a variable <code>SLURM_ARRAY_TASK_ID</code> for that job, which contains the array task number (i.e. between 1 and 4 in the case above). We use this to run the command on the sample that matches this array task ID. I.e. array job 3 will run the commands on \"sample3\" (<code>sample${SLURM_ARRAY_TASK_ID}</code> is read in as <code>sample3</code>).</p> <p>The filtered reads are now available in <code>host_filtered_reads/</code> for downstream use.</p>"},{"location":"day1/ex3_assembly/","title":"Assembly I: Assembling contigs","text":"<p>Objectives</p> <ul> <li>Become familiar with the standard input files for <code>SPAdes</code> and <code>IDBA-UD</code></li> <li>Understand the basic parameters that should be modified when using these assemblers</li> <li>Prepare an assembly job to run under slurm</li> </ul> <p> </p> <p>All work for this exercise will occur in the <code>3.assembly/</code> directory.</p>"},{"location":"day1/ex3_assembly/#the-standard-input-files-for-spades-and-idba-ud","title":"The standard input files for <code>SPAdes</code> and <code>IDBA-UD</code>","text":"<p>Although they both make use of the same types of data, both <code>SPAdes</code> and <code>IDBA-UD</code> have their own preferences for how sequence data is provided to them. To begin, we will look at the types of data accepted by <code>SPAdes</code>:</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/3.assembly\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>Load <code>SPAdes</code> and check parameters</p> <pre><code># Load module\nmodule purge\nmodule load SPAdes/4.0.0-foss-2023a-Python-3.11.6\n\n# Check parameters\nspades.py -h\n</code></pre> <code>SPAdes</code> parameters <pre><code>...\nInput data:\n  --12 &lt;filename&gt;             file with interlaced forward and reverse paired-end reads\n  -1 &lt;filename&gt;               file with forward paired-end reads\n  -2 &lt;filename&gt;               file with reverse paired-end reads\n  -s &lt;filename&gt;               file with unpaired reads\n  --merged &lt;filename&gt;         file with merged forward and reverse paired-end reads\n  --pe-12 &lt;#&gt; &lt;filename&gt;      file with interlaced reads for paired-end library number &lt;#&gt;.\n                              Older deprecated syntax is -pe&lt;#&gt;-12 &lt;filename&gt;\n  --pe-1 &lt;#&gt; &lt;filename&gt;       file with forward reads for paired-end library number &lt;#&gt;.\n                              Older deprecated syntax is -pe&lt;#&gt;-1 &lt;filename&gt;\n  --pe-2 &lt;#&gt; &lt;filename&gt;       file with reverse reads for paired-end library number &lt;#&gt;.\n                              Older deprecated syntax is -pe&lt;#&gt;-2 &lt;filename&gt;\n  --pe-s &lt;#&gt; &lt;filename&gt;       file with unpaired reads for paired-end library number &lt;#&gt;.\n                              Older deprecated syntax is -pe&lt;#&gt;-s &lt;filename&gt;\n  --pe-m &lt;#&gt; &lt;filename&gt;       file with merged reads for paired-end library number &lt;#&gt;.\n                              Older deprecated syntax is -pe&lt;#&gt;-m &lt;filename&gt;\n  --pe-or &lt;#&gt; &lt;or&gt;            orientation of reads for paired-end library number &lt;#&gt; \n                              (&lt;or&gt; = fr, rf, ff).\n                              Older deprecated syntax is -pe&lt;#&gt;-&lt;or&gt;\n  --s &lt;#&gt; &lt;filename&gt;          file with unpaired reads for single reads library number &lt;#&gt;.\n                              Older deprecated syntax is --s&lt;#&gt; &lt;filename&gt;\n  --mp-12 &lt;#&gt; &lt;filename&gt;      file with interlaced reads for mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -mp&lt;#&gt;-12 &lt;filename&gt;\n  --mp-1 &lt;#&gt; &lt;filename&gt;       file with forward reads for mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -mp&lt;#&gt;-1 &lt;filename&gt;\n  --mp-2 &lt;#&gt; &lt;filename&gt;       file with reverse reads for mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -mp&lt;#&gt;-2 &lt;filename&gt;\n  --mp-s &lt;#&gt; &lt;filename&gt;       file with unpaired reads for mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -mp&lt;#&gt;-s &lt;filename&gt;\n  --mp-or &lt;#&gt; &lt;or&gt;            orientation of reads for mate-pair library number &lt;#&gt; \n                              (&lt;or&gt; = fr, rf, ff).\n                              Older deprecated syntax is -mp&lt;#&gt;-&lt;or&gt;\n  --hqmp-12 &lt;#&gt; &lt;filename&gt;    file with interlaced reads for high-quality mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -hqmp&lt;#&gt;-12 &lt;filename&gt;\n  --hqmp-1 &lt;#&gt; &lt;filename&gt;     file with forward reads for high-quality mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -hqmp&lt;#&gt;-1 &lt;filename&gt;\n  --hqmp-2 &lt;#&gt; &lt;filename&gt;     file with reverse reads for high-quality mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -hqmp&lt;#&gt;-2 &lt;filename&gt;\n  --hqmp-s &lt;#&gt; &lt;filename&gt;     file with unpaired reads for high-quality mate-pair library number &lt;#&gt;.\n                              Older deprecated syntax is -hqmp&lt;#&gt;-s &lt;filename&gt;\n  --hqmp-or &lt;#&gt; &lt;or&gt;          orientation of reads for high-quality mate-pair library number &lt;#&gt; \n                              (&lt;or&gt; = fr, rf, ff).\n                              Older deprecated syntax is -hqmp&lt;#&gt;-&lt;or&gt;\n  --sanger &lt;filename&gt;         file with Sanger reads\n  --pacbio &lt;filename&gt;         file with PacBio reads\n  --nanopore &lt;filename&gt;       file with Nanopore reads\n  --trusted-contigs &lt;filename&gt;\n                              file with trusted contigs\n  --untrusted-contigs &lt;filename&gt;\n                              file with untrusted contigs\n...\n</code></pre> <p>At a glance, you could provide any of the following data types to <code>SPAdes</code> and have it perform an assembly:</p> <ol> <li>Illumina paired-end sequencing data, either as standard library or Mate Pairs</li> <li>Sanger sequences</li> <li>PacBio reads</li> <li>Oxford Nanopore reads</li> <li>Pre-assembled scaffolds for guiding the assembly</li> </ol> <p>Awkwardly, while <code>SPAdes</code> accepts multiple input libraries (i.e. samples) in a single assembly, this behaviour does not work with the <code>-meta</code> flag enabled, which is needed in our example to activate metagenome assembly mode. We therefore need to concatenate our four individual samples together ready for sequencing.</p> <p>Concatenate samples by read direction</p> <pre><code>cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz &gt; for_spades_R1.fq.gz\ncat sample1_R2.fastq.gz sample2_R2.fastq.gz sample3_R2.fastq.gz sample4_R2.fastq.gz &gt; for_spades_R2.fq.gz\n</code></pre> <p>Note that these FASTQ files are compressed, yet we can concatenate them together with the <code>cat</code> command regardless. This is a nice feature of <code>.gz</code> files that is handy to remember.</p> <p>By contrast, what does <code>IDBA-UD</code> accept?</p> <p>Load <code>IDBA-UD</code> and check parameters</p> <pre><code># Load module\nmodule purge\nmodule load IDBA-UD/1.1.3-GCC-11.3.0\n\n# Check parameters\nidba_ud --help\n</code></pre> <code>IDBA-UD</code> parameters <pre><code>  -o, --out arg (=out)                   output directory\n  -r, --read arg                         fasta read file (&lt;=128)\n      --read_level_2 arg                 paired-end reads fasta for second level scaffolds\n      --read_level_3 arg                 paired-end reads fasta for third level scaffolds\n      --read_level_4 arg                 paired-end reads fasta for fourth level scaffolds\n      --read_level_5 arg                 paired-end reads fasta for fifth level scaffolds\n  -l, --long_read arg                    fasta long read file (&gt;128)\n      --mink arg (=20)                   minimum k value (&lt;=124)\n      --maxk arg (=100)                  maximum k value (&lt;=124)\n      --step arg (=20)                   increment of k-mer of each iteration\n      --inner_mink arg (=10)             inner minimum k value\n      --inner_step arg (=5)              inner increment of k-mer\n      --prefix arg (=3)                  prefix length used to build sub k-mer table\n      --min_count arg (=2)               minimum multiplicity for filtering k-mer when building the graph\n      --min_support arg (=1)             minimum supoort in each iteration\n      --num_threads arg (=0)             number of threads\n      --seed_kmer arg (=30)              seed kmer size for alignment\n      --min_contig arg (=200)            minimum size of contig\n      --similar arg (=0.95)              similarity for alignment\n      --max_mismatch arg (=3)            max mismatch of error correction\n      --min_pairs arg (=3)               minimum number of pairs\n      --no_bubble                        do not merge bubble\n      --no_local                         do not use local assembly\n      --no_coverage                      do not iterate on coverage\n      --no_correct                       do not do correction\n      --pre_correction                   perform pre-correction before assembly\n</code></pre> <p>'Short' or 'long' reads, and only a single file for each. This means that if we want to assemble our community data using <code>IDBA-UD</code> we will need to pool the paired-end data into a single, interleaved FASTA file. Interleaved means that instead of having a pair of files that contain the separate forward and reverse sequences, the read pairs are in a single file in alternating order. For example</p> <pre><code># Paired-end file, forward\n&gt;read1_1\n...\n&gt;read2_1\n...\n\n# Paired-end file, reverse\n&gt;read1_2\n...\n&gt;read2_2\n...\n\n# Interleaved file\n&gt;read1_1\n...\n&gt;read1_2\n...\n&gt;read2_1\n...\n&gt;read2_2\n...\n</code></pre> <p>Fortunately, the <code>IDBA</code> set of tools comes with some helper scripts to achieve just this. Unfortunately we cannot apply this shuffling operation to compressed data, so we must decompress the data first.</p> <p>Decompress, interleave, and then concatenate sequence files</p> <pre><code>module load pigz/2.7\n\nfor i in sample1 sample2 sample3 sample4;\ndo\n  pigz --keep --decompress ${i}_R1.fastq.gz ${i}_R2.fastq.gz\n  fq2fa --merge ${i}_R1.fastq ${i}_R2.fastq ${i}.fna\ndone\n\ncat sample1.fna sample2.fna sample3.fna sample4.fna &gt; for_idba.fna    \n</code></pre>"},{"location":"day1/ex3_assembly/#basic-assembly-parameters","title":"Basic assembly parameters","text":"<p>For any assembler, there are a lot of parameters that can be fine-tuned depending on your data. As no two data sets are the same, it is almost impossible to predict which parameter combinations will yield the best outcome for your dataset. That said, an assembly can be quite a resource-intensive process and it is generally not practical to test every permutation of parameter values with your data. In genomics, the saying goes that the best assembly is the one that answers your question. As long as the data you are receiving is meaningful to the hypothesis you are seeking to address, then your assembly is as good as it needs to be.</p> <p>Generally speaking, assemblers are developed in a way where they run with default parameters that have been empirically demonstrated to produce the best outcome on average across multiple data sets. For most purposes, there is not a lot of need to change these, but some parameters that we would always want to look at include:</p> <ol> <li>k-mer sizes to be assembled over, and step size if using a range</li> <li>Number of threads to use during assembly</li> <li>Memory limit to prevent the assembler from using up all available RAM and forcing the computer to use its swap space</li> </ol>"},{"location":"day1/ex3_assembly/#setting-the-k-mer-size","title":"Setting the k-mer size","text":"<p>Depending on which assembler you are using, the commands for choosing the k-mer sizes for the assembly vary slightly, but they are recognisable between programs. In <code>SPAdes</code>, you can set the k-mer size using either</p> <p>code</p> <pre><code>spades.py -k 21,33,55,77,99,121 ...\n\nspades.py -k auto ...\n</code></pre> <p>The first command lets us specify the k-mers ourselves, or we are letting <code>SPAdes</code> automatically pick the most appropriate size. For <code>IDBA-UD</code>, we can select the k-mer size using</p> <p>code</p> <pre><code>idba_ud --mink 21 --maxk 121 --step 22\n</code></pre> <p>Unlike <code>SPAdes</code>, we do not have fine-scale control over the k-mer sizes used in the assembly. We instead provide <code>IDBA-UD</code> with the first and last k-mer size to use, then specify the increment to use between these. In either case, it is important that we are always assembling using a k-mer of odd lengths in order to avoid the creation of palindromic k-mers.</p>"},{"location":"day1/ex3_assembly/#specifying-the-number-of-threads","title":"Specifying the number of threads","text":"<p>This is simple in either assembler:</p> <p>code</p> <pre><code>spades.py -t 20 ...\n\nidba_ud --num_threads 20 ...\n</code></pre> <p>The only thing to keep in mind is that these tools have different default behaviour. If no thread count is specified by the user, <code>SPAdes</code> will assemble with 16 threads. <code>IDBA-UD</code> will use all available threads, which can be problematic if you are using a shared compute environment that does not use a resource management system like slurm.</p>"},{"location":"day1/ex3_assembly/#setting-a-memory-limit","title":"Setting a memory limit","text":"<p>By far, the worst feature of <code>SPAdes</code> is the high memory requirement for performing an assembly. In the absence of monitoring, <code>SPAdes</code> will request more and more memory as it proceeds. If this requires more memory than is available on your computer, your system will start to store memory to disk space. This is an extremely slow operation and can render your computer effectively unusable. In managed environments such as NeSI a memory limit is imposed upon all running jobs, but if you are not using such a system you are advised to set a memory limit when executing <code>SPAdes</code>:</p> <p>code</p> <pre><code>spades.py -m 400GB ...\n</code></pre> <p>No such parameter exists in <code>IDBA-UD</code>, but it requires far less RAM than <code>SPAdes</code>, so you are less likely to need it.</p>"},{"location":"day1/ex3_assembly/#preparing-an-assembly-job-for-slurm","title":"Preparing an assembly job for slurm","text":"<p>NeSI does not allow users to execute large jobs interactively on the terminal. Instead, the node that we have logged in to (lander02) has only a small fraction of the computing resources that NeSI houses. The lander node is used to write small command scripts, which are then deployed to the large compute nodes by a system called slurm. The ins and outs of working in slurm are well beyond the scope of this workshop (and may not be relevant if your institution uses a different resource allocation system). In this workshop, we will therefore only be showing you how to write minimal slurm scripts sufficient to achieve our goals. By the end of the workshop, you should have built up a small collection of slurm scripts for performing the necessary stages of our workflow and with experience you will be able to modify these to suit your own needs.</p>"},{"location":"day1/ex3_assembly/#submitting-a-spades-job-to-nesi-using-slurm","title":"Submitting a <code>SPAdes</code> job to NeSI using slurm","text":"<p>To begin, we need to open a text file using the <code>nano</code> text editor. </p> <p>Create script named <code>spades_assembly.sl</code> using <code>nano</code></p> <pre><code>nano spades_assembly.sl\n</code></pre> <p>Into this file, either write or copy/paste the following commands:</p> <p>Warning</p> <p>Paste or type in the following. Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own directory.</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_assembly\n#SBATCH --partition     milan\n#SBATCH --time          00:30:00\n#SBATCH --mem           10G\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load SPAdes/4.0.0-foss-2023a-Python-3.11.6\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/3.assembly\n\n# Run SPAdes\nspades.py --meta -k 33,55,77,99,121 -t $SLURM_CPUS_PER_TASK \\\n          -1 for_spades_R1.fq.gz -2 for_spades_R2.fq.gz \\\n          -o spades_assembly/\n</code></pre> <p>To save your file, use Ctrl + O to save the file, then Ctrl + X to exit <code>nano</code>. Going through those lines one by one:</p> Slurm parameters and their functions Parameter Description <code>#!/bin/bash -e</code> Header for the file, letting NeSI know how to interpret the following commands. The <code>-e</code> flag means that the slurm run will halt at the first failed command (rather than pushing through and trying to execute subsequent ones) <code>--account</code> The name of the project account to run the job under. You are provided with this when you create a project on NeSI <code>--job-name</code> The name of the job, to display when using the <code>squeue</code> command <code>--partition</code> This is a parameter that enables us to access the milan nodes (think of a node as a single computer within the cluster) where resources are reserved for this workshop <code>--time</code> Maximum run time for the job before it is killed <code>--mem</code> Amount of server memory to allocate to the job. If this is exceeded, the job will be terminated <code>--cpus-per-task</code> number of processing cores to assign to the job. This should match with the number used by your assembler <code>--error</code> File to log the standard error stream of the program.This is typically used to prove error reports, or to just inform the user of job progress <code>--output</code> File to log the standard output stream of the program.This is typically used to inform the user of job progress and supply messages <p>The <code>module load</code> command needs to be invoked within your slurm script. It is also a good idea to explicitly set the path to your files within the job so that</p> <ol> <li>There is no chance of having the job fail immediately because it cannot find the relevant files</li> <li>When looking back through your slurm logs, you know where the data is meant to be</li> </ol> <p><code>SPAdes</code> parameters of note</p> Parameter Description <code>--meta</code> Activate metagenome assembly mode. Default is to assemble your metagenome using single genome assembly assumptions <code>-k</code> k-mer sizes for assembly. These choices will provide the output we will use in the Binning session, but feel free to experiment with these to see if you can improve the assembly <code>-t</code> Number of threads (see above). Here, we use a special slurm environment variable: <code>$SLURM_CPUS_PER_TASK</code> to tell the programme to use the same number of threads allocated for this job via <code>--cpus-per-task</code>. <code>-1</code> Forward reads, matched to their reverse partners <code>-2</code> Reverse reads, matched to their forward partners <code>-o</code> Output directory for all files <p>We don't explicitly set memory for this job, simply for the sake of keeping the command uncluttered. The default memory limit of <code>SPAdes</code> (250 GB) is much higher than the 10 GB we have allowed our job here. If the memory cap was violated then both slurm and <code>SPAdes</code> will terminate the assembly. We did specify the number of threads at the value of 12, which is specified by the special slurm variable <code>$SLURM_CPUS_PER_TASK</code>.</p> <p>It is a good idea to match your number of threads request in the slurm script with what you intend to use with <code>SPAdes</code> because your project usage is calculated based off what you request in your slurm scripts rather than what you actually use. Requesting many unused threads simply drives your project down the priority queue. By contrast, requesting fewer threads than you attempt to use in the program (i.e., request 10 in slurm, set thread count to 30 in <code>SPAdes</code>) will result in reduced performance, as your <code>SPAdes</code> job will divide up jobs as though it has 30 threads, but only 10 will be provided. This is discussed in this blog post.</p> <p>Once you are happy with your slurm script, execute the job by navigating to the location of your script and entering the command</p> <p>Submitting the slurm script for <code>SPAdes</code> assembly</p> <pre><code>sbatch spades_assembly.sl\n</code></pre> <p>You will receive a message telling you the job identifier for your assembly. Record this number, as we will use it in the next exercise.</p>"},{"location":"day1/ex3_assembly/#monitoring-job-progress","title":"Monitoring job progress","text":"<p>You can view the status of your current jobs using the command</p> <p>code</p> <pre><code>squeue --me\n</code></pre> <p>Terminal output</p> <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n31491555      jboe440  nesi02659 spawner-jupy   2      4G infill  2022-11-23T1     7:44:17 RUNNING  wbl001              \n31491999      jboe440  nesi02659 spades_assem  12     10G large   2022-11-23T1       30:00 PENDING  wbn069  \n</code></pre> <p>We can see here that the job has not yet begun, as NeSI is waiting for resources to come available. At this stage the <code>START_TIME</code> is an estimation of when the resources are expected to become available. When they do, the output will change to</p> <p>Terminal output</p> <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n31491555      jboe440  nesi02659 spawner-jupy   2      4G infill  2022-11-23T1     7:44:15 RUNNING  wbl001              \n31491999      jboe440  nesi02659 spades_assem  12     10G large   2022-11-23T1       29:58 RUNNING  wbn069          \n</code></pre> <p>Which allows us to track how far into our run we are, and see the remaining time for the job. The <code>START_TIME</code> column now reports the time the job actually began.</p>"},{"location":"day1/ex3_assembly/#submitting-an-idba-ud-job-to-nesi-using-slurm","title":"Submitting an <code>IDBA-UD</code> job to NeSI using slurm","text":"<p>Create a new slurm script using <code>nano</code> to run an equivalent assembly with <code>IDBA-UD</code></p> <pre><code>nano idbaud_assembly.sl\n</code></pre> <p>Paste or type in the following:</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      idbaud_assembly\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           4GB\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Prepare modules\nmodule purge\nmodule load IDBA-UD/1.1.3-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/3.assembly\n\n# Run IDBA-UD\nidba_ud --num_threads $SLURM_CPUS_PER_TASK --mink 33 --maxk 99 --step 22 \\\n        -r for_idba.fna -o idbaud_assembly/\n</code></pre> <p>Submit the script as a slurm job</p> <pre><code>sbatch idbaud_assembly.sl\n</code></pre> <p>When your job starts running, files with suffixes <code>.err</code> and <code>.out</code> will be created in the directory from where you submitted your job. These files will have have your job name and job identification number as file names.</p>"},{"location":"day1/ex4_assembly/","title":"Assembly II: Varying the parameters","text":"<p>Objectives</p> <ul> <li>Examining the effect of changing parameters for assembly</li> </ul> <p>All work for this exercise will occur in the <code>3.assembly/</code> directory.</p>"},{"location":"day1/ex4_assembly/#examining-the-effect-of-changing-assembly-parameters","title":"Examining the effect of changing assembly parameters","text":"<p>For this exercise, there is no real structure. Make a few copies of your initial slurm scripts and tweak a few of the assembly parameters. Make sure to also change the assembly output file (e.g. from <code>spades_assembly</code> to <code>spades_assembly_var1</code>). We will compare the effects of these changes in the next lesson.</p>"},{"location":"day1/ex4_assembly/#spades-parameters","title":"<code>SPAdes</code> parameters","text":"<p>Make a few copies of your <code>SPAdes</code> slurm script like so;</p> <p>code</p> <pre><code>cp spades_assembly.sl spades_assembly_var1.sl\n</code></pre> <p>Change a few of the parameters for run time. Some potential options include</p> <ol> <li>Change the k-mer sizes to either a different specification, or change to the <code>auto</code> option</li> <li>Disable error correction</li> <li>Assemble without the <code>--meta</code> flag</li> <li>Employ a coverage cut-off for assembling</li> </ol>"},{"location":"day1/ex4_assembly/#idba-ud-parameters","title":"<code>IDBA-UD</code> parameters","text":"<p>Make variants of your <code>IDBA-UD</code> assembly script and change some parameters. Some potential options include</p> <ol> <li>Change the minimum/maximum k-mer sizes, or the k-mer step size</li> <li>Change the alignment similarity parameter</li> <li>Adjust the prefix length for the k-mer sub-table </li> </ol> <p>Submit two or three jobs per variation.</p> <p>Outputs per variant job</p> <p>For all variations of your assemblies, please remember to modify <code>-o</code> argument from <code>-o spades_assembly/</code> and <code>-o idbaud_assembly/</code> in your slurm scripts to another name (e.g. <code>-o spades_assembly_var_kmer/</code>)</p>"},{"location":"day1/ex5_evaluating_assemblies/","title":"Assembly evaluation","text":"<p>Objectives</p> <ul> <li>Evaluating the resource consumption of various assemblies</li> <li>Evaluating the assemblies using <code>BBMap</code></li> <li>Sequence taxonomic classification using <code>Kraken2</code></li> <li>Reconstruct rRNA using <code>PhyloFlash</code>-<code>EMIRGE</code></li> <li>(Optional) Evaluating assemblies using <code>MetaQUAST</code></li> </ul> <p> </p>"},{"location":"day1/ex5_evaluating_assemblies/#evaluating-the-resource-consumption-of-various-assemblies","title":"Evaluating the resource consumption of various assemblies","text":"<p>Check to see if your assembly jobs have completed. If you have multiple jobs running or queued, the easiest way to check this is to simply run the <code>squeue</code> command.</p> <p>Check job progress</p> <pre><code>squeue --me\n</code></pre> <p>Terminal output</p> <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n39035482      jboe440  nesi02659 spawner-jupy   2      4G interac 2023-08-31T1     7:47:42 RUNNING  wbn004      \n</code></pre> <p>If there are no jobs besides your Jupyter session listed, either everything running has completed or failed. To get a list of all jobs we have run in the last day, we can use the <code>sacct</code> command. By default this will report all jobs for the day but we can add a parameter to tell the command to report all jobs run since the date we are specifying.</p> <p>Check progress for jobs started on specific date</p> <pre><code>sacct -S 2023-08-12\n</code></pre> <p>Terminal output</p> <pre><code>JobID           JobName          Alloc     Elapsed     TotalCPU  ReqMem   MaxRSS State      \n--------------- ---------------- ----- ----------- ------------ ------- -------- ---------- \n38483216        spawner-jupyter+     2    07:45:01     00:00:00      4G          NODE_FAIL  \n38483216.batch  batch                2    07:45:01     00:00:00                  CANCELLED  \n38483216.extern extern               2    07:45:01     00:00:00                  CANCELLED  \n38485254        spades_assembly     12    00:14:38     01:56:40     10G          COMPLETED  \n38485254.batch  batch               12    00:14:38     01:56:40         7227872K COMPLETED  \n38485254.extern extern              12    00:14:38     00:00:00                0 COMPLETED\n</code></pre> <p>Each job has been broken up into several lines, but the main ones to keep an eye on are the base <code>JobID</code> values. </p> Using <code>srun</code> <p>If you use <code>srun</code>, the JobID will have values suffixed with .0. The first of these references the complete job. The later (and any subsequent suffixes like .1, .2) are the individual steps in the script that were called with the <code>srun</code> command.</p> <p>We can see here the time elapsed for each job, and the number of CPU hours used during the run. If we want a more detailed breakdown of the job we can use the <code>nn_seff</code> command</p> <p>Check job resource use</p> <pre><code>nn_seff 38485254\n</code></pre> <p>Terminal output</p> <pre><code>Cluster: mahuika\nJob ID: 38485254\nState: COMPLETED\nCores: 6\nTasks: 1\nNodes: 1\nJob Wall-time:   48.8%  00:14:38 of 00:30:00 time limit\nCPU Efficiency: 132.9%  01:56:40 of 01:27:48 core-walltime\nMem Efficiency:  68.9%  6.89 GB of 10.00 GB0\n</code></pre> <p>Here we see some of the same information, but we also get some information regarding how well our job used the resources we allocated to it. You can see here that my CPU and memory usage was somewhat efficient but had high memory efficiency. In the future, I can request less time and retain the same RAM and still had the job run to completion.</p> <p>CPU efficiency is harder to interpret as it can be impacted by the behaviour of the program. For example, mapping tools like <code>bowtie</code> and <code>BBMap</code> can more or less use all of their threads, all of the time and achieve nearly 100% efficiency. More complicated processes, like those performed in <code>SPAdes</code> go through periods of multi-thread processing and periods of single-thread processing, drawing the average efficiency down.</p>"},{"location":"day1/ex5_evaluating_assemblies/#evaluating-the-assemblies-using-bbmap","title":"Evaluating the assemblies using <code>BBMap</code>","text":"<p>Evaluating the quality of a raw metagenomic assembly is quite a tricky process. Since, by definition, our community is a mixture of different organisms, the genomes from some of these organisms assemble better than those of others. It is possible to have an assembly that looks 'bad' by traditional metrics that still yields high-quality genomes from individual species, and the converse is also true.</p> <p>A few quick checks I recommend are to see how many contigs or scaffolds your data were assembled into, and then see how many contigs or scaffolds you have above a certain minimum length threshold. We will use <code>seqmagick</code> for performing the length filtering, and then just count sequence numbers using <code>grep</code>.</p> <p>These steps will take place in the <code>4.evaluation/</code> folder, which contains copies of our <code>SPAdes</code> and <code>IDBA-UD</code> assemblies.</p> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code># Load seqmagick\nmodule purge\nmodule load seqmagick/0.8.4-gimkl-2020a-Python-3.8.2\n\n# Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation/\n\n# Filter assemblies and check number of contigs\nseqmagick convert --min-length 1000 spades_assembly/spades_assembly.fna \\\n                                    spades_assembly/spades_assembly.m1000.fna\ngrep -c '&gt;' spades_assembly/spades_assembly.fna spades_assembly/spades_assembly.m1000.fna\n\nseqmagick convert --min-length 1000 idbaud_assembly/idbaud_assembly.fna \\\n                                    idbaud_assembly/idbaud_assembly.m1000.fna\ngrep -c '&gt;' idbaud_assembly/idbaud_assembly.fna idbaud_assembly/idbaud_assembly.m1000.fna\n</code></pre> <p>Terminal output</p> <code>SPAdes</code><code>IDBA-UD</code> <pre><code>spades_assembly/spades_assembly.fna:1327\nspades_assembly/spades_assembly.m1000.fna:933\n</code></pre> <pre><code>idbaud_assembly/idbaud_assembly.fna:5056\nidbaud_assembly/idbaud_assembly.m1000.fna:1996\n</code></pre> <p>If you have your own assemblies and you want to try inspect them in the same way, try that now. Note that the file names will be slightly different to the files provided above. If you followed the exact commands in the previous exercise, you can use the following commands.</p> <p>code</p> <pre><code>seqmagick convert \\\n    --min-length 1000 \\\n    ../3.assembly/my_spades_assembly/scaffolds.fasta my_spades_assembly.m1000.fna\n\nseqmagick convert \\\n    --min-length 1000 \\\n    ../3.assembly/my_idbaud_assembly/scaffold.fa my_idbaud_assembly.m1000.fna\n</code></pre> <p>Choice of software: sequence file manipulation</p> <p>The tool <code>seqtk</code> is also available on NeSI and performs many of the same functions as <code>seqmagick</code>. My choice of <code>seqmagick</code> is mostly cosmetic as the parameter names are more explicit so it's easier to understand what's happening in a command when I look back at my log files. Regardless of which tool you prefer, we strongly recommend getting familiar with either <code>seqtk</code> or <code>seqmagick</code> as both perform a lot of common FASTA and FASTQ file manipulations.</p> <p>As we can see here, the <code>SPAdes</code> assembly has completed with fewer contigs assembled than the <code>IDBA-UD</code>, both in terms of total contigs assembled and contigs above the 1,000 bp size. This doesn't tell us a lot though - has <code>SPAdes</code> managed to assemble fewer reads, or has it managed to assemble the sequences into longer (and hence fewer) contigs? We can check this by looking at the N50/L50 (see more information about this statistic here) of the assembly with <code>BBMap</code>.</p> <p>code</p> <pre><code># Load BBMap module\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Generate statistics for filtered SPAdes assembly\nstats.sh in=spades_assembly/spades_assembly.m1000.fna\n</code></pre> <p>This gives quite a verbose output:</p> <p>Terminal output</p> <pre><code>A       C       G       T       N       IUPAC   Other   GC      GC_stdev\n0.2541  0.2475  0.2453  0.2531  0.0018  0.0000  0.0000  0.4928  0.0958\n\nMain genome scaffold total:             934\nMain genome contig total:               2703\nMain genome scaffold sequence total:    34.293 MB\nMain genome contig sequence total:      34.231 MB       0.182% gap\nMain genome scaffold N/L50:             53/160.826 KB\nMain genome contig N/L50:               107/72.909 KB\nMain genome scaffold N/L90:             302/15.325 KB\nMain genome contig N/L90:               812/4.643 KB\nMax scaffold length:                    1.222 MB\nMax contig length:                      1.045 MB\nNumber of scaffolds &gt; 50 KB:            152\n% main genome in scaffolds &gt; 50 KB:     77.10%\n\n\nMinimum         Number          Number          Total           Total           Scaffold\nScaffold        of              of              Scaffold        Contig          Contig  \nLength          Scaffolds       Contigs         Length          Length          Coverage\n--------        --------------  --------------  --------------  --------------  --------\n    All                    934           2,703      34,293,018      34,230,627    99.82%\n   1 KB                    934           2,703      34,293,018      34,230,627    99.82%\n 2.5 KB                    744           2,447      33,967,189      33,909,776    99.83%\n   5 KB                    581           2,131      33,379,814      33,328,450    99.85%\n  10 KB                    394           1,585      32,031,121      31,994,989    99.89%\n  25 KB                    236             916      29,564,265      29,545,757    99.94%\n  50 KB                    152             599      26,441,339      26,429,391    99.95%\n 100 KB                     92             415      22,238,822      22,230,143    99.96%\n 250 KB                     31             153      12,606,774      12,603,418    99.97%\n 500 KB                      6              32       4,821,355       4,821,095    99.99%\n   1 MB                      1               2       1,221,548       1,221,538   100.00%\n</code></pre> <p>N50 and L50 in <code>BBMap</code></p> <p>Unfortunately, the N50 and L50 values generated by <code>stats.sh</code> are switched. N50 should be a length and L50 should be a count. The results table below shows the corrected values based on <code>stats.sh</code> outputs.</p> <p>But what we can highlight here is that the statistics for the <code>SPAdes</code> assembly, with short contigs removed, yielded an N50 of 72.5 kbp at the contig level. We will now compute those same statistics from the other assembly options.</p> <p>code</p> <pre><code>stats.sh in=spades_assembly/spades_assembly.fna\n\nstats.sh in=idbaud_assembly/idbaud_assembly.m1000.fna\nstats.sh in=idbaud_assembly/idbaud_assembly.fna\n</code></pre> Assembly N50 (contig) L50 (contig) SPAdes (filtered) 72.9 kbp 107 SPAdes (unfiltered) 72.3 kbp 108 IDBA-UD (filtered) 103.9 kbp 82 IDBA-UD (unfiltered) 96.6 kbp 88"},{"location":"day1/ex5_evaluating_assemblies/#reconstruct-rrna-using-phyloflash-emirge","title":"Reconstruct rRNA using <code>PhyloFlash</code>-<code>EMIRGE</code>","text":"<p>Another method that we can use to obtain taxonomic composition of our metagenomic libraries is to extract/reconstruct the 16S and 18S ribosomal RNA gene. However, modern assemblers often struggle to produce contiguous 16S rRNA genes due to (1) taxa harbouring multiple copies of the gene and (2) repeat regions that short reads cannot span (see here for details). As such, software such as EMIRGE (Miller et al., 2011) and PhyloFlash (Gruber-Vodicka et al., 2020) were developed to reconstruct 16S and 18S rRNA genes. These methods leverage the expansive catalogue of 16S rRNA genes available in databases such as SILVA in order to subset reads and then reconstruct the full-length gene.</p> <p>For this workshop, we will use PhyloFlash to obtain sequences and abundances of 16S rRNA sequences. The reason being that PhyloFlash can also run EMIRGE as part of its routine, compare those reconstructions with sequences generated by PhyloFlash's map-assemble, and produce a set of 16S rRNA gene sequences as well as estimate relative abundances via coverage estimation.</p> <p><code>-lib</code></p> <p>A quirk of PhyloFlash is that all outputs will be placed in the current running directory. The only punctuation allowed is \"-\" and \"_\". Make sure to name your \"LIB\" sensibly so you can track which library you're working with.</p> <p>code</p> <pre><code>nano phyloflash.sl\n</code></pre> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      phyloflash\n#SBATCH --partition     milan\n#SBATCH --time          15:00\n#SBATCH --mem           16G\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x.%j.err\n#SBATCH --output        %x.%j.out\n\nmodule purge\nmodule load Miniconda3/23.10.0-1\nmodule load USEARCH/11.0.667-i86linux32\n\n# Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation/\n\n# Set conda variables\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\n\n# Activate environment\nexport CONDA_ENVS_PATH=/nesi/project/nesi02659/MGSS_2024/resources/tools\nsource activate phyloflash\n\n# Run PhyloFlash per sample pair\nfor r1 in ../3.assembly/sample*_R1.fastq.gz; do\n    phyloFlash.pl -lib $(basename $r1 _R1.fastq.gz) \\\n                  -read1 ${r1} -read2 ${r1/R1/R2} \\\n                  -everything \\\n                  -CPUs $SLURM_CPUS_PER_TASK\ndone\n\nphyloFlash_compare.pl \\\n    --zip $(ls -1 sample*.phyloFlash.tar.gz | paste -sd \",\") \\\n    --task heatmap,barplot,matrix,ntu_table \\\n    --out allsamples_compare\n\nconda deactivate\n</code></pre> <p>A successful run of PhyloFlash (lines 24-29) would have generated the following per-sample outputs:</p> <ul> <li><code>tar.gz</code> archive containing relevant data files</li> <li>An <code>.html</code> page summary</li> <li>A timestamped <code>.log</code> detailing what was done</li> </ul> <p>The reconstructed 16S rRNA sequences are stored in the per-sample archive. We will extract them to explore the data files inside.</p> <p>code</p> <pre><code>for pf in *phyloFlash.tar.gz; do\n    outdir=$(basename ${pf} .tar.gz)\n    mkdir -p ${outdir}\n    tar -xvzf ${pf} -C ${outdir}\ndone\n</code></pre> <p>The reconstructed sequences based on the <code>phyloFlash</code> and <code>EMIRGE</code> workflows are stored in <code>sample?.phyloFlash/sample?.all.final.fasta</code>.</p> <p>In lines 32-34 of our slurm script, we also ran a cross-sample comparison script. This generated 4 files:</p> <ul> <li><code>allsamples_compare.barplot.pdf</code> A relative abundance barplot of identified taxa</li> <li><code>allsamples_compare.heatmap.pdf</code> A heatmap of identified taxa</li> <li><code>allsamples_compare.matrix.tsv</code> A dissimilarity matrix (abundance-weighted, UniFrac-like)</li> <li><code>allsamples_compare.ntu_table.tsv</code> (long/tidy-format table of relative abundance per sample per taxa)</li> </ul> <p>The <code>allsamples_compare.ntu_table.tsv</code> is especially useful if you intend to perform downstream diversity analyses in <code>R</code>.</p>"},{"location":"day1/ex5_evaluating_assemblies/#sequence-taxonomic-classification-using-kraken2","title":"Sequence taxonomic classification using <code>Kraken2</code>","text":"<p>Most, if not all, of the time, we never know the taxonomic composition of our metagenomic reads a priori. For some environments, we can make good guesses (e.g., Prochlorococcus in marine samples, members of the Actinobacteriota in soil samples, various Bacteroidota in the gut microbiome, etc.). Here, we can use <code>Kraken2</code>, a k-mer based taxonomic classifier to help us interrogate the taxonomic composition of our samples. This is helpful if there are targets we might be looking for (e.g. working hypotheses, well characterised microbiome). Furthermore, we can estimate the abundance of the classified taxa using <code>Bracken</code>.</p> <p>code</p> <pre><code>nano kraken2_bracken.sl\n</code></pre> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      kraken2_bracken\n#SBATCH --partition     milan\n#SBATCH --time          30:00\n#SBATCH --mem           80G\n#SBATCH --cpus-per-task 20\n#SBATCH --error         %x.%j.err\n#SBATCH --output        %x.%j.out\n\n# Load modules\nmodule purge\nmodule load \\\n    Kraken2/2.1.3-GCC-11.3.0 \\\n    Bracken/2.7-GCC-11.3.0\n\n# Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation/\n\n# Point to database\nK2DB=/nesi/project/nesi02659/MGSS_2024/resources/databases/k2_standard_20240605\n\n# Create output directory\nmkdir -p read_classification/\n\n# Run Kraken2 and Bracken\nfor r1 in ../3.assembly/sample?_R1.fastq.gz; do\n  # Output basename\n  outbase=read_classification/$(basename ${r1} _R1.fastq.gz)\n  # Taxonomic classification of reads\n  kraken2 --paired --threads $SLURM_CPUS_PER_TASK \\\n          --classified-out ${outbase}#.k2_classified.fq \\\n          --unclassified-out ${outbase}#.k2_unclassified.fq \\\n          --report ${outbase}.k2_report.txt \\\n          --output ${outbase}.k2_out \\\n          --db ${K2DB} \\\n          ${r1} ${r1/R1/R2}\n  # Estimate taxa abundance\n  bracken -d ${K2DB} -i ${outbase}.k2_report.txt -r 100 \\\n          -o ${outbase}.bracken\ndone\n</code></pre> <p>The main output is quite dense and verbose with columns indicated here. The report is more human-readable, with nicely spaced columns that indicate:</p> <ol> <li>Percentage reads mapped to taxon</li> <li>Number of reads mapped to taxon</li> <li>Number of reads directly assigned to taxon</li> <li>Rank of taxon</li> <li>NCBI taxonomy ID</li> <li>Scientific name of taxon</li> </ol> <p>Remember to produce the <code>--report</code> as Bracken bases its estimation on the report</p> <p>The code tells <code>Kraken2</code> that the inputs <code>${r1}</code> and <code>${r1/R1/R2}</code> are paired-end reads. The <code>#</code> after <code>${base}</code> will be replaced with the read orientation (i.e., forward <code>_1</code> or reverse <code>_2</code>).</p> <p>The Bracken outputs provides us with adjusted number (columns <code>added_reads</code> and <code>new_est_reads</code>) and fraction of reads that were assigned to each species identified by Kraken2 for downstream analyses.</p> Database construction for <code>Kraken2</code> and <code>Bracken</code> <p>For this workshop (and most applications), the standard or other extensive pre-built Kraken2 databases are sufficient. However, if you require specific reference sequences to be present, the database building process (outlined here) can be quite time and resource intensive.</p> <p>The Kraken2 database also comes with a few pre-built Bracken databases (filenames look like this: <code>database100mers.kmer_distrib</code>). Note that the \"100mers\" doesn't refer to k-mers (oligonucleotide frequency), but it is the length of the read the database was built for. In this workshop, we've used the 100bp database to estimate abundances for 126bp reads. According to the developers, this is acceptable. As long as the read length of the built database is \\(\\le\\) than the length of reads to be classified, Bracken will provide good enough estimates.</p> <p>If you want to build your own Bracken database based on your library minimum read lengths, you can follow the process here. Take note that you will require the initial sequence and taxonomy files used for building the <code>Kraken2</code> database (not part of the files in the pre-built databases) and these (especially for NCBI nt and bacterial databses) will take a long time to download and lots of memory to build.</p>"},{"location":"day1/ex5_evaluating_assemblies/#optional-evaluating-assemblies-using-metaquast","title":"(Optional) Evaluating assemblies using <code>MetaQUAST</code>","text":"<p>For more genome-informed evaluation of the assembly, we can use the <code>MetaQUAST</code> tool to view our assembled metagenome. This is something of an optional step because, like <code>QUAST</code>, <code>MetaQUAST</code> aligns your assembly against a set of reference genomes. Under normal circumstances we wouldn't know the composition of the metagenome that led to our assembly. In this instance determining the optimal reference genomes for a <code>MetaQUAST</code> evaluation is a bit of a problem. For your own work, the following tools could be used to generate taxonomic summaries of your metagenomes to inform your reference selection:</p> <ol> <li>CLARK (DNA based. k-mer classification)</li> <li>Kaiju (Protein based, BLAST classification)</li> <li>Centrifuge (DNA based, sequence alignment classification)</li> <li>MeTaxa2 or SingleM (DNA based, 16S rRNA recovery and classification)</li> <li>MetaPhlAn2 (DNA based, clade-specific marker gene classification)</li> </ol> <p>A good summary and comparison of these tools (and more) was published by Ye et al..</p> <p>However, since we do know the composition of the original communities used to build this mock metagenome, <code>MetaQUAST</code> will work very well for us today. In your <code>4.evaluation/</code> directory you will find a file called <code>ref_genomes.txt</code>. This file contains the names of the genomes used to build these mock metagenomes. We will provide these as the reference input for <code>MetaQUAST</code>.</p> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      metaquast\n#SBATCH --partition     milan\n#SBATCH --time          00:15:00\n#SBATCH --mem           10GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load module\nmodule purge\nmodule load QUAST/5.0.2-gimkl-2018b\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/4.evaluation\n\n# Run metaquast    \nmetaquast.py --references-list ref_genomes.txt --max-ref-number 21 \\\n             -t $SLURM_CPUS_PER_TASK \\\n             --labels SPAdes,SPAdes.m1000,IDBAUD,IDBAUD.m1000 \\\n             --output-dir metaquast_results/ \\\n             spades_assembly/spades_assembly.fna \\\n             spades_assembly/spades_assembly.m1000.fna \\\n             idbaud_assembly/idbaud_assembly.fna \\\n             idbaud_assembly/idbaud_assembly.m1000.fna\n\n# Create archive for download\ntar -cvzf metaquast_results.tar.gz metaquast_results\n</code></pre> <p>By now, you should be getting familiar enough with the console to understand what most of the parameters here refer to. The one parameter that needs explanation is the <code>--max-ref-number</code> flag, which we have set to 21. This caps the maximum number of reference genomes to be downloaded from NCBI which we do in the interest of speed. Since there are 21 taxa in the file <code>ref_genomes.txt</code> (10 prokaryote species and 11 viruses), <code>MetaQUAST</code> will download one reference genome for each. If we increase  the <code>--max-ref-number</code> flag we will start to get multiple reference genomes per taxa provided which is usually desirable.</p> <p>We will now look at a few interesting assembly comparisons.</p> <p>Viewing HTML in Jupyter Hub</p> <p>The NeSI Jupyter hub does not currently support viewing HTML that require Javascript (even if the browser you are running it in does). To view a basic version of the report, download the report file by navigating to the <code>4.evaluation/</code> directory, right-click <code>metaquast_results.tar.gz</code> and select download. You will need to decompress and extract the archive on your computer. Navigate to the folder with the extracted contents to open the <code>report.html</code>.</p> Brief summaryof assembliesComparison of NGA50between assembliesComparison ofaligned contigsInspection ofunaligned contigs <p></p> <p></p> <p></p> <p></p>"},{"location":"day2/ex10.1_viruses/","title":"Identifying viral contigs in metagenomic data","text":"<p>Objectives</p> <ul> <li>Identifying viral contigs</li> <li>Identifying viral contigs using <code>VirSorter2</code></li> </ul>"},{"location":"day2/ex10.1_viruses/#identifying-viral-contigs","title":"Identifying viral contigs","text":"<p>Viral metagenomics is a rapidly progressing field, and new software are constantly being developed and released each year that aim to better identify and characterise viral genomic sequences from assembled metagenomic sequence reads. </p> <p>Currently, the most commonly used methods are <code>VirSorter2</code>, <code>VIBRANT</code>, and <code>VirFinder</code> (or the machine learning implementation of this, <code>DeepVirFinder</code>). A number of recent studies use one of these tools or a combination of several at once.</p> <code>VirSorter2</code><code>VIBRANT</code><code>DeepVirFinder</code> <p>Uses a predicted protein homology reference database-based approach, together with searching for a number of pre-defined metrics based on known viral genomic features. <code>VirSorter2</code> includes dsDNAphage, ssDNA, and RNA viruses, and the viral groups Nucleocytoviricota and lavidaviridae.* </p> <p>More info </p> <ul> <li>VirSorter2 GitHub</li> <li>Guo et al. (2021) VirSorter2: a multi-classifier, expert-guided approach to detect diverse DNA and RNA viruses</li> </ul> <p>Uses a machine learning approach based on protein similarity (non-reference-based similarity searches with multiple HMM sets), and is in principle applicable to bacterial and archaeal DNA and RNA viruses, integrated proviruses (which are excised from contigs by <code>VIBRANT</code>), and eukaryotic viruses. </p> <p>More info </p> <ul> <li>VIBRANT GitHub</li> <li>Kieft, Zhou, and Anantharaman (2020) VIBRANT: automated recovery, annotation and curation of microbial viruses, and evaluation of viral community function from genomic sequences</li> </ul> <p>Uses a machine learning based approach based on k-mer frequencies. Having developed a database of the differences in k-mer frequencies between prokaryote and viral genomes, VirFinder examines assembled contigs and identifies whether their k-mer frequencies are comparable to known viruses in the database, using this to predict viral genomic sequence. This method has some limitations based on the viruses that were included when building the database (bacterial DNA viruses, but very few archaeal viruses, and, at least in some versions of the software, no eukaryotic viruses). However, tools are also provided to build your own database should you wish to develop an expanded one. Due to its distinctive k-mer frequency-based approach, VirFinder may also have the capability of identifying some novel viruses overlooked by tools such as VIBRANT or VirSorter.</p> <p>More info </p> <ul> <li>DeepVirFinder GitHub</li> </ul>"},{"location":"day2/ex10.1_viruses/#identifying-viral-contigs-using-virsorter2","title":"Identifying viral contigs using <code>VirSorter2</code>","text":"<p>For this exercise, we will use <code>VirSorter2</code> to identify viral contigs from our assembled contigs. We can also use <code>VirSorter2</code> to prepare files for later use with the gene annotation tool <code>DRAM-v</code>, which we'll run later in the day.</p>"},{"location":"day2/ex10.1_viruses/#checking-quality-and-estimate-completeness-of-the-viral-contigs-via-checkv","title":"Checking quality and estimate completeness of the viral contigs via <code>CheckV</code>","text":"<p><code>CheckV</code> was developed as an analogue to <code>CheckM</code>. <code>CheckV</code> first performs a 'contaminating sequence' trim, removing any retained (prokaryote) host sequence on the end of contigs with integrated prophage, and then assesses the quality and completeness of the assembled viral contigs. The quality of the contigs are also categoriesed based on the recently developed Minimum Information about an Unclutivated Virus Genome (MIUViG) standards for reporting sequences of unclutivated virus geneomes (such as those recovered from metagenomic sequencing data). The MIUViG were developed as an extension of the Minimum Information about any (x) Sequence (MIxS) standards, which include, among others, standards for Metagenome-Assembled Genomes (MIMAG).</p>"},{"location":"day2/ex10.1_viruses/#run-virsorter2-and-checkv","title":"Run <code>VirSorter2</code> and <code>CheckV</code>","text":"<p>These exercises will take place in the <code>7.viruses/</code> folder.</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n</code></pre> <p>For <code>VirSorter2</code>, we will input the assembled contigs from the <code>SPAdes</code> assembly we performed earlier. These assembly files have been copied to <code>7.viruses/spades_assembly/</code> for this exercise.</p> <p>We will then run <code>CheckV</code> in the same script, providing the FASTA file of viral contigs output by <code>VirSorter2</code> as input (<code>final-viral-combined.fa</code>).</p> <p>Create a script named <code>VirSorter2_and_checkv.sl</code></p> <pre><code>nano VirSorter2_and_checkv.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      VirSorter2_and_checkv\n#SBATCH --partition     milan\n#SBATCH --time          05:00:00\n#SBATCH --mem           2GB\n#SBATCH --cpus-per-task 28\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n\n# VirSorter2\n## Load modules\nmodule purge\nmodule unload XALT\nmodule load VirSorter/2.2.3-gimkl-2020a-Python-3.8.2\n\n## Output directory\nmkdir -p VirSorter2\n\n## Run VirSorter2\nvirsorter run -j $SLURM_CPUS_PER_TASK \\\n  -i spades_assembly/spades_assembly.m1000.fna \\\n  -d /nesi/project/nesi02659/MGSS_2024/resources/databases/virsorter2_20210909 \\\n  --min-score 0.7 --include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae \\\n  --prep-for-dramv \\\n  -l mgss -w VirSorter2 \\\n  --tmpdir ${SLURM_JOB_ID}.tmp --rm-tmpdir \\\n  all \\\n  --config LOCAL_SCRATCH=${TMPDIR:-/tmp}\n\n# CheckV\n## Load module\nmodule purge\nmodule load CheckV/1.0.1-gimkl-2022a-Python-3.10.5\n\n## Output directory\nmkdir -p checkv_out\n\n## Run CheckV\ncheckv end_to_end VirSorter2/mgss-final-viral-combined.fa checkv_out/ -t $SLURM_CPUS_PER_TASK\n</code></pre> <p><code>module unload XALT</code></p> <p>For the <code>VirSorter/2.2.3-gimkl-2020a-Python-3.8.2</code> NeSI module to work properly, we must also include <code>module unload XALT</code> in the script above.</p> <p><code>VirSorter2</code> parameters</p> <p>The key parameters you may want to consider altering for your own work are <code>--min-score</code> and <code>--include-groups</code>. For today's excersice we will include all available groups (<code>--include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae</code>), and will set the min-score to 0.7. You can experiment with this value for your own data (see the Virsorter2 github page for more information).</p> <p>Submit the script as a slurm job</p> <pre><code>sbatch VirSorter2_and_checkv.sl\n</code></pre> <p><code>VirSorter2</code> for your own work</p> <p>The required databases for <code>VirSorter2</code> are not loaded with the NeSI module. For your own work, you will need to first download these databases and provide the path to the <code>-d</code> flag. For today's workshop this is already set up.</p>"},{"location":"day2/ex6_initial_binning/","title":"Introduction to binning","text":"<p>Objectives</p> <ul> <li>Remove short contigs from the data set</li> <li>Obtain coverage profiles for assembled contigs via read mapping</li> <li>Optional: Read mapping using a slurm array</li> </ul>"},{"location":"day2/ex6_initial_binning/#remove-short-contigs-from-the-data-set","title":"Remove short contigs from the data set","text":"<p>Ideally, we do not want to be creating bins from all of the assembled contigs, as there is often a long tail of contigs which are only several k-mers long. These have little biological meaning, as they are too short for robust gene annotation, and they can introduce a significant degree of noise in the clustering algorithms used for binning. We therefore identify a suitable threshold for a minimum length of contigs to be considered for binning.</p> <p>We have already done this in the previous exercise so we could either use the existing filtering at 1,000 bp in length, or move to something stricter. Most binning tools have a default cut-off for minimum contig size - <code>MetaBAT</code> uses a default minimum of 2,500 bp, and recommends at least 1,500 bp. By contrast, <code>MaxBin</code> sets the minimum length at 1,000 bp.</p>"},{"location":"day2/ex6_initial_binning/#obtain-coverage-profiles-for-assembled-contigs-via-read-mapping","title":"Obtain coverage profiles for assembled contigs via read mapping","text":"<p>Binning is done using a combination of information encoded in the composition and coverage of the assembled contigs. Composition refers to k-mer (usually tetranucleotide) frequency profiles of the contigs, which are generally conserved within a genome. By contrast, coverage is a reflection of the abundance of the contigs in the assembly. Organisms which are more abundant will contribute more genomic material to the metagenome, and hence their DNA will be, on average, more abundant in the sample. When binning, we can look for pieces of DNA which are not assembled together, but have similar composition and occur at approximately equal abundances in the sample to identify contigs which likely originate in the same genome.</p> <p>The composition of the contigs is calculated by the binning tool at run time, but to obtain coverage information we must map our unassembled reads from each sample against the assembly to generate the differential abundance profiles for each contig. This is achieved using <code>bowtie2</code> to map the reads against the assembly, then <code>samtools</code> to sort and compress the resulting file.</p>"},{"location":"day2/ex6_initial_binning/#create-a-mapping-index","title":"Create a mapping index","text":"<p>Before we can map reads, we need to create a <code>bowtie2</code> index file from the assembly, for use in read mapping. Navigate into the <code>5.binning/</code> folder to begin.</p> <p>code</p> <pre><code># Load modules\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0\n\n# Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Build index\nbowtie2-build spades_assembly/spades_assembly.m1000.fna spades_assembly/bw_spades\n</code></pre> <p>If you look inside the <code>spades_assembly/</code> folder you will now see the following:</p> <p>code</p> <pre><code>ls spades_assembly/\n</code></pre> <p>Terminal output</p> <pre><code>bw_spades.1.bt2  bw_spades.3.bt2  bw_spades.rev.1.bt2  spades_assembly.m1000.fna\nbw_spades.2.bt2  bw_spades.4.bt2  bw_spades.rev.2.bt2\n</code></pre> <p>These files ending in .bt2 are the index files for <code>bowtie2</code>, and are specific to this tool. If you wish to map using an alternate tool (for example <code>bowtie</code> or <code>BBMap</code>) you will need to create index/database files using these programs.</p> <p>Generally speaking, we don't need to know the names of the index files, as they are simply referred to be the output name we specified (bw_spades) when running <code>bowtie2</code>.</p>"},{"location":"day2/ex6_initial_binning/#mapping-the-reads","title":"Mapping the reads","text":"<p>We will create a slurm script to perform the mapping steps, as these benefit greatly from the multithreaded capacity of NeSI and we will use a for loop to iterate over each set of reads to simplify our script.</p> <p>The full script is provided here, and we will discuss it below.</p> <p>Open a new script using nano:</p> <p>Create script named <code>spades_mapping.sl</code></p> <pre><code>nano spades_mapping.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_mapping\n#SBATCH --partition     milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0 SAMtools/1.19-GCC-12.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Step 1\nfor i in sample1 sample2 sample3 sample4;\ndo\n\n  # Step 2\n  bowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n          -x spades_assembly/bw_spades \\\n          -1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \\\n          -S ${i}.sam\n\n  # Step 3\n  samtools sort -@ $SLURM_CPUS_PER_TASK -o ${i}.bam ${i}.sam\n\ndone\n</code></pre> <p>Submit the script using <code>sbatch</code></p> <pre><code>sbatch spades_mapping.sl\n</code></pre>"},{"location":"day2/ex6_initial_binning/#step-1-loop-through-the-sample-files","title":"Step 1 - Loop through the sample files","text":"<p>Since we just want to perform the same set of operations on each file, we can use a for loop to repeat each operation on a set of files. The structure of the loop, and use of variables was covered on the first day.</p> <p>For large sets of files, it can be beneficial to use a slurm array to send the jobs out to different nodes and distribute the process across many independent jobs. An example of how we could modify the above script is given at the bottom of this exercise, but is not necessary for the purposes of this workshop.</p>"},{"location":"day2/ex6_initial_binning/#step-2-map-the-reads-using-bowtie2","title":"Step 2 - Map the reads using bowtie2","text":"<p>This is performed using the following parameters</p> Parameter Function <code>--minins</code> Minimum insert size, determines the minimum distance between the start of each read pair <code>--maxins</code> Maximum insert size, determines the maximum distance between the start of each read pair <code>--threads</code> Number of threads to use in read mapping <code>--sensitive</code> Specifies where we want to be positioned in the trade-off between speed and sensitivity. See the manual for more information <code>-x</code> The base name of our assembly index. Should be exactly the same as what was specified when running <code>bowtie2-build</code> <code>-1</code> / <code>-2</code> The forward and reverse read pairs to map to the assembly <code>-S</code> Name of the output file, to be written in sam format"},{"location":"day2/ex6_initial_binning/#step-3-sort-and-compress-results","title":"Step 3 - Sort and compress results","text":"<p>The default output format for most mapping tools is the Sequence Alignment/Map (sam) format. This is a compact text representation of where each short read sits in the contigs. You can view this file using any text viewer, although owing to the file size <code>less</code> is a good idea.</p> <p>Generally I wouldn't bother with this - there is a lot of information in here and unless you are looking to extract specific information from the alignment directly, this is just an intermediate file in our workflow. In order to save disk space, and prepare the file for downstream analysis we now perform two final steps:</p> <ol> <li>Sort the mapping information</li> <li>Compress the sam file into its binary equivalent, bam</li> </ol> <p>Which is achieved with the following parameters</p> Parameter Function <code>sort</code> Subcommand for <code>samtools</code> to invoke the <code>sort</code> operation <code>-@</code> Number of threads to use for sorting and compressing <code>-o</code> Output file name. When we specify the bam extension <code>samtools</code> automatically compresses the output <p>Do not open a <code>.bam</code> file, your terminal might crash!</p> <p>Compressing the sam files to bam format is an important step as sam files can be massive. It is also helpful to sort the mapping information so that reads mapped to a contig are listed in order of their start position. For example</p> <p>Unsorted reads</p> <pre><code>Ref: REFERENCECONTIG\nMap: --------ECONT--\nMap: REFE-----------\nMap: --FERENCECO----\nMap: -----------NTIG\n</code></pre> <p>Sorted reads</p> <pre><code>Ref: REFERENCECONTIG\nMap: REFE-----------\nMap: --FERENCECO----\nMap: --------ECONT--\nMap: -----------NTIG\n</code></pre> <p>Reads will initially be mapped in an unsorted order, as they are added to the sam file in more or less the same order as they are encountered in the original fastQ files.</p> <p>Sorting the mapping information is an important prerequisite for performing certain downstream processes. Not every tool we use requires reads to be sorted, but it can be frustrating having to debug the instances where read sorting matters, so we typically just get it done as soon as possible and then we don't have to worry about it again.</p>"},{"location":"day2/ex6_initial_binning/#optional-read-mapping-using-an-array","title":"(Optional) Read mapping using an array","text":"<p>If you have a large number of files to process, it might be worth using a slurm array to distribute your individual mapping jobs across many separate nodes. An example script for how to perform this is given below. We do not need to use an array for read mapping in this workshop, but we will revisit array jobs in further lessons.</p> <p>Open a new script using nano:</p> <p>Create script named <code>spades_mapping_array.sl</code></p> <pre><code>nano spades_mapping_array.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_mapping_array\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           20GB\n#SBATCH --array         0-3\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n\n# Load modules\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0 SAMtools/1.19-GCC-12.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Load the sample names into a bash array\nsamples=(sample1 sample2 sample3 sample4)\n\n# Run Bowtie2 and SAMTools, using the SLURM_ARRAY_TASK_ID variable to\n# identify which position in the `samples` array to use\nbowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n        -x spades_assembly/bw_spades \\\n        -1 ../3.assembly/${samples[ $SLURM_ARRAY_TASK_ID ]}_R1.fastq.gz \\\n        -2 ../3.assembly/${samples[ $SLURM_ARRAY_TASK_ID ]}_R2.fastq.gz \\\n        -S ${samples[ $SLURM_ARRAY_TASK_ID ]}_a.sam\n\nsamtools sort -@ $SLURM_CPUS_PER_TASK \\\n              -o ${samples[ $SLURM_ARRAY_TASK_ID ]}_a.bam \\\n              ${samples[ $SLURM_ARRAY_TASK_ID ]}_a.sam\n</code></pre> <p>Submit the job to slurm</p> <pre><code>sbatch spades_mapping_array.sl\n</code></pre>"},{"location":"day2/ex7_initial_binning/","title":"Binning with multiple tools","text":"<p>Objectives</p> <ul> <li>Overview</li> <li>Create initial bins using <code>MetaBAT</code></li> <li>Create initial bins using <code>MaxBin</code></li> </ul>"},{"location":"day2/ex7_initial_binning/#overview","title":"Overview","text":"<p>With the mapping information computed in the last exercise, we can now perform binning. There are a multitude of good binning tools currently published, and each have their strengths and weaknesses. As there is no best tool for binning, the current strategy for binning is to use a number of different tools on your data, then use the tool <code>DAS_Tool</code> to evaluate all potential outcomes and define the best set of bins across all tools used.</p> <p>In our own workflow, we use the tools <code>MetaBAT</code>, <code>MaxBin</code>, and <code>CONCOCT</code> for binning, but there are many alternatives that are equally viable. In the interests of time, we are only going to demonstrate the first two tools. However, we recommend that you experiment with some of the following tools when conducting your own research.</p> <ol> <li>Tetra-ESOM</li> <li>VAMB</li> </ol>"},{"location":"day2/ex7_initial_binning/#metabat","title":"<code>MetaBAT</code>","text":"<p><code>MetaBAT</code> binning occurs in two steps. First, the bam files from the last exercise are parsed into a tab-delimited table of the average coverage depth and variance per sample mapped. Binning is then performed using this table.</p> <p>The .bam files can be passed in via either a user-defined order, or using wildcards.</p> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n</code></pre> <p>code</p> <pre><code>module purge\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# Manual specification of files\njgi_summarize_bam_contig_depths --outputDepth metabat.txt sample1.bam sample2.bam sample3.bam sample4.bam\n\n# Wildcard\njgi_summarize_bam_contig_depths --outputDepth metabat.txt sample?.bam\n</code></pre> Bash wildcards <p>The question mark <code>?</code> is also a Bash wildcard. The difference between <code>?</code> and <code>*</code> is that:</p> <ul> <li><code>?</code> represents any character once</li> <li><code>*</code> represents any character zero or more times</li> </ul> <p>The reason we use a <code>?</code> here instead is because if you did the SPAdes contig mapping with the <code>for</code> loop AND with the array, you will capture each sample twice (i.e., <code>sample1.bam</code> and <code>sample1_a.bam</code>) with the asterisk <code>*</code>. For this exercise, we really want each sample file to be counted only once.</p> <p>Both give the same result, although the sample order may vary.</p> <p>We can then pass the table <code>metabat.txt</code> into the <code>MetaBAT</code> binning tool.</p> <p>Before we proceed, note that when you run <code>MetaBAT</code> on NeSI you will see the text <code>vGIT-NOTFOUND</code> appear in your command line. This has no impact on the performance of the tool.</p> <p>Run MetaBAT</p> <pre><code>metabat2 -t 2 -m 1500 \\\n         -i spades_assembly/spades_assembly.m1000.fna \\\n         -a metabat.txt \\\n         -o metabat/metabat\n</code></pre> <p>Note here that we are specifying a minimum contig size of 1,500 bp, which is the lower limit allowed by the authors of <code>MetaBAT</code>. This is larger than the minimum threshold of 1,000 bp when we filtered the assembly, which means there are some assembled contigs which cannot be binned. Consider the choice of this parameter and your initial contig filtering carefully when binning your own data.</p> <p>When specifying the output file, notice that we pass both a folder path (metabat/) and file name (metabat). The reason I do this is that <code>MetaBAT</code> writes its output files using the pattern</p> <p><code>[USER VALUE].[BIN NUMBER].fa</code></p> <p>If we only provided the path, without a file name prefix, <code>MetaBAT</code> would create output like the following:</p> <pre><code>metabat/.1.fa\nmetabat/.2.fa\nmetabat/.3.fa\n</code></pre> <p>The problem with this is that on Linux systems, prefixing a file or folder name with a '.' character means the the file is hidden. This can lead to a lot of confusion when your binning job completes successfully but no files are visible!</p>"},{"location":"day2/ex7_initial_binning/#maxbin","title":"<code>MaxBin</code>","text":"<p>Like <code>MetaBAT</code>, <code>MaxBin</code> requires a text representation of the coverage information for binning. Luckily, we can be sneaky here and just reformat the <code>metabat.txt</code> file into the format expected by <code>MaxBin</code>. We use <code>cut</code> to select only the columns of interest, which are the contigName and coverage columns, but not the contigLen, totalAvgDepth, or variance columns.</p> <p>We can inspect the <code>metabat.txt</code> file with <code>head</code> or <code>less</code> to identify the correct column indices for <code>cut</code>.</p> <p>code</p> <pre><code>less metabat.txt\n</code></pre> <p>Terminal output: Snapshot of <code>metabat.txt</code></p> <pre><code>contigName      contigLen       totalAvgDepth   sample1.bam     sample1.bam-var sample2.bam     sample2.bam-var sample3.bam     sample3.bam-var sample4.bam     sample4.bam-var\nNODE_1_length_1221431_cov_0.752208      1.22143e+06     18.9178 3.55592 3.51379 10.865  11.3668 4.49688 4.51001 0       0\nNODE_2_length_871377_cov_1.172283       871377  29.4909 6.14223 6.1249  2.81105 2.77014 20.5376 20.6339 0       0\nNODE_3_length_835083_cov_0.372452       835083  9.2898  1.79405 1.74475 4.2584  4.19122 3.23735 3.23068 0       0\nNODE_4_length_803085_cov_0.518929       803085  13.054  2.63945 2.65518 2.53475 2.48676 7.87984 8.38717 0       0\nNODE_5_length_686960_cov_0.527949       686960  13.2164 2.64157 2.98377 2.59676 2.90338 7.97809 10.8339 0       0\nNODE_6_length_607162_cov_1.000759       607162  25.0853 16.7701 17.576  2.759   2.74899 5.55618 5.69041 0       0\n</code></pre> <p>Create <code>maxbin.txt</code> based on <code>metabat.txt</code></p> <pre><code>cut -f1,4,6,8,10 metabat.txt &gt; maxbin.txt\n</code></pre> <p>Generally speaking, this pattern of first, fourth, then [n + 2]<sup>th</sup> should work for any number of mapping files, although we always recommend that you check and confirm before you continue.</p> <p>This table is then passed to <code>MaxBin</code>. Unlike the case with <code>MetaBAT</code>, if we want to direct the output files into a folder, we must create that folder in advance.</p> <p>Create a new script to submit as a slurm job</p> <pre><code>nano maxbin_clustering.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      maxbin_clustering\n#SBATCH --time          00:05:00\n#SBATCH --partition     milan\n#SBATCH --mem           10GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load MaxBin/2.2.7-GCC-11.3.0-Perl-5.34.1\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Output directory\nmkdir -p maxbin/\n\n# Run MaxBin\nrun_MaxBin.pl -thread $SLURM_CPUS_PER_TASK -min_contig_length 1500 \\\n              -contig spades_assembly/spades_assembly.m1000.fna \\\n              -abund maxbin.txt \\\n              -out maxbin/maxbin\n</code></pre> <p>Submit the script as a slurm job</p> <pre><code>sbatch maxbin_clustering.sl\n</code></pre> <p><code>MaxBin</code> runtime</p> <p>This will take a bit longer to complete, as <code>MaxBin</code> uses gene prediction tools to identify the ideal contigs to use as the start of each bin.</p>"},{"location":"day2/ex8_bin_dereplication/","title":"Bin dereplication","text":"<p>Objectives</p> <ul> <li>Bin dereplication using <code>DAS_Tool</code> - Creating input tables</li> <li>Bin dereplication using <code>DAS_Tool</code></li> <li>Evaluating bins using <code>CheckM</code></li> <li>Dereplicating bins across multiple assemblies</li> </ul>"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool-creating-input-tables","title":"Bin dereplication using <code>DAS_Tool</code> - Creating input tables","text":"<p>As we discussed in the previous exercise, we have now generated two sets of bins from the same single assembly. With this mock data set we can see that <code>MetaBAT</code> recovered 12 bins, while <code>MaxBin</code> recovered 10. Note that we are aiming to recover prokaryote genomes using these binning tools (we will use other tools to investigate viral genomes in later exercises), and 10 bacterial and archaeal genomes were used in the creation of this mock community. If our mock community only contained these 10 prokaryote genomes and omitted the viral genomes, we shouldn't expect to see more than 10 bins total. In our case here, these tools have likely recovered 10 bins of the same genomes. The additional two bins identified by <code>MetaBAT</code> may be the result of noise introduced into the binning process by the viral contigs included in the data. Furthermore, it is not clear which tool has done a better job of recruiting contigs to each bin - we very rarely expect to see the complete genome recovered from these kinds of data, so while it is probably the case that while an equivalent bin is present in the <code>MetaBAT</code> and <code>MaxBin</code> outputs, they will likely be of differing quality.</p> <p><code>DAS_Tool</code> is a program designed to analyse the bins in each of our binning sets and determine where these equivalent pairs (or triplets if we use three binners) exist and return the 'best' one. <code>DAS_Tool</code> does not use the actual bins, but a set of text files that link contigs to their corresponding bins in each of the bin sets. We can produce these files using <code>bash</code>.</p> <p>For this exercise, we will continue working in the <code>5.binning/</code> directory.</p>"},{"location":"day2/ex8_bin_dereplication/#create-contigbin-tables-metabat","title":"Create contig/bin tables - <code>MetaBAT</code>","text":"<p>For each of our binning tools, we need to extract the contigs assigned to each bin and create a single file that reports these as</p> <pre><code>Contig[tab]Bin\n</code></pre> <p>This can be done with a bit of <code>bash</code> scripting. There's quite a bit going on here, so we'll provide the full command, and then a step-by-step explanation of what's happening.</p> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n</code></pre> <p>Create <code>metabat_associations.txt</code></p> <pre><code>for bin_path in metabat/*.fa; do\n    bin_name=$(basename ${bin_path} .fa)\n\n    grep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; metabat_associations.txt\ndone\n</code></pre> <p>You can check the contents of this file using <code>less</code> or <code>head</code>:</p> <p>Inspect <code>metabat_association.txt</code></p> <pre><code>head -n 5 metabat_associations.txt\n</code></pre> <p>Terminal output</p> <pre><code>NODE_44_length_186282_cov_0.255381      metabat.10\nNODE_51_length_159085_cov_0.252240      metabat.10\nNODE_68_length_132434_cov_0.251275      metabat.10\nNODE_71_length_129650_cov_0.256954      metabat.10\nNODE_73_length_121437_cov_0.254006      metabat.10\n</code></pre> <p>We will now walk through the content of the \"Create <code>metabat_associations.txt</code>\" code block above, breaking apart each individual step.</p> <p>Line 1: Initiate a <code>for</code> loop</p> <pre><code>for bin_path in metabat/*.fa; do\n</code></pre> <p>This is the initial loop, returning each file in the <code>metabat/</code> folder that ends with the file extension .fa.</p> <p>Line 2: Remove path prefix</p> <pre><code>    bin_name=$(basename ${bin_path} .fa)\n</code></pre> <p>As we have previously seen, the <code>basename</code> command removes the path information from the input variable <code>bin_path</code> (i.e. - <code>metabat/metabat.1.fa</code> becomes <code>metabat.1.fa</code>) and assigns it to a new variable, <code>bin_name</code>. As an optional additional parameter, we can pass extra pieces of text to be removed from the variable, in this case the .fa extension.</p> <p>Line 4: Subset information to create and update <code>metabat_associations.txt</code></p> <pre><code>    grep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; metabat_associations.txt\n</code></pre> <p>We will break this line down step-by-step:</p> Command Description <code>grep \"&gt;\" ${bin_path}</code> The <code>grep</code> command searches the input file <code>bin_path</code> for lines containing the <code>&gt;</code> character, which is the fastA demarcation for a sequence name. <code>sed 's/&gt;//g'</code> The first <code>sed</code> command replaces the <code>&gt;</code> character with the empty character <code>''</code>, as we do not need this character in the final file. <code>sed \"s/$/\\t${bin_name}/g\"</code> We now use <code>sed</code> again, this time to replace the <code>$</code> character. In many command line tools and software environments (including <code>R</code> and <code>python</code>) the characters <code>^</code> and <code>$</code> are used as shortcuts for the beginning and ending of a line, respectively. By using the character in this way, we are telling <code>sed</code> to replace the end-of-line with the text <code>\\t${bin_name}</code>. <code>sed</code> will parse this text to mean the tab character followed by the content of the <code>bin_name</code> variable. The nature of <code>sed</code> is that it will automatically insert a new end-of-line. <code>&gt;&gt; metabat_associations.txt</code> This is similar to the stdout redirection we have previously used, but the double use of the <code>&gt;</code> character means that we are appending our text to the end of the file <code>metabat_associations.txt</code>. Because we are looping through several files in this exercise, if we were to use the single <code>&gt;</code> character then on each new fastA file read, the content of <code>metabat_associations.txt</code> would be replaced.It is important to note that because we are appending to the file, not replacing the contents, if you make a mistake in the command and need to re-run it, you will need to explicitly delete the <code>metabat_associations.txt</code> file using <code>rm</code>, otherwise your new (correct) output will be pasted to the end of your old (incorrect) output."},{"location":"day2/ex8_bin_dereplication/#create-contigbin-tables-maxbin","title":"Create contig/bin tables - <code>MaxBin</code>","text":"<p>The process for creating the <code>MaxBin</code> table is basically the same, we just need to change the file extension, as <code>MaxBin</code> writes outputs using the .fasta suffix rather than the .fa one.</p> <p>Create <code>maxbin_associations.txt</code></p> <pre><code>for bin_path in maxbin/*.fasta;\ndo\n    bin_name=$(basename ${bin_path} .fasta)\n    grep \"&gt;\" ${bin_path} | sed 's/&gt;//g' | sed \"s/$/\\t${bin_name}/g\" &gt;&gt; maxbin_associations.txt\ndone\n</code></pre> <p>Warning for unbinned contigs</p> <p>Both <code>MetaBAT</code> and <code>MaxBin</code> have the option to output unbinned contigs after binning completes. We have not used that parameter here, but if you do choose to enable it you will end up with another fastA file in your output directory which you will need to avoid in the loops for creating <code>DAS_Tool</code> tables.</p>"},{"location":"day2/ex8_bin_dereplication/#bin-dereplication-using-das_tool","title":"Bin dereplication using <code>DAS_Tool</code>","text":"<p>We are now ready to run <code>DAS_Tool</code>. This can be done from the command line, as it does not take a particularly long time to run for this data set.</p> <p>Run <code>DAS_Tool</code></p> <pre><code># Remove modules to ensure a clean environment\nmodule purge\n\n# Load DAS Tool\nmodule load DAS_Tool/1.1.5-gimkl-2022a-R-4.2.1\n\n# Create DAS_Tool output directory\nmkdir -p dastool_out/\n\n# Run DAS_Tool\nDAS_Tool -i metabat_associations.txt,maxbin_associations.txt \\\n         -l MetaBAT,MaxBin \\\n         -t 2 --write_bins --search_engine diamond \\\n         -c spades_assembly/spades_assembly.m1000.fna \\\n         -o dastool_out/\n</code></pre> <p>Terminal output</p> <pre><code>DAS Tool 1.1.5 \nAnalyzing assembly \nPredicting genes \nAnnotating single copy genes using diamond \nDereplicating, aggregating, and scoring bins \nWriting bins\n</code></pre> <p>As usual, we will break down the parameters:</p> Parameter Description <code>-i</code> A comma-separated list of the contig/bin files we wish to process <code>-l</code> A comma-separated list of the binning tools used <code>-t</code> Number of threads to use <code>--write_bins</code> An argument telling <code>DAS_Tool</code> whether or not to write out a new set of binsThis is recommended, because <code>DAS_Tool</code> can create slices of old bins based on marker composition (see the paper for details) <code>--search_engine diamond</code> Specify whether to use <code>usearch</code>, <code>diamond</code>, or <code>BLAST</code> as the alignment tool for comparing gene sequences <code>-c</code> Path to the assembly used in binning <code>-o</code> Output directory for all files <p>When <code>DAS_Tool</code> has completed, we will have a final set of bins located in the folder path <code>dastool_out/_DASTool_bins</code>. Have a look at the output and see which bins made it to the final selection. Did a single binning tool pick the best bins, or are the results a split between <code>MetaBAT</code> and <code>MaxBin</code>?</p>"},{"location":"day2/ex8_bin_dereplication/#evaluating-bins-using-checkm","title":"Evaluating bins using <code>CheckM</code>","text":"<p>Now that we have our dereplicated set of bins, it is a good idea to determine estimates of their completeness (how much of the genome was recovered) and contamination (how many contigs we believe have been incorrectly assigned to the bin). For organisms that lack a reference genome there is not definitive way to do this, but the tool <code>CheckM</code> provides a robust estimate for these statistics by searching each of your bins for a number of highly conserved, single copy genes. The number of markers depends on whether or not you are working with bacterial (120 markers) or archaeal (122 markers) genomes, but <code>CheckM</code> is able to determine which set is more appropriate for each of your bins as it runs.</p> <p>There are several characteristics of the <code>CheckM</code> marker set worth noting:</p> <p>Highly conserved, single copy markers</p> <p>The marker sets used in <code>CheckM</code> were chosen because they are present in at least 95% of bacterial/archaeal genomes, and are single copy in \u226597% genomes tested. This means that if a gene is missing from a genome, it is likely due to incompleteness in either the original assembly or the binning approach. Similarly, if a marker is observed more than once in a bin it is likely the result of over-clustering of the data.</p> <p>Genes are considered as co-located clusters</p> <p>Rather than test the raw presence/absence of genes in the marker sets, the genes are organised into operon-like structures where genes known to be co-located are placed together. This is advantageous for two reasons</p> <ol> <li>These co-located groups are distributed around the prokaryotic genome, so estimates are not biased by lucky/unlucky recovery of a gene hotspot</li> <li><code>CheckM</code> can account for how complete each individual gene cluster is, rather than just whether or not genes are present</li> </ol> <p>Lineage-specific duplications and losses can be identified</p> <p>As part of determining the correct marker set to use for each bin (bacterial or archaeal), <code>CheckM</code> uses a set of 43 conserved prokaryotic markers to insert each bin into a guide tree to estimate the phylogeny of the bin. There are several lineages which are known to have lost particular markers, or to have acquired a additional copies, and if <code>CheckM</code> places a bin into one of these lineages it can adjust its completeness/contamination estimates accordingly.</p> <p>This process isn't perfect, however, and we will discuss some times when you might need to create your own marker set in the next session.</p> <p>We will need to run <code>CheckM</code> under a slurm script. This is because the tree placement process requires a large amount of memory to perform, independently of the size of your data set. A basic script for submitting a <code>CheckM</code> job would be as follows:</p> <p>Create a new script</p> <p>Create script named <code>checkm.sl</code></p> <pre><code>nano checkm.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      CheckM\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load CheckM/1.2.3-foss-2023a-Python-3.11.6\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\n# Run CheckM\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK --pplacer_threads $SLURM_CPUS_PER_TASK \\\n                  -x fa --tab_table -f checkm.txt \\\n                  dastool_out/_DASTool_bins/ checkm_out/\n</code></pre> <p>Submit the script as a slurm job</p> <pre><code>sbatch checkm.sl\n</code></pre> <p>The breakdown of parameters is as follows</p> Parameter Function <code>lineage_wf</code> Specifies with mode of <code>CheckM</code> to run. This is the most common to use, but several others exist <code>-t</code> Number of threads to use for the initial marker gene detection and clustering <code>--pplacer_threads</code> Number of threads to use when inserting bins into the guide tree <code>-x</code> The fastA file extension to look for in the input folder. Default is .fna <code>--tab_table</code> If this parameter is present, a summary table will be written for the <code>CheckM</code> run <code>-f</code> The name of the file for the summary <code>dastool/_DASTool_bins/</code> The location of the bins to test <code>checkm_out/</code> The location to write intermediate and output files <p>Increasing <code>--pplacer_threads</code></p> <p>Increasing this parameter results in a linear increase in memory requirement - seting it to 10 means that <code>CheckM</code> will need about 10 times more memory than with a single thread</p> <p>When your job completes, we will download the summary file and examine it.</p> <p>CheckM2</p> <p>An independent update to CheckM was made in late 2022 in the form of a new software CheckM2 (see publication). It uses machine learning to estimate completeness and contamination. Computationally, it uses less resources than CheckM and can be used on putative genomes with reduced genomes or unusual biology. Keep in mind that because both software use different methods for assessing genome completeness, the output statistics will be different (see discussion here for a case example) and it does not estimate strain heterogeneity.</p> <p>If you would like to try it out, here's a script analogous to that we ran using CheckM:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      CheckM2\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load CheckM2/1.0.1-Miniconda3\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/5.binning/\n\ncheckm2 predict \\\n  -t $SLURM_CPUS_PER_TASK \\\n  -x .fa --input dastool_out/_DASTool_bins/ \\\n  --output-directory checkm2_out/\n</code></pre>"},{"location":"day2/ex8_bin_dereplication/#dereplicating-bins-across-multiple-assemblies","title":"Dereplicating bins across multiple assemblies","text":"<p>In this workshop, we have generated a set of putative MAGs by binning scaffolds taken from a single co-assembly. Alternatively, we may have chosen to generate multiple assemblies (for example, mini-co-assemblies for each sample group, or individual assemblies for each sample). In this case, it would be necessary to work through the binning process for each assembly, and then conduct an additional dereplication step across the multiple assemblies to generate a single set of dereplicated bins for all assemblies. </p> <p>This is beyond the scope of this workshop (and unnecessary here, since we are working with a single co-assembly). For future reference for your own work, further information about how to dereplicate bins and viral contigs across multiple assemblies via <code>dRep</code> and <code>dedupe</code> has been provided as an appendix here.</p>"},{"location":"day2/ex9_refining_bins/","title":"Manual bin refinement","text":"<p>Objectives</p> <ul> <li>Preparing input files for <code>VizBin</code></li> <li>Projecting a t-SNE using <code>VizBin</code> and examining bin clusters</li> <li>Picking and exporting sequences</li> <li>Refining bins</li> </ul>"},{"location":"day2/ex9_refining_bins/#preparing-input-files-for-vizbin","title":"Preparing input files for <code>VizBin</code>","text":"<p>What is <code>VizBin</code>?</p> <p><code>VizBin</code> is a handy, GUI-based tool for creating ordinations of our binning data using the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on YouTube that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS.</p> <p>On its own, <code>VizBin</code> takes a set of contigs and performs the t-SNE projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage <code>VizBin</code> only allows a single coverage value per contig, which is not ideal. This is because <code>VizBin</code> only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own t-SNE projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing <code>VizBin</code> with coverage values for sample1 only. </p> <p>The only required input file for <code>VizBin</code> is a single .fna file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement.</p> <p>In the interests of time today, the input files have been generated and are provided in the <code>6.bin_refinement/</code> folder: </p> <ul> <li><code>all_fragments.fna</code> is a concatenation of the bins of fragmented sub-contigs (fragmented to 20k)</li> <li><code>all_fragments.sample1.vizbin.ann</code> is the annotation file containing per-subcontig coverage, label (bin ID), and length values.</li> </ul> <p>Contig fragments as input for <code>VizBin</code></p> <p>When running <code>VizBin</code>, it is often preferable to split long contigs into smaller pieces in order to increase the density of clustering in the t-SNE. The data we are working with today are based on our bins output by <code>DAS_Tool</code> in the last binning exercise, but have been further processed using the <code>cut_up_fasta.py</code> script that comes with the binning tool <code>CONCOCT</code> to cut long contigs into 20k fragments. When reviewing our <code>VizBin</code> plots and outputs, it is important to remember that here we are looking at the fragmented sub-contigs, rather than the full complete contigs.</p> <p>For future reference, and for working with your own data, a step-by-step process for generating these files from the dereplicated bins generated by <code>DAS_Tool</code> has been provided as an Appendix.</p> <p>For this section, we will be working within <code>6.bin_refinement/</code>. Let's first have a quick look at the annotation file.</p> <p>Navigate to working directory</p> <pre><code># Navigate to correct directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement\n</code></pre> <p>Inspect <code>all_bins.sample1.vizbin.ann</code></p> <pre><code>head -n 5 all_fragments.sample1.vizbin.ann\n</code></pre> <p>Terminal output</p> <pre><code>coverage,label,length\n17.6626,bin_0.chopped,20000\n15.9561,bin_0.chopped,20000\n17.294,bin_0.chopped,20000\n15.8157,bin_0.chopped,20000\n</code></pre> <p>This file is a comma-delimited table (csv file) that presents the information in the way that <code>VizBin</code> expects it. The order of rows in this file corresponds to the order of contigs in the concatenated FASTA file of our fragmented bins, <code>all_fragments.fna</code>.</p> <p>Create a few variations of the .ann file with various columns removed, in order to examine the different outputs they can generate.</p> <p>code</p> Bin ID onlyBin ID and coverage without length <pre><code>cut -f2 -d ',' all_fragments.sample1.vizbin.ann &gt; all_fragments.sample1.vizbin.bin_only.ann\n</code></pre> <pre><code>cut -f1,2 -d ',' all_fragments.sample1.vizbin.ann &gt; all_fragments.sample1.vizbin.no_length.ann\n</code></pre> <p>Symbolic links for easy access</p> <p>By default, <code>VizBin</code> redirects to your home directory. To make it easy to come back to our working directory, we can make a symbolic link (aka shortcut) that points to here from home.</p> <pre><code>ln -sr $(pwd) ~/\n</code></pre> <p>The flags mean:</p> <ul> <li><code>-s</code> designates it as a symbolic, instead of hard link</li> <li><code>-r</code> indicates that paths are relative</li> </ul>"},{"location":"day2/ex9_refining_bins/#projecting-a-t-sne-and-examining-bin-clusters","title":"Projecting a t-SNE and examining bin clusters","text":"<p>We can now use these files in <code>VizBin</code> to curate the contigs in our bins. We will load and view the data in a few different steps.</p> <p>For this exercise, we will be using the Virtual Desktop on NeSI which allows us to use programmes with graphical user interfaces (GUI) within the NeSI computing environment.</p> <p>Initiate the Virtual Desktop</p> <ol> <li>Click on File on the top left corner of the Jupyter Hub, and then select New Launcher. You can also click on the small + on the tabbed bar next to your terminal tab.</li> <li>Look for the Virtual Desktop icon and then click on centre of the icon.      </li> <li>A new browser tab named 'TurboVNC:...' should appear and the Virtual Desktop should load.</li> <li>A successful instance of the Virtual Desktop should look like a desktop environment.</li> </ol> <p> </p> <p>Running <code>VizBin</code>: local vs remote</p> <p>Running <code>VizBin</code> remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like <code>VizBin</code>) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing <code>VizBin</code> on your local machine and downloading the relevant input files (e.g. via <code>scp ...</code>) to run locally.</p>"},{"location":"day2/ex9_refining_bins/#initiate-vizbin-within-the-virtual-desktop-environment","title":"Initiate <code>VizBin</code> within the Virtual Desktop environment","text":"<ol> <li> <p>In the Virtual Desktop, click on the terminal icon.      </p> </li> <li> <p><code>VizBin</code> is a Java programme, therefore we will need to load the Java module for it to work. In the terminal, type the following to load the Java module</p> <p>Copy/paste in the Virtual Desktop</p> <p>You will not be able to copy text from outside the Virtual Desktop and paste into the Virtual Desktop, in which case you will need to manually type these commands.</p> <p>code</p> <pre><code>module load Java/20.0.2\n</code></pre> </li> <li> <p>In the terminal, navigate to your directory where the Java file resides</p> <p>Remember to replace <code>YOUR FOLDER</code> with your user name.</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\nls\n</code></pre> <p>Terminal output</p> <pre><code>all_fragments.fna                 example_data_unchopped  mock_fragments.fna\nall_fragments.sample1.vizbin.ann  mock_bins               mock_fragments.sample3.vizbin.bin_only.ann\nexample_data_20k                  mock_bins_checkm_out    vizbin_count_table.sh\nexample_data_20k_cov.txt          mock_bins.checkm.txt    VizBin-dist.jar\n</code></pre> </li> <li> <p>Type the following into your Virtual Desktop terminal to initiate VizBin.</p> <p>code</p> <pre><code>java -jar VizBin-dist.jar\n</code></pre> </li> <li> <p>A successful launch of <code>VizBin</code> will look like the following:     </p> </li> </ol>"},{"location":"day2/ex9_refining_bins/#load-input-files","title":"Load input files","text":"<p>Once <code>VizBin</code> is open, to get started, click the 'Choose...' button then navigate to the FASTA file <code>all_fragments.fna</code>.</p> <p><code>VizBin</code> directory</p> <p>It is likely that when we click on the 'Choose...' button, the directory that is open will be our NeSI home directory. In that case, we can type in the input FASTA file in the 'File Name' bar: <code>/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/all_fragments.fna</code></p> <p></p> <p>Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your 'bin only' .ann file into the 'Annotation file (optional)' field.</p> <p></p>"},{"location":"day2/ex9_refining_bins/#executing-the-t-sne","title":"Executing the t-SNE","text":"<p>For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following:</p>"},{"location":"day2/ex9_refining_bins/#contigs-coloured-by-bin","title":"Contigs coloured by bin","text":"Additional annotations by length and coverage <p>If you input <code>all_bins.sample1.vizbin.ann</code> as your annotation file, you can see that the visualisation takes contig length (represented by point size) and coverage (represented by opacity) into account.</p> <p> </p> <p>Similar to other projection techniques, we interpret the closeness of points as a proxy for how similar they are, and because of our .ann file we can see which contigs belong to the same bin.</p> <p>What do scaffolds look like?</p> <p>In the example above, we used fragmented scaffolds as input files for VizBin. Take a look at what scaffolds look like with VizBin. Run the following code to generate input files for VizBin to visualise scaffolds.</p> <p>code</p> <pre><code>echo \"label\" &gt; all_scaffolds.vizbin.ann\nfor fasta in example_data_unchopped/*.fna; do\n    bin=$(basename ${fasta} .fna)\n    cat ${fasta} &gt;&gt; all_scaffolds.fna\n    grep '&gt;' ${fasta} | sed \"s/.*/${bin}/g\" &gt;&gt; all_scaffolds.vizbin.ann\ndone\n</code></pre> <p>Import the newly generated <code>all_scaffolds.fna</code> and <code>all_scaffolds.vizbin.ann</code> into VizBin to visualise it.</p>"},{"location":"day2/ex9_refining_bins/#1-picking-and-exporting-sequences","title":"1. Picking and exporting sequences","text":"<p>We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands:</p> <ol> <li>Left-click and drag: Highlight an area of the ordination to zoom into</li> <li>Right-click, 'Zoom Out', 'Both Axes': Rest of the view</li> <li>Left-click several points: Create a selection of contigs to extract from the data</li> <li>Right-click, 'Selection', 'Export': Save the selected contigs into a new file</li> <li>Right-click, 'Selection', 'Clear selection': Clear the current selection</li> </ol> <p>How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed.</p> <p>First, for each <code>VizBin</code> cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as <code>cluster_1.fna</code>. </p> <p>Try this for one or two clusters. In practice, we would do this for each <code>VizBin</code> cluster, saving each as a new <code>cluster_n.fna</code> file.</p>"},{"location":"day2/ex9_refining_bins/#highlight-a-cluster-to-zoom-into","title":"Highlight a cluster to zoom into","text":""},{"location":"day2/ex9_refining_bins/#select-the-cluster-to-export","title":"Select the cluster to export","text":"<p>Left-click several points around the cluster</p> <p> </p>"},{"location":"day2/ex9_refining_bins/#export-the-cluster","title":"Export the cluster","text":"<p>Right-click, 'Selection', 'Export'. Save the output as <code>cluster_1.fna</code>. </p> <p> </p>"},{"location":"day2/ex9_refining_bins/#2-refining-bins","title":"2. Refining bins","text":"<p>VizBin is a general purpose tool for contig/scaffold/fragment visualisation. For this workshop, we're going to attempt to refine a few bins. Refinement can mean reducing contamination, improving completeness and/or splitting strains. Here, we will:</p> <ul> <li>Diagnose and visualise contig fragments</li> <li>Export clusters of contig fragments</li> <li>Check distribution of contig fragments across clusters</li> <li>Pull new contigs from concatenated bin files to form new bin clusters</li> <li>Check if genome metrics improve after manual refinement</li> </ul> <p>For this exercise, we will be using data from the <code>mock_fragments*</code> files, <code>mock_bins/</code> sub-directory, and additional genome metrics in <code>mock_bins.checkm.txt</code>. These files were generated using a different assembly (reads assembled using MEGAHIT) based on a modified sample 3 library, and then binned using MetaBAT2 and MaxBin as per previous lessons.</p>"},{"location":"day2/ex9_refining_bins/#inspect-the-genome-metrics","title":"Inspect the genome metrics","text":"<p>Open up the file named <code>mock_bins.checkm.txt</code>. Take note of the metrics of each bin and consider what we might want to improve on.</p>"},{"location":"day2/ex9_refining_bins/#prepare-output-directory","title":"Prepare output directory","text":"<p>We also need to prepare an output directory for the clusters that we export.</p> <p>code</p> <pre><code>mkdir -p vb_export\n</code></pre>"},{"location":"day2/ex9_refining_bins/#load-the-files-into-vizbin","title":"Load the files into VizBin","text":"<p>Return to the VizBin set-up dialogue in the Virtual Desktop. Select the following input files:</p> <p>File to visualize: <code>mock_fragments.fna</code></p> <p>Annotation file: <code>mock_fragments.sample3.vizbin.bin_only.ann</code></p> <p>Your VizBin will look like this:</p> <p>image</p> <p>Legends</p> <p>When you have projected the tSNE, you can show the bin identity by right-clicking anywhere on the ordination and then click \"Legend\". This will give you a box that shows you which contig belongs to which bin.</p>"},{"location":"day2/ex9_refining_bins/#export-clusters","title":"Export clusters","text":"<p>Based on information you have on hand, make your selection around points that you think:</p> <ul> <li>Should form bins (name these sequences <code>cluster_n.bin_n.fa</code>, where <code>bin_n</code> reminds you which bin most of the fragments come from)</li> <li>Are problematic fragments or fragments in doubt (name these sequences <code>contigs_n.fa</code>)</li> </ul> <p>You don't have to circle everything, just the ones you think need refining!</p> <p>Export the sequences into the <code>vb_export/</code> directory we made earlier.</p>"},{"location":"day2/ex9_refining_bins/#check-distribution-of-contig-fragments","title":"Check distribution of contig fragments","text":"<p>We have provided you with a script that collects information from your exported fragments in order to check if you need to remove contigs from the aggregated <code>mock_fragments.fna</code>. We run this script like so:</p> <p>code</p> <pre><code>./vizbin_count_table.sh -i vb_export/ \\\n                        -e fa \\\n                        -c cluster \\\n                        -s contig\n</code></pre> <p><code>vizbin_count_table.sh</code> flags</p> Flags Description <code>-i</code> Input directory where FASTA files of drawn clusters are located <code>-e</code> File extension for drawn clusters <code>-c</code> File prefix for fragments that form new bins <code>-s</code> File prefix for problematic fragments or whose placements are doubtful. <p>The script counts how problematic fragments are distributed across different clusters (based on the contig they came from). This helps us make an informed decision if removing a contig from a cluster would impact other clusters.</p> <p>The outputs are:</p> <ul> <li><code>vb_count_table.txt</code>: A table of fragments and where other fragments of the same contig are also distributed in different clusters.</li> <li><code>vb_omit_contigs_tmp.txt</code>: A list of contigs (not fragments) that were designated as problematic to be removed.</li> </ul>"},{"location":"day2/ex9_refining_bins/#remove-problematic-contigs","title":"Remove problematic contigs","text":"<p>If your <code>vb_count_table.txt</code> has fragments of a contig that are spread across different clusters, you may need to reconsider removing that contig (i.e., there are non-zero counts in multiple cluster columns). This is because removing them may impact the composition of other clusters. </p> <p>Otherwise, if problematic contig fragments do not form clusters, we can simply ignore them when reforming new bins. In this exercise, it is highly unlikely you will encounter contig fragments shared across multiple clusters. As such, we will simply ignore the problematic contigs from the concatenated contigs file.</p>"},{"location":"day2/ex9_refining_bins/#form-new-bins","title":"Form new bins","text":"<p>Start by creating a new directory for refined bins.</p> <p>code</p> <pre><code>mkdir -p refined_bins/\n</code></pre> <p>We then create a file with the relevant contigs from our newly formed clusters.</p> <p>code</p> <pre><code>for cluster in vb_export/cluster*.fa; do\n    grep '&gt;' ${cluster} \\\n    | sed -e 's/&gt;//g' -e 's/\\.concoct_part_[0-9]*//g' \\\n    | sort -u \\\n    &gt; ${cluster/.fa/.contigID}\ndone\n</code></pre> <p>Then, we use SeqKit to extract the required contigs.</p> <p>code</p> <pre><code>module purge\nmodule load SeqKit/2.4.0\n\nfor ids in vb_export/cluster*.contigID; do\n    seqkit grep -f ${ids} &lt;(cat mock_bins/*.fna) &gt; refined_bins/$(basename ${ids} .contigID).fna\ndone\n</code></pre> Refining by omission <p>If you want to use the <code>vb_omit_contigs_tmp.txt</code> as the search pattern to omit contigs from the bins, you can do that following:</p> <p>code</p> <pre><code>module purge\nmodule load SeqKit/2.4.0\n\nfor bins in mock_bins/*.fna; do\n    seqkit grep -v -f vb_omit_contigs.tmp.txt &gt; refined_bins/$(basename ${bins} .fna).refined.fna\ndone\n</code></pre> <p>In the code above, we added a <code>-v</code> to the <code>grep</code> command to indicate invert matches.</p>"},{"location":"day2/ex9_refining_bins/#check-your-new-bins","title":"Check your new bins!","text":"<p>Moment of truth! How did your decisions impact the genome metrics of the refined bins? Run your selections through CheckM and see how you did!</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      CheckM_vb_exports\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load CheckM/1.2.3-foss-2023a-Python-3.11.6\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Run CheckM\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK \\\n                  --pplacer_threads $SLURM_CPUS_PER_TASK \\\n                  -x fna --tab_table -f refined_bins.checkm.txt \\\n                  refined_bins/ refined_bins.checkm_out/\n</code></pre>"},{"location":"day2/ex9_refining_bins/#human-refinement-vs-automated-binning","title":"Human refinement vs automated binning","text":"<p>Did you do better than the automated binning software? Consider the following:</p> <ul> <li>Were the new clusters more complete?</li> <li>How much contamination was removed?</li> <li>Was there an substantial trade-off between completeness and contamination?</li> <li>How would you pre-process the sequences differently prior to manual refinement?</li> <li>Are there additional information that would have helped you in the decision-making process?</li> </ul> <p>When your CheckM run finishes, check and compare how you did!</p>"},{"location":"day3/ex10.2_viruses/","title":"Virus taxonomy","text":"<p>Objectives</p> <ul> <li>Explore outputs of <code>VirSorter2</code> and <code>CheckV</code></li> <li>Introduction to <code>vConTACT2</code> for predicting taxonomy of viral contigs</li> <li>Visualising the <code>vConTACT2</code> gene-sharing network in <code>Cytoscape</code></li> </ul>"},{"location":"day3/ex10.2_viruses/#explore-outputs-of-virsorter2-and-checkv","title":"Explore outputs of <code>VirSorter2</code> and <code>CheckV</code>","text":"<p>Key outputs from <code>VirSorter2</code> include:</p> <ul> <li><code>mgss-final-viral-combined.fa</code>: FASTA file of identified viral sequences</li> <li><code>mgss-final-viral-score.tsv</code>: table with score of each viral sequences across groups and a few more key features, which can also be used for further filtering</li> <li><code>mgss-for-dramv/</code>: files to be used as input to <code>DRAM-v</code> for gene prediction and annotation (we will be running <code>DRAM-v</code> later today during the gene annotation session)</li> </ul> <p><code>CheckV</code> provides summary outputs for contamination, completeness, repeats, and an overall quality summary. Later today we will have a brief look at some examples of the information you can draw from these <code>CheckV</code> outputs. </p>"},{"location":"day3/ex10.2_viruses/#exercise-examining-viral-output-files-from-virsorter2-and-checkv","title":"Exercise: Examining viral output files from <code>VirSorter2</code> and <code>CheckV</code>","text":"<p><code>VirSorter2</code> and <code>CheckV</code> provide several of different output files that are important for identifying and understanding the viruses present in your data. Explore through the following files: </p> <ul> <li><code>7.viruses/VirSorter2/mgss-final-viral-score.tsv</code></li> <li><code>7.viruses/checkv_out/quality_summary.tsv</code></li> </ul> <p>When viewing these files, see if you can find the following information:</p> <ul> <li>How many viral contigs did <code>VirSorter2</code> identify?</li> <li>How many viral contigs meet the \"High-quality\" (MIUViG) standard?</li> <li>How many might we consider \"complete\" genomes based on <code>CheckV</code>'s completeness estimation?</li> <li>Are any of the identified viral contigs complete circular genomes (based on identifying direct terminal repeat regions on both ends of the genome)? If not, think about why this might be the case for this dataset (hint: the workshop materials are a manufactured \"metagenome\" data set based on compiling several individual genomes)</li> <li>Are there any suspicious contigs that you might want to flag for closer examination (and/or careful consideration in downstream analyses)? (Note that standard practice would be to use these <code>CheckV</code> results as one basis for filtering to remove potential false positives)</li> </ul>"},{"location":"day3/ex10.2_viruses/#introduction-to-vcontact2-for-predicting-taxonomy-of-viral-contigs","title":"Introduction to <code>vConTACT2</code> for predicting taxonomy of viral contigs","text":"<p>Even more so than prokaryote taxonomy, establishing a coherent system for viral taxonomy is complex and continues to evolve. In 2020, the International Committee on Taxonomy of Viruses (ICTV) overhauled the classification code into 15 hierarchical ranks. Furthermore, the knowledge gap in databases of known and taxonomically assigned viruses remains substantial, and so identifying the putative taxonomy of viral contigs from environmental metagenomics data remains challenging.</p> <p>There are a number of approaches that can be used to attempt to predict the taxonomy of the set of putative viral contigs output by programs such as <code>VIBRANT</code>, <code>VirSorter</code>, and <code>VirFinder</code>. <code>vConTACT2</code> is one such method that uses 'guilt-by-contig-association' to predict the potential taxonomy of viral genomic sequence data based on relatedness to known viruses within a reference database (such as viral RefSeq). The principle is that, to the extent that the 'unknown' viral contigs cluster closely with known viral genomes, we can then expect that they are closely related enough to be able to predict a shared taxonomic rank. </p> <p>Note</p> <p>Anecdotally, however, in my own experience with this process I have unfortunately been unable to directly predict the taxonomy of the vast majority of the viral contigs output by <code>VIBRANT</code>, <code>VirSorter</code>, or <code>VirFinder</code> from an environmental metagenomic data set (due to not clustering closely enough with known viruses in the reference database). You can, however, visualise the gene-sharing network generated to infer the likely taxonomy of each of your viruses at higher taxonomic ranks due to the relatedness to known reference viral genomes.</p> <p>Running <code>vConTACT2</code> can require a considerable amount of computational resources, and so we won't be running this in the workshop today. The required process is outlined for reference in an Appendix for this exercise, should you wish to experiment with this on your own data in the future. </p> <p>For today, we have provided some of the output files from this process when applied to our mock metagenome data. A selection of these can be viewed in the folder <code>7.viruses/vConTACT2_Results/</code> via <code>head</code> or <code>less</code>.</p> <p>code</p> <pre><code>less vConTACT2_Results/genome_by_genome_overview.csv\n</code></pre> <p>code</p> <pre><code>less vConTACT2_Results/tax_predict_table.tsv\n</code></pre> <p>A few notes to consider: </p> <ul> <li>You will see that the <code>genome_by_genome_overview.csv</code> file contains entries for the full reference database used as well as the input viral contigs (contigs starting with <code>NODE</code>). </li> <li> <p>You can use a command such as <code>grep \"NODE\" vConTACT2_Results/genome_by_genome_overview.csv | less</code> to view only the lines for the input contigs of interest. </p> <ul> <li>Note also that these lines however will not contain taxonomy information. </li> <li>See the notes in the Appendix for further information about why this might be.</li> </ul> </li> <li> <p>As per the notes in the Appendix, the <code>tax_predict_table.tsv</code> file contains predictions of potential taxonomy (and or taxonomies) of the input viral contigs for order, family, and genus, based on whether they clustered with any viruses in the reference database.</p> <ul> <li>Note that these may be lists of multiple potential taxonomies, in the cases where viral contigs clustered with multiple reference viruses representing more than one taxonomy at the given rank.</li> </ul> <p>The taxonomies are deliberately enclosed in square brackets (<code>[ ]</code>) to highlight the fact that these are predictions, rather than definitive taxonomy assignments.</p> </li> </ul>"},{"location":"day3/ex10.2_viruses/#visualising-the-vcontact2-gene-sharing-network-in-cytoscape","title":"Visualising the <code>vConTACT2</code> gene-sharing network in <code>Cytoscape</code>","text":"<p>We can visualise the gene-sharing network generated by <code>vConTACT2</code> (<code>c1.ntw</code>) using the software <code>Cytoscape</code>. <code>Cytoscape</code> runs as a GUI (graphical user interface), so we will need to either download and install this software or open <code>Cytoscape</code> using NeSI's Virtual Desktop (instructions can be found here). With our Virtual Desktop open, <code>Cytoscape</code> can then be loaded as follows.</p> <p>Copy/paste in the Virtual Desktop</p> <p>You will not be able to copy text from outside the Virtual Desktop and paste into the Virtual Desktop, in which case you will need to manually type these commands.</p> <p>Open a terminal in Virtual Desktop prior to running code below</p> <p>code</p> <pre><code># Load the module\nmodule purge\nmodule load Cytoscape/3.9.1\n# Run cytoscape\nCytoscape\n</code></pre> <p>Do not update <code>Cytoscape</code>!</p> <p>A dialog box will appear telling you about a new version of <code>Cytoscape</code>. Click \"close\", as we will not be installing any new versions today!</p>"},{"location":"day3/ex10.2_viruses/#load-the-network","title":"Load the network","text":"<ol> <li>With <code>Cytoscape</code> open, click on File \\(\\rightarrow\\) Import \\(\\rightarrow\\) Network from file</li> <li>Open the <code>c1.ntw</code> file by (a) typing in the absolute path in the File Name box: <code>/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/vConTACT2_Results/c1.ntw</code> or (b) navigate to the file using the GUI.</li> <li> <p>In the Import Network From Table pop-up box:</p> <ol> <li>Click on Advanced Options<ol> <li>Select SPACE as the delimiter</li> <li>Uncheck Use first line as column names</li> <li>Click OK</li> </ol> </li> </ol> <p></p> <ol> <li>In the drop-down menu for 'Column 1', select the green dot (source node)</li> <li>For Column 2 select the red target (Target node)</li> <li>Click OK</li> </ol> <p></p> </li> </ol> <p>It will now ask if you want to create a view for your large networks now. Click OK. This may take a minute to generate the network visualisation. </p>"},{"location":"day3/ex10.2_viruses/#annotate-the-network","title":"Annotate the network","text":"<p>There are many ways to modify and inspect this visualisation. One basic addition that will help us interpret the results here is to colour code the viral genomes based on reference (RefSeq) genomes and the viral contigs recovered from our dataset. We can do this by loading the <code>genome_by_genome_overview.csv</code> file. </p> <p><code>Dataset</code> column</p> <p>For the purposes of this workshop, we have added the additional column <code>Dataset</code> to the <code>genome_by_genome_overview.csv</code> file stating whether each viral sequence originated from either the reference database (<code>RefSeq</code>) or our own data (<code>Our_data</code>). This column is not generated by <code>vConTACT2</code>, but you can open the file in Excel to add any additional columns you would like to colour code the nodes by.</p> <p>Click <code>File/Import/Table from file</code> and select the <code>genome_by_genome_overview.csv</code> file to open. In the pop-up box, leave the settings as default and click OK. This will add a bunch of metadata to your network table. We can now use this to colour code the nodes in the network.</p> <p>To colour code the nodes by genome source (<code>Refseq</code> or <code>Our_data</code>): </p> <ol> <li>Click the <code>Style</code> tab on the far left</li> <li>Select the dropdown arrow by <code>Fill Color</code></li> <li>Next to <code>Column</code> click on --select value-- and select <code>Dataset</code> </li> <li>For <code>Mapping Type</code>, select <code>Discrete Mapping</code>. </li> <li>Then for each of <code>Refseq</code> and <code>Our_data</code>, click the three dots in the empty box to the right of the label and select a colour for that dataset.</li> </ol> <p>You can now zoom in on the network and select viral contigs from your own dataset, and examine which reference viruses may be the most closely related. This can give an indication of the possible taxonomy of your own viruses based on their gene-sharing relatedness to the known reference genomes.</p>"},{"location":"day3/ex11.1_phylogenomics/","title":"Phylogenomics","text":"<p>Objectives</p> <ul> <li>Introduction to phylogenetic inference</li> <li>Building a phylogenetic tree</li> <li>Visualising the tree</li> </ul>"},{"location":"day3/ex11.1_phylogenomics/#introduction-to-phylogenetic-inference","title":"Introduction to phylogenetic inference","text":"<p><code>GTDB-Tk</code> is very helpful in inferring the taxonomy of our MAGs. It also has a bunch of additional files that is generated so we can visualise the phylogenetic relationships between the MAGs (both in the larger context of the entire GTDB tree, or just our MAGs). </p> <p>There are several software options for inferring phylogenetic relationships, some more commonly used than others. Popular options include <code>BEAST</code> (Bayesian Evolutionary Analysis Sampling Trees), <code>FastTree 2</code>, <code>Geneious</code>, <code>IQ-TREE 2</code>, <code>MEGA</code> (Molecular Evolutionary Genetics Analysis), and <code>RAxML</code> (Randomized Axelerated Maximum Likelihood). A requirement common to all software listed is that they require a multiple sequence alignment (MSA). For this workshop, GTDB-Tk has conveniently output this file for us.</p> Software for MSAs <p>GTDB-Tk generates an MSA based on a concatenated set of core single-copy genes identified within each of our MAGs. However, if you are studying other genes, you will need to generate (and curate) your own MSA. Popular software include <code>MAFFT</code>, <code>ClustalW</code>, <code>Clustal Omega</code>, and <code>MUSCLE</code>. </p> <p>Phylogenetic inference methods </p> <p>To start, we need a set of aligned (and ideally, trimmed) orthologous genes. These are genes in different species that, by speciation, have evolved from a common ancestor and tend to retain the same function over the course of evolution. From here, there are two ways to generate an input for phylogenetic inference:</p> Super-tree approachSupermatrix approach (most common) <p>Alignment of each gene is analysed individually to estimate a tree. Then, these distinct trees are integrated to estimate the species tree.</p> <p>Aligned orthologous genes are concatenated (this is the supermatrix) and then used to estimate a tree. This is the most commonly used method.</p> <p>From here, we then choose one of two ways to reconstruct the phylogeny:</p> Distance-basedCharacter-based <p>This involves the calculation of a pairwise genetic distance matrix and then using it to iteratively construct a tree. A common implementation of this method is the Neighbour-Joining algorithm. While computationally efficient, it tends not to perform well for distantly related organisms.</p> <p>Infers phylogeny based on the characters (nucleic/amino acid alphabet) in the MSA via one of three methods:</p> <ul> <li>Maximum parsimony This method tries to find a tree topology with the least the number of character changes required to explain the data (i.e., the most parsimonious tree). While computationally efficient, this method is the least commonly applied due to unrealistic assumptions about evolution.</li> <li>Maximum likelihood (ML) Estimates the parameters of a statistical model such that the probability of observing the data (i.e., the likelihood) is maximal. The general idea behind this method is that if the estimated parameter values of a model makes it more likely to observe the data, it is assumed to approximate the true topology of the tree. This framework is employed in software such as RAxML, IQ-TREE2, and FastTree2.</li> <li>Bayesian methods This method uses a similar approach to ML in that it relies on a model and maximises the likelihood of observing the data. The difference is in its implementation, where parameters are inferred under the Bayesian framework. Here, unknown model parameter values are described using probability distributions (i.e., these values have a degree of uncertainty and are not fixed values). These are then integrated with the likelihood of observing the data (as in ML) to generate a posterior distribution from which tree model parameters are actually inferred/sampled from.</li> </ul> <p>Confidence in tree inference</p> <p>How would we know if the placement of branches are correct in our inferred trees? For ML- and parsimony-based tree reconstruction, a common way to test that is via bootstrapping. This is the generation of pseudo-data of similar size to the observed data via iterative resampling of the observed data. New trees are then inferred from the resampled data, and the number of times the same tree or branch placements (clades) are observed based on the pseudo-data. This is analogous to permutations in non-parametric tests. For Bayesian method-based inference, confidence is estimated based on posterior probabilities (a.k.a. the posterior support; i.e., the product of prior probabilities and maximal likelihood).</p>"},{"location":"day3/ex11.1_phylogenomics/#building-a-phylogenetic-tree","title":"Building a phylogenetic tree","text":"<p>The exercises for this section is performed in the <code>8.prokaryotic_taxonomy/</code> directory. Here, we will build a bootstrapped ML tree using <code>FastTree2</code> with an MSA provided by <code>GTDB-Tk</code> of our MAGs.</p> <p>We begin by loading the required modules:</p> <p>code</p> <pre><code>module purge\nmodule load FastTree/2.1.11-GCCcore-9.2.0\n</code></pre> <p>Then, we prepare the input file based on our GTDB-Tk outputs:</p> <p>code</p> <pre><code># Make sure you're in the right place!\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/8.prokaryotic_taxonomy\n\n# Copy MSA produced by GTDB-Tk\ncp gtdbtk_out/align/gtdbtk.bac120.user_msa.fasta.gz .\n\n# Decompress file\ngzip -d gtdbtk.bac120.user_msa.fasta.gz\n</code></pre> <p>Finally, we can build the tree using the following code:</p> <p>code</p> <pre><code>FastTree gtdbtk.bac120.user_msa.fasta &gt; bin.tree\n</code></pre> <p>We can inspect what the output looks like:</p> <p>code</p> <pre><code>cat bin.tree\n</code></pre> Output <pre><code>(bin_3:0.67071,(bin_2:0.72960,bin_6:0.50416)0.940:0.06864,((bin_0:0.37758,bin_1:0.30061)1.000:0.49407,(bin_4:0.63792,(bin_9:0.58897,(bin_5:0.37075,(bin_7:0.25166,bin_8:0.26745)1.000:0.12499)1.000:0.18677)1.000:0.08332)0.929:0.05225)0.996:0.06772);\n</code></pre>"},{"location":"day3/ex11.1_phylogenomics/#visualising-the-tree","title":"Visualising the tree","text":"<p>We will use iTOL (Interactive Tree Of Life) to visualise the tree we made. iToL is a powerful online, browser-based tool for the display, annotation and management of phylogenetic and trees. </p> <p>iToL subscription</p> <p>For the purposes of today's workshop, the free version of iToL will suffice. However, if you think you will need to perform more phylogenetic analyses, you should consider creating an account and subscribing to the web service. This allows you to save your trees online for future use.</p> <p>Navigate to iTOL</p> <p>On your web browser, navigate to iTOL.</p> <p></p> <p>Click on the Upload button on the top left.</p> <p></p> <p>Add tree information</p> <p>As we are working with a small tree, we can go back to the Jupyter terminal to copy the contents of <code>bin.tree</code> into the 'Tree text' area.</p> <p>code</p> <pre><code>cat bin.tree\n</code></pre> Content of <code>bin.tree</code> <pre><code>((bin_0.filtered:0.39270,bin_1.filtered:0.27312)1.000:0.53352,(bin_7.filtered:0.73278,(bin_2.filtered:0.51665,bin_6.filtered:0.66105)0.930:0.06144)1.000:0.07298,(bin_4.filtered:0.62639,(bin_8.filtered:0.57815,(bin_3.filtered:0.36294,(bin_5.filtered:0.24898,bin_9.filtered:0.28774)1.000:0.11846)1.000:0.19589)1.000:0.07212)0.970:0.05113);\n</code></pre> <p>We can also name our tree.</p> <p>Once we are done, click Upload.</p> <p></p> <p>Annotate tree</p> <p>We can also add additional information to highlight some or all of the tips. iTOL requires annotation files in a specific format. We have provided you with some example ones to use in this workshop:</p> <ul> <li>Relabel the bin IDs to their taxon</li> <li>Add colour to highlight relevant taxa</li> <li>Add shapes to highlight risk-associated taxa</li> </ul> <p>After downloading the files, simply drag and drop them into the iTOL page and it will annotate the tips accordingly.</p> <p></p> <p>Play around with the tree</p> <p>We can edit and add annotations by clicking within the iTol website environment. However, the annotation files are a great way to maintain annotations for the phylogenetic trees and automatically display information you need to highlight on the tree.</p> <p>Exercise Alternative display and additional information</p> <p>See if you can add/change the following from your tree:</p> <ul> <li>Show bootstrap support values in blue</li> <li>Show branch lengths in red</li> <li>Change display format of your tree</li> </ul> <p>References</p> <p>Kapli, P., Yang, Z. and Telford M.J. (2020) Phylogenetic tree building in the genomic age. Nat Rev Genet 21: 428-444</p> <p>Yang, Z. and Rannala, B. (2012) Molecular phylogenetics: principles and practice. Nat Rev Genet 13: 303-314.</p>"},{"location":"day3/ex11_coverage_and_taxonomy/","title":"Assigning taxonomy to refined prokaryotic bins","text":"<p>Objectives</p> <ul> <li>Assigning taxonomy to the refined bins</li> </ul>"},{"location":"day3/ex11_coverage_and_taxonomy/#assigning-taxonomy-to-the-dereplicated-bins","title":"Assigning taxonomy to the dereplicated bins","text":"<p>It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us:</p> <ol> <li>Extract 16S rRNA gene sequences from the MAGs and classify them</li> <li>Annotate each gene in the MAG and take the consensus taxonomy</li> <li>Use a profiling tool like <code>Kraken</code>, which matches pieces of DNA to a reference database using k-mer searches</li> <li>Identify a core set of genes in the MAG, and use these to compute a species phylogeny</li> </ol> <p>For this exercise, we will use the last option in the list, making use of the <code>GTDB-TK</code> software (available on github) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, <code>GTDB-TK</code> will perform the following steps on a set of bins.</p> <ol> <li>Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG.</li> <li>Based on the recovered numbers, identify which domain is a more likely assignment for each MAG</li> <li>Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions</li> <li>Filter the alignment down to approximately 5,000 informative sites</li> <li>Insert each MAG into a reference tree create from type material and published MAGs</li> <li>Scale the branch lengths of the resulting tree, as described in Parks et al., to identify an appropriate rank to each branch event in the tree</li> <li>Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree</li> <li>Report the resulting taxonomic assignment, and gene alignment</li> </ol> <p>This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process.</p> <p>For the following exercises, we will be working in <code>8.prokaryotic_taxonomy/</code>.</p> <p>Create a new script</p> <p>Create script named <code>gtdbtk.sl</code></p> <pre><code>nano gtdbtk.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      gtdbtk\n#SBATCH --partition     milan\n#SBATCH --time          01:00:00\n#SBATCH --mem           140GB\n#SBATCH --cpus-per-task 24\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load GTDB-Tk/2.4.0-foss-2023a-Python-3.11.6\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/8.prokaryotic_taxonomy\n\n# Run GTDB-Tk\ngtdbtk classify_wf -x fna --cpus $SLURM_CPUS_PER_TASK \\\n                   --keep_intermediates \\\n                   --skip_ani_screen \\\n                   --genome_dir dastool_bins/ \\\n                   --out_dir gtdbtk_out/\n</code></pre> <p>Submit the script</p> <pre><code>sbatch gtdbtk.sl\n</code></pre> <p>As usual, lets look at the parameters here</p> Parameter Description <code>classify_wf</code> Specifies the sub-workflow from <code>GTDB-TK</code> that we wish to use <code>-x</code> Specify the file extension for MAGs within our input directory.Default is .fna, but it's always good practice to specify it anyway <code>--cpus</code> Number of threads/CPUs to use when finding marker genes, and performing tree insertion operations <code>--keep_intermediates</code> Keep intermediate outputs <code>--genome_dir</code> Input directory containing MAGs as individual FASTA files <code>--out_dir</code> Output directory to write the final set of files <p>Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw <code>DAS_Tool</code> outputs in the <code>../6.bin_refinement/dastool_out/_DASTool_bins/</code> folder, the renamed set of bins in the <code>../6.bin_refinement/example_data_unchopped/</code> folder (as we have here), the set of curated bins in the <code>filtered_bins/</code> folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here.</p> <p>When the task completes, you will have a number of output files provided. The main ones to look for are <code>gtdbtk.bac120.summary.tsv</code> and <code>gtdbtk.arch122.summary.tsv</code> which report the taxonomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the <code>gtdbtk.arch122.summary.tsv</code> file.</p> <p>If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the <code>gtdbtk.bac120.msa.fasta</code> and <code>gtdbtk.arch122.msa.fasta</code> files.</p> <p>Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop.</p>"},{"location":"day3/ex12_gene_prediction/","title":"Gene prediction","text":"<p>Objectives</p> <ul> <li>Overview/refresher of <code>prodigal</code></li> <li>Predicting protein coding sequences in metagenome-assembled genomes</li> <li>Predicting RNA features and non-coding regions</li> </ul>"},{"location":"day3/ex12_gene_prediction/#overviewrefresher-of-prodigal","title":"Overview/refresher of <code>prodigal</code>","text":"<p>At this stage we have recovered a number of high quality genomes or population genomes. While there are interesting biological questions we can ask of the genomes at the DNA/organisational level, it is more likely that we are interested in the genes present in the organism.</p> <p>How we predict genes in the metagenomic data varies depending on what features we are trying to detect. Most often, we are interested in putatively protein coding regions and open reading frames. For features that are functional but not not translated, such as ribosomal RNA and tRNA sequences we need to use alternative tools. When considering protein coding sequences, we avoid the use of the term 'open reading frame' (ORF). The nature of a fragmented assembly is that you may encounter a partial gene on the start or end of a contig that is a function gene, but lacks the start or stop codon due to issues with assembly or sequencing depth.</p> <p>There are many software tools to predict gene sequences and in this workshop we will start with the tool <code>prodigal</code> (PROkaryotic\u202fDynamic Programming Genefinding\u202fALgorithm). <code>prodigal</code> has gone on to become one of the most popular microbial gene prediction algorithms as in incorporates modelling algorithms to profile the coding sequences within your genome and better identify the more cryptic (or partial) genes.</p> <p><code>prodigal</code> is excellent for the following use cases:</p> <ol> <li>Predicting protein-coding genes in draft genomes and metagenomes  </li> <li>Quick and unsupervised execution, with minimal resource requirements</li> <li>Ability to handle gaps, scaffolds, and partial genes </li> <li>Identification of translation initiation sites </li> <li>Multiple output formats, including either straight fastA files or the DNA sequence and protein translation for genes, as well as detailed summary statistics for each gene (e.g. contig length, gene length, GC content, GC skew, RBS motifs used, and start and stop codon usage)</li> </ol> <p><code>prodigal</code> is not the best tool to use for the following cases:</p> <ol> <li>Predicting RNA genes </li> <li>Handling genes with introns </li> <li>Deal with frame shifts</li> </ol> <p>It is also not advised to use <code>prodigal</code> when making predictions through your unassembled reads. If you are working with unassembled data, <code>FragGeneScan</code> is a better tool, as it is more sensitive for partial genes and does not assume every piece of DNA in the input fastA file must be coding.</p>"},{"location":"day3/ex12_gene_prediction/#predicting-protein-coding-sequences-in-mags","title":"Predicting protein coding sequences in MAGs","text":"<p>To get started, move into the exercise directory.</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction/\n</code></pre>"},{"location":"day3/ex12_gene_prediction/#examine-the-prodigal-parameters","title":"Examine the <code>prodigal</code> parameters","text":"<p>Before we start running <code>prodigal</code>, we will take a quick look at the parameters.</p> <p>code</p> <pre><code>module purge\nmodule load prodigal/2.6.3-GCCcore-7.4.0\n\nprodigal -h\n</code></pre> <p>Terminal output</p> <pre><code>Usage:  prodigal [-a trans_file] [-c] [-d nuc_file] [-f output_type]\n                 [-g tr_table] [-h] [-i input_file] [-m] [-n] [-o output_file]\n                 [-p mode] [-q] [-s start_file] [-t training_file] [-v]\n\n         -a:  Write protein translations to the selected file.\n         -c:  Closed ends.  Do not allow genes to run off edges.\n         -d:  Write nucleotide sequences of genes to the selected file.\n         -f:  Select output format (gbk, gff, or sco).  Default is gbk.\n         -g:  Specify a translation table to use (default 11).\n         -h:  Print help menu and exit.\n         -i:  Specify FASTA/Genbank input file (default reads from stdin).\n         -m:  Treat runs of N as masked sequence; don't build genes across them.\n         -n:  Bypass Shine-Dalgarno trainer and force a full motif scan.\n         -o:  Specify output file (default writes to stdout).\n         -p:  Select procedure (single or meta).  Default is single.\n         -q:  Run quietly (suppress normal stderr output).\n         -s:  Write all potential genes (with scores) to the selected file.\n         -t:  Write a training file (if none exists); otherwise, read and use\n              the specified training file.\n         -v:  Print version number and exit.\n</code></pre> <p>There are a few parameters that are worth considering in advance.</p> <p>Output files</p> <p>When running <code>prodigal</code> the default behaviour is to create a gbk file (this is a Genbank-like feature table, see here for more information) from your genome and write it to the stdout of your interface. This can either be captured using a redirect, or the output can instead be placed into a file using the <code>-o</code> flag. You can also change the format of the file using the <code>-f</code> flag.</p> <p>Since we often want to go straight from gene prediction to annotation, <code>prodigal</code> also has the option to create fastA files of the gene prediction (<code>-d</code>) and protein translation (<code>-a</code>) at the same time. This is an extremely helpful feature, and it is worth running all three outputs at the same time. Generally speaking, you will probably find that the amino acid sequence for your genes is all you need for most practical purposes, but having the corresponding nucleotide sequence can sometimes be useful if we want to mine other data sets.</p> <p>Modes of gene prediction</p> <p>As mentioned in the introduction to this exercise, <code>prodigal</code> uses the profiles of genes it detects in your data set to better tune its prediction models and improve coding sequence recovery. It has three algorithms for how the training is performed which you must determine in advance:</p> Parameter Mode Description Normal mode <code>-p single</code> Take the sequence(s) you provide and profiles the sequence(s) properties. Gene predictions are then made based upon those properties.Normal mode\u202fshould be used on finished genomes, reasonable quality draft genomes, and big viruses. Anonymous mode <code>-p meta</code> Apply pre-calculated training files to the provided input sequences.Anonymous mode\u202fshould be used on metagenomes, low quality draft genomes, small viruses, and small plasmids. Training mode <code>-p train</code> Works like normal mode, but <code>prodigal</code> saves a training file for future use. <p>Anecdotally, when applied to a MAG or genome, anonymous mode (<code>-p meta</code>) will identify slightly fewer genes than normal mode (<code>-p single</code>). However, single mode can miss laterally transferred elements. There is not necessarily a best choice for which version to use and this is at the users discretion.</p>"},{"location":"day3/ex12_gene_prediction/#execute-prodigal","title":"Execute <code>prodigal</code>","text":"<p>We will now run <code>prodigal</code> over the 10 bins in single mode using an array.</p> <p>Create script named <code>prodigal.sl</code></p> <pre><code>nano prodigal.sl\n</code></pre> <p>Remember to update  to your own folder <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      prodigal\n#SBATCH --partition     milan\n#SBATCH --time          00:10:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 1\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n\n# Load modules\nmodule purge\nmodule load prodigal/2.6.3-GCCcore-7.4.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction\n\n# Output directory\nmkdir -p predictions/\n\n# Variables\nbin_file=dastool_bins/bin_${SLURM_ARRAY_TASK_ID}.fna\npred_file=$(basename ${bin_file} .fna)\n\n# Run prodigal\nprodigal -i ${bin_file} -p single \\\n         -d predictions/${pred_file}.genes.fna \\\n         -a predictions/${pred_file}.genes.faa \\\n         -o predictions/${pred_file}.genes.gbk\n</code></pre> <p>Submit the script</p> <pre><code>sbatch prodigal.sl\n</code></pre> <p>Once <code>prodigal</code> has completed, let's check one of the output files:</p> <p>code</p> <pre><code>head -n 5 predictions/bin_0.genes.faa\n</code></pre> <p>Terminal output</p> <pre><code>&gt;bin_0_NODE_6_length_607162_cov_1.000759_1 # 2 # 667 # -1 # ID=1_1;partial=10;start_type=ATG;rbs_motif=AGGAG;rbs_spacer=5-10bp;gc_cont=0.321\nMIKSNGILFTGKKFVMGAVVSALLATSGIAADYTLKFSHVVSPNTPKGKAADFFAKRLEE\nLSGGKIDVQVYPSSQLYNDSAVLKALRLDSVQMAAPSFSKFGKIVPQLALFNLPFLFKDI\nDQLHRVQDGPVGEKLKSLVTAKGFVALNFWDNGFKQLSSSKEPLLMPKDAEGQKFRIMSS\nKVLEAQFKAVGANPQMMPFSEVYSGLQQGVIDAAENPFSNIY\n</code></pre> <p>There are a few thing to unpack here. First lets look at the first line (the header) of the FASTA file:</p> <p>Sequence header</p> <pre><code>&gt;bin_0_NODE_6_length_607162_cov_1.000759_1 # 2 # 667 # -1 # ID=1_1;partial=10;start_type=ATG;rbs_motif=AGGAG;rbs_spacer=5-10bp;gc_cont=0.321\n|                                        |   |   |     |    |      |\nA                                        B   C   D     E    F      G\n</code></pre> Section Example text Description A <code>&gt;bin_0_NODE_6_length_607162_cov_1.000759</code> Contig ID B <code>_1</code> Gene suffix for the contig C <code>2</code> Start position relative to contig D <code>667</code> Stop position relative to contig E <code>-1</code> Orientation F <code>ID=1_1</code> Unique gene ID G <code>partial=10</code> Completeness of gene boundaries (0 = complete; 1 = partial) <p>Here are the first few pieces of information in the FASTA header identified by what they mean. <code>prodigal</code> names genes using the contig name followed by an underscore then the number of the gene along the contig. The next two pieces of information are the start and stop coordinates of the gene. Next, a 1 is report for a gene that is in the forward orientation (relative to the start of the contig) and a -1 for genes that are in reverse orientation.</p> <p>There is also a unique gene ID provided although this may not be necessary. As long as your contig names are unique, then all gene names generated from them will also be unique.</p> <p>The last option that is important to check is the partial parameter. This is reported as two digits which correspond to the start and end of the gene and report whether or not the gene has the expected amino acids for the start (M) and end of a gene (* in the protein file). A 0 indicates a complete gene edge, and 1 means partial. In this case, we have '10' which indicates the gene runs off the left edge of the contig. Alternate outcomes for this field are '00', '01', or '11'.</p>"},{"location":"day3/ex12_gene_prediction/#strip-metadata","title":"Strip metadata","text":"<p>While this header information can be very informative, its presence in the FASTA files can lead to some downstream issues. The FASTA file format specifies that the sequence name for each entry runs from the '&gt;' character to the first space, and everything after the space is metadata. Some bioinformatic programs are aware of this convention and will strip the metadata when producing their outputs, but some tools do not do this. It's really easy to end up in situations where your gene names are failing to match between analyses because of this inconsistency, so we recommend creating new FASTA files with the metadata removed to preempt this problem.</p> <p>code</p> <pre><code>for pred_file in predictions/*.fna;\ndo\n    file_base=$(basename ${pred_file} .fna)\n\n    cut -f1 -d ' ' predictions/${file_base}.fna &gt; predictions/${file_base}.no_metadata.fna\n    cut -f1 -d ' ' predictions/${file_base}.faa &gt; predictions/${file_base}.no_metadata.faa\ndone\n</code></pre> <p><code>cut</code> flags</p> <ul> <li><code>-f</code> argument is how we specify which columns to keep. It can be used to specify a range as well</li> <li><code>-d-</code> delimiter : <code>cut</code> uses tab as a default field delimiter but can also work with other delimiter by using <code>-d</code> option</li> </ul> <p>code</p> <pre><code>head -n 5 predictions/bin_0.genes.no_metadata.faa\n</code></pre> <p>Terminal output</p> <pre><code>&gt;bin_0_NODE_6_length_607162_cov_1.000759_1\nMIKSNGILFTGKKFVMGAVVSALLATSGIAADYTLKFSHVVSPNTPKGKAADFFAKRLEE\nLSGGKIDVQVYPSSQLYNDSAVLKALRLDSVQMAAPSFSKFGKIVPQLALFNLPFLFKDI\nDQLHRVQDGPVGEKLKSLVTAKGFVALNFWDNGFKQLSSSKEPLLMPKDAEGQKFRIMSS\nKVLEAQFKAVGANPQMMPFSEVYSGLQQGVIDAAENPFSNIY\n</code></pre>"},{"location":"day3/ex12_gene_prediction/#predicting-rna-features-and-non-coding-regions","title":"Predicting RNA features and non-coding regions","text":""},{"location":"day3/ex12_gene_prediction/#predict-rrna-sequences","title":"Predict rRNA sequences","text":"<p>While they will not be covered in great detail here, there are a few other prediction tools that are useful when working with metagenomic data. The first of these is <code>MeTaxa2</code>, which can be used to predict ribosomal RNA sequences in a genome. Detection of these is a handy way to link your MAGs to the scientific literature and taxonomy, although recovery of ribosomal sequences like the 16S rRNA subunit is often not successful.</p> <p>To attempt to find the small (16S, SSU) and large (28S, LSU) ribosomal subunits in our data, use the following commands.</p> <p>Create script named <code>metaxa2.sl</code></p> <pre><code>nano metaxa2.sl\n</code></pre> <p>Remember to update  to your own folder <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      metaxa2\n#SBATCH --partition     milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 4\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n\n# Load modules\nmodule purge\nmodule load Metaxa2/2.2.3-gimkl-2022a\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction\n\n# Output directory\nmkdir -p ribosomes/\n\n# Variables\nbin_file=dastool_bins/bin_${SLURM_ARRAY_TASK_ID}.fna\npred_file=$(basename ${bin_file} .fna)\n\n# Run Metaxa2\nfor ribosome_type in ssu lsu; do\n  metaxa2 --cpu $SLURM_CPUS_PER_TASK -g ${ribosome_type} --mode genome \\\n          -i ${bin_file} -o ribosomes/${pred_file}.${ribosome_type}\ndone\n</code></pre> <p>Submit the script</p> <pre><code>sbatch metaxa2.sl\n</code></pre> <p>The parameters here are fairly self-explanatory, so we won't discuss them in detail. Briefly, <code>--cpu</code> tells the program how many CPUs to use in sequence prediction, and the <code>-g</code> flag determines whether we are using the training data set for SSU or LSU regions. the <code>-i</code> and <code>-o</code> flags denote the input file and output prefix.</p> <p>The only other parameter that can be helpful is the <code>-t</code> flag to indicate taxa type (<code>b</code>, bacteria, <code>a</code>, archaea, <code>e</code>, eukaryota, <code>m</code>, mitochondrial, <code>c</code>, chloroplast, <code>A</code>, all, <code>o</code>, other). By default, <code>MeTaxa2</code> will search your genome for the following ribosomal signatures:</p> <ol> <li>Bacteria</li> <li>Archaea</li> <li>Chloroplast</li> <li>Mitochondira</li> <li>Eukaryote</li> </ol> <p>It is usually worth letting it search for all options as detecting multiple rRNAs from different lineages can be a good sign of binning contamination. However, if you want to restrict the search or provide a custom training set this can be set with the <code>-t</code> flag.</p>"},{"location":"day3/ex12_gene_prediction/#predict-trna-and-tmrna-sequences","title":"Predict tRNA and tmRNA sequences","text":"<p>The MIMAG standard specifies that in order to reach particular quality criteria, a MAG must contain a certain number or tRNA sequences. We can search a MAG or genome for these using <code>Aragorn</code> (link here).</p> <p></p>"},{"location":"day3/ex13_gene_annotation_part1/","title":"Gene annotation I: BLAST-like and HMM","text":"<p>Objectives</p> <ul> <li>Annotation methods</li> <li>Annotating MAGs against the UniProt database with <code>DIAMOND</code></li> <li>Annotating MAGs against the Pfam database with <code>HMMER</code></li> <li>Annotating signal peptides with <code>SignalP6</code></li> <li>Evaluating the quality of gene assignment</li> <li>Differences in taxonomies</li> </ul>"},{"location":"day3/ex13_gene_annotation_part1/#annotation-methods","title":"Annotation methods","text":"<p>Broadly speaking, there are two ways we perform gene annotations with protein sequences. Both compare our sequences of interest against a curated set of protein sequences for which function is known, or is strongly suspected. In each case, there are particular strengths to the approach and for particular research questions, one option may be favoured over another.</p>"},{"location":"day3/ex13_gene_annotation_part1/#blast-like-annotation","title":"<code>BLAST</code>-like annotation","text":"<p>The first of these is the <code>BLAST</code> algorithm for sequence alignment. This approach performs pairwise alignment between the gene of interest (query sequence) and the sequences in the database (target sequence). <code>BLAST</code> searches each potential target sequence for k-mers identified in the query sequence. Where these k-mers are found in targets, the ends are extended out to try to create longer regions of highly similar sequence spans. Across this span, the tool identifies the longest span of characters (nucleotide or amino acid) that match within a scoring framework to return the length of the region (coverage) and the sequence identity over the span (identity).</p> <p>The original tool for performing this kind of analysis was the <code>BLAST</code> tool. While <code>BLAST</code> and its variants are still excellent tools for performing this kind of sequence annotation, they suffer from a slow runtime speed due to the need to test each query sequence against every target sequence in the database. For this reason, several tools have been published which take the basic approach of <code>BLAST</code>, but augment it with methods to reduce the number of pairwise comparisons needed to identify targets with high sequence similarity to the query. Two popular pieces of software are the tools <code>USEARCH</code> and <code>DIAMOND</code>.</p>"},{"location":"day3/ex13_gene_annotation_part1/#hmm-profiling-of-domains","title":"HMM-profiling of domains","text":"<p>An alternate method for attributing function to query sequences is to consider them as a collection of independently functioning protein folding domains. This is the approach used in the HMMER software, and the Pfam, TIGRfam, and PANTHER databases. In these analyses, the database consists not of individual sequences, but of Hidden Markov models built from a collection of proteins that share a common domain. These profiles build out a statistical map of the amino acid transitions (from position to position), variations (differences at a position), and insertions/deletions between positions in the domain across the different observations in the training database and apply these maps to the query data.</p> <p>These exercises will take place in the <code>10.gene_annotation_and_coverage/</code> folder. </p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n</code></pre>"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-uniprot-database-with-diamond","title":"Annotating MAGs against the UniProt database with <code>DIAMOND</code>","text":"<p>For this exercise we are going to use <code>DIAMOND</code> for performing our annotation. We have chosen to use this tool because it is faster than <code>BLAST</code>, and <code>USEARCH</code> comes with licensing restrictions that make it hard to work with in a shared computing environment like NeSI.</p> <p>For this exercise we have created a diamond-compatible database from the 2024 release of the UniProt database.</p> <p>For input files, the <code>predictions/</code> results from the previous gene prediction exercise have been copied over to <code>10.gene_annotation_and_coverage/predictions/</code>.</p> <p>In general, <code>diamond</code> takes a pair of input files - the protein coding sequences we wish to annotate and the database we will use for this purpose. There are a few parameters that need to be tweaked for obtaining a useful output file, however.</p> <p>code</p> <pre><code>module purge\nmodule load DIAMOND/2.1.6-GCC-11.3.0\n\ndiamond help\n</code></pre> <p>Terminal output</p> <pre><code>diamond v2.0.15.153 (C) Max Planck Society for the Advancement of Science\nDocumentation, support and updates available at http://www.diamondsearch.org\nPlease cite: http://dx.doi.org/10.1038/s41592-021-01101-x Nature Methods (2021)\n\nSyntax: diamond COMMAND [OPTIONS]\n\nCommands:\n...\nblastp  Align amino acid query sequences against a protein reference database\n...\n\nGeneral options:\n--threads (-p)         number of CPU threads\n--db (-d)              database file\n--out (-o)             output file\n--outfmt (-f)          output format\n...\n</code></pre> <p>There are two output formats we can chose from which are useful for our analysis. We will obtain our output in the BLAST tabular format, which provides the annotation information in a simple-to-parse text file that can be viewed in any text or spreadsheet viewing tool. This will allow us to investigate and evaluate the quality of our annotations. </p> <p>Awkwardly, <code>DIAMOND</code> does not provide the headers for what the columns in the output table mean. This table is a handy reference for how to interpret the output.</p> <p>From here we can view important statistics for each query/target pairing such as the number of identical residues between sequences and the aligned length between query and target.</p> <p>Before we begin, we need to create an directory for outputs.</p> <p>code</p> <pre><code>mkdir -p gene_annotations\n</code></pre> <p>Now, lets set up a slurm job to annotate each of our MAGs. </p> <p>Create a new script</p> <p>Create script named <code>annotate_uniprot.sl</code></p> <pre><code>nano annotate_uniprot.sl\n</code></pre> <p>Remember to update  to your own folder <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_uniprot\n#SBATCH --partition     milan\n#SBATCH --time          01:00:00\n#SBATCH --mem           20GB\n#SBATCH --cpus-per-task 20\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n\n# Load modules\nmodule purge\nmodule load DIAMOND/2.1.6-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Variables\nprot_file=predictions/bin_${SLURM_ARRAY_TASK_ID}.genes.no_metadata.faa\nout_file=$(basename ${prot_file} .faa)\ndb=/nesi/nobackup/nesi02659/MGSS_2024/resources/databases/uniprot.20240724.dmnd\n\n# Run DIAMOND\ndiamond blastp --threads $SLURM_CPUS_PER_TASK --max-target-seqs 5 --evalue 0.001 \\\n               --db $db --query ${prot_file} --outfmt 6 \\\n               --out gene_annotations/${out_file}.uniprot.txt\n</code></pre> <p>Submit the script</p> <pre><code>sbatch annotate_uniprot.sl\n</code></pre>"},{"location":"day3/ex13_gene_annotation_part1/#annotating-mags-against-the-pfam-database-with-hmmer","title":"Annotating MAGs against the Pfam database with <code>HMMER</code>","text":"<p>The standard software for performing HMM-profiling annotation is HMMER. Compared to <code>BLAST</code>, <code>FASTA</code>, and other sequence alignment and database search tools based on older scoring methodology, <code>HMMER</code> aims to be significantly more accurate and more able to detect remote homologs because of the strength of its underlying mathematical models. In the past, this strength came at significant computational expense, but in the new <code>HMMER3</code> project, <code>HMMER</code> is now essentially as fast as <code>BLAST</code>. </p> <p><code>HMMER</code> will search one or more profiles against a sequence database for sequence hommologs, and for making sequence alignments, implementing profile hidden Markov models. In this exercise, we will perform a search using <code>hmmsearch</code>. For each profile in hmmfile, <code>HMMER</code> uses that query profile to search the target database of sequences indicated in seqdb, and output ranked lists of the sequences with the most significant matches to the profile. <code>hmmsearch</code> accepts any fastA file as target database input. It also accepts EMBL/UniProtKB text format, and Genbank format. It will automatically determine what format your file is in so you don\u2019t have to specify it. </p> <p>As we did with <code>diamond</code>, we will also have to modify some parameters to get the desired ouotput. </p> <p>code</p> <pre><code>module load HMMER/3.3.2-GCC-11.3.0\n\nhmmsearch -h\n</code></pre> <code>hmmsearch -h</code> <pre><code>hmmsearch :: search profile(s) against a sequence database\nHMMER 3.3.2 (Nov 2020); http://hmmer.org/\nCopyright (C) 2020 Howard Hughes Medical Institute.\nFreely distributed under the BSD open source license.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsearch [options] &lt;hmmfile&gt; &lt;seqdb&gt;\n\nBasic options:\n -h : show brief help on version and usage\n\nOptions directing output:\n...\n--tblout &lt;f&gt;     : save parseable table of per-sequence hits to file &lt;f&gt;\n....\n\nOptions controlling reporting thresholds:\n...\n-E &lt;x&gt;     : report sequences &lt;= this E-value threshold in output  [10.0]  (x&gt;0)\n...\n\nOther expert options:\n...\n--cpu &lt;n&gt;     : number of parallel CPU workers to use for multithreads\n...\n</code></pre> <p>We are now going to submit another slurm job to annotate our MAGs using the Pfam database. Pfam used to have a standalone website, but it has recently been integrated into InterPro maintained by the European Bioinformatics Institute (see announcement). Matching sequences to a <code>Pfam</code> entry allows us to transfer the functional information from an experimentally characterised sequence to uncharacterised sequences in the same entry. <code>Pfam</code> then provides comprehensive annotation for each entry.</p> <p>Create script named <code>annotate_pfam.sl</code></p> <pre><code>nano annotate_pfam.sl\n</code></pre> <p>Remember to update  to your own folder <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_pfam\n#SBATCH --partition     milan\n#SBATCH --time          00:20:00\n#SBATCH --mem           5GB\n#SBATCH --cpus-per-task 10\n#SBATCH --array         0-9\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n\n# Load modules\nmodule purge\nmodule load HMMER/3.3.2-GCC-11.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Variables\nprot_file=predictions/bin_${SLURM_ARRAY_TASK_ID}.genes.no_metadata.faa\nout_file=$(basename ${prot_file} .faa)\ndb=/nesi/nobackup/nesi02659/MGSS_2024/resources/databases/Pfam-A.hmm\n\n# Run HMMER\nhmmsearch --tblout gene_annotations/${out_file}.pfam.txt -E 0.001 \\\n          --cpu $SLURM_CPUS_PER_TASK \\\n          ${db} ${prot_file}\n</code></pre> <p>Submit the script</p> <pre><code>sbatch annotate_pfam.sl\n</code></pre>"},{"location":"day3/ex13_gene_annotation_part1/#annotating-signal-peptides-of-predicted-genes-using-signalp6","title":"Annotating signal peptides of predicted genes using <code>SignalP6</code>","text":"<p>In addition to putative function, we can also predict whether a protein is secreted or not using <code>SignalP</code>. In the 5<sup>th</sup> version, users needed to know the Gram status of the organism for the software to predict signal peptides. In 2022, the developers (Teufel et al., 2022) produced a 6<sup>th</sup> version that uses protein language models (like ChatGPT, for proteins!) to predict the presence of signal peptides across the domains of life.</p> <p><code>SignalP6</code> has several models depending on required accuracy and hardware availability. Here, we are running the fast model on CPU. There are models built for GPU (they are interconvertible and the software package comes with utilities for easy conversion) that is blazingly fast (most language models require GPUs for accelerations). However, GPUs on NeSI are a coveted resource. For our small dataset, the fast model on CPU is sufficient as an example.</p> <p><code>SignalP6</code> is licensed software</p> <p>In terms of computational capability to detect signal peptides, there is nothing like SignalP6. It is free for academic use and other models can be requested from the developers. If you are not running this on NeSI, you will need to acquire your own license.</p> <p>code</p> <pre><code>nano annotate_signal_peptide.sl\n</code></pre> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      signal_peptide\n#SBATCH --partition     milan\n#SBATCH --time          00:30:00\n#SBATCH --mem           10GB\n#SBATCH --cpus-per-task 24\n#SBATCH --error         %x_%A_%a.err\n#SBATCH --output        %x_%A_%a.out\n#SBATCH --array         0-9\n\n# Load modules\nmodule purge\nmodule load SignalP/6.0g-gimkl-2022a-Python-3.10.5\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Array variables\nfiles=(predictions/*.faa)\nbin=${files[$SLURM_ARRAY_TASK_ID]}\nprefix=$(basename ${bin} genes.no_metadata.faa)\n\n# Run SignalP\nsignalp6 --fastafile ${bin} --output_dir signal_peptides/${prefix} \\\n         --format none --organism other --bsize 50 \\\n         --write_procs $SLURM_CPUS_PER_TASK\n</code></pre> <p><code>SignalP</code> will probably <code>TIMEOUT</code></p> <p>The above script will almost, inevitably, time out before completing every MAG. However, that is not critical for our purposes today. We simply need at least one of the array jobs to complete (several will) for us to explore the output.</p> <p>The output of the script above are directories per bin that contain 5 files:</p> <ul> <li><code>predicted_results.txt</code> the main output with gene ID and the predicted signal peptide (\"Other\" means none predicted), the prediction probability (i.e., confidence), and the cleavage site</li> <li><code>output.gff3</code> A GFF3-format output of sequences with start and end positions of signal peptides relative to each amino acid sequence</li> <li><code>output.json</code> Similar to above, with more information about run parameters</li> <li><code>processed_entries.fasta</code> Amino acid sequences with the signal peptide cleaved off (i.e., predicted mature proteins)</li> <li><code>region_output.gff3</code> Signal peptides have specific regions characteristic of each type of signal peptide. These usually inform us about cleavage sites for those proteins. This output provides this information. </li> </ul> <p><code>--format none</code></p> <p>If you choose to run the above script with other formats (e.g., txt, png, all), per-sequence data outputs and prediction plots (if requested) will be produced. For large numbers of amino acid sequences, this can be problematic as it uses up a lot of space. Unless you're needing to query specific parts of a few proteins, it is advised to stick to \"none\" to output summaries only.</p>"},{"location":"day3/ex13_gene_annotation_part1/#evaluating-the-quality-of-gene-assignment","title":"Evaluating the quality of gene assignment","text":"<p>Determining how trustworthy a gene annotation is can be a very tricky process. How similar do protein sequences need to be to perform the same function? The answer is surprisingly low. A bioinformatic analysis performed in 1999 identified that proteins with as little as 20 - 35% sequence identity can still share the same function (Rost, 1999), but this is not a universal occurrence. When evaluating annotations, consider the following questions:</p> <ol> <li>What is the amino acid identity along the aligned region?</li> <li>What is the amino acid similarity between the aligned region?</li> <li>What is the coverage as a percentage of the query and target genes?</li> <li>If we infer a phylogeny of this query gene with references from the target family, is a stable tree resolved?</li> <li>Does the inclusion of this gene function make sense in the context of the organism's taxonomy?</li> <li>Does the gene sit on a long contig that is core to the MAG, or is it a short contig carrying only a single gene?</li> <li>If we are uncertain of a particular annotation, does the predicted gene occur in an operon? If so, are the other genes present in the annotation?</li> </ol> <p>We must also remain aware of the potential for incorrectly annotated genes in the annotation database and that proteins can perform multiple functions (and may therefore be attributed multiple, inconsistent annotations). Furthermore, it is also important to consider exactly which part of the target gene the alignment is happening across. There are several catalytic centers of enzymes, such as the Fe-S cofactor, which are shared across many different proteins, and if your annotation is only spanning one of these regions then it may simply be the case that you are identifying a generic electron accepting or donating domain.</p>"},{"location":"day3/ex13_gene_annotation_part1/#differences-in-taxonomies","title":"Differences in taxonomies","text":"<p>Another way to determine if an annotation 'belongs' in the MAG of interest is to consider the predicted taxonomy of the query gene with that of the MAG itself. For example, if you detect a Desulfovibrio-like dsrA sequence in a bin that has been classified as belonging to the genus Desulfovibrio then it is probably a safe bet that the annotation is correct.</p> <p>However, when comparing taxonomic assignments, it is important to be aware of the differing taxonomic schemas that are circulating in the microbiological and bioinformatic literature and to know how to reconcile their differences. Similar to how the 16S rRNA gene taxonomies provided by SILVA, Greengenes, and RDP taxonomies all differ in some aspects, there are multiple competing taxonomies in protein databases.</p> <p>This problem exists despite the existence of a formal Code for the naming of bacteria and archaea, because </p> <ol> <li>There are no rules governing how we define the grouping of these names together, other than for type species</li> <li>Defunct synonyms and basonyms are not correctly purged from taxonomy lists (this is quite noticeable with the NCBI taxonomy)</li> <li>Valid names cannot be assigned for uncultivated organisms, meaning there are many informal placeholder names in the literature. For example, clades like WPS-2, SAR324, and SAUL are widely cited in the literature despite having no official standing</li> </ol> <p>It is therefore important to periodically sanity check your taxonomic annotations in order to avoid splitting taxa based on spelling differences or the use of historic names that have since been reclassified.</p>"},{"location":"day3/ex14_gene_annotation_part2/","title":"Gene annotation II: DRAM and coverage calculation","text":"<p>Objectives</p> <ul> <li>Gene prediction and annotation with <code>DRAM</code></li> <li>Annotating MAGs with <code>DRAM</code></li> <li>Annotating viral contigs with <code>DRAM-v</code></li> <li>Calculating per-sample coverage stats for prokaryotic bins</li> <li>Calculating per-sample coverage stats for viral contigs</li> <li>Select initial goal</li> </ul>"},{"location":"day3/ex14_gene_annotation_part2/#gene-prediction-and-annotation-with-dram-distilled-and-refined-annotation-of-metabolism","title":"Gene prediction and annotation with <code>DRAM</code> (Distilled and Refined Annotation of Metabolism)","text":"<p><code>DRAM</code> is a tool designed to profile microbial (meta)genomes for metabolisms known to impact ecosystem functions across biomes. <code>DRAM</code> annotates MAGs and viral contigs using KEGG (if provided by user), UniRef90, PFAM, CAZy, dbCAN, RefSeq viral, VOGDB (Virus Orthologous Groups), and the MEROPS peptidase database. It is also highly customizable to other custom user databases.</p> <p><code>DRAM</code> only uses assembly-derived FASTA files input by the user. These input files may come from unbinned data (metagenome contig or scaffold files) or genome-resolved data from one or many organisms (isolate genomes, single-amplified genome (SAGs), MAGs).</p> <p><code>DRAM</code> is run in two stages: annotation and distillation.</p> <p></p>"},{"location":"day3/ex14_gene_annotation_part2/#1-annotation","title":"1. Annotation","text":"<p>The first step in <code>DRAM</code> is to annotate genes by assigning database identifiers to genes. Short contigs (default &lt; 2,500 bp) are initially removed. Then, <code>Prodigal</code> is used to detect open reading frames (ORFs) and to predict their amino acid sequences. Next, <code>DRAM</code> searches all amino acid sequences against multiple databases, providing a single Raw output. When gene annotation is complete, all results are merged in a single tab-delimited annotation table, including the best hit for each database for user comparison.</p>"},{"location":"day3/ex14_gene_annotation_part2/#2-distillation","title":"2. Distillation","text":"<p>After genome annotation, a distill step follows with the aim to curate these annotations into useful functional categories, creating genome statistics and metabolism summary files, which are stored in the Distillate output. The genome statistics provides most genome quality information required for MIMAG standards, including <code>GTDB-tk</code> and <code>CheckM</code> information if provided by the user. The summarised metabolism table includes the number of genes with specific metabolic function identifiers (KO, CAZY ID, etc) for each genome, with information obtained from multiple databases. The Distillate output is then further distilled into the Product, an html file displaying a heatmap, as well as the corresponding data table. We will investigate all these files later on.  </p>"},{"location":"day3/ex14_gene_annotation_part2/#annotating-mags-with-dram","title":"Annotating MAGs with <code>DRAM</code>","text":"<p>Beyond annotation, <code>DRAM</code> aims to be a data compiler. For that reason, output files from both <code>CheckM</code> and <code>GTDB_tk</code> steps can be input to <code>DRAM</code> to provide both taxonomy and genome quality information of the MAGs. </p>"},{"location":"day3/ex14_gene_annotation_part2/#dram-input-files","title":"<code>DRAM</code> input files","text":"<p>For these exercises, we have copied the relevant input files into the folder <code>10.gene_annotation_and_coverage/DRAM_input_files/</code>. <code>gtdbtk.bac120.summary.tsv</code> was taken from the earlier <code>8.prokaryotic_taxonomy/gtdbtk_out/</code> outputs, and <code>dastool_bins_checkm.txt</code> from the result of re-running <code>CheckM</code> on the final refined filtered bins in <code>6.bin_refinement/dastool_bins</code>.</p> <p>Navigate to working directory</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n</code></pre> <p>Along with our filtered bins, the <code>CheckM</code> output file (<code>checkm.txt</code>) and <code>GTDB-Tk</code> summary output <code>gtdbtk.bac120.summary.tsv</code> are used as inputs as is.</p>"},{"location":"day3/ex14_gene_annotation_part2/#dram-annotation","title":"<code>DRAM</code> annotation","text":"<p>In default annotation mode, <code>DRAM</code> only requires as input the directory containing all the bins we would like to annotate in fastA format (either .fa or .fna). There are few parameters that can be modified if not using the default mode. Once the annotation step is complete, the mode <code>distill</code> is used to summarise the obtained results.</p> <p>UniRef and RAM requirements</p> <p>Due to the increased memory requirements, annotations against the UniRef90 database is not set by default and the flag <code>\u2013use_uniref</code> should be specified in order to search amino acid sequences against UniRef90. In this exercise, due to memory and time constraints, we won't be using the UniRef90 database.</p> <p>We will start by glancing at some of the options for <code>DRAM</code>.</p> <p>code</p> <pre><code>module purge\nmodule load DRAM/1.3.5-Miniconda3\n\nDRAM.py --help\n</code></pre> <p>Terminal output</p> <pre><code>usage: DRAM.py [-h] {annotate,annotate_genes,distill,strainer,neighborhoods,merge_annotations} ...\n\npositional arguments:\n  {annotate,annotate_genes,distill,strainer,neighborhoods,merge_annotations}\n    annotate            Annotate genomes/contigs/bins/MAGs\n    annotate_genes      Annotate already called genes, limited functionality compared to annotate\n    distill             Summarize metabolic content of annotated genomes\n    strainer            Strain annotations down to genes of interest\n    neighborhoods       Find neighborhoods around genes of interest\n    merge_annotations   Merge multiple annotations to one larger set\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre> <p>To look at some of the arguments in each command, type the following:</p> <p>code</p> <pre><code># DRAM.py &lt;command&gt; --help\n\n# For example:\nDRAM.py annotate --help\n</code></pre>"},{"location":"day3/ex14_gene_annotation_part2/#submitting-dram-annotation-as-a-slurm-job","title":"Submitting <code>DRAM</code> annotation as a slurm job","text":"<p>To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. </p> <p>Create script named <code>annotate_dram.sl</code></p> <pre><code>nano annotate_dram.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_DRAM\n#SBATCH --partition     milan\n#SBATCH --time          5:00:00\n#SBATCH --mem           30Gb\n#SBATCH --cpus-per-task 24\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load DRAM/1.3.5-Miniconda3\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run DRAM\nDRAM.py annotate -i 'dastool_bins/*.fna' \\\n                 --checkm_quality DRAM_input_files/checkm.txt \\\n                 --gtdb_taxonomy DRAM_input_files/gtdbtk.bac120.summary.tsv \\\n                 -o dram_annotations --threads $SLURM_CPUS_PER_TASK\n</code></pre> <p>Submit the job</p> <pre><code>sbatch annotate_dram.sl\n</code></pre> <p>The program will take 4-4.5 hours to run, so we will submit the jobs and inspect the results tomorrow morning.</p>"},{"location":"day3/ex14_gene_annotation_part2/#annotating-viral-contigs-with-dram-v","title":"Annotating viral contigs with <code>DRAM-v</code>","text":"<p><code>DRAM</code> also has an equivalent program (<code>DRAM-v</code>) developed for predicting and annotating genes of viral genomes. A number of the options are similar to the standard <code>DRAM</code> run above, although the selection of databases differs slightly, and the subsequent distill step is focussed on identifying and classifying auxilliary metabolic genes (AMGs) rather than the metabolic pathways output by <code>DRAM</code>'s standard distill step.</p> <p>To see more details on options for <code>DRAM-v</code> we can run the same <code>--help</code> command as above:</p> <p>code</p> <pre><code># DRAM-v.py &lt;command&gt; --help\n\n# For example:\nDRAM-v.py annotate --help\nDRAM-v.py distill --help\n</code></pre>"},{"location":"day3/ex14_gene_annotation_part2/#submit-dram-v-annotation-as-a-slurm-job","title":"Submit <code>DRAM-v</code> annotation as a slurm job","text":"<p>To run this exercise we first need to set up a slurm job. We will use the results for tomorrow's distillation step. </p> <p>Note</p> <p><code>DRAM-v</code> requires the <code>mgss-for-dramv/</code> files <code>final-viral-combined-for-dramv.fa</code> and <code>viral-affi-contigs-for-dramv.tab</code> that were generated by <code>VirSorter2</code>. These have been copied into <code>10.gene_annotation_and_coverage/</code> for this exercise.</p> <p>Create script named <code>annotate_dramv.sl</code></p> <pre><code>nano annotate_dramv.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      annotate_DRAMv\n#SBATCH --partition     milan\n#SBATCH --time          02:00:00\n#SBATCH --mem           10Gb\n#SBATCH --cpus-per-task 12\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load DRAM/1.3.5-Miniconda3\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run DRAM-v\nDRAM-v.py annotate --threads ${SLURM_CPUS_PER_TASK} \\\n                   --min_contig_size 1000 \\\n                   -i mgss-for-dramv/final-viral-combined-for-dramv.fa \\\n                   -v mgss-for-dramv/viral-affi-contigs-for-dramv.tab \\\n                   -o dramv_annotations\n</code></pre> <p>Submit the job</p> <pre><code>sbatch annotate_dramv.sl\n</code></pre> <p>We will submit this job now and inspect the results tomorrow morning.</p>"},{"location":"day3/ex14_gene_annotation_part2/#calculating-per-sample-coverage-stats-for-prokaryotic-bins","title":"Calculating per-sample coverage stats for prokaryotic bins","text":"<p>One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? With bins of metagenome-assembled genome (MAG) data, we can investigate this by mapping the quality-filtered unassembled reads back to the refined bins to then generate coverage profiles. Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. </p> <p>As per the preparation step at the start of the binning process, we can do this using read mapping tools such as <code>Bowtie</code>, <code>Bowtie2</code>, and <code>BBMap</code>. Here we will follow the same steps as before using <code>Bowtie2</code>, <code>samtools</code>, and <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>, but this time inputting our refined filtered bins. </p> <p>These exercises will take place in the <code>10.gene_annotation_and_coverage/</code> folder. Our final filtered refined bins from the previous bin refinement exercise have been copied to the <code>10.gene_annotation_and_coverage/dastool_bins/</code> folder.</p> <p>First, concatenate the bin data into a single file to then use to generate an index for the read mapper.</p> <p>Concatenate bin nucleotide FASTA files</p> <pre><code>cat dastool_bins/*.fna &gt; dastool_bins.fna\n</code></pre> <p>Now build the index for <code>Bowtie2</code> using the concatenated bin data. We will also make a new directory <code>bin_coverage/</code> to store the index and read mapping output into.</p> <p>Build <code>Bowtie2</code> index</p> <pre><code>mkdir -p bin_coverage/\n\n# Load Bowtie2\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0\n\n# Build Bowtie2 index\nbowtie2-build dastool_bins.fna bin_coverage/bw_bins\n</code></pre> <p>Map the quality-filtered reads (from <code>../3.assembly/</code>) to the index using <code>Bowtie2</code>, and sort and convert to <code>.bam</code> format via <code>samtools</code>.</p> <p>Create script named <code>mapping_dastool_bins.sl</code></p> <pre><code>nano mapping_dastool_bins.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      mapping_dastool_bins\n#SBATCH --partition     milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0 SAMtools/1.19-GCC-12.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run Bowtie2\nfor i in {1..4}; do\n    bowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n            -x bin_coverage/bw_bins \\\n            -1 ../3.assembly/sample${i}_R1.fastq.gz \\\n            -2 ../3.assembly/sample${i}_R2.fastq.gz \\\n            -S bin_coverage/sample${i}.sam\n    samtools sort -@ $SLURM_CPUS_PER_TASK -o bin_coverage/sample${i}.bam bin_coverage/sample${i}.sam\ndone\n</code></pre> <p>Submit the script</p> <pre><code>sbatch mapping_dastool_bins.sl\n</code></pre> <p>Finally, generate the per-sample coverage table for each contig in each bin via <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>.</p> <p>Obtain coverage values</p> <pre><code># Load MetaBAT\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# Calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth bins_cov_table.txt bin_coverage/sample*.bam\n</code></pre> <p>The coverage table will be generated as <code>bins_cov_table.txt</code>. As before, the key columns of interest are the <code>contigName</code>, and each <code>sample[1-n].bam</code> column.</p> <p>Note</p> <p>Here we are generating a per-sample table of coverage values for each contig within each bin. To get per-sample coverage of each bin as a whole, we will need to generate average coverage values based on all contigs contained within each bin. We will do this in <code>R</code> during our data visualisation exercises on day 4 of the workshop, leveraging the fact that we added bin IDs to the sequence headers.*</p>"},{"location":"day3/ex14_gene_annotation_part2/#calculating-per-sample-coverage-stats-for-viral-contigs","title":"Calculating per-sample coverage stats for viral contigs","text":"<p>Here we can follow the same steps as outlined above for the bin data, but with a concatenated FASTA file of viral contigs. </p> <p>A quick recap</p> <ul> <li>In previous exercises, we first used <code>VirSorter2</code> to identify viral contigs from the assembled reads, generating a new fasta file of viral contigs: <code>final-viral-combined.fa</code> </li> <li>We then processed this file using <code>CheckV</code> to generate quality information for each contig, and to further trim any retained (prokaryote) sequence on the ends of prophage contigs. </li> </ul> <p>The resultant fasta files generated by <code>CheckV</code> (<code>proviruses.fna</code> and <code>viruses.fna</code>) have been copied to to the <code>10.gene_annotation_and_coverage/checkv</code> folder for use in this exercise.</p> <p>Note</p> <p>Due to the rapid mutation rates of viruses, with full data sets it will likely be preferable to first further reduce viral contigs down based on a percentage-identity threshold using a tool such as <code>BBMap</code>'s <code>dedupe.sh</code> or Cluster_genomes_5.1.pl from Simon Roux's group. This would be a necessary step in cases where you had opted for generating multiple individual assemblies or mini-co-assemblies (and would be comparable to the use of a tool like <code>dRep</code> for prokaryote data), but may still be useful even in the case of single co-assemblies incorporating all samples.*</p> <p>We will first need to concatenate these files together.</p> <p>code</p> <pre><code>cat checkv/proviruses.fna checkv/viruses.fna &gt; checkv_combined.fna\n</code></pre> <p>Now build the index for <code>Bowtie2</code> using the concatenated viral contig data. We will also make a new directory <code>viruses_coverage/</code> to store the index and read mapping output into.</p> <p>code</p> <pre><code>mkdir -p viruses_coverage/\n\n# Load Bowtie2\nmodule load Bowtie2/2.5.4-GCC-12.3.0\n\n# Build Bowtie2 index\nbowtie2-build checkv_combined.fna viruses_coverage/bw_viruses\n</code></pre> <p>Map the quality-filtered reads (from <code>../3.assembly/</code>) to the index using <code>Bowtie2</code>, and sort and convert to <code>.bam</code> format via <code>samtools</code>.</p> <p>Create script named <code>mapping_viruses.sl</code></p> <pre><code>nano mapping_viruses.sl\n</code></pre> <p>Remember to update <code>&lt;YOUR FOLDER&gt;</code> to your own folder</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      mapping_viruses\n#SBATCH --partition     milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Load modules\nmodule purge\nmodule load Bowtie2/2.5.4-GCC-12.3.0 SAMtools/1.19-GCC-12.3.0\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# Run Bowtie2\nfor i in {1..4}; do\n  bowtie2 --minins 200 --maxins 800 --threads $SLURM_CPUS_PER_TASK --sensitive \\\n            -x viruses_coverage/bw_viruses \\\n            -1 ../3.assembly/sample${i}_R1.fastq.gz \\\n            -2 ../3.assembly/sample${i}_R2.fastq.gz \\\n            -S viruses_coverage/sample${i}.sam\n  samtools sort -@ $SLURM_CPUS_PER_TASK -o viruses_coverage/sample${i}.bam viruses_coverage/sample${i}.sam\ndone\n</code></pre> <p>Run the script</p> <pre><code>sbatch mapping_viruses.sl\n</code></pre> <p>Finally, generate the per-sample coverage table for each viral contig via <code>MetaBAT</code>'s <code>jgi_summarize_bam_contig_depths</code>.</p> <p>code</p> <pre><code># Load MetaBAT\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth viruses_cov_table.txt viruses_coverage/sample*.bam\n</code></pre> <p>The coverage table will be generated as <code>viruses_cov_table.txt</code>. As before, the key columns of interest are the <code>contigName</code>, and each <code>sample[1-n].bam</code> column.</p> <p>Note</p> <p>Unlike the prokaryote data, we have not used a binning process on the viral contigs (since many of the binning tools use hallmark characteristics of prokaryotes in the binning process). Here, <code>viruses_cov_table.txt</code> is the final coverage table. This can be combined with <code>CheckV</code> quality and completeness metrics to, for example, examine the coverage profiles of only those viral contigs considered to be \"High-quality\" or \"Complete\".* </p>"},{"location":"day3/ex14_gene_annotation_part2/#normalise-coverage-values","title":"Normalise coverage values","text":"<p>Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. </p> <p>For an example of one way in which the <code>cov_table.txt</code> output generated by <code>jgi_summarize_bam_contig_depths</code> above could then be normalised based on average library size, see the Normalise per-sample coverage Appendix.</p>"},{"location":"day3/ex14_gene_annotation_part2/#select-initial-goal","title":"Select initial goal","text":"<p>It is now time to select the goals to investigate the genomes you have been working with. We ask you to select one of the following goals:</p> <ol> <li>Denitrification (Nitrate or nitrite to nitrogen)</li> <li>Ammonia oxidation (Ammonia to nitrite or nitrate)</li> <li>Anammox (Ammonia and nitrite to nitrogen)</li> <li>Sulfur oxidation (SOX pathway, thiosulfate to sulfate)</li> <li>Sulfur reduction (DSR pathway, sulfate to sulfide)</li> <li>Photosynthetic carbon fixation</li> <li>Non-photosynthetic carbon fixation (Reverse TCA or Wood-Ljundahl)</li> <li>Non-polar flagella expression due to a chromosomal deletion</li> <li>Plasmid-encoded antibiotic resistance</li> <li>Aerobic (versus anaerobic) metabolism</li> </ol> <p>Depending on what you are looking for, you will either be trying to find gene(s) of relevance to a particular functional pathway, or the omission of genes that might be critical in function. In either case, make sure to use the taxonomy of each MAG to determine whether it is likely to be a worthwhile candidate for exploration, as some of these traits are quite restricted in terms of which organisms carry them.</p> <p>To conduct this exersise, you should use the information generated with <code>DRAM</code> as well as the annotation files we created previously that will be available in the directory <code>10.gene_annotation_and_coverage/gene_annotations</code>. </p>"},{"location":"day4/ex15_gene_annotation_part3/","title":"Gene annotation III: DRAM distillation","text":"<p>Objectives</p> <ul> <li>Overview of <code>DRAM.py annotate</code> and <code>DRAM-v.py annotate</code> output</li> <li><code>DRAM</code> and <code>DRAM-v</code> distillation step and visualization of results</li> <li>Tie findings to your initial goal</li> </ul>"},{"location":"day4/ex15_gene_annotation_part3/#overview-of-drampy-annotate-output","title":"Overview of <code>DRAM.py annotate</code> output","text":"<p>The submitted jobs from the previous session should now be completed. If we examine the output directory <code>10.gene_annotation_and_coverage/dram_annotations/</code> we will see the following files:</p> File name Description <code>genes.faa</code> and <code>genes.fna</code> FASTA files with all the genes called by <code>Prodigal</code>, with additional header information gained from the annotation as nucleotide and amino acid records, respectively <code>genes.gff</code> GFF3 file with the same annotation information as well as gene locations <code>scaffolds.fna</code> A collection of all scaffolds/contigs given as input to <code>DRAM.py annotate</code> with added bin information <code>annotations.tsv</code> This file includes all annotation information about every gene from all MAGs <code>trnas.tsv</code> Summary of the tRNAs found in each MAG <code>rrnas.tsv</code> Summary of the rRNAs found in each MAG <p>If we inspect the head of the annotation file we will see the following</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n\nhead -n 5 dram_annotations/annotations.tsv \n</code></pre> <p>Terminal output</p> <pre><code>        fasta   scaffold        gene_position   start_position  end_position    strandednessrank     ko_id   kegg_hit        peptidase_id    peptidase_family        peptidase_hit   peptidase_RBH        peptidase_identity      peptidase_bitScore      peptidase_eVal  pfam_hits   cazy_id  cazy_hits       heme_regulatory_motif_count     bin_taxonomy    bin_completeness    bin_contamination\nbin_0_bin_0_NODE_11_length_360679_cov_0.995524_1        bin_0   bin_0_NODE_11_length_360679_cov_0.995524     1       1       213     -1      C       K00873  pyruvate kinase [EC:2.7.1.40]Pyruvate kinase, barrel domain [PF00224.24]                     0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis       99.59   3.39\nbin_0_bin_0_NODE_11_length_360679_cov_0.995524_2        bin_0   bin_0_NODE_11_length_360679_cov_0.995524     2       253     909     -1      C       K22293  GntR family transcriptional regulator, rspAB operon transcriptional repressor                                            FCD domain [PF07729.15]; Bacterial regulatory proteins, gntR family [PF00392.24]             0d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis  99.59   3.39\nbin_0_bin_0_NODE_11_length_360679_cov_0.995524_3        bin_0   bin_0_NODE_11_length_360679_cov_0.995524     3       1010    1843    -1      D                                           Universal stress protein family [PF00582.29]                     0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis       99.59   3.39\nbin_0_bin_0_NODE_11_length_360679_cov_0.995524_4        bin_0   bin_0_NODE_11_length_360679_cov_0.995524     4       1865    3397    -1      C       K01708  galactarate dehydratase [EC:4.2.1.42]                                                                D-galactarate dehydratase / Altronate hydrolase, C terminus [PF04295.16]                     0       d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis       99.59   3.39\n</code></pre> <p>For each gene annotated, <code>DRAM</code> provides a summary rank (from A to E), representing the confidence of the annotation based on reciprocal best hits (RBH). The following figure briefly explains how this summary rank is calculated:</p> <p></p>"},{"location":"day4/ex15_gene_annotation_part3/#overview-of-dram-vpy-annotate-output","title":"Overview of <code>DRAM-v.py annotate</code> output","text":"<p><code>DRAM-v</code> generates the same output files as <code>DRAM</code>, but this time for the viral contigs. These files can be viewed in the output directory <code>10.gene_annotation_and_coverage/dramv_annotations/</code>. In this case, <code>annotations.tsv</code> also includes some viral-specific columns, including viral gene database matches (<code>vogdb</code>), and categories that are used by <code>DRAM-v.py distill</code> to identify putative auxiliary metabolic genes (AMGs) (<code>virsorter_category</code>, <code>auxiliary_score</code>,  <code>is_transposon</code>, <code>amg_flags</code>)</p>"},{"location":"day4/ex15_gene_annotation_part3/#dram-and-dram-v-distillation-of-the-results","title":"<code>DRAM</code> and <code>DRAM-v</code> distillation of the results","text":"<p>After the annotation is finished, we will summarise and visualise these annotations with the so-called distillation step. We do so by running the following commands directly in the terminal. This will generate the distillate and liquor files for each dataset.</p> <p>For the viral annotations, we will also include the parameters <code>--remove_transposons</code> (\"Do not consider genes on scaffolds with transposons as potential AMGs\") and <code>--remove_fs</code> (\"Do not consider genes near ends of scaffolds as potential AMGs\") to filter out some potential false positives for auxiliary metabolic gene identification.</p> <p>code</p> <pre><code>module purge\nmodule load DRAM/1.3.5-Miniconda3\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage/\n\n# Prokaryote annotations\nDRAM.py distill -i dram_annotations/annotations.tsv \\\n                -o dram_distillation \\\n                --trna_path dram_annotations/trnas.tsv \\\n                --rrna_path dram_annotations/rrnas.tsv\n\n# viral annotations\nDRAM-v.py distill --remove_transposons --remove_fs \\\n                  -i dramv_annotations/annotations.tsv \\\n                  -o dramv_distillation\n</code></pre>"},{"location":"day4/ex15_gene_annotation_part3/#drampy-distill-output-files","title":"<code>DRAM.py distill</code> output files","text":"<p>The <code>DRAM</code> distillation step generates the following files that can be found within the <code>dram_distillation</code> directory :</p> File name Description <code>genome_stats.tsv</code> Genome quality information required for MIMAG <code>metabolism_summary.xlsx</code> Summarised metabolism table containing number of genes with specific metabolic function identifiers <code>product.html</code> HTML file displaying a heatmap summarising pathway coverage, electron transport chain component completion, and presence/absence of specific functions <code>product.tsv</code> Data table visualised in <code>product.html</code> <p>First, let's have a look at the <code>genome_stats.tsv</code> file to check the assembly quality of our bins by double-clicking the file within the <code>Jupyter</code> environment, viewing from the terminal via <code>less</code> or <code>cat</code>, or downloading the files from here and opening locally (e.g. via Excel).</p> Content of <code>genome_stats.tsv</code> genome number of scaffolds taxonomy completeness score contamination score 5S rRNA 16S rRNA 23S rRNA tRNA count assembly quality bin_0 23 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Arcobacteraceae;g__Arcobacter;s__Arcobacter nitrofigilis 99.59 3.39 2 present bin_0_bin_0_NODE_25_length_262222_cov_1.020389 (260223, 261736) - 44 med bin_1 10 d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Nautiliales;f__Nautiliaceae;g__Nautilia;s__Nautilia profundicola 99.59 0.41 bin_1_bin_1_NODE_179_length_42779_cov_1.586174 (42578, 42683) bin_1_bin_1_NODE_179_length_42779_cov_1.586174 (37726, 39229) bin_1_bin_1_NODE_179_length_42779_cov_1.586174 (39618, 42521) 43 high bin_2 14 d__Bacteria;p__Cyanobacteriota;c__Cyanobacteriia;o__PCC-6307;f__Cyanobiaceae;g__Prochlorococcus_C;s__ 99.73 0.27 - bin_2_bin_2_NODE_16_length_329212_cov_0.767174 (155033, 156512) bin_2_bin_2_NODE_16_length_329212_cov_0.767174 (151325, 154198) 51 med bin_3 375 d__Bacteria;p__Planctomycetota;c__Brocadiia;o__Brocadiales;f__Brocadiaceae;g__Kuenenia;s__Kuenenia stuttgartiensis_A 91.21 0.61 - bin_3_bin_3_NODE_465_length_7730_cov_0.184781 (1, 929) - 37 med bin_4 12 d__Bacteria;p__Desulfobacterota;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Desulfovibrio;s__Desulfovibrio desulfuricans 94.67 0 - - - 46 med bin_5 59 d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Burkholderiales;f__Nitrosomonadaceae;g__Nitrosomonas;s__Nitrosomonas europaea 99.97 0.9 bin_5_bin_5_NODE_53_length_160826_cov_1.123462 (86680, 86787) bin_5_bin_5_NODE_53_length_160826_cov_1.123462 (90154, 91687) bin_5_bin_5_NODE_53_length_160826_cov_1.123462 (86863, 89745) 42 high bin_6 73 d__Bacteria;p__Bacillota;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__Staphylococcus aureus 98.58 0.11 3 present bin_6_bin_6_NODE_110_length_77108_cov_0.318002 (42475, 44023) bin_6_bin_6_NODE_110_length_77108_cov_0.318002 (39078, 41996) 34 med bin_7 57 d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__Pseudomonas aeruginosa 96.87 0.11 2 present - - 42 med bin_8 16 d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Vibrio;s__Vibrio cholerae 99.41 0.03 6 present bin_8_bin_8_NODE_68_length_133755_cov_0.640623 (22138, 23676) bin_8_bin_8_NODE_68_length_133755_cov_0.640623 (24392, 27264) 77 med bin_9 70 d__Bacteria;p__Pseudomonadota;c__Alphaproteobacteria;o__Rhizobiales;f__Xanthobacteraceae;g__Nitrobacter;s__Nitrobacter winogradskyi 99.8 0 bin_9_bin_9_NODE_50_length_179569_cov_0.671348 (103643, 103751) bin_9_bin_9_NODE_50_length_179569_cov_0.671348 (98442, 99927) bin_9_bin_9_NODE_50_length_179569_cov_0.671348 (100757, 103567) 49 high <p>To finish, we visualize the Product, an .HTML file produced in the distillation step, by double-clicking on it in our Jupyter lab notebook or downloading from here. The Product has three primary parts:</p> <p>Product visualisation</p> ModulesETC ComplexesOther functions <p>Central metabolism pathways coverage. Completion of pathways is based on the structure of KEGG modules, with the pathway coverage calculated as the percent of steps with at least one gene present.</p> <p> </p> <p>Electron Transport Chain component completion</p> <p> </p> <p>Presence of specific functions, including CAZy, Nitrogen metabolism, Sulfur metabolism and Photosynthesis. Note that the taxonomic classification of each of the bins is also shown in the first figure</p> <p> </p>"},{"location":"day4/ex15_gene_annotation_part3/#dram-vpy-distill-output-files","title":"<code>DRAM-v.py distill</code> output files","text":"<p>The <code>DRAM-v</code> distillation step for the viral contigs generates the following files that can be found within the <code>dramv_distillation/</code> directory :</p> File name Description <code>vMAG_stats.tsv</code> \"Genome\" (in this case viral contigs of varying completeness) information including: total gene counts, viral vs host gene counts, and counts of genes related to viral replication, structure, and those with presumed viral or host benefits <code>amg_summary.tsv</code> Genes identified as putative auxiliary metabolic genes (AMGs) and various columns for metabolic characterisation of each gene <code>product.html</code> HTML file displaying a heatmap summarising AMG counts and presence/absence for different broad metabolic categories for each viral contig <p>When viewing these files, see if you can find the following information:</p> <ul> <li>What are some annotations of interest within the output annotations file? </li> <li>NOTE: the *VirSorter2 annotations file includes multiple columns for both prokaryote and viral protein predictions. Be careful as to which column you are looking at (as well as its associated confidence score) when assessing viral annotations vs. AMGs*.</li> <li>Among these annotations, how many were flagged as AMGs by <code>DRAM-v</code>?</li> <li>What broad metabolic categories did the AMGs fall into? </li> <li>Discussion point: How might we investigate whether identified putative AMGs are actually within the viral genomes, rather than residual contaminating host genomic sequence attached to the end of integrated prophage (but incompletely trimmed off in the excision process)?</li> </ul>"},{"location":"day4/ex16a_data_presentation_Intro/","title":"Introduction to data presentation","text":"<p>Objectives</p> <ul> <li>Overview, <code>RStudio</code>, and using <code>R</code> in the <code>Jupyter</code> environment</li> <li>Data visualisation and accessibility</li> <li>Logging in to the NeSI <code>Jupyter hub</code></li> </ul>"},{"location":"day4/ex16a_data_presentation_Intro/#overview-rrstudio-and-using-r-in-the-jupyter-environment","title":"Overview, R/RStudio, and using R in the Jupyter environment","text":"<p>There are a number of powerful packages within the <code>R</code> software environment which can be used to create high quality images of genomic and metagenomic data. While each of these packages comes with its own documentation, these documents and tutorials usually assume that your data is already in some already-prepared format. Our data will almost never be in this format, though, so these exercises show a few brief examples of how we can scrape data from our existing files to create useful figures. As such, these examples are more complicated than what you would get reading the tutorials and manuals of the plotting tools, but will be transferable to your own work.</p> <p>In your own work, it may be preferable to download the relevant files from NeSI (e.g. via <code>scp ...</code>, download from <code>Jupyter</code> file explorer pane) and work with them on a locally installed version of <code>RStudio</code> on your own machine. That way you have control over what packages you can install. For today, to be able to run these <code>R</code> exercises in a stable environment within the NeSI platform, we will be running an <code>RStudio</code> from within a Jupyter environment.</p> <p>By now you should be very familiar with running the terminal window from within the NeSI Jupyter hub. In addition to the terminal, NeSI has recently integrated <code>RStudio</code> into the <code>Jupyter</code> hub. You can also opt to run R code on <code>Jupyter Notebooks</code> which also provides an interactive space that allows for mixing multiple languages within a single document, including Markdown, <code>Python</code>, and <code>R</code> (by default, <code>Markdown</code> and one coding language such as <code>R</code> can be used within one document, but there are add-ons available to expand this, should you wish to). <code>Jupyter Notebooks</code> can be extremely useful as a workspace that is the equivalent of an electronic \"lab book\".</p> <p>These exercises will take place with files in the <code>11.data_presentation/</code> folder.</p>"},{"location":"day4/ex16a_data_presentation_Intro/#data-visualisation-and-accessibility","title":"Data visualisation and accessibility","text":"<p>In this section, we will work through a number of example exercises for visualising various aspects of metagenomic data.</p> <p>As the fundamental point of data visualisation is communication, when building figures it is important to be mindful of aspects of your figures that might affect the accessibility of what you're trying to communicate (i.e. to maximise the number of people you will be communicating effectively with). A considerable number of your intended audience will be affected by one of the forms of colour vision deficiency (colour blindness). There are a number of useful resources available online for both selecting and testing the appropriateness of your colour selection. Some include:</p> <ul> <li>ColorBrewer2 (select 'colorblindsafe')</li> <li>chroma.js</li> <li>Selecting and checking your colour choice using Viz Palette</li> <li>An article featuring tips for visual accessibility</li> <li>Several useful colour palettes designed by Martin Krzywinski are available here</li> <li>Stack Overflow community suggestions</li> </ul> <p>We have been mindful to make appropriate colour selections throughout these examples, but please do let us know if you spot any we might have overlooked.</p>"},{"location":"day4/ex16a_data_presentation_Intro/#getting-started-logging-in-to-the-nesi-jupyter-hub","title":"Getting started: logging in to the NeSI Jupyter hub","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub.</p> <p>Within the <code>Jupyter</code> launcher, click on the <code>RStudio</code> button to start a session.</p> <p>All of the required packages for these exercises are already installed. </p> Local RStudio <p>If you are running this on your local <code>R</code> or <code>RStudio</code>, you will need to run the following:</p> <p>code</p> <pre><code>install.packages('ade4')\ninstall.packages('genoPlotR')\ninstall.packages('pheatmap')\ninstall.packages('gplots')\ninstall.packages('vegan')\n\n# Install 'tidyverse' (includes: ggplot2, tibble, tidyr, readr, purrr, dplyr, string4, forcats)\ninstall.packages('tidyverse')\n\n# Install 'pathview' package (part of Bioconductor)\nif (!require(BiocManager)) {\n  install.packages(\"BiocManager\")\n  BiocManager::install(\"pathview\", update = FALSE)\n}\n\n# Install 'KEGGREST' package (part of Bioconductor)\nif (!require(BiocManager)) {\n  install.packages(\"BiocManager\")\n  BiocManager::install(\"KEGGREST\", update = FALSE)\n}\n</code></pre> <p>If you are new to <code>RStudio</code>, spend a few minutes familiarising yourself with the environment (also known as the \"workspace\"). There is a pane for the <code>R</code> console on the left that prints information on the <code>R</code> version you are using on start-up. On the top right pane, there are 2 important tabs to note: <code>Environment</code> (where you can explore your data) and <code>History</code> (all code that you have run). The bottom right pane has tabs for <code>Files</code> (where you can navigate the directory structure), <code>Plots</code> (where your plots appear), <code>Packages</code> (available packages), <code>Help</code> (all help pages and manual are shown here), and <code>Viewer</code> (some packages output interactive content and it will show up here). At the very top, there are two toolbars: the first leads to other settings and options (you can explore this on your own time), the second one has icons for (starting from the far left and hover mouse pointer to see description):</p> <ul> <li>Open new file</li> <li>Create an <code>R</code> project</li> <li>Open existing file</li> <li>Save current document</li> <li>Save all open documents</li> <li>Print current file</li> <li>Search bar to navigate open documents</li> <li>Manage workspace panes</li> <li>Options for additional plugins</li> </ul> <p>To start, open a new file to start writing code in.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/","title":"Coverage heatmaps","text":"<p>Objectives</p> <ul> <li>Building a heatmap of MAG coverage per sample</li> <li>Building a heatmap of viral contigs per sample</li> </ul>"},{"location":"day4/ex16b_data_presentation_Coverage/#build-a-heatmap-of-average-coverage-per-sample-using-r","title":"Build a heatmap of average coverage per sample using R","text":"<p>One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? In the previous coverage calculation exercises we generated per-sample coverage tables by mapping the quality-filtered unassembled reads back to the refined bins and the viral contigs to then generate coverage profiles for each. </p> <p>As a reminder:</p> <p>Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample.</p> <p>A simple way to present this information is via a heatmap. In this exercise we will build a clustered heatmap of these coverage profiles in <code>R</code>. Since we also have tables of taxonomy assignments (via <code>gtdb-tk</code> for MAGs) and/or predictions (via <code>vContact2</code> for viral contigs), we will also use these to add taxonomy information to the plot.</p> <p>The coverage and taxonomy tables generated in earlier exercises have been copied to <code>11.data_presentation/coverage/</code> for use in these exercises.</p> <p>In addition to this, a simple mapping file has also been created (<code>11.data_presentation/coverage/mapping_file.txt</code>). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column (Group_A, Group_B, and Group_C). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination, or correlation analyses) you could also add new columns to this file and load them at the same time.</p> <p>Note</p> <p>As discussed in the coverage and taxonomy exercises, it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the normalised coverage tables into the steps outlined below.*</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#part-1-building-a-heatmap-of-mag-coverage-per-sample","title":"Part 1 - Building a heatmap of MAG coverage per sample","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub and make sure you are working within RStudio with the required packages installed (see the data presentation intro for more information).</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#11-prepare-environment","title":"1.1 Prepare environment","text":"<p>First, set the working directory and load the required libraries.</p> <p>code</p> <pre><code># Set working directory ----\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/coverage')\n\n# Load libraries ----\n# Tidyverse packages \nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(tibble)\n\n# Visualisation package\nlibrary(gplots)\n\n# Ecological analyses\nlibrary(vegan)\n</code></pre> <p>NOTE: after copying this code into the empty script file in <code>Rstudio</code>, remember that, to run the code, press <code>&lt;shift&gt; + &lt;enter&gt;</code> with the code block selected.</p> <p>Import all relevant data as follows:</p> <p>code</p> <pre><code># Read files ----\ncontig_cov &lt;- read_tsv(\"bin_cov_table.txt\") # Bin contig coverage table\ngtdbtk_out &lt;- read_tsv(\"gtdbtk.bac120.summary.tsv\") # Prokaryotic taxonomy from GTDB-Tk\nvirus_cov &lt;- read_tsv(\"viruses_cov_table.txt\") # Viral contig coverage table\ncheckv_summary &lt;- read_tsv(\"checkv_quality_summary.tsv\") # Viral contig quality from CheckV\nvContact2_out &lt;- read_tsv(\"tax_predict_table.tsv\") # Viral taxonomy from vContact2\nmetadata &lt;- read_tsv(\"mapping_file.txt\") # Metadata/mapping file of environmental parameters\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#12-wrangle-data","title":"1.2 Wrangle data","text":"<p>After importing the data tables, we need to subset the tables to only relevant columns. As noted during the coverage exercises, it is important to remember that we currently have a table of coverage values for all contigs contained within each MAG. Since we're aiming to present coverage for each MAG, we need to reduce these contig coverages into a single mean coverage value per MAG per sample.</p> <p>In the following code, we first <code>select()</code> columns of interest (i.e. the contig ID and sample coverages). We then remove the <code>.bam</code> suffix using the <code>rename_with()</code> function. Given that we require mean coverages per MAG, we create a column of MAG/bin IDs (via <code>mutate</code>) by extracting the relevant text from the contig ID using <code>str_replace()</code>. Finally, we group the data by the bin ID (via <code>group_by()</code>) then use a combination of <code>summarise()</code> and <code>across()</code> to obtain a mean of coverage values per bin per sample. Here, <code>summarise()</code> calculates the <code>mean</code> based on grouping variables set by <code>group_by()</code>, and this is done <code>across()</code> columns that have the column header/name which <code>contains(\"sample\")</code>.</p> <p>code</p> <pre><code>## 1. Obtain average coverage per MAG per sample ----\nMAG_cov &lt;- contig_cov %&gt;%\n  # Pick relevant columns\n  select(contigName, contigLen, ends_with(\".bam\")) %&gt;%\n  # Remove \".bam\" from sample columns\n  rename_with(\n    .fn = function(sample_name) str_remove(sample_name, \".bam\"),\n    .cols = ends_with(\".bam\")\n  ) %&gt;%\n  # Add bin ID to data\n  mutate(\n    binID = str_replace(contigName, \"(.*)_NODE_.*\", \"\\\\1\")\n  ) %&gt;%\n  # Designate \"binID\" as a grouping factor\n  group_by(binID) %&gt;%\n  # Calculate average coverage per sample per bin\n  summarise(\n    across(\n      contains(\"sample\"), function(coverage) weighted.mean(coverage, contigLen)\n    )\n  )\n</code></pre> <p>Next, we would also like to append the lowest identified taxonomy to each MAG. But before that, we will need to tidy up the taxonomy table to make it more readable.</p> <p>In the following code, we first <code>select()</code> the <code>user_genome</code> (which is equivalent to <code>binID</code> above) and the relevant taxonomic <code>classification</code> columns. We then <code>separate()</code> the taxonomic <code>classification</code> column from a long semicolon-separated (<code>;</code>) string to 7 columns describing the MAG's <code>domain</code>, <code>phylum</code>, <code>class</code>, <code>order</code>, <code>family</code>, <code>genus</code>, and <code>species</code>. Finally, we polish the taxonomy annotations. We remove the hierarchical prefixes by retaining everything after the double underscores (<code>__</code>) via <code>str_replace</code> across all columns except <code>user_genome</code> (via inverse selection).</p> <p>code</p> <pre><code>## 2. Clean up taxonomy table ----\nbin_taxonomy &lt;- gtdbtk_out %&gt;%\n  # Select relevant columns\n  select(user_genome, classification) %&gt;%\n  # Split taxonomic classifications into individual columns\n  separate(\n    col = classification,\n    into = c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"),\n    sep = ';'\n  ) %&gt;%\n  mutate(\n    # Remove \"d__\" from all taxonomy columns\n    across(\n      # Negative selection: change all columns except \"user_genome\"\n      -user_genome,\n      .fns = function(taxa) str_replace(taxa, \"[a-z]__(.*)\", \"\\\\1\")\n    ),\n    # If species is empty, fill with genus\n    species = if_else(species == \"\", genus, species)\n  )\n</code></pre> <p>After obtaining a tidy taxonomy table, we append the species annotations to the bin IDs as follows:</p> <ol> <li><code>left_join()</code> a subset of the columns (<code>user_genome</code>, <code>species</code>) from <code>bin_taxonomy</code> generated above based on bin IDs. Bin IDs is <code>binID</code> in <code>MAG_cov</code> and in <code>user_genome</code> in <code>bin_taxonomy</code>, so we must specifically tell <code>left_join()</code> which columns to join <code>by</code>.</li> <li>We <code>unite()</code> the columns <code>binID</code> and <code>species</code> as 1 column and place an underscore <code>_</code> betwee the strings.</li> <li>We then convert the <code>binID</code> column into <code>rownames</code> for the <code>data.frame</code> in preparation for the other steps.</li> </ol> <p>code</p> <pre><code>## 3. Add species to bin ID ----\nMAG_cov &lt;- MAG_cov %&gt;%\n  left_join(\n    select(bin_taxonomy, user_genome, species),\n    by = c(\"binID\" = \"user_genome\")\n  ) %&gt;%\n  unite(col = \"binID\", c(binID, species), sep = \"_\") %&gt;%\n  column_to_rownames(\"binID\")\n</code></pre> Too much data? <p>In your own work, if you have too many MAGs, it can be difficult to present them concisely in a heatmap. Thus, you may want to only retain MAGs that fulfill some criteria. </p> <p>For example, you may only be interested in the top 10 MAGs based on coverage:</p> <p>code</p> <pre><code># Create a vector of top 10 MAGs\ntop10 &lt;- sort(rowSums(MAG_cov), decreasing = TRUE)[1:10]\n\n# Retain only top 10 MAGs\nMAG_cov_top10 &lt;- MAG_cov[rownames(MAG_cov) %in% names(top10), ]\n</code></pre> <p>Perhaps you want to filter MAGs based on their prevalence across your sampling regime? Assuming you would like to retain MAGs that are present in at least 50% of your samples:</p> <p>code</p> <pre><code># Create a presence/absence matrix\nMAG_prevalence &lt;- ifelse(MAG_cov &gt; 0, 1, 0)\n\n# Create a logical vector that fulfill criteria\ntop_prev &lt;- rowSums(MAG_prevalence) &gt; (0.5 * ncol(MAG_prevalence))\n\n# Retain only MAGs in coverage table that fulfill criteria\nMAG_cov_top_prev &lt;- MAG_cov[top_prev, ]\n</code></pre> <p>Very often, coverage data generated using metagenomic methods can be sparse and/or have values that differ by large orders of magnitude. This can hamper effective visualisation of the data. To remedy this, we can perform numeric transformations that can enhance visualisation of relative differences between values. Here, we will use a logarithm (base 2) transform with 1 added as a pseudo-count (because \\(\\log 0\\) is undefined).</p> <p>On data transformation</p> <p>Transformations applied to enhance data visualisation are not necessarily suitable for downstream statistical analyses. Depending on the attributes of your data (shape, sparsity, potential biases, etc.) and choice of analyses, it is recommended that you become familiar with field and tool-specific assumptions, norms and requirements for data transformation.</p> <p>code</p> <pre><code>## 4. Transform coverage data ----\nMAG_cov_log2 &lt;- log2(MAG_cov + 1)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#13-order-the-heatmap","title":"1.3 Order the heatmap","text":"<p>We then need to prepare some form of ordering for our heatmap. This is usually presented in the form of a dendrogram based on results from hierarchical clustering. To do this, we first need to generate two dissimilarity/distance matrices based on transformed coverage values:</p> <ul> <li>Sample-wise dissimilarities</li> <li>MAG-wise dissimilarities</li> </ul> <p>Here, we calculate Bray-Curtis dissimilarity using <code>vegdist()</code> to generate dissimilarities between samples and MAGs. Then, the matrices are used as input to perform hierarchical clustering and plotted.</p> <p>code</p> <pre><code># Perform hierarchical clustering ----\n## Dissimilarities between samples\nsample_dist &lt;- vegdist(t(MAG_cov_log2), method = \"bray\")\nsample_hclust &lt;- hclust(sample_dist, \"average\")\n\nplot(\n  sample_hclust,\n  main = \"Bray-Curtis dissimilarities between samples\\n(MAG)\",\n  xlab = \"Sample\",\n  ylab = \"Height\",\n  sub = \"Method: average linkage\",\n  hang = -1\n)\n\n## Dissimilarities between MAGs\nMAG_dist &lt;- vegdist(MAG_cov_log2)\nMAG_hclust &lt;- hclust(MAG_dist, \"average\")\n\nplot(\n  MAG_hclust,\n  main = \"Bray-Curtis dissimilarities between MAGs\",\n  xlab = \"MAGs\",\n  ylab = \"Height\",\n  sub = \"Method: average linkage\",\n  hang = -1,\n  cex = 0.75\n)\n</code></pre> <p> </p> <p>Exporting figures</p> <p>To export figures you have made, navigate your mouse pointer to the bottom right pane, then click on 'Export' on the second toolbar of the pane. You can export your figure as an image (e.g. TIFF, JPEG, PNG, BMP), a vector image (i.e. PDF, SVG) or PostScript (EPS). </p> <p>If you would like to do this via code, you can wrap the plot code in between functions for graphic devices (i.e. <code>png()</code>, <code>jpeg()</code>, <code>tiff()</code>, <code>bmp()</code>, <code>pdf()</code>, and <code>postscript()</code>) and <code>dev.off()</code>. The following is an example:</p> <pre><code>png(filename = \"file.png\", ...)\nplot(...)\ndev.off()\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#14-set-colour-palettes","title":"1.4 Set colour palettes","text":"<p>The penultimate step before building our heatmap is to set the colours that will be used to represent annotations and the cell/tile values. In this case, annotations are sample groups as designated in the metadata (a.k.a. mapping) file and MAG taxonomy. In the code below, we will set 3 palettes:</p> <ul> <li>Sample groups</li> <li>MAG phylum</li> <li>Cell/tile values</li> </ul> <p>Pre-installed colour palettes</p> <p>Base <code>R</code> has many colour palettes that come pre-installed. Older versions of R had colour palettes (also known as palette <code>R3</code>) that were not ideal for people with colour vision deficiencies. However, newer versions (4.0+) now carry better palettes that are more colour-blind friendly (see here).</p> <p>You can also quickly check what these colour palettes are:</p> <p>code</p> <pre><code># What colour palettes come pre-installed?\npalette.pals()\n\n# Plotting the \"Okabe-Ito\" palette\n# (Requires the \"scales\" package)\nscales::show_col(\n  palette.colors(palette = \"Okabe-Ito\")\n)\n</code></pre> <p></p> <p>code</p> <pre><code># Set colour palette ----\n## Prepare sample groups and colour\nmetadata &lt;- metadata %&gt;%\n  mutate(\n    Group = as.factor(Group)\n  )\n\ngroup_colour &lt;- data.frame(\n  Group = levels(metadata$Group),\n  colour = palette.colors(\n    n = length(levels(metadata$Group)),\n    palette = \"Okabe-Ito\")\n)\n\nsample_colour &lt;- left_join(metadata, group_colour)\n\n## Prepare MAG colours based on taxonomic class\n## Remember that the new labels (bin ID) are 'binID_species'\nbin_phylum &lt;- bin_taxonomy %&gt;%\n  select(user_genome, species, phylum) %&gt;%\n  unite(col = \"binID\", user_genome, species, sep = \"_\") %&gt;%\n  mutate(\n    phylum = as.factor(phylum)\n  )\n\nphylum_colour &lt;- data.frame(\n  phylum = levels(bin_phylum$phylum),\n  colour = palette.colors(\n    n = length(levels(bin_phylum$phylum)),\n    palette = \"R4\")\n)\n\nMAG_colour &lt;- left_join(bin_phylum, phylum_colour)\n\n## Set a grayscale for cell colours\ncell_colour &lt;- colorRampPalette(c(\"white\", \"black\"), space = \"Lab\")(2^8)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#15-build-heatmap","title":"1.5 Build heatmap","text":"<p>code</p> <pre><code>heatmap.2(\n    x = as.matrix(MAG_cov_log2), # Log2 transformed coverage table\n    Colv = as.dendrogram(sample_hclust), # Arrange columns based on sample-wise dissimilarities\n    Rowv = as.dendrogram(MAG_hclust), # Arrange rows based on MAG-wise dissimilarities\n    margins = c(30, 20), # Numbers represent margins for bottom &amp; right side\n    RowSideColors = MAG_colour$colour, # Colours on the row represent MAG phylum\n    ColSideColors = sample_colour$colour, # Colours on the column represent sample groups\n    scale = \"none\", # Do not scale values provided\n    col = cell_colour, # Colour shading based on values\n    trace = \"none\", \n    density.info = \"none\"\n)\n## Legend for MAG phyla\nlegend(\n  x = \"bottomleft\", # Legend position\n  legend = phylum_colour$phylum, # Legend text\n  col = phylum_colour$colour, # Legend colour\n  lty = 1, # Line type\n  lwd = 6, # Line width\n  ncol = 1, # Number of columns for legend,\n  bty = \"n\", # Border type (set to no border)\n  title = \"Y-axis key: MAG taxonomy\"\n)\n## Legend for sample grouping\nlegend(\n  x = \"bottomright\",\n  legend = group_colour$Group,\n  col = group_colour$colour,\n  lty = 1,\n  lwd = 6,\n  ncol = 1,\n  bty = \"n\",\n  title = \"X-axis key: Sample group\"\n)\n</code></pre> <p> </p> <p>Results at a glance</p> <p>The heatmap shows us that:</p> <ul> <li>4 MAGs belong to the phylum Pseudomonadota</li> <li>Sample 4 belonging to Group C is somewhat of an outlier, with only 3 MAGs recovered from this sample</li> <li><code>bin_9</code>, a Nitrobacter, is only found in sample 4</li> <li>Presumably, sample 4 has a high abundance of nitrogen cycling MAGs</li> <li>Samples from Groups A and B recovered the same bins</li> </ul> <p>You can and should experiment with different paramters and arguments within the <code>heatmap.2</code> function. A good starting point is <code>cex = ...</code> which controls \"character expansion\" (i.e. text size). Here, we use the <code>...SideColors</code> argument to annotate MAG phylum and sample grouping. We also use <code>legend</code> to provide additional information on what those colours represent. Read the help page for <code>heatmap.2()</code> by typing <code>?heatmap.2</code> into the console. The help manual will appear on the bottom right pane.</p> <p>Here, we arrange our columns based Bray-Curtis dissimilarities between samples. However, you may want to enforce or preserve some form of sample order (samples from the same transect, chronological/topological order, etc.). You can do this by setting <code>Colv = FALSE</code>.</p> <p>Arranging figure elements</p> <p>As observed in the above plot, the positioning and arrangement of the colour key and legends relative to the main heatmap may not be ideal. Often times, it is desirable to manually arrange or resize elements of a figure (legends, keys, text). This is usually done by exporting the figure in a format capable of preseving vectors (e.g. SVG or PDF) and then post-processing them in vector graphics software such as Adobe Illustrator or Inkscape.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#part-2-building-a-heatmap-of-viral-contigs-per-sample","title":"Part 2 - Building a heatmap of viral contigs per sample","text":"<p>We can run through the same process for the viral contigs. Many of the steps are as outlined above, so we will work through these a bit quicker and with less commentary along the way. However, we will highlight a handful of differences compared to the commands for the MAGs above, for example:  steps for selecting and/or formatting the taxonomy; importing the quality output from <code>CheckV</code>; and the (optional) addition of filtering out low quality contigs.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#21-prepare-environment","title":"2.1 Prepare environment","text":"<p>This has already been done above. We will immediately begin wrangling the data.</p>"},{"location":"day4/ex16b_data_presentation_Coverage/#22-wrangle-data","title":"2.2 Wrangle data","text":"<p>To start, we need to identify viral contigs of at least medium quality. THis will be used as a filtering vector.</p> <p>code</p> <pre><code>## 1. Clean up viral taxonomy annotation quality ----\nvirus_quality &lt;- checkv_summary %&gt;%\n  mutate(\n    checkv_quality = factor(\n      checkv_quality,\n      levels = c(\n        \"Not-determined\",\n        \"Low-quality\",\n        \"Medium-quality\",\n        \"High-quality\",\n        \"Complete\"\n      ))\n  ) %&gt;%\n  select(contig_id, checkv_quality)\n\n## Subset good quality predictions\nvirus_hq &lt;- virus_quality %&gt;%\n  filter(!(checkv_quality %in% c(\"Not-determined\", \"Low-quality\"))) %&gt;%\n  pull(contig_id)\n\nvirus_taxonomy_hq &lt;- vContact2_out %&gt;%\n  filter(Genome %in% virus_hq) %&gt;%\n  mutate(\n    Order_VC_predicted = as.factor(Order_VC_predicted)\n  )\n</code></pre> <p>We then obtain the coverage matrix and transform the values to enhance visualisation.</p> <p>code</p> <pre><code>## 2. Obtain viral contig coverage matrix ----\n## Unlike MAGs, we do not need to average coverage values.\n## However, we do need to filter out annotations of lesser quality.\n## We find 15 contigs that needs to be kept\nvirus_cov_matrix &lt;- virus_cov %&gt;%\n  select(contigName, ends_with(\".bam\")) %&gt;%\n  rename_with(\n    .fn = function(name) str_remove(name, \".bam\"),\n    .cols = everything()\n  ) %&gt;%\n  filter(contigName %in% virus_hq) %&gt;%\n  column_to_rownames(\"contigName\") %&gt;%\n  as.matrix()\n\n## 3. Log2 transform coverage matrix ----\nvirus_cov_matrix_log2 &lt;- log2(virus_cov_matrix + 1)\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#23-order-the-heatmap","title":"2.3 Order the heatmap","text":"<p>code</p> <pre><code># Perform hierarchical clustering (viral contigs) ----\n## Dissimilarities between samples\nvirus_sample_dist &lt;- vegdist(t(virus_cov_matrix_log2))\nvirus_sample_hclust &lt;- hclust(virus_sample_dist, \"average\")\n\nplot(\n  virus_sample_hclust,\n  main = \"Bray-Curtis dissimilarities between samples\\n(virus)\",\n  xlab = \"Sample\",\n  ylab = \"Height\",\n  sub = \"Method: average linkage\",\n  hang = -1\n)\n\n## Dissimilarities between viruses\nvirus_dist &lt;- vegdist(virus_cov_matrix_log2)\nvirus_hclust &lt;- hclust(virus_dist, \"average\")\n\nplot(\n  virus_hclust,\n  main = \"Bray-Curtis dissimilarities between viruses\",\n  xlab = \"Viral contigs\",\n  ylab = \"Height\",\n  sub = \"Method: average linkage\",\n  hang = -1,\n  cex = 0.75\n)\n</code></pre> <p> </p>"},{"location":"day4/ex16b_data_presentation_Coverage/#24-set-colour-palettes","title":"2.4 Set colour palettes","text":"<p>For colours based on sample group, we will retain the colours set as above. Here, we will only set a new palette (Tableau 10) for viruses based on the predicted taxonomy.</p> <p>code</p> <pre><code># Prepare grouping variables and colour palettes ----\n# Check palette colours\nscales::show_col(\n  palette.colors(\n    palette = \"Tableau 10\"\n  )\n)\n</code></pre> <p> </p> <p>code</p> <pre><code>## Set colours for viral orders\nvirus_order_colour &lt;- data.frame(\n  \"virus_order\" = levels(virus_taxonomy_hq$Order_VC_predicted),\n  \"colour\" = palette.colors(\n    n = length(levels(virus_taxonomy_hq$Order_VC_predicted)),\n    palette = \"Tableau 10\"\n  )\n)\n\n## Set colours for viruses\nvirus_colour &lt;- virus_taxonomy_hq %&gt;%\n  select(Genome, Order_VC_predicted) %&gt;%\n  left_join(virus_order_colour, by = c(\"Order_VC_predicted\" = \"virus_order\"))\n</code></pre>"},{"location":"day4/ex16b_data_presentation_Coverage/#25-build-heatmap","title":"2.5 Build heatmap","text":"<p>code</p> <pre><code>heatmap.2(\n  x = virus_cov_matrix_log2,\n  Colv = as.dendrogram(virus_sample_hclust),\n  Rowv = as.dendrogram(virus_hclust),\n  margins = c(30, 20),\n  RowSideColors = virus_colour$colour,\n  ColSideColors = sample_colour$colour,\n  scale = \"none\",\n  col = cell_colour,\n  trace = \"none\",\n  keysize = 1,\n  density.info = \"none\"\n)\n## Legends\nlegend(\n  x = \"bottomleft\",\n  legend = virus_order_colour$virus_order,\n  col = virus_order_colour$colour,\n  lty = 1,\n  lwd = 6,\n  ncol = 1,\n  bty = \"n\",\n  title = \"Y-axis key: Virus taxonomy\"\n)\nlegend(\n  x = \"bottomright\",\n  legend = group_colour$Group,\n  col = group_colour$colour,\n  lty = 1,\n  lwd = 6,\n  ncol = 1,\n  bty = \"n\",\n  title = \"X-axis key: Sample group\"\n)\n</code></pre> <p> </p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/","title":"Ordinations","text":"<p>Objectives</p> <ul> <li>Import and wrangle data in <code>R</code></li> <li>Calculate weighted and unweighted dissimilarities</li> <li>Generate ordination</li> <li>Extract data from ordination</li> <li>Build the ordination plot</li> <li>Follow-up analyses</li> </ul>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#introduction","title":"Introduction","text":"<p>A common method to investigate the relatedness of samples to one another is to calculate ordinations and to visualise this in the form of a principal components analysis (PCA) or non-metric multidimensional scaling (nMDS) plot. In this exercise, we will calculate ordinations based on weighted and unweighted (binary) Bray-Curtis dissimilarity and present these in nMDS plots.</p> <p>The coverage tables generated in earlier exercises have been copied to <code>11.data_presentation/coverage/</code> a for use in these exercises.</p> <p>In addition to this, a simple mapping file has also been created (<code>11.data_presentation/coverage/mapping_file.txt</code>). This is a tab-delimited file listing each sample ID in the first column, and the sample \"Group\" in the second column (Group_A, Group_B, and Group_C). This grouping might represent, for example, three different sampling sites that we want to compare between. If you had other data (such as environmental measures, community diversity measures, etc.) that you wish to incorporate in other downstream analyses (such an fitting environmental variables to an ordination) you could also add new columns to this file and load them at the same time.</p> <p>Note</p> <p>As discussed in the coverage exercises, it is usually necessary to normalise coverage values across samples based on equal sequencing depth. This isn't necessary with the mock metagenome data we're working with, but if you include this step in your own work you would read the normalised coverage tables into the steps outlined below.*</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#1-import-and-wrangle-data-in-r","title":"1. Import and wrangle data in R","text":"<p>To get started, open <code>RStudio</code> and start a new document.</p> <p>Note</p> <p>You will recognise that the first few steps will follow the same process as the previous exercise on generating coverage heatmaps. In practice, these two workflows can be combined to reduce the repetitive aspects.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#11-prepare-environment","title":"1.1 Prepare environment","text":"<p>First, set the working directory and load the required libraries.</p> <p>code</p> <pre><code># Set working directory\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/coverage')\n\n# Load libraries ----\n# Tidyverse libraries\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(ggplot2)\n\n# Ecological analyses\nlibrary(vegan)\n</code></pre> <p>Import coverage tables and mapping file.</p> <p>code</p> <pre><code># Read files ----\ncontig_cov &lt;- read_tsv(\"bin_cov_table.txt\") # Bin contig coverage table\nvirus_cov &lt;- read_tsv(\"viruses_cov_table.txt\") # Viral contig coverage table\nmetadata &lt;- read_tsv(\"mapping_file.txt\") # Metadata/mapping file of environmental parameters\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#12-wrangle-data","title":"1.2 Wrangle data","text":"<p>As before in coverage exercise, we need to obtain per MAG and sample average coverage values. We begin by selecting relevant columns and renaming them.</p> <p>code</p> <pre><code>## Select relevant columns and rename them\ncontig_cov &lt;- contig_cov %&gt;%\n  select(contigName, contigLen, ends_with(\".bam\")) %&gt;%\n  rename_with(\n    .fn = function(sample_name) str_remove(sample_name, \".bam\"),\n    .cols = everything()\n  )\n\nvirus_cov &lt;- virus_cov %&gt;%\n  select(contigName, ends_with(\".bam\")) %&gt;%\n  rename_with(\n    .fn = function(sample_name) str_remove(sample_name, \".bam\"),\n    .cols = everything()\n  )\n\n## Calculate average bin coverage based on contig coverage\nMAG_cov &lt;- contig_cov %&gt;%\n  mutate(\n    binID = str_replace(contigName, \"(.*)_NODE_.*\", \"\\\\1\")\n  ) %&gt;%\n  group_by(binID) %&gt;%\n  summarise(\n    across(\n      contains(\"sample\"),\n      .fns = function(coverage) weighted.mean(coverage, contigLen)\n    )\n  )\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#2-calculate-weighted-and-unweighted-dissimilarities","title":"2. Calculate weighted and unweighted dissimilarities","text":"<p>It is often useful to examine ordinations based on both weighted and unweighted (binary) dissimilarity (or distance) metrics. Weighted metrics take into account the proportions or abundances of each variable (in our case, the coverage value of each bin or viral contig). This can be particularly useful for visualising broad shifts in overall community structure (while the membership of the community may remain relatively unchanged). Unweighted metrics are based on presence/absence alone, and can be useful for highlighting cases where the actual membership of communities differs (ignoring their relative proportions within the communities).</p> <p>Here we will use the functions <code>vegdist()</code> and <code>metaMDS()</code> from the <code>R</code> package <code>vegan</code> to generate weighted and unweighted Bray-Curtis dissimilarity matrices and nMDS solutions for the microbial bin data and viral contigs data.</p> <p>Setting seed</p> <p>You may also wish to make use of the <code>set.seed()</code> function before each calculation to ensure that you obtain consistent results if the same commands are re-run at a later date.</p> <p>code</p> <pre><code># Calculate dissimilarities ----\n## Unweighted dissimilarities (presence/absence)\nMAG_binary_bray &lt;- MAG_cov %&gt;%\n  # Convert \"binID\" column into rownames\n  # (vegan prefers to work on numeric matrices)\n  column_to_rownames(\"binID\") %&gt;%\n  # Transpose the data frame\n  t() %&gt;%\n  # Calculate dissimilarities\n  vegdist(x = ., method = \"bray\", binary = T)\n\nvirus_binary_bray &lt;- virus_cov %&gt;%\n  column_to_rownames(\"contigName\") %&gt;%\n  t() %&gt;%\n  vegdist(x = ., method = \"bray\", binary = T)\n\n## Weighted dissimilarities\nMAG_bray &lt;- MAG_cov %&gt;%\n  column_to_rownames(\"binID\") %&gt;%\n  t() %&gt;%\n  vegdist(x = ., method = \"bray\")\n\nvirus_bray &lt;- virus_cov %&gt;%\n  column_to_rownames(\"contigName\") %&gt;%\n  t() %&gt;%\n  vegdist(x = ., method = \"bray\")\n</code></pre> <p>From here on out, we will process the data using the same functions/commands. We can make our code less redundant by compiling all necessary inputs as a list, then processing them together. This is achieved by using the <code>map(...)</code> family of functions from the <code>purrr</code> package.</p> <p>code</p> <pre><code># Collect dissimilarities into a list for collective processing ----\nbray_list &lt;- list(\n  \"MAG_binary_bray\" = MAG_binary_bray,\n  \"MAG_bray\" = MAG_bray,\n  \"virus_binary_bray\" = virus_binary_bray,\n  \"virus_bray\" = virus_bray\n)\n</code></pre>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#3-generate-ordination","title":"3. Generate ordination","text":"<p>We now generate and visualise all ordinations in 4 panels them using native plotting methods.</p> <p>code</p> <pre><code># Perform non-metric multidimensional scaling (nMDS) ----\nnmds_list &lt;- map(bray_list, function(bray) metaMDS(bray, trymax = 999))\n\n## Check nMDS plot natively\npar(mfrow = c(2, 2)) # Sets panels\nmap2(nmds_list, names(nmds_list), function(nmds, lbl) {\n  ordiplot(nmds, main = lbl, type = \"t\", display = \"sites\")\n})\n</code></pre> <p> </p> <p>Plotting via this method is a quick and easy way to look at what your ordination looks like. However, this method is often tedious to modify to achieve publication quality figures. In the following section, we will use <code>ggplot2</code> to generate a polished and panelled figure.</p> <p>The <code>map(...)</code> function</p> <p>The <code>map(...)</code> function iterates through a list and applies the same set of commands/functions to each of them. For <code>map2(...)</code>, two lists are iterated concurrently and one output is generated based on inputs from both lists. The base <code>R</code> equivalent is the <code>apply(...)</code> family of functions. Here, we need to perform the same analyses on similar data types (dissimilarities of different kinds and organisms). These are powerful functions that can make your workflow more efficient, less repetitive, and more readable. However, this is not a panacea and there will be times where <code>for</code> loops or brute coding is necessary.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#4-extract-data-from-ordination","title":"4. Extract data from ordination","text":"<p>Before proceeding to plotting using <code>ggplot2</code>. We need to extract the X and Y coordinates from the ordination result using <code>scores()</code>. We then also need to \"flatten\" the list to a single data frame as well as extract other relevant statistics (e.g. stress values).</p> <p>code</p> <pre><code># Extract data from nMDS ----\n## Obtain coordinates\nscrs_list &lt;- map(nmds_list, function(nmds) {\n  scores(nmds, display = \"sites\") %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(\"sample\")\n})\n\n## Collect nMDS scores in a single data frame\nscrs_all &lt;- bind_rows(scrs_list, .id = \"data_type\")\n\n## Collect nMDS statistics (stress values)\nstress_values &lt;- map(nmds_list, function(nmds) {\n  data.frame(\"label\" = paste(\"Stress =\", nmds$stress))\n}) %&gt;%\n  bind_rows(.id = \"data_type\")\n</code></pre> <p>If you click on <code>scrs_all</code> in the 'Environment' pane (top right), you will see that it is a data frame with the columns:</p> <ul> <li><code>data_type</code>: Label for data output (weighted MAG coverage; unweighted MAG coverage, ...)</li> <li><code>sample</code>: Sample names</li> <li><code>NMDS1</code> and <code>NMDS2</code>: X and Y coordinates for each plot</li> </ul>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#5-build-the-ordination-plot","title":"5. Build the ordination plot","text":"<p>After obtaining the coordinates (and associated statistics), we can use it as input to <code>ggplot()</code> (the function is <code>ggplot()</code> from the package called <code>ggplot2</code>). As before, we will set our colour palette first. We will also generate a vector of panel headers.</p> <p>code</p> <pre><code># Plot nMDS using ggplot2 ----\n## Set up colours\nmetadata &lt;- metadata %&gt;%\n  mutate(\n    Group = as.factor(Group)\n  )\n\ngroup_colour &lt;- palette.colors(n = length(levels(metadata$Group)),\n                               palette = \"Okabe-Ito\") %&gt;%\n  setNames(., levels(metadata$Group))\n\n## Append sample grouping to scores\nscrs_all &lt;- left_join(scrs_all, metadata, by = c(\"sample\" = \"SampleID\"))\n\n## Create panel headers\npanel_labels &lt;- c(\n  \"MAG_binary_bray\" = \"MAG coverage\\n(unweighted Bray-Curtis)\",\n  \"MAG_bray\" = \"MAG coverage\\n(weighted Bray-Curtis)\",\n  \"virus_binary_bray\" = \"Virus coverage\\n(unweighted Bray-Curtis)\",\n  \"virus_bray\" = \"Virus coverage\\n(weighted Bray-Curtis)\"\n)\n\n## Call ggplot\nggplot(data = scrs_all, aes(x = NMDS1, y = NMDS2, colour = Group)) +\n  # Plot scatterplot\n  geom_point() +\n  # Designate sample group colours\n  scale_colour_manual(values = group_colour, name = \"Sample group\") +\n  # Split plots based on \"data_type\"\n  facet_wrap(~ data_type, labeller = labeller(data_type = panel_labels)) +\n  # Add text annotations\n  geom_text(data = stress_values, aes(label = label), inherit.aes = F,\n            x = 0.4, y = 0.4, vjust = 0, hjust = 0) +\n  # Set general theme\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(), # remove grid lines\n    legend.position = \"bottom\" # set legend position at the bottom\n  )\n</code></pre> <p>The code above can be summarised to the following:</p> <ol> <li>Call <code>ggplot()</code> which initiates an empty plot. It reads in the data frame <code>scrs_all</code> as the main source of data and sets <code>NMDS1</code> and <code>NMDS1</code> as <code>x</code> and <code>y</code> coordinates, as well as designates colour based on the <code>Group</code> column.</li> <li><code>geom_point</code> specifies that we want to plot points (i.e. a scatterplot), and we also set a size for all points.</li> <li><code>scale_colour_manual</code> specifies our preferred colour palette and names our colour legend.</li> <li><code>facet_wrap</code> specifies that we want to panel our figure based on the <code>data_type</code> column and that the labels/headers (a.k.a. <code>labellers</code>) should be printed according to the <code>panel_label</code> object.</li> <li>We also add text annotations using <code>geom_text</code>, but that it should read data from the <code>stress_values</code> data frame and do not use data arguments (via <code>inherit.aes = F</code>) specified above in <code>ggplot()</code>. Text adjustments were made using <code>size</code>, <code>vjust</code> and <code>hjust</code>.</li> <li><code>theme_bw</code> specifies the application of a general, monochromatic theme.</li> <li>Elements of the monochromatic theme were modified based on arguments within <code>theme()</code></li> </ol> <p>You should try and modify any of the arguments above to see what changes: Change sizes, colours, labels, etc...</p> <p><code>ggplot2</code> implements extremely flexible plotting methods. It uses the concept of layers where each line after <code>+</code> adds another layer that modifies the plot. Each geometric layer (the actual plotted data) can also read different inputs. Nearly all aspects of a figure can be modified provided one knows which layer to modify. To start out, it is recommended that you take a glance at the cheat sheet.</p> <p></p> <p>Note</p> <p>How informative these types of analyses are depends in part on the number of samples you actually have and the degree of variation between the samples. As you can see in the nMDS plots based on unweighted (binary) Bray-Curtis dissimilarities (especially for the MAGs data) there are not enough differences between any of the samples (in this case, in terms of community membership, rather than relative abundances) for this to result in a particularly meaningful or useful plot in these cases.</p>"},{"location":"day4/ex16c_OPTIONAL_data_presentation_Ordination/#follow-up-analyses","title":"Follow-up analyses","text":"<p>It is often valuable to follow these visualisations up with tests for \\(\\beta\\)-dispersion (whether or not sample groups have a comparable spread to one another, i.e. is one group of communities more heterogeneous than another?) and, provided that beta-dispersion is not significantly different between groups, PERMANOVA tests (the extent to which the variation in the data can be explained by a given variable (such as sample groups or other environmental factors, based on differences between the centroids of each group).</p> <p>Beta-dispersion can be calculated using the  <code>betadisper()</code> function from the <code>vegan</code> package (passing the <code>bray.dist</code> data and the <code>map$Group</code> variable to group by), followed by <code>anova()</code>, <code>permutest()</code>, or <code>TukeyHSD()</code> tests of differences between the groups (by inputting the generated <code>betadisper</code> output). PERMANOVA tests can be conducted via the <code>adonis()</code> function from the <code>vegan</code> package. For example: </p> <p>code</p> <pre><code>adonis2(MAG_bray ~ Group, data=metadata, permutations=999)\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/","title":"KEGG pathway maps","text":"<p>Objectives</p> <ul> <li>Build a KEGG pathway map using <code>R</code></li> </ul>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#build-a-kegg-pathway-map-using-r","title":"Build a KEGG pathway map using R","text":"<p>In this exercise, we will generate KEGG a pathways map from genome annotations to visualize potential pathways present in our assembled, binned genome sequences.</p> <p>The key package used here is pathview, available from <code>Bioconductor</code> (for installation on your local version of <code>RStudio</code>, see the previous intro to data presentation section). <code>pathview</code> is mainly used in transcriptomic analyses but can be adapted for any downstream analyses that utilise the KEGG database. For this exercise, we are interested in visualising the prevalence of genes that we have annotated in a pathway of interest.</p> <p>To get started, if you're not already, log back in to NeSI's Jupyter hub and open <code>RStudio</code>.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#1-prepare-environment","title":"1. Prepare environment","text":"<p>Set the working directory, load the required packages, and import data.</p> <p>code</p> <pre><code># Set working directories ----\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/kegg_map')\n\n# Load libraries ----\n# Tidyverse packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(tibble)\n\n# Colour package\nlibrary(viridis)\n\n# KEGG maps\nlibrary(pathview)\nlibrary(KEGGREST)\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#2-subset-the-data","title":"2. Subset the data","text":"<p>For this exercise, we will use KEGG Orthologies generated via <code>DRAM</code>. We'll start by loading the annotation file and extracting the relevant column. </p> <p>code</p> <pre><code>dram &lt;- read_tsv(\"dram_annotations.tsv\")\n\nKEGG_annotations &lt;- dram %&gt;%\n  # Subset relevant data\n  select(fasta, scaffold, gene_position, ko_id) %&gt;%\n  # Create a gene_id that matches what is in the bin FASTA files\n  mutate(gene_id = paste0(scaffold, \"_\", gene_position))\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#3-summarise-ko-per-bin","title":"3. Summarise KO per bin","text":"<p>Here, we are interested in the available KO in each bin. Thus, we can summarise the data by the bin to generate a list of KO per bin. Note that some annotations do not have KO numbers attached to them. In these cases, we will filter these data out.</p> <p>Multiple KOs per bin</p> <p>Multiple annotations per bin is possible and not entirely rare, even if you did filter by E-value/bitscore. Some genes may just be very difficult to tell apart based on pairwise sequence alignment annotations. In this case, we are looking for overall trends. Our question here is: Does this MAG have this pathway? We can further refine annotations by comparing domains and/or gene trees to known, characterised gene sequences if gene annotations look suspicious.</p> <p>code</p> <pre><code>KO_bins &lt;- KEGG_annotations %&gt;%\n  # Tally by bin (as column fasta) and KO\n  group_by(fasta, ko_id) %&gt;%\n  tally(name = \"hits\") %&gt;%\n  # Remove rows without valid annotations\n  drop_na()\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#4-identify-pathway-maps-of-interest","title":"4. Identify pathway maps of interest","text":"<p>Before moving on, we must first identify the pathway map ID of our pathway of interest. Lets say, for this exercise, we are interested in the TCA cycle. Here, we will use <code>KEGGREST</code> to access the KEGG database and query it with a search term. </p> <p><code>KEGGREST</code> can also help you identify other information stored in the KEGG database. For more information, the <code>KEGGREST</code> vignette can be viewed using the <code>vignette</code> function in <code>R</code>: <code>vignette(\"KEGGREST-vignette\")</code></p> <p>code</p> <pre><code>keggFind(database = \"pathway\", query = \"TCA cycle\")\n</code></pre> <p>Console output</p> <pre><code>              path:map00020 \n\"Citrate cycle (TCA cycle)\"\n</code></pre> <p>code</p> <pre><code># We find the map ID is 00020 and assign it to an object.\ntca_map_id &lt;- \"00020\"\n</code></pre>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#5-reshaping-the-data-for-input-into-pathview","title":"5. Reshaping the data for input into pathview","text":"<p><code>pathview</code> needs the data as a numeric matrix with IDs as row names and samples/experiments/bins as column names. Here, we will reshape the data into a matrix of counts per KO number in each bin.</p> <p>code</p> <pre><code>KO_matrix &lt;- KO_bins %&gt;%\n  # Create a wide format table\n  pivot_wider(names_from = \"fasta\", \n              values_from = \"hits\", \n              values_fill = NA) %&gt;%\n  # KO numbers become our rownames\n  column_to_rownames(\"ko_id\") %&gt;%\n  # Coercion to matrix\n  as.matrix()\n</code></pre> <p>If you click on <code>KO_matrix</code> in the Environment pane, you can see that it is now a matrix of counts per KO per bin. Bins that do not possess a particular KO number is given NA. Do not worry about that as <code>pathview</code> can deal with that.</p>"},{"location":"day4/ex16d_data_presentation_KEGG_pathways/#6-creating-pathway-map-of-genes-related-to-tca-cycle","title":"6. Creating pathway map of genes related to TCA cycle","text":"<p>Now we can generate images of the KEGG pathway maps using the matrix we just made. For this section, we will try to find genes invovled in the TCA cycle.</p> <p>code</p> <pre><code>pv_bin_0 &lt;- pathview(\n  gene.data = KO_matrix[, \"bin_0\"],\n  pathway.id = tca_map_id,\n  species = \"ko\",\n  out.suffix = \"pv_bin_0\"\n)\n</code></pre> <p>There is no plot output for this command as it automatically writes the results into the current working directory. By default, it names the file as <code>&lt;species&gt;&lt;mapID&gt;.&lt;out.suffix&gt;.png</code>. If this is the first time this is run, it will also write the pathway map's original image file <code>&lt;species&gt;&lt;mapID&gt;.png</code> and the <code>&lt;species&gt;&lt;mapID&gt;.xml</code> with information about how the pathway is connected.</p> <p>Lets take a look at our first output.</p> <p> </p> <p>Boxes highlighted in red means that our MAG has this gene. However, the colour scale is a little strange seeing as there cannot be negative gene annotation hits (its either NA or larger than 0). Also, we know that there are definitely bins with more than 1 of some KO, but the colour highlights do not show that. Lets tweak the code further and perhaps pick better colours. For the latter, we will use the <code>viridis</code> colour package that is good for showing a gradient.</p> <p>code</p> <pre><code># Set colours\npath_colours &lt;- viridis(n = 3, begin = 0.65, end = 1, direction = 1)\n\n# For more information on the viridis package: \n# vignette(\"intro-to-viridis\")\n\n# Plot pathway\npv_bin_0.2 &lt;- pathview(\n  gene.data = KO_matrix[, \"bin_0\"],\n  pathway.id = tca_map_id,\n  species = \"ko\",\n  # Lets make an arbitrary assumption that 5 copies is a lot\n  limit = list(\n    gene = c(1, 5),\n    cpd = c(1, 5)\n  ),\n  bins = list(\n    gene = 4,\n    cpd = 4\n  ),\n  # We are plotting number of hits, so specify TRUE for this\n  # If plotting, say, gene/transcript abundance, set this to FALSE\n  discrete = list(\n    gene = TRUE,\n    cpd = TRUE\n  ),\n  # Tally colours\n  low = path_colours[1],\n  mid = path_colours[2],\n  high = path_colours[3],\n  out.suffix = \"pv_bin_0.2\"\n)\n</code></pre> <p> </p> <p>This plot looks much better. We can see that some genes do have more hits than others. Now, lets propagate this using <code>map(...)</code> based on our bin IDs.</p> <p>code</p> <pre><code>bin_ids &lt;- colnames(KO_matrix)\n\npv_bin_all &lt;- map(bin_ids, function(bin) {\n  # Get column with correct bin ID\n  bin_data &lt;- KO_matrix[, bin]\n  # Prepare output suffix\n  out.suffix = paste0(\"TCA.\", bin)\n  # Plot\n  pathview(\n    gene.data = bin_data,\n    pathway.id = tca_map_id,\n    species = \"ko\",\n    # Lets make an arbitrary assumption that 5 copies is a lot\n    limit = list(\n      gene = c(1, 5),\n      cpd = c(1, 5)\n    ),\n    bins = list(\n      gene = 4,\n      cpd = 4\n    ),\n    # We are plotting number of hits, so specify TRUE for this\n    # If plotting, say, gene/transcript abundance, set this to FALSE\n    discrete = list(\n      gene = TRUE,\n      cpd = TRUE\n    ),\n    # Tally colours\n    low = path_colours[1],\n    mid = path_colours[2],\n    high = path_colours[3],\n    out.suffix = out.suffix\n  )\n})\n</code></pre> <p>Results</p> Bin 0Bin 1Bin 2Bin 3Bin 4Bin 5Bin 6Bin 7Bin 8Bin 9 <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Based on the plots, it seems that not all bins have a complete TCA cycle.</p> <p>Now that you know how to make pathway maps, try it using different pathways of interest!</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/","title":"Gene synteny","text":"<p>Objectives</p> <ul> <li>Build a sulfur assimilation gene alignment figure to investigate gene synteny using <code>R</code></li> </ul>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#build-a-sulfur-assimilation-gene-alignment-figure-to-investigate-gene-synteny-using-r","title":"Build a sulfur assimilation gene alignment figure to investigate gene synteny using <code>R</code>","text":"<p>When investigating the evolution of genomes, we sometimes want to consider not only the presence/absence of genes in a genome, but also how they are arranged (e.g. in an operon). For this exercise, we are going to visualise several sulfur assimilation genes from our MAGs and compare their arrangements.</p> <p>This exercise involves the use of commands in <code>bash</code> and <code>R</code>. Before starting, make sure you have an active terminal, then navigate to the directory <code>11.data_presentation/gene_synteny/</code>. Here, you'll find a copy of the annotations that came from DRAM (<code>dram_annotations.tsv</code>) and a directory of dereplicated bins in <code>dastool_bins/</code>. </p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#1-obtain-contigs-containing-genes-of-interest-r","title":"1. Obtain contigs containing genes of interest (R)","text":"<p>In an open RStudio session, set your working directory to <code>11.data_presentation/gene_synteny/</code> and then load the required libraries and annotation file.</p> <p>code</p> <pre><code># Working directory\nsetwd(\"/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/gene_synteny/\")\n\n# Tidyverse packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(tidyr)\n\n# Gene synteny\nlibrary(genoPlotR)\n\n# KEGG API\nlibrary(KEGGREST)\n\n# Import annotations\ndram &lt;- read_tsv(\"dram_annotations.tsv\")\n</code></pre> <p>In order to be able to assimilate sulfate, prokaryotic genomes need to encode for the following KEGG orthologies which encode for an ATP-dependent thiosulfate/sulfate uptake system:</p> <ul> <li>K02045 (cysA)</li> <li>K02046 (cysU)</li> <li>K02047 (cysW)</li> <li>K02048 (cysP) or K23163 (sbp)</li> </ul> <p>We'll subset the annotation file to obtain relevant rows and columns. In addtion to the bin and KO information, we require the gene position relative to the contig, strandedness, and the start and end position of the gene. </p> <p>code</p> <pre><code>genes_of_interest &lt;- c(\"K02048\", \"K23163\", \"K02046\", \"K02047\", \"K02045\")\n\nsulfate_assimilation &lt;- dram %&gt;%\n  filter(ko_id %in% genes_of_interest) %&gt;%\n  select(fasta, scaffold, gene_position, ko_id, strandedness, start_position, end_position)\n</code></pre>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#2-check-contiguity-of-genes","title":"2. Check contiguity of genes","text":"<p>We need to check that the genes are contiguous (all on the same contig) and sequential. If there gaps between genes, we will need to fill them.</p> <p>code</p> <pre><code>sulfate_assimilation %&gt;%\n  group_by(fasta) %&gt;%\n  distinct(scaffold) %&gt;%\n  tally(name = \"n_contigs\")\n</code></pre> <p>Console output</p> <pre><code># A tibble: 4 \u00d7 2\n  fasta n_contigs\n  &lt;chr&gt;     &lt;int&gt;\n1 bin_5         1\n2 bin_7         2\n3 bin_8         1\n4 bin_9         1\n</code></pre> <p>We find that <code>bin_7</code> is discontiguous. We need to identify how the genes of interest are distributed across the 2 contigs.</p> <p>code</p> <pre><code>sulfate_assimilation %&gt;%\n  group_by(fasta, scaffold) %&gt;%\n  tally(name = \"n_genes\")\n</code></pre> <p>Console output</p> <pre><code># A tibble: 5 \u00d7 3\n# Groups:   fasta [4]\n  fasta scaffold                                 n_genes\n  &lt;chr&gt; &lt;chr&gt;                                      &lt;int&gt;\n1 bin_5 bin_5_NODE_55_length_158395_cov_1.135272       4\n2 bin_7 bin_7_NODE_54_length_158668_cov_0.373555       1\n3 bin_7 bin_7_NODE_96_length_91726_cov_0.379302        4\n4 bin_8 bin_8_NODE_12_length_355954_cov_0.516970       3\n5 bin_9 bin_9_NODE_62_length_149231_cov_0.651774       4\n</code></pre> <p>Let's also check which gene is not contiguous.</p> <p>code</p> <pre><code>sulfate_assimilation %&gt;%\n  filter(fasta == \"bin_7\") %&gt;%\n  select(scaffold, gene_position, ko_id)\n</code></pre> <p>Console output</p> <pre><code># A tibble: 5 \u00d7 3\n  scaffold                                 gene_position ko_id \n  &lt;chr&gt;                                            &lt;dbl&gt; &lt;chr&gt; \n1 bin_7_NODE_54_length_158668_cov_0.373555           136 K02048\n2 bin_7_NODE_96_length_91726_cov_0.379302             18 K02045\n3 bin_7_NODE_96_length_91726_cov_0.379302             19 K02047\n4 bin_7_NODE_96_length_91726_cov_0.379302             20 K02046\n5 bin_7_NODE_96_length_91726_cov_0.379302             21 K23163\n</code></pre> <p>Looks like there is a an operon that consists of sbp and cysAUW, then a discontiguous cysP. </p> <p>We also need to check for gaps between genes of interest.</p> <p>code</p> <pre><code>select(sulfate_assimilation, scaffold, gene_position)\n</code></pre> <p>Console output</p> <pre><code># A tibble: 16 \u00d7 2\n   scaffold                                 gene_position\n   &lt;chr&gt;                                            &lt;dbl&gt;\n 1 bin_5_NODE_55_length_158395_cov_1.135272           128\n 2 bin_5_NODE_55_length_158395_cov_1.135272           132\n 3 bin_5_NODE_55_length_158395_cov_1.135272           133\n 4 bin_5_NODE_55_length_158395_cov_1.135272           134\n 5 bin_7_NODE_54_length_158668_cov_0.373555           136\n 6 bin_7_NODE_96_length_91726_cov_0.379302             18\n 7 bin_7_NODE_96_length_91726_cov_0.379302             19\n 8 bin_7_NODE_96_length_91726_cov_0.379302             20\n 9 bin_7_NODE_96_length_91726_cov_0.379302             21\n10 bin_8_NODE_12_length_355954_cov_0.516970           148\n11 bin_8_NODE_12_length_355954_cov_0.516970           149\n12 bin_8_NODE_12_length_355954_cov_0.516970           150\n13 bin_9_NODE_62_length_149231_cov_0.651774            55\n14 bin_9_NODE_62_length_149231_cov_0.651774            56\n15 bin_9_NODE_62_length_149231_cov_0.651774            57\n16 bin_9_NODE_62_length_149231_cov_0.651774            58\n</code></pre> <p>Here, we can see there is a gap in bin_5 where we need to fill 3 genes.</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#3-subset-relevant-genes-and-positions","title":"3. Subset relevant genes and positions","text":"<p>Based on the explorations above, we need to:</p> <ul> <li>Add genes 129-131 for scaffold <code>bin_5_NODE_55_length_158395_cov_1.135272</code></li> <li>Remove <code>bin_7_NODE_54_length_158668_cov_0.373555</code></li> </ul> <p>code</p> <pre><code># Add genes to bridge genes of interest for bin_5\nbin_5_additions &lt;- dram %&gt;%\n  filter(scaffold == \"bin_5_NODE_55_length_158395_cov_1.135272\" &amp; gene_position %in% 129:131) %&gt;%\n  select(fasta, scaffold, gene_position, ko_id, strandedness, start_position, end_position)\n\n# Remove extra scaffold from bin_7\nsulfate_assimilation &lt;- sulfate_assimilation %&gt;%\n  group_by(scaffold) %&gt;%\n  # Our discontiguous scaffold only has one entry\n  filter(n() &gt; 1)\n\n# Combine both tables\nsulfate_assimilation_coords &lt;- bind_rows(sulfate_assimilation, bin_5_additions) %&gt;%\n  arrange(scaffold, gene_position) %&gt;%\n  mutate(gene_id = paste0(scaffold, \"_\", gene_position))\n</code></pre> <p>We will write out the results for processing in <code>bash</code>.</p> <p>code</p> <pre><code>walk(\n  unique(sulfate_assimilation_coords$fasta),\n  function(bin) {\n    write_tsv(\n      filter(sulfate_assimilation_coords, fasta == bin),\n      paste0(bin, \".goi.tsv\")\n    )\n  }\n)\n</code></pre>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#4-subset-relevant-scaffolds-from-bin-sequences","title":"4. Subset relevant scaffolds from bin sequences","text":"<p>Now, we need to return to the terminal. </p> <p>Check that you're in <code>11.data_presentation/gene_synteny/</code></p> <p>We'll use the previously generated table information to extract the scaffolds (using <code>seqtk</code>) we need from bin files.</p> <p>code</p> <pre><code>module purge\nmodule load seqtk/1.4-GCC-11.3.0\n\nfor i in *.goi.tsv; do\n  bin=$(basename ${i} .goi.tsv)\n  cut -f 2 ${i} | sed '1d' &gt; tmp.scaffolds\n  seqtk subseq dastool_bins/${bin}.fna tmp.scaffolds &gt; ${bin}.goi.fna\ndone\n</code></pre>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#5-compare-scaffolds-using-tblastx","title":"5. Compare scaffolds using tBLASTx","text":"<p>With our scaffolds ready, we can now perform pairwise comparisons between them. You can choose to generate complete combinations for comparisons. Here, we will only compare between bins sequentially. As we are using the <code>-subject</code> flag, the comparisons are single-threaded (we cannot change this).</p> <p>code</p> <pre><code>module purge\nmodule load BLAST/2.16.0-GCC-12.3.0\n\nbin_array=($(ls *.goi.fna | sed -e 's/.goi.fna//g'))\n\ni=0\nmax_i=$(( ${#bin_array[@]}-1 ))\n\nwhile [ $i -lt $max_i ]; do\n  qbin=${bin_array[$i]} # Current bin as query\n  sbin=${bin_array[$(($i+1))]} # Next bin as subject \n\n  printf \"Comparing %s against %s\\n\" ${qbin} ${sbin}\n\n  tblastx -query ${qbin}.goi.fna \\\n          -subject ${sbin}.goi.fna \\\n          -out blast_${qbin}_${sbin}.txt \\\n          -outfmt \"7 std ppos frames\"\n\n  ((i++)) # Increment i after each comparison\ndone\n</code></pre> <code>-outfmt 7</code> <p>You may be familiar with BLAST output format 6 as the tabular output from BLAST. Here, format 7 is the one with a commented header lines. This allows <code>genoPlotR</code> downstream to parse the columns appropriately. The proceeding arguments specifies a standard output <code>std</code> with additional columns for percentage of positive matches <code>ppos</code> and the translated frames for query/subject <code>frames</code>.</p> <p>Now we have all the necessary files. We will return to the RStudio session for plotting.</p>"},{"location":"day4/ex16e_data_presentation_Gene_synteny/#6-plot-gene-synteny","title":"6. Plot gene synteny","text":"<p>We first need to import all the relevant per bin files.</p> <p>code</p> <pre><code># Gene coordinates\ngoi_files &lt;- list.files(pattern = \".*.goi.tsv\")\ngoi &lt;- map(goi_files, function(x) read_tsv(x)) %&gt;%\n  setNames(nm = str_remove(goi_files, \"\\\\..*\"))\n\n# BLAST results\nblast_files &lt;- list.files(pattern = \"blast.*\")\nblast &lt;- map(blast_files, function(x) {\n  read_comparison_from_blast(x)\n}) %&gt;%\n  setNames(nm = str_remove(blast_files, \".txt\"))\n</code></pre> <p>We will also create a data frame that has relevant information about the gene. In this case, we are using the gene symbol.</p> <p>code</p> <pre><code>ko &lt;- unique(bind_rows(goi)$ko_id)\nkegg_records &lt;- keggGet(ko[!is.na(ko)])\nko_annot &lt;- map(kegg_records, function(entry) {\n    data.frame(\"ko_id\" = entry[[\"ENTRY\"]], \n               \"gene_symbol\" = entry[[\"SYMBOL\"]])\n  }) %&gt;%\n  bind_rows()\n</code></pre> <p>We then create tracks of DNA segments and annotations that can be parsed by <code>genoPlotR</code>.</p> <p>code</p> <pre><code># DNA segments\nds_list &lt;- map(goi, function(x) {\n  coords &lt;- x %&gt;%\n    select(gene_id, start_position, end_position, strandedness)\n  colnames(coords) &lt;- c(\"name\", \"start\", \"end\", \"strand\")\n  dna_seg(coords)\n})\n\n# Annotations that include gene symbols\nannot_list &lt;- map(goi, function(x) {\n  data &lt;- left_join(x, ko_annot, by = \"ko_id\") %&gt;%\n    select(gene_id, start_position, end_position, strandedness, ko_id, gene_symbol)\n  annotation(\n    x1 = data$start_position,\n    x2 = data$end_position,\n    text = ifelse(is.na(data$gene_symbol), \"\", data$gene_symbol)\n  )\n})\n</code></pre> <p>Finally, we can plot!</p> <p>code</p> <pre><code>plot_gene_map(\n  dna_segs = ds_list,\n  comparisons = blast,\n  annotations = annot_list,\n  dna_seg_labels = names(ds_list),\n  dna_seg_scale = TRUE,\n  gene_type = \"arrows\"\n)\n</code></pre> <p></p> <p>Seems a little messy, let's clean it up a little by only keeping those with the best matches (i.e., minimum E-value).</p> <p>code</p> <pre><code>filt_blast &lt;- map(blast, function(x) {\n  group_by(x, name1, name2) %&gt;%\n    filter(e_value == min(e_value)) %&gt;%\n    ungroup() %&gt;%\n    as.comparison()\n})\n\n# Re-plot\nplot_gene_map(\n  dna_segs = ds_list,\n  comparisons = filt_blast,\n  annotations = annot_list,\n  dna_seg_labels = names(ds_list),\n  dna_seg_scale = TRUE,\n  gene_type = \"arrows\"\n)\n</code></pre> <p></p> <p>Much better! To further improve the plot, you might want to rearrange the tBLASTx comparisons and perhaps add additional information to the un-annotated genes based on hits to other databases (e.g., Pfam, TIGRFam, UniProt, etc.).</p>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/","title":"CAZy heatmaps","text":"<p>Objectives</p> <ul> <li>Build a basic heatmap from annotation data using <code>R</code></li> </ul>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#build-a-basic-heatmap-from-annotation-data-using-r","title":"Build a basic heatmap from annotation data using <code>R</code>","text":"<p>To get started, if you're not already, log back in to NeSI's Jupyter hub and open <code>RStudio</code>.</p> <p>For this exercise, set <code>11.data_presentation/cazy_heatmap/</code> as the working directory. We will use the DRAM annotation file as well.</p>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#1-import-the-data-into-an-r-dataframe","title":"1. Import the data into an <code>R</code> <code>data.frame</code>","text":"<p>The first thing we need to do is set up the environment by loading relevant libraries then import annotations into <code>R</code>. </p> <p>First, we import our <code>R</code> libraries with the <code>library()</code> command. For this workflow, we need three libraries from the <code>tidyverse</code> package:</p> <p>code</p> <pre><code># Set working directory\nsetwd('/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/cazy_heatmap/')\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(vegan)    # Useful functions for ecological data\nlibrary(pheatmap) # \"Pretty\" heatmap package\nlibrary(viridis)  # Colour palette\n\n# Import data\ndram &lt;- read_tsv(\"dram_annotations.tsv\")\n</code></pre>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#2-subset-the-data","title":"2. Subset the data","text":"<p>Not every column is useful, so we will subset only those that we need from it. We will also create a more informative bin ID for the heatmap that incorporates genus and species level information.</p> <p>code</p> <pre><code>cazy &lt;- dram %&gt;%\n  select(fasta, cazy_id, bin_taxonomy) %&gt;%\n  mutate(cazy_id = str_split(cazy_id, \"; \"),\n         species = if_else(str_detect(bin_taxonomy, \"s__$\"),\n                           str_replace(bin_taxonomy, \".*;g__([^;]+);.*\", \"\\\\1 sp.\"),\n                           str_replace(bin_taxonomy, \".*;s__([^;]+)$\", \"\\\\1\")),\n         bin = paste0(fasta, \": \", species)) %&gt;%\n  unnest(cazy_id)\n</code></pre>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#3-aggregate-the-data","title":"3. Aggregate the data","text":"<p>We will now perform a summarising step, aggregating instances of multiple genes with the same annotation into a single count for each genome. We do this by</p> <ul> <li>For each bin in the data frame<ul> <li>For each annotation in the bin<ul> <li>Count the number of times the annotation is observed</li> </ul> </li> </ul> </li> </ul> <p>For the majority of cases this will probably be one, but there will be a few cases where multiple annotations have been seen.</p> <p>This process is done using the <code>group_by</code> and <code>tally</code> functions from <code>dplyr</code>, again using pipes to pass the data between functions.</p> <p>code</p> <pre><code>cazy_tally &lt;- cazy %&gt;%\n  mutate(cazy_id = str_remove(cazy_id, \"_.*\")) %&gt;%\n  group_by(bin, cazy_id) %&gt;%\n  tally(name = \"hits\") %&gt;%\n  drop_na()\n</code></pre> <p>Console output</p> <pre><code># A tibble: 314 \u00d7 3\n# Groups:   bin [10]\n   bin                            cazy_id  hits\n   &lt;chr&gt;                          &lt;chr&gt;   &lt;int&gt;\n 1 bin_0: Arcobacter nitrofigilis AA3         2\n 2 bin_0: Arcobacter nitrofigilis AA4         1\n 3 bin_0: Arcobacter nitrofigilis AA7         1\n 4 bin_0: Arcobacter nitrofigilis CBM50       1\n 5 bin_0: Arcobacter nitrofigilis CE11        1\n 6 bin_0: Arcobacter nitrofigilis CE4         1\n 7 bin_0: Arcobacter nitrofigilis GH102       1\n 8 bin_0: Arcobacter nitrofigilis GH103       1\n 9 bin_0: Arcobacter nitrofigilis GH13       10\n10 bin_0: Arcobacter nitrofigilis GH17        1\n# \u2026 with 304 more rows\n</code></pre>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#4-coerce-the-data-into-a-wide-data-matrix","title":"4. Coerce the data into a wide data matrix","text":"<p>We now have a data.frame-like object (a tibble) with three columns. We can convert this into a gene matrix using the <code>pivot_wider</code> function from the <code>tidyr</code> library to create a genome x gene matrix in the following form:</p> Bin CAZy_1 CAZy_2 ... CAZy_n bin_0 N. of genes N. of genes ... N. of genes bin_1 N. of genes ... ... ... ... ... ... ... ... bin_9 N. of genes ... ... ... <p>code</p> <pre><code>cazy_matrix &lt;- cazy_tally %&gt;%\n  pivot_wider(names_from = \"bin\", values_from = \"hits\", values_fill = 0) %&gt;%\n  arrange(cazy_id) %&gt;%\n  # pheatmap requires the rownames to make axis labels\n  column_to_rownames(\"cazy_id\") %&gt;%\n  as.matrix()\n</code></pre>"},{"location":"day4/ex16f_OPTIONAL_data_presentation_CAZy_annotations/#5-build-the-plot","title":"5. Build the plot","text":"<p>Finally, we create the actual plot by passing this matrix into the <code>pheatmap</code> library. Given that it is quite big, we'll subset to plot only glycosyl hydrolases (GH).</p> <p>code</p> <pre><code># Set cell colours\ncolours &lt;- viridis(n = 100)\n\n# Normalise the matrix to reduce visual effect of extreme values\nnorm_cazy_matrix &lt;- decostand(cazy_matrix, method = \"log\", 2)\n\n# Plot heatmap for GH hits\npheatmap(norm_cazy_matrix[str_detect(rownames(norm_cazy_matrix), \"GH\"), ], \n         col = colours, fontsize = 5)\n</code></pre> CAZy heatmap plot <p> </p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/","title":"APPENDIX (ex8): Dereplicating data from multiple assemblies","text":"<p>Objectives</p> <ul> <li>Understand the common issues with using <code>dRep</code> and <code>CheckM</code></li> <li>Use <code>CheckM</code> and <code>dRep</code> together to dereplicate a set of genomes</li> <li>De-duplicate viral contigs using <code>BBMap</code>'s <code>dedupe.sh</code></li> </ul>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#using-drep-and-checkm","title":"Using <code>dRep</code> and <code>CheckM</code>","text":"<p>Before we begin to use <code>dRep</code>, it is important to understand the workflow that it applies to a data set. The basic idea of <code>dRep</code> is that genomes or MAGs are processed as follows:</p> <ol> <li>Genomes are scored for completeness and contamination estimates using <code>CheckM</code></li> <li>Genomes are assigned to primary clusters using a quick and rough average nucleotide identity (ANI) calculation</li> <li>Clusters of genomes sharing greater than 90% ANI are grouped together and ANI is calculated using a more sensitive method</li> <li>Where groups of genomes sharing &gt;99% ANI are found, the best (determined by completeness and contamination statistics) is picked as a representative of the cluster</li> </ol> <p>When run on its own, <code>dRep</code> will automatically try to run <code>CheckM</code> in the background. There are two problems with this approach, namely:</p> <ol> <li><code>dRep</code> is written in version 3.6 of the <code>python</code> language, and the version of <code>CheckM</code> avaiable on NeSI is written in version 2.7. These are not compatible with each other</li> <li>There are two parameters in <code>CheckM</code> which speed up the workflow through multithreading, but <code>dRep</code> only has access to one of them</li> </ol> <p>For these reasons, when working on NeSI we run <code>CheckM</code> on our data set first, and then pass the results directly into <code>dRep</code>, avoiding the need for <code>dRep</code> to try to call <code>CheckM</code>.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#use-checkm-and-drep-together-to-dereplicate-a-set-of-mags","title":"Use <code>CheckM</code> and <code>dRep</code> together to dereplicate a set of MAGs","text":"<p>For this exercise, we will be working with a different set of MAGs to the mock community, as there is not enough strain-level variation in the mock metagenome for <code>dRep</code> to actually remove any MAGs.</p> <p>We will write a single slurm script to run all necessary commands, then analyse the content.</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      checkm_drep\n#SBATCH --partition \u00a0 \u00a0 milan\n#SBATCH --time          30:00\n#SBATCH --mem           80GB\n#SBATCH --cpus-per-task 16\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/12.drep_example/\n\n# Step 1\nmodule purge\nmodule load CheckM/1.2.1-gimkl-2022a-Python-3.10.5\ncheckm lineage_wf -t $SLURM_CPUS_PER_TASK \\\n                  --pplacer_threads $SLURM_CPUS_PER_TASK \\\n                  -x fa --tab_table -f checkm.txt \\\n                  input_bins/ checkm_out/\n\n# Step 2\necho \"genome,completeness,contamination\" &gt; dRep.genomeInfo\ncut -f1,12,13 checkm.txt \\\n  | sed 's/\\t/.fa\\t/' \\\n  | sed 's/\\t/,/g' \\\n  | tail -n+2 &gt;&gt; dRep.genomeInfo\n\n# Step 3\nmodule purge\nmodule load drep/2.3.2-gimkl-2018b-Python-3.7.3\n\ndRep dereplicate --genomeInfo dRep.genomeInfo \\\n                 -g input_bins/*.fa \\\n                 -p $SLURM_CPUS_PER_TASK \\\n                 drep_output/\n</code></pre> <p>Walking through this script, step by step, we are performing the following tasks:</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-1","title":"Step 1","text":"<p>This should look familiar to you. Here we are simply loading <code>CheckM</code>, then running it over a set of MAGs to get the completeness and contamination estimates for our data.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-2","title":"Step 2","text":"<p>When running <code>dRep</code>, we have the option to either let <code>dRep</code> execute <code>CheckM</code> in the background, or we can pass a comma-separated file of the MAG name and its statistics. Unfortunately, <code>dRep</code> does not take the <code>CheckM</code> output itself, so we must use some shell commands to reformat the data. To achieve this, we use the following steps:</p> <p>code</p> <pre><code>echo \"genome,completeness,contamination\" &gt; dRep.genomeInfo\n</code></pre> <p>This line creates the header row for the <code>dRep</code> file, which we are calling <code>dRep.genomeInfo</code>.</p> <p>code</p> <pre><code>cut -f1,12,13 checkm.txt | sed 's/\\t/.fa\\t/' | sed 's/\\t/,/g' | tail -n+2 &gt;&gt; dRep.genomeInfo\n</code></pre> <p>This line cuts the columns 1, 12, and 13 from the <code>CheckM</code> output table, which correspond to the MAG name and completeness/contamination estimates. We then redirect these columns using the <code>|</code> character and use them as input in a <code>sed</code> command. Because <code>CheckM</code> reports our MAG names without their FASTA file extension, but <code>dRep</code> requires the extension to be present in the MAG name, we add the trailing <code>.fa</code> with our <code>sed</code> command. This also gives us an opportunity to replace the tab-delimiting character from the <code>CheckM</code> output with the comma character that <code>dRep</code> uses for marking columns in the table. We then pass the output into a second <code>sed</code> command to replace the tab between columns 12 and 13 with a comma.</p> <p>We then use another redirect (<code>|</code>) to pass the resulting text stream to the <code>tail</code> command. The way we are calling <code>tail</code> here will return every row in the text stream except for the first, which means that we are getting all the MAG rows but not their column names. We remove these names because <code>`dRep</code> uses a different naming convention for specifying columns.</p> <p>We append the MAG statistics to the end of the <code>dRep.genomeInfo</code> file, which contains the correct column names for <code>dRep</code>.</p>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#step-3","title":"Step 3","text":"<p>Here we simply load the modules required for <code>dRep</code>, then execute the command. Because of the compatibility issues between the <code>python</code> version required by <code>CheckM</code> and <code>dRep</code>, we use the <code>module purge</code> command to unload all current modules before loading <code>dRep</code>. This removes the <code>CheckM</code> library, and its <code>python2.7</code> dependency, allowing the <code>dRep</code> and <code>python3.6</code> to load correctly.</p> <p>The parameters for <code>dRep</code> are as follows:</p> Parameter Function <code>dereplicate</code> Activate the dereplicate workflow from <code>dRep</code> <code>--genomeInfo</code> Skip quality checking via <code>CheckM</code>, instead use the values in the table provided <code>-g</code> List of MAGs to dereplicate, passed by wildcard <code>-p</code> Number of processors to use <code>drep_output/</code> Output folder for all outputs <p>When <code>dRep</code> finishes running, there are a few useful outputs to examine:</p> <p>Outputs</p> <pre><code>drep_output/dereplicated_genomes/   # The representative set of MAGs\ndrep_output/figures/                # Dendrograms to visualise the clustering of genomes\ndrep_output/data_tables/            # The primary and secondary clustering of the MAGs, and scoring information\n</code></pre>"},{"location":"resources/1_APPENDIX_ex8_Dereplication/#de-duplicate-viral-contigs-using-bbmaps-dedupesh","title":"De-duplicate viral contigs using <code>BBMap</code>'s <code>dedupe.sh</code>","text":"<p>Part of the process for <code>dRep</code> includes measures specific to prokaryotes. Hence, the above approach will not be appropriate for dereplicating viral contigs derived from different assemblies. <code>dedupe.sh</code> from the <code>BBMap</code> suite of tools is one alternative to achieve a similar process for these data.</p> <p><code>dedupe.sh</code> takes a comma separated list of assembly FASTA files as input, and filters out any contigs that are either full duplicates of another contig, or fully contained within another (longer) contig (i.e. matching alignment within another longer contig). <code>minidentity=...</code> sets the minimum identity threshold, and <code>out=...</code> results in a single deduplicated set of contigs as output.</p> <p>An example of how <code>dedupe.sh</code> might be run on multiple FASTA files of assembled viral contigs (e.g. those output by tools such as <code>VIBRANT</code> or <code>VirSorter</code>) is as follows:</p> <p>code</p> <pre><code>cd /path/to/viral/contigs/from/multiple/assemblies/\nmkdir -p dedupe\n\n# load BBMap\nmodule load BBMap/39.01-GCC-11.3.0\n\n# Set infiles\ninfiles=\"assembly_viral_1.fna,assembly_viral_2.fna,assembly_viral_3.fna,assembly_viral_4.fna\"\n\n# Run main analyses \ndedupe.sh threads=1 in=${infiles} \\\n  minidentity=98 exact=f sort=length mergenames=t mergedelimiter=___ overwrite=t \\\n  out=dedupe/dedupe.fa\n</code></pre> <p>Note</p> <p><code>dedupe.sh</code> will dereplicate contigs that are duplicates or are fully contained by another contig, but unfortunately not those that share a partial overlap (i.e. sharing an overlapping region, but with non-overlapping sections hanging off the ends). <code>dedupe.sh</code> does include the functionality to cluster these contigs together (via <code>c</code> and <code>mo</code>) and output as separate FASTA files, but not to then merge these sequences together into a single representative (this appears to have been a \"to do\" item for a number of years). One option in this case could be to develop a method that: outputs all of the clusters, aligns sequences within each cluster, generates a consensus sequence from the alignment (i.e. effectively performing new mini-assemblies on each of the clusters of overlapping contigs), and then adds this back to the deduplicated FASTA output from <code>dedupe.sh</code> (n.b. this is unfortunately a less trivial process than it sounds...).</p>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/","title":"APPENDIX (ex9): Generating input files for VizBin from DAS_Tool curated bins","text":"<p>The final bins that we obtained in the previous step (output from <code>DAS_Tool</code>) have been copied into <code>6.bin_refinement/dastool_out/_DASTool_bins/</code></p>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#1-generalise-bin-naming-and-add-bin-ids-to-sequence-headers","title":"1. Generalise bin naming and add bin IDs to sequence headers","text":"<p>We will first modify the names of our bins to be simply numbered 1 to n bins. We will use a loop to do this, using the wildcard ( <code>*</code> ) to loop over all files in the <code>_DASTool_bins</code> folder, copying to the new <code>example_data_unchopped/</code> folder and renaming as <code>bin_[1-n].fna</code>. The <code>sed</code> command then adds the bin ID to the start of sequence headers in each of the new bin files (this will be handy information to have in the sequence headers for downstream processing).</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Make a new directory the renamed bins\nmkdir example_data_unchopped/\n\n# Copy and rename bins into generic &lt;bin_[0-n].fna&gt; filenames\ni=0\nfor file in dastool_out/_DASTool_bins/*;\ndo\n    # Copy and rename bin file\n    cp ${file} example_data_unchopped/bin_${i}.fna\n    # extract bin ID\n    binID=$(basename example_data_unchopped/bin_${i}.fna .fna)\n    # Add bin ID to sequence headers\n    sed -i -e \"s/&gt;/&gt;${binID}_/g\" example_data_unchopped/bin_${i}.fna\n    # Increment i\n    ((i+=1))\ndone\n</code></pre>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#2-fragment-contigs","title":"2. Fragment contigs","text":"<p>Using the <code>cut_up_fasta.py</code> script that comes with the binning tool <code>CONCOCT</code>, cut contigs into 20k fragments to add better density to the cluster.</p> <p>code</p> <pre><code># Load CONCONCT module\nmodule load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16\n\n# Make directory to add chopped bin data into\nmkdir example_data_20k/\n\n# loop over .fna files to generate chopped (fragmented) files using CONCONT's cut_up_fasta.py\nfor bin_file in example_data_unchopped/*;\ndo\n    bin_name=$(basename ${bin_file} .fna)\n    cut_up_fasta.py -c 20000 -o 0 --merge_last ${bin_file} &gt; example_data_20k/${bin_name}.chopped.fna\ndone\n</code></pre>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#3-concatenate-fragmented-bins","title":"3. Concatenate fragmented bins","text":"<p>Concatenate chopped bins into a single fastA file.</p> <p>Concatenate chopped bins into a single <code>all_bins.fna</code> file to use as input for both subcontig read mapping via <code>Bowtie2</code> and visualisation via <code>VizBin</code>.</p> <p>code</p> <pre><code>cat example_data_20k/*.fna &gt; all_bins.fna\n</code></pre>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#4-read-mapping-of-subcontigs-fragmented-contigs-based-on-20k-length","title":"4. Read mapping of subcontigs (fragmented contigs based on 20k length)","text":""},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#4a-build-mapping-index","title":"4a. Build mapping index","text":"<p>Build <code>Bowtie2</code> mapping index based on the concatenated chopped bins.</p> <p>code</p> <pre><code>mkdir -p read_mapping/\n\nmodule load Bowtie2/2.4.5-GCC-11.3.0\n\nbowtie2-build all_bins.fna read_mapping/bw_bins\n</code></pre>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#4b-map-sample-reads-to-index","title":"4b. Map sample reads to index","text":"<p>Map quality filtered reads to the index using <code>Bowtie2</code>.</p> <p>Example slurm script:</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      6.bin_refinement_mapping\n#SBATCH --partition \u00a0 \u00a0 milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n\nmodule purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0 SAMtools/1.15.1-GCC-11.3.0\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/6.bin_refinement/\n\n# Step 1\nfor i in sample1 sample2 sample3 sample4;\ndo\n\n  # Step 2\n  bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \\\n          -x read_mapping/bw_bins \\\n          -1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \\\n          -S read_mapping/${i}.sam\n\n  # Step 3\n  samtools sort -@ 10 -o read_mapping/${i}.bam read_mapping/${i}.sam\n\ndone\n</code></pre> <p>Resource allocations</p> <p>These settings are appropriate for this workshop's mock data. Full data sets will likely require considerably greater memory and time allocations.</p>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#5-generate-coverage-table-of-subcontigs-contigs-fragmented-based-on-20k-length","title":"5. Generate coverage table of subcontigs (contigs fragmented based on 20k length)","text":"<p>Use <code>MetaBAT</code>'s <code>jgi_summarise_bam_contig_depths</code> to generate a coverage table.</p> <p>code</p> <pre><code># Load module\nmodule load MetaBAT/2.15-GCC-11.3.0\n\n# calculate coverage table\njgi_summarize_bam_contig_depths --outputDepth example_data_20k_cov.txt read_mapping/sample*.bam\n</code></pre>"},{"location":"resources/2_APPENDIX_ex9_Generating_input_files_for_VizBin/#6-generate-annotation-table-for-vizbin","title":"6. Generate annotation table for <code>VizBin</code>","text":"<p>Using the chopped bin files (<code>example_data_20k/</code>) and the coverage table generated above (<code>example_data_20k_cov.txt</code>), we can use the following script to generate an annotation table in the format that <code>VizBin</code> is expecting. Note that here we are including columns for per-sub-contig coverage based on sample1 (see note at the start of this exercise), label (bin ID), and length values, and storing this in <code>all_bins.sample1.vizbin.ann</code>.</p> <p>What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information for sample1 which is associated with that contig in the <code>example_data_20k_cov.txt</code> file, and adding that as a new column to the file. (If you wished to instead look based on sample2, you would need to modify the <code>cut</code> command in the line <code>sample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4)</code> accordingly (e.g. <code>cut -f6</code>).</p> <p>code</p> <pre><code># Set up annotation file headers\necho \"coverage,label,length\" &gt; all_bins.sample1.vizbin.ann\n\n# loop through bin .fna files\nfor bin_file in example_data_20k/*.fna; do\n    # extract bin ID\n    binID=$(basename ${bin_file} .fna)\n    # loop through each sequence header in bin_file\n    for header in `grep \"&gt;\" ${bin_file}`; do\n        contigID=$(echo ${header} | sed 's/&gt;//g')\n        # identify this line from the coverage table (example_data_20k_cov.txt), and extract contigLen (column 2) and coverage for sample1.bam (column 4)\n        contigLen=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f2)\n        sample1_cov=$(grep -P \"${contigID}\\t\" example_data_20k_cov.txt | cut -f4)\n        # Add to vizbin.ann file\n        echo \"${sample1_cov},${binID},${contigLen}\" &gt;&gt; all_bins.sample1.vizbin.ann\n    done\ndone\n</code></pre> <p>We now have the <code>all_bins.fna</code> and <code>all_bins.sample1.vizbin.ann</code> files that were provided at the start of the VizBin exercise.</p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/","title":"APPENDIX (ex11): Normalise per-sample coverage values by average library size (example)","text":"<p>Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by normalising to minimum or average library size alone. </p> <p>In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the <code>cov_table.txt</code> output generated by <code>jgi_summarize_bam_contig_depths</code> above could then be normalised based on average library size. </p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#normalise-to-average-read-depth-via-the-python-script-norm_jgi_cov_tablepy","title":"Normalise to average read depth via the <code>python</code> script <code>norm_jgi_cov_table.py</code>","text":"<p>The script <code>norm_jgi_cov_table.py</code> is available in the folder <code>10.gene_annotation_and_coverage/</code>, and is also available for download for future reference at this link. </p> <p>Note</p> <p>This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected.</p> <p>In brief, this <code>python</code> script leverages the fact that the standard error output from <code>bowtie2</code> includes read counts for each sample. This has been saved in <code>mapping_filtered_bins_&lt;ADD JOB ID&gt;.err</code>, as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that <code>bowtie2</code> processed the samples (based on the loop we provided to <code>bowtie2</code>: <code>for i in sample1 sample2 sample3 sample4</code>), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the <code>*.bam</code> columns) are normalised for each sample based on: <code>(coverage / sample_read_depth) * average_read_depth</code>. Finally, this is saved to the new file with the prefix 'normalised_' (e.g. <code>normalised_bins_cov_table.txt</code>).</p> <p>Note</p> <p>In your own work, if you alternatively chose to use <code>BBMap</code> (and <code>BBMap</code>s <code>covstats</code> output) for the previous coverage calculation step, read counts can similarly be extracted from the <code>scafstats</code> output by searching for the line \"Reads Used: ...\".</p> <p><code>norm_jgi_cov_table.py</code> requires two input arguments, and takes an additional optional output path argument.</p> <ul> <li><code>-c</code>: Coverage table generated by <code>jgi_summarize_bam_contig_depths</code></li> <li><code>-e</code>: Standard error file created by read mapping via <code>bowtie2</code></li> <li><code>-o</code>: (Optional) Path to output directory (must already exist) (default is the current directory).</li> </ul> <p>Run <code>norm_jgi_cov_table.py</code> for microbial bins data and viral contigs, inputting:</p> <ul> <li>A. the <code>bins_cov_table.txt</code> and <code>mapping_filtered_bins_&lt;ADD JOB ID&gt;.err</code> files</li> <li>B. the <code>viruses_cov_table.txt</code> and <code>mapping_viruses_&lt;ADD JOB ID&gt;.err</code> files. </li> </ul> <p>This will generate the outputs <code>normalised_bins_cov_table.txt</code> and <code>normalised_viruses_cov_table.txt</code>. </p> <p>Note</p> <p>If this <code>python</code> script is in the directory you are currently in, you can call it simply by adding <code>./</code> in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path.*</p> <p>code</p> <pre><code>module purge\nmodule load Python/3.8.2-gimkl-2020a\n\n./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins_&lt;ADD JOB ID&gt;.err\n./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses_&lt;ADD JOB ID&gt;.err\n</code></pre>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#what-if-i-only-have-bam-files","title":"What if I only have BAM files?","text":"<p>You can still normalise your data even with BAM files alone. But this will involve an additional step of obtaining library size/read depth information from the BAM files using <code>SAMtools</code>.</p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#1-obtain-library-size-information","title":"1. Obtain library size information","text":"<p>We can extract this based on the output from <code>SAMtools</code> <code>flagstat</code> command.</p> <p>code</p> <pre><code>module purge\nmodule load SAMtools/1.15.1-GCC-11.3.0\n\nfor i in bin_coverage/*.bam; do\n  filename=$(basename $i)\n  libsize=$(($(samtools flagstat $i | head -n 1 | cut -f 1 -d ' ')/2))\n  printf \"%s\\t%d\\n\" $filename $libsize &gt;&gt; libsize.txt\ndone\n</code></pre> <p>Most of the commands in the above code block should be familiar to you. Here, we use a loop to go through all the BAM files (mapping outputs from <code>bowtie2</code>). Something that might be new here is <code>$(($(samtools flagstat sample1.bam | head -n 1 | cut -f 1 -d ' ')/2))</code>. Let's go through the code chunk by chunk, starting inside and moving our way outwards:</p> <ul> <li> <p><code>samtools flagstat $i</code>: This calls the <code>flagstat</code> subcommand which provides some stats about our mapping.If you were to run it on sample1.bam, the results would look like the following (notice in the first line that the first number is double that of read 1 or read 2):</p> <p><code>flagstat</code> output</p> <pre><code>1099984 + 0 in total (QC-passed reads + QC-failed reads)\n1099984 + 0 primary\n0 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n0 + 0 primary duplicates\n1002053 + 0 mapped (91.10% : N/A)\n1002053 + 0 primary mapped (91.10% : N/A)\n1099984 + 0 paired in sequencing\n549992 + 0 read1\n549992 + 0 read2\n549784 + 0 properly paired (49.98% : N/A)\n998676 + 0 with itself and mate mapped\n3377 + 0 singletons (0.31% : N/A)\n6466 + 0 with mate mapped to a different chr\n6267 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre> </li> <li> <p><code>head -n 1</code>: We only want the first line of this output.</p> </li> <li><code>cut -f 1 -d ' '</code>: We only want the first column (first number) in the line that is space delimited.</li> <li><code>$(( ))</code>: Anything encased in these double round brackets will be parsed arithmetically. If you were to do <code>echo $((1+2))</code>, it will print <code>3</code>. Here, we want to perform a division based on the outputs of<code>samtools flagstat ...</code> (notice the <code>/2</code> at the end).</li> </ul> <p>Altogether, we generate, for each BAM file, the output of flagstat, take the first number and divide it by 2. This is our library size (written out to a file as <code>libsize.txt</code>). </p>"},{"location":"resources/3_APPENDIX_ex11_Normalise_coverage_example/#2-normalise-and-scale-contig-coverage","title":"2. Normalise and scale contig coverage","text":"<p>We then use the coverage table and library size files as inputs for a custom R script (you can also find it here). We can call this script via the command line like so:</p> <p>code</p> <pre><code>module purge\nmodule load R/4.2.1-gimkl-2022a\n\n./normalise_jgi_cov.r bins_cov_table.txt libsize.txt\n</code></pre> <p>This script will generate an output in the current directory with the <code>normalised_</code> prefix before the coverage table file name. It will also inform you if there are unequal or unmatched sample names.</p>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/","title":"APPENDIX (ex11) : Viral taxonomy prediction via vContact2","text":"<p>Prior to running this, you need to have run VirSorter2 and CheckV as indicated here.</p>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/#1-predict-genes-via-prodigal-gv","title":"1. Predict genes via prodigal-gv","text":"<p>Prodigal-gv is a variant of prodigal that has been modified to improve gene calling for giant viruses and viruses that use alternative genetic codes. More information is available here.</p> <p>Note</p> <p>If you have already run DRAM-v to annotate viral genes, DRAM-v runs prodigal-gv in the background, so you can also use the genes.faa file generated by DRAM-v as input for vConTACT2 here.</p> <p>code</p> <pre><code># Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Create output directory\nmkdir -p viral_taxonomy\n\n# Create combined nucleotide sequence file\ncat checkv_out/*.fna &gt; checkv_combined.fna\n</code></pre> <p>Example slurm script:</p> <p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      prodigal\n#SBATCH --partition     milan\n#SBATCH --time          00:05:00\n#SBATCH --mem           1GB\n#SBATCH --cpus-per-task 2\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n\n# Load dependencies\nmodule purge\nmodule load prodigal-gv/2.9.0-GCC-11.3.0\n\n# Set up working directories\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Run Prodigal-gv to predict viral genes \nprodigal-gv -p meta -q \\\n         -i checkv_combined.fna \\\n         -a viral_taxonomy/checkv_combined.faa \n</code></pre> <p>Deprecation warning</p> <p>When running the above, you might get a warning:</p> <p><code>/opt/conda/bin/vcontact2_gene2genome:174: DeprecationWarning: 'U' mode is deprecated</code></p> <p>This is okay and will not impact the outcome.</p>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/#2-generate-required-mapping-file-for-vcontact2","title":"2. Generate required mapping file for <code>vContact2</code>","text":"<p>Use <code>vContact2</code>'s <code>vcontact2_gene2genome</code> script to generate the required mapping file from the output of <code>prodigal</code>.</p> <p>code</p> <pre><code># Ensure you are in the correct working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Load modules\nmodule purge\nmodule unload XALT\nmodule load Apptainer/1.2.5 \\\n        MCL/14.137-gimkl-2020a \\\n        DIAMOND/2.1.9-GCC-11.3.0\n\n# All NeSI Filesystems are pre-bound to APPTAINER_BIND\ncontainer=/opt/nesi/containers/vContact2\n\n# Run script\napptainer run $container/vcontact2.simg \\\nvcontact2_gene2genome --proteins viral_taxonomy/checkv_combined.faa \\\n                      --output viral_taxonomy/viral_genomes_g2g.csv \\\n                      -s 'Prodigal-FAA'\n</code></pre>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/#3-run-vcontact2","title":"3. Run <code>vContact2</code>","text":"<p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      vConTACT2\n#SBATCH --partition     milan\n#SBATCH --time          02:00:00\n#SBATCH --mem           20GB\n#SBATCH --cpus-per-task 16\n#SBATCH --error         %x_%j.err\n#SBATCH --output        %x_%j.out\n\n# Working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses\n\n# Load modules\nmodule purge\nmodule unload XALT\nmodule load Apptainer/1.2.5 \\\n        MCL/14.137-gimkl-2020a \\\n        DIAMOND/2.1.9-GCC-11.3.0\n\n# Bind paths\ncontainer=/opt/nesi/containers/vContact2\n\n# Run vConTACT2\napptainer run $container/vcontact2.simg \\\nvcontact2 --raw-proteins viral_taxonomy/checkv_combined.faa \\\n          --rel-mode Diamond \\\n          --threads $SLURM_CPUS_PER_TASK \\\n          --proteins-fp viral_taxonomy/viral_genomes_g2g.csv \\\n          --db 'ProkaryoticViralRefSeq201-Merged' \\\n          --c1-bin /opt/conda/bin/cluster_one-1.0.jar \\\n          --output-dir vConTACT2_Results\n</code></pre>"},{"location":"resources/4_APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2/#4-predict-taxonomy-of-viral-contigs-based-on-output-of-vcontact2","title":"4. Predict taxonomy of viral contigs based on output of <code>vContact2</code>","text":"<p><code>vContact2</code> doesn't actually assign taxonomy to your input viral contigs. It instead provides an output outlining which reference viral genomes your viral contigs clustered with (if they clustered with any at all). Based on how closely they clustered with any reference genome(s), you can then use this to predict the likely taxonomy of the contig. </p> <p>From the <code>vContact2</code> online docs:</p> <p>One important note is that the taxonomic information is not included for user sequences. This means that each user will need to find their genome(s) of interest and check to see if reference genomes are located in the same VC. If the user genome is within the same VC sub-cluster as a reference genome, then there's a very high probability that the user genome is part of the same genus. If the user genome is in the same VC but not the same sub-cluster as a reference, then it's highly likely the two genomes are related at roughly genus-subfamily level. If there are no reference genomes in the same VC or VC sub-cluster, then it's likely that they are not related at the genus level at all.</p> <p>The summary output of <code>vContact2</code> is the file <code>vConTACT2_Results/genome_by_genome_overview.csv</code>. As the comment above notes, one approach would be to search this file for particular contigs of interest, and see if any reference genomes fall into the same viral cluster (VC), using this reference to predict the taxonomy of the contig of interest.</p> <p>The following <code>python</code> script is effectively an automated version of this for all input contigs (Note: this script has not been widely tested, and so should be used with some degree of caution). This script groups together contigs (and reference genomes) that fall into each VC, and then for each, outputs a list of all taxonomies (at the ranks of 'Order', 'Family', and 'Genus', separately) that were found in that cluster. The predictions (i.e. the list of all taxonomies found in the same VC) for each rank and each contig is output to the table <code>tax_predict_table.txt</code>. </p> <p>Note</p> <p>The taxonomies are deliberately enclosed in square brackets (<code>[ ]</code>) to highlight the fact that these are predictions, rather than definitive taxonomy assignments.</p> <p>For future reference, a copy of this script is available for download here</p> <p>Note</p> <p>This script was written for vConTACT2_0.9.19 and vConTACT2_0.11.3, and has not been tested with later versions. </p> <p>A comparable script for earlier versions of vConTACT2 is available here</p> <p>code</p> <pre><code>module purge\nmodule load Python/3.11.6-foss-2023a\n\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/7.viruses/\n\n./tax_predict_vConTACT2_0.9.19.py \\\n-i vConTACT2_Results/genome_by_genome_overview.csv \\\n-o vConTACT2_Results/\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/","title":"APPENDIX (ex15): Prepare input for gene synteny visualisation","text":"<p>In order to produce gene synteny plots using <code>genoPlotR</code> as outlined in Gene synteny, we need to know the annotations and relative nucleotide positions of our genes of interest. We can use our annotations file generated via homology and domain searches (here, we have parsed and aggregated these annotations using an in-house custom script) or DRAM in order to obtain relevant genes and their labels. Gene nucleotide positions relative to assembled contigs/scaffolds can be obtained from <code>prodigal</code> outputs. </p> <p>Additionally, we need BLAST outputs for comparing between genes along their contigs. For this, we rely on outputs from pairwise <code>tBLASTx</code> (translates a nucleotide database then searches it using a translated nucleotide query) to perform sequential comparisons across different bins.</p> <p>For this example, we use our aggregated annotations (provided in <code>10.gene_annotation_and_coverage/example_annotation_tables</code>) and prodigal outputs to generate input files for visualising the synteny of genes that encode the sulfate/thiosulfate ABC transporter involved in assimilating extracellular sulfate. We will begin by subsetting our genes of interest based on KEGG orthology numbers and relevant annotation labels, followed by a pairwise <code>tBLASTx</code> of our contigs.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#1-navigating-the-working-directory","title":"1. Navigating the working directory","text":"<p>We will be working in <code>10.gene_annotation_and_coverage/</code> where you will find the <code>example_annotation_tables/</code>, <code>filtered_bins/</code> and <code>predictions/</code> sub-directories already prepared for you. </p> <p>code</p> <pre><code># Navigate to working directory\ncd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/10.gene_annotation_and_coverage\n\n# View relevant sub-directories\nls example_annotation_tables/\n</code></pre> <p>Terminal output</p> <pre><code>bin_0.annotation.aa  bin_3.annotation.aa  bin_6.annotation.aa  bin_9.annotation.aa\nbin_1.annotation.aa  bin_4.annotation.aa  bin_7.annotation.aa\nbin_2.annotation.aa  bin_5.annotation.aa  bin_8.annotation.aa\n</code></pre> <p>code</p> <pre><code>ls predictions/\n</code></pre> <p>Terminal output</p> <pre><code>bin_0.filtered.genes.faa              bin_5.filtered.genes.faa\nbin_0.filtered.genes.fna              bin_5.filtered.genes.fna\nbin_0.filtered.genes.gbk              bin_5.filtered.genes.gbk\nbin_0.filtered.genes.no_metadata.faa  bin_5.filtered.genes.no_metadata.faa\nbin_0.filtered.genes.no_metadata.fna  bin_5.filtered.genes.no_metadata.fna\nbin_1.filtered.genes.faa              bin_6.filtered.genes.faa\nbin_1.filtered.genes.fna              bin_6.filtered.genes.fna\nbin_1.filtered.genes.gbk              bin_6.filtered.genes.gbk\nbin_1.filtered.genes.no_metadata.faa  bin_6.filtered.genes.no_metadata.faa\nbin_1.filtered.genes.no_metadata.fna  bin_6.filtered.genes.no_metadata.fna\nbin_2.filtered.genes.faa              bin_7.filtered.genes.faa\nbin_2.filtered.genes.fna              bin_7.filtered.genes.fna\nbin_2.filtered.genes.gbk              bin_7.filtered.genes.gbk\nbin_2.filtered.genes.no_metadata.faa  bin_7.filtered.genes.no_metadata.faa\nbin_2.filtered.genes.no_metadata.fna  bin_7.filtered.genes.no_metadata.fna\nbin_3.filtered.genes.faa              bin_8.filtered.genes.faa\nbin_3.filtered.genes.fna              bin_8.filtered.genes.fna\nbin_3.filtered.genes.gbk              bin_8.filtered.genes.gbk\nbin_3.filtered.genes.no_metadata.faa  bin_8.filtered.genes.no_metadata.faa\nbin_3.filtered.genes.no_metadata.fna  bin_8.filtered.genes.no_metadata.fna\nbin_4.filtered.genes.faa              bin_9.filtered.genes.faa\nbin_4.filtered.genes.fna              bin_9.filtered.genes.fna\nbin_4.filtered.genes.gbk              bin_9.filtered.genes.gbk\nbin_4.filtered.genes.no_metadata.faa  bin_9.filtered.genes.no_metadata.faa\nbin_4.filtered.genes.no_metadata.fna  bin_9.filtered.genes.no_metadata.fna\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#2-obtain-annotations-for-genes-of-interest","title":"2. Obtain annotations for genes of interest","text":""},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#21-subset-annotations","title":"2.1 Subset annotations","text":"<p>Based on the KEGG module for sulfate-sulfur assimilation, we need annotations that match <code>K02048</code>, <code>K23163</code>, <code>K02046</code>, <code>K02047</code>, and <code>K02045</code>. Out of all the columns, we are only interested in the gene ID and the KEGG annotations. We know that column 1 is the gene ID, so lets find the column index that contains the KEGG annotations.</p> <p>code</p> <pre><code>grep -P \"K0204[5-8]|K23163\" example_annotation_tables/*.aa \\\n  | awk -F '\\t' '\n      {\n        for (i = 1; i &lt;= NF; i++) {\n          if (match($i, \"K[0-9]{5}\"))\n            printf(\"Column %d contains KO of interest.\\n\", i)\n        }\n      }\n    ' \\\n  | uniq\n\n# Column 26 contains KO of interest.\n</code></pre> <p>Deconstruct the code block above:</p> Code Action <code>grep -P \"K0204[5-8]\\|K23163\" *.aa</code> Finds the rows that contain the pattern for our KO of interest in all annotation files within this directory. <code>awk -F '\\t' '</code> Starts the <code>awk</code> programme to parse the output of <code>grep</code> and tells it that the outputs have tab-delimited columns. <code>for (i = 1; i &lt;= NF; i++) {</code> Starts the C-style <code>for</code> loop within <code>awk</code> by assigning it an initial value of 1 (<code>i = 1</code>), continue the loop as long as the value of <code>i</code> is lower than the total number of rows (<code>i &lt;= NF</code>), and increment by 1 as the loop progresses (<code>i++</code>). <code>if (match($i, \"K[0-9]{5}\"))</code> Starts an <code>if</code> loop and evaluates if a column contains string that matches the pattern <code>K[0-9]{5}</code> (which is the regex for a KO number). This is evaluated per row. <code>printf(\"Column %d contains KO of interest.\\n\", i)</code> If there is a match in the row, print out the resulting column index. <code>uniq</code> Returns only unique results. <p>The code above says that all KO annotations are in column 26. This is true for all annotation files. Lets move on to subset to the KO numbers we want and cut out the columns we need. We will do this per bin.</p> <p>code</p> <pre><code>for bin_number in {0..9}; do\n\n  grep -P \"K0204[5-8]|K23163\" example_annotation_tables/bin_${bin_number}.annotation.aa \\\n    | cut -f 1,26 &gt; bin_${bin_number}.goi.aa\n\n  if [ ! -s bin_${bin_number}.goi.aa ]; then\n    rm bin_${bin_number}.goi.aa\n  fi\n\ndone\n</code></pre> Deconstructing the code block <p>The above code block loops through each bin annotation file to subset relevant rows from the main annotation files (<code>grep -P \"K0204[5-8]|K23163\" example_annotation_tables/bin_${bin_number}.annotation.aa</code>) then selects the first and 26<sup>th</sup> columns (<code>cut -f 1,26</code>). </p> <p>It also evaluates if the output is an empty file (in the case where the KO of interest is not found in the bin annotation) (<code>if [ ! -s bin_${bin_number}.goi.aa ]</code>) and removes it if it is. </p> <p>We have 4 files after running the above code.</p> <p>code</p> <pre><code>ls *.aa\n</code></pre> <p>Terminal output</p> <pre><code>bin_3.goi.aa  bin_5.goi.aa  bin_8.goi.aa  bin_9.goi.aa\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#22-check-for-contiguous-and-sequential-outputs","title":"2.2 Check for contiguous and sequential outputs","text":"<p>We need to check that the genes are contiguous (all on the same contig) and sequential (identify gaps between genes and fill them). Remember that prodigal headers (for which were propagated through our annotation workflows) always have <code>contigID_geneID</code> where <code>geneID</code> is always relative to the order it was found on the contig's nucleotide sequence. With that in mind, lets check for gene contiguity first.</p> <p>code</p> <pre><code>for aa_file in *goi.aa; do\n  bin=$(basename ${aa_file} .goi.aa)\n  contig=$(cut -f 1 ${aa_file} | sed -E 's/_[0-9]+$//g' | sort -u | wc -l)\n\n  if [ $contig -gt 1 ]; then\n    printf \"%s has %d contig(s).\\n\" $bin $contig\n  fi\n\ndone\n</code></pre> <p>Terminal output</p> <pre><code>bin_5 has 2 contig(s).\n</code></pre> Deconstructing the code block <p>The code above loops through all annotation subsets and finds the number of contigs in each file. It does this by using <code>cut -f 1</code> which selects the first columns, then <code>sed -E 's/_[0-9]+$//g'</code> removes the trailing gene numbers of the gene ID, followed by a dereplication of contigs via <code>sort -u</code> and finally counts the number of lines of the results using <code>wc -l</code>.</p> <p>if the bin annotation subset has more than 1 contig (<code>if [ $contig -gt 1 ]; then</code>), then it reports/prints the bin and number of contigs found (<code>printf \"%s has %d contig(s).\\n\" $bin $contig</code>). </p> <p>The code block above shows that we have non-contiguous genes in bin 5. We will need to remove the extra gene(s) originating from another contig. Lets check which contig we need to remove.</p> <p>code</p> <pre><code>cat bin_5.goi.aa\n</code></pre> <p>Terminal output</p> <pre><code>bin_5_NODE_52_length_158668_cov_0.373555_136    K02048: cysP; sulfate ABC transporter substrate-binding protein\nbin_5_NODE_95_length_91726_cov_0.379302_67      K02048: sbp; sulfate-binding protein\nbin_5_NODE_95_length_91726_cov_0.379302_68      K02046: cysT; sulfate transporter CysT\nbin_5_NODE_95_length_91726_cov_0.379302_69      K02047: cysW; sulfate transporter CysW\nbin_5_NODE_95_length_91726_cov_0.379302_70      K02045: cysA; sulfate.thiosulfate ABC transporter ATP-binding protein CysA\n</code></pre> <p>We find that contig <code>bin_5_NODE_52_length_158668_cov_0.373555</code> is not contiguous with the set of genes in contig <code>bin_5_NODE_95_length_91726_cov_0.379302</code>. We will make a note of that. Now lets check for continuity of genes.</p> <p>code</p> <pre><code>for aa_file in *goi.aa; do\n  bin=$(basename ${aa_file} .goi.aa)\n  gene_number=$(cut -f 1 ${aa_file} | sed -E 's/.*_([0-9]+$)/\\1/g' | uniq)\n  echo \"$bin: $(echo ${gene_number[@]})\"\ndone\n</code></pre> <p>Terminal output</p> <pre><code>bin_3: 128 132 133 134\nbin_5: 136 67 68 69 70\nbin_8: 55 56 57 58\nbin_9: 147 148 149 150\n</code></pre> Deconstructing the code block <p>We loop through each of the annotation subset to select the first column (<code>cut -f 1</code>), extract the trailing gene order/number (<code>sed -E 's/.*_([0-9]+$)/\\1/g'</code>) and returns unique entries (<code>uniq</code>). The result is stored as an array named <code>gene_number</code>.</p> <p>It also reports the results of each bin and the array of gene numbers.</p> <p>Results from the above code block shows that bin 3 has a gap between genes. We need to make sure we get the genes in between gene number 128 and 132.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#23-create-subsets-of-annotation-files-with-correct-genes","title":"2.3 Create subsets of annotation files with correct genes","text":"<p>Based on previous checks, we find that we need to:</p> <ul> <li>remove contig <code>bin_5_NODE_52_length_158668_cov_0.373555</code> from bin 5 when we create the final annotation file.</li> <li>fill in gaps in genes for bin 3.</li> </ul> <p>code</p> <pre><code># Remove non-contiguous gene from bin_5.goi.aa\nsed -i '/bin_5_NODE_52_length_158668_cov_0.373555/d' bin_5.goi.aa\n\n# Obtain genes between 129 and 131 in contig bin_3_NODE_53_length_158395_cov_1.135272 and add to annotations\ncat example_annotation_tables/bin_3.annotation.aa \\\n  | grep -E \"bin_3_NODE_53_length_158395_cov_1.135272_\" \\ # Search for required contig\n  | grep -E \"_129|_130|_131\" \\                            # Search for required gene numbers\n  | cut -f 1,26 &gt;&gt; bin_3.goi.aa                           # Select columns 1 and 26\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#24-clean-up-annotation-tables","title":"2.4 Clean up annotation tables","text":"<p>Now we have gene annotations that are contiguous and continuous. Lets clean up the tables by:</p> <ol> <li>Sorting entries by gene order</li> <li>Separating KO from other annotation information</li> <li>Adding headers</li> <li>Replace anything that doesn't have KO numbers with an annotation so we do not get empty fields</li> </ol> <p>code</p> <pre><code>for aa_file in *.aa; do\n  bin=$(basename ${aa_file} .goi.aa)\n  sort -u -k 1 ${aa_file} \\\n    | sed -e 's/: /\\t/g' \\\n    | sed '1i\\Query gene\\tKO\\tAnnotation' \\\n    | awk -F '\\t' -v OFS='\\t' '\n        {\n          if (!match($2, \"K\")) {\n            $3=$2\n          }\n\n          print\n        }\n      ' &gt; ${bin}_cys.txt\ndone\n</code></pre> Deconstructing the code block <p>We loop through each annotation subset, then:</p> Code Action <code>sort -u -k 1</code> Dereplicate and sort entries based on the first column <code>sed '1i\\Query gene\\tKO\\tAnnotation'</code> Add header <code>awk -F '\\t' -v OFS='\\t'</code> Initiate the <code>awk</code> programme by setting the input and output field separator as tabs <code>if (!match($2, \"K\")) {$3=$2}</code> If entries in the second column that do not have a \"K\", use value in the second column as values for the third column <code>print</code> Return/print out all lines"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#3-create-comparison-tables","title":"3. Create comparison tables","text":"<p>After obtaining relevant annotations for our genes of interest, we need to align the sequences against each other in order to identify sequence homology and directionality across our genes of interest. This can be achieved via pairwise sequence alignment using BLAST (or BLAST-like algorithms). Here, we will use <code>tBLASTx</code> to obtain requisite comparison tables. Here, we will do this using the command line. However, you can also use online BLAST as outlined here. This is a 2 step process:</p> <ol> <li>Obtain nucleotide sequences for relevant contigs in each bin</li> <li>Run (sequentially) a pairwise <code>tBLASTx</code> between each bin</li> </ol> Which BLAST algorithms to use? <p>For visualising gene synteny using <code>genoPlotR</code>, one can use outputs of <code>BLASTn</code> or <code>tBLASTx</code>. The determination of which algorithm to use depends on the phylogenetic relationship between MAGs. For closely related genomes, you can use <code>BLASTn</code> (or its variants) given that the genes will likely be conserved at the nucleotide sequence level. However, if your genomes are phylogenetically distant, you will need to use <code>tBLASTx</code>. This allows us to compare gene homology at the amino acid sequence level and retain nucleotide position information.</p>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#31-subset-contigs-per-bin","title":"3.1 Subset contigs per bin","text":"<p>We begin by getting contig headers from our annotation files then using that to search and subset binned contigs. To subset sequences in FASTA format, we will be using <code>seqtk</code></p> <p>code</p> <pre><code>module purge\nmodule load seqtk/1.3-gimkl-2018b\n\nfor gene_file in *_cys.txt; do\n  bin=$(basename ${gene_file} _cys.txt)\n  cut -f 1 ${gene_file} | tail -n+2 | sed -e 's/_[0-9]*$//g' | uniq &gt; ${bin}_cys.contigID\n  seqtk subseq filtered_bins/${bin}.filtered.fna ${bin}_cys.contigID &gt; ${bin}_cys_contig.fna\ndone\n</code></pre>"},{"location":"resources/5_APPENDIX_ex15_Prepare_gene_synteny_inputs/#33-run-tblastx","title":"3.3 Run <code>tBLASTx</code>","text":"<p>We do not need to perform pairwise comparisons for all bin combinations (you can, if you want to). We will run tBLASTx in the order which we want to observe the genes in: <code>bin_3</code>, <code>bin_5</code>, <code>bin_8</code>, then <code>bin_9</code>. To do this, we set an array for bin IDs then run them through sequentially.</p> <p>code</p> <pre><code># Set array\nbin_array=(bin_3 bin_5 bin_8 bin_9)\n\n# Initiate index\ni=0\n\n# Set maximum array index\nmax_i=$(( ${#bin_array[@]}-1 ))\n\n# Run tBLASTx, comparing bins sequentially in the order of the array\nwhile [ $i -lt $max_i ]; do\n  qbin=${bin_array[$i]}\n  sbin=${bin_array[$(($i+1))]}\n  printf \"Comparing %s against %s\\n\" $qbin $sbin\n  tblastx -query ${qbin}_cys_contig.fna -subject ${sbin}_cys_contig.fna \\\n          -out blast_${qbin}_${sbin}.txt -outfmt \"7 std ppos frames\"\n  ((i++)) # Increment i after each comparison\ndone\n</code></pre> <p>The above workflow should generate the required files for the annotation subset (<code>&lt;binID&gt;_cys.txt</code>) and pairwise <code>BLAST</code> comparisons (<code>blast_&lt;query binID&gt;_&lt;subject binID&gt;.txt</code>). Along with these outputs, copy the <code>prodigal</code> predictions (either <code>*.faa</code> or <code>*.fna</code>; do not use <code>no_metadata</code> files) from relevant bins into your working directory and follow the steps outlined in Presentation of data: gene synteny to generate synteny plots.</p>"},{"location":"resources/5_APPENDIX_ex15_gene_synteny_Generate_blast_files/","title":"APPENDIX (ex15): Generating pairwise contig comparisons using online BLAST","text":"<p>There are several ways of getting the BLAST files. <code>genoplotR</code> can read tabular files: either user-generated tab files (read_comparison_from_tab), or from BLAST output (read_comparison_from_blast). To produce files that are readable by <code>genoPlotR</code>, the <code>-m</code> 8 or 9 option should be used in blastall, or <code>-outfmt</code> 6 or 7 with the BLAST+ suite.</p> <p>In this exercise, we are using <code>tblastx</code> on the NCBI website. Alternatively, you can use the command line version of <code>tblastx</code> in BLAST suite to get the same output (but remember to create the database first).</p> <p>Firstly, we will need to get the input <code>.fna</code> files for blast. Navigate to the <code>11.data_presentation/gene_synteny/</code> folder, then we can grab the node of interest and load <code>seqtk</code> on Jupyter to grab the FASTA sequence.</p> <p>code</p> <pre><code>cd /nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/11.data_presentation/gene_synteny/\n\n# Subset node name\nfor i in *cys.txt ;do \ngrep 'bin_' $i | sed 's/.*bin/bin/g;s/cov_\\(.*\\)_.*/cov_\\1/g' | uniq &gt; node_$i;\ndone\n\n# Subset sequence using seqtk\nexport dir=/nesi/nobackup/nesi02659/MGSS_U/&lt;YOUR FOLDER&gt;/9.gene_prediction/filtered_bins/\nmodule load seqtk\n\nfor i in {4,5,7};do \n  seqtk subseq ${dir}/bin_${i}.filtered.fna node_bin_${i}_cys.txt &gt; bin_${i}_cys.fna;\ndone\n</code></pre> <p>Download the <code>*cys.fna</code> files to your local computer and then upload them to the NCBI website for blasting between bin 4 and bin 5, and then again between bin 5 and bin 7. </p> <p>  You will get the output that looks like this. Click 'Download All', and select 'Hit Table (text)'.</p> <p> </p> <p>That's it! Now you will have downloaded two files (one comparing between bin 4 and bin 5, and another between bin 5 and bin 7).</p>"},{"location":"resources/7_command_line_shortcuts/","title":"Useful command line shortcuts","text":""},{"location":"resources/7_command_line_shortcuts/#overview","title":"Overview","text":"<ul> <li>Bash</li> <li>Nano</li> <li>SLURM commands</li> <li>SBATCH flags</li> </ul>"},{"location":"resources/7_command_line_shortcuts/#bash-commands","title":"Bash commands","text":"Command Function Example <code>cat</code> Read and print contents of a file <code>cat my_file.txt</code> <code>grep</code> Search a file for lines containing text of interest <code>grep \"words\" my_file.txt</code> <code>sed</code> Find and replace text in a file <code>sed 's/words/pictures/' my_file.txt</code> <code>head</code> Print the first n lines from a file <code>head -n 10 my_file.txt</code> <code>tail</code> Print the last n lines from a file <code>tail -n 10 my_file.tx</code>t <code>less</code> Partially open a text file for viewing,reading lines as the user scrolls <code>less my_file.txt</code> <code>cut</code> Retrieve particular columns of a file.Uses tab as the delimiting character,can change with <code>-d</code> parameter <code>cut -f1,2,5 -d \",\" my_file.txt</code> <code>paste</code> Opposite of <code>cut</code>, stitch together files in a row-wise manner <code>paste my_file.txt my_other_file.txt</code>"},{"location":"resources/7_command_line_shortcuts/#nano-shortcuts","title":"Nano shortcuts","text":"Command Function Ctrl + X Close the file Ctrl + O Save the file Ctrl + W Search for a string in the file Ctrl + Space Go forward one word in a line Alt + Space Go backwards one word in a line Ctrl + A Go to the beginning of a line Ctrl + E Go to the end of a line Ctrl + W, Ctrl + V Go to the last line of the file Ctrl + K Cut the current line and save it to the clipboard Ctrl + U Paste (**U**n-cut) the text in the clipboard Ctrl + Shift + 6 Select text"},{"location":"resources/7_command_line_shortcuts/#slurm-commands","title":"SLURM commands","text":"Command Function Example <code>sbatch</code> Submits a SLURM script to the queue manager <code>sbatch job.sl</code> <code>squeue</code> Display jobs in the queueDisplay jobs belonging to user usr9999Display jobs on the long partition <code>squeue</code><code>squeue -u usr9999</code><code>squeue -p long</code> <code>sacct</code> Displays all the jobs run by you that dayDisplay job 123456789 <code>sacct</code><code>sacct -j 123456789</code> <code>scancel</code> Cancel a queued or running jobCancel all jobs belonging to usr9999 <code>scancel 123456789</code><code>scancel -u usr9999</code> <code>sshare</code> Shows the Fair Share scores for all projects of which usr9999 is a member <code>sshare -U usr9999</code> <code>sinfo</code> Shows the current state of the SLURM partitions <code>sinfo</code> <code>sview</code> Launches a GUI for monitoring SLURM jobs <code>sview</code>"},{"location":"resources/7_command_line_shortcuts/#sbatch-flags","title":"SBATCH flags","text":"<p>Most of these commands have a single character shortcut that you can use instead. These can be found by running</p> <pre><code>man sbatch\n</code></pre> <p>While logged into NeSI.</p> Command Description Example <code>--job-name</code> The name that will appear when using <code>squeue</code> or <code>sacct</code> <code>#SBATCH --job-name=MyJob</code> <code>--account</code> The account to which your core hours will be 'charged' <code>#SBATCH --account=nesi9999</code> <code>--time</code> Job max walltime. After this duration,the job will be terminated <code>#SBATCH --time=DD-HH:MM:SS</code> <code>--mem</code> Memory required by the job. If exceededthe job will be termintated <code>#SBATCH --mem=30GB</code> <code>--cpu</code> Number of CPUs to use for multithreading <code>#SBATCH --cpu=20</code> <code>--partition</code> NeSI partition for job to run (default is large) <code>#SBATCH --partition=long</code> <code>--output</code> Path and name of standard output file <code>#SBATCH --output=output.out</code> <code>--error</code> Path and name of standard error file <code>#SBATCH --output=output.err</code> <code>--mail-user</code> Address to send mail notifications <code>#SBATCH --mail-user=bob123@gmail.com</code> <code>--mail-type</code> When to send mail notificationsWill send a mail notification at BEGIN/END/FAILWill send message at 80% walltime <code>#SBATCH --mail-type=ALL</code><code>#SBATCH --mail-type=TIME_LIMIT_80</code> <code>--ntasks</code> Number of MPI tasks to useIt is very rare to use this option in bioinformatics"},{"location":"supplementary/supplementary_1/","title":"NeSI Setup","text":"S.1.1 : NeSI Mahuika Jupyter login <ol> <li><p>Follow https://jupyter.nesi.org.nz/hub/login</p></li> <li><p>Enter NeSI username, HPC password and 6 digit second factor token </p></li> <li> <p><p>Choose server options as below</p></p> <ul> <li>make sure to choose the correct project code <code>nesi02659</code>, number of CPUs <code>CPUs=4</code>, memory <code>8 GB</code> prior to pressing  button.</li> </ul> <p></p> </li> <li> <p><p>Once logged in, click the Terminal tile/icon to make sure a terminal session can be launched without any issues</p> <p></p> S.1.2 : Set NeSI HPC Password <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and set your NeSI HPC password as below</li> </ol> <p></p> S.1.3 : Set NeSI HPC Second Factor <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and set your NeSI HPC Second factor as below</li> </ol> <p></p> S.1.4 : Reset NeSI HPC Password <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and Reset your NeSI HPC Second factor as below</li> </ol> <p></p> S.1.5 : Reset NeSI HPC Second Factor <ol> <li>Log into mynesi portal with your Institutional credentials (OR Tuakiri Virtual Home) and Reset your NeSI HPC Second factor as below</li> </ol> <p></p>"},{"location":"supplementary/supplementary_2/","title":"NeSI File system, Working directory and Symlinks","text":"<p>The part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called \u2018folders\u2019), which hold files or other directories.</p> <p>Directories are like places \u2014 at any time while we are using the shell, we are in exactly one place called our current working directory. Commands mostly read and write files in the current working directory, i.e. \u2018here\u2019, so knowing where you are before running a command is important.</p> <p>NeSI Filesystem (For Researchers)</p> <p>All HPC platforms have custom File Systems for the purpose of general use and admin. NeSI Filesystem looks like above </p> <p> </p> <p>This may look a bit obscure but thing of them as different labels for some familiar names such as Desktop, Downloads, Documents</p> <ul> <li><code>/home/username</code> is for user-specific files such as configuration files, environment setup, source code, etc. This will be the default landing file system during a login</li> <li><code>/nesi/project/projectcode</code> is for persistent project-related data, project-related software, etc</li> <li><code>/nesi/nobackup/projectode</code> is a 'scratch space', for data you don't need to keep long term. Old data is periodically deleted from nobackup</li> </ul> <p><code>projectode</code> for this event is <code>nesi02659</code>. If you are to open a NeSI project for your own research, it will have a unique project code with a prefix to represent your affiliated institute and a five digit number (randomly generated). </p> <p>Therefore, full path to persistent and nobackup/scratch file systems will be in the format of </p> <ul> <li><code>/nesi/project/nesi02659</code></li> <li><code>/nesi/nobackup/nesi02659</code></li> </ul>"},{"location":"supplementary/supplementary_2/#symlinks-shortcuts","title":"Symlinks (shortcuts ?)","text":"<p>All of the SummerSchool material will be hosted on <code>/nesi/nobackup/nesi02659</code> file system as it is the largest and fastest filesystem. Also, each one of the attendee has an individual working space in <code>/nesi/nobackup/nesi02659/MGSS_U/</code>. Although this is great in everyway, having to type the full path to access this particular path (or having to remember it) from the default login site (<code>/home</code> filesystem) can be a tedious task. Therefore, we recommend creating a Symbolic link to your individual working directory from <code>/home</code> Think of it as a shortcut from your Desktop \ud83d\ude42</p> <p>Creating a symlink from <code>/home/$USER</code> to nobackup(scratch)</p> <ul> <li>Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material and open a terminal session</li> <li>Let's call the symlink (shortcut) <code>mgss</code></li> <li> <p>Following command will create the <code>mgss</code> symlink from your <code>/home</code> directory to individual working directory in <code>/nesi/nobackup/nesi02659/MGSS_U/</code> <pre><code>ln -s /nesi/nobackup/nesi02659/MGSS_U/$USER ~/mgss\n</code></pre></p> </li> <li> <p>Now, you can access your working directory with    <pre><code>cd ~/mgss\n</code></pre></p> </li> <li>Run <code>pwd</code> to check the current working directory</li> <li><code>pwd</code> commands on symlinks will print the \"relative path\" (location from where we are, rather than from the root of the file system) with respect to \"absolute path\" (entire path from the root directory)</li> <li>Run the command <code>realpath /path/you/want/to/know</code>  to show the absolute path of the symlink. i..e <pre><code>realpath ~/mgss\n</code></pre> OR if you are already at the symlinked path <pre><code>cd ~/mgss\n</code></pre> <pre><code>realpath .\n</code></pre></li> <li>Summary</li> </ul> <p></p>"},{"location":"supplementary/supplementary_2/#jupyter-file-explorer","title":"Jupyter File explorer","text":"<p>Guide  File Explorer to correct working directory</p> <p>Jupyter terminal and file explorer (on left) operate independently of each other. Therefore, changing the directory via terminal to your individual directory will not change the default working directory in explorer. Changing it to your individual directories can be done will couple of click </p> <p></p>"}]}