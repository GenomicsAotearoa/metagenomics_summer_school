# Manually refining bins

### Objectives

* Prepare input files for `VizBin`
* Project a *t-SNE* using `VizBin` and examine bin clusters
* Refine bins by identifying incorrectly assigned contigs
* *Optional:* Refine and filter problematic contigs from bins
* *Optional:* Creating new `VizBin` profiles with different fragment lengths
* *Optional:* Scripts for processing data with `ESOMana`
* *Appendix:* Generating input files for `VizBin` from `DAS_Tool` curated bins

---

### Prepare input files for *VizBin*

[**VizBin**](http://claczny.github.io/VizBin/) is a handy, GUI-based tool for creating ordinations of our binning data using the [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://lvdmaaten.github.io/tsne/) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on [YouTube](https://www.youtube.com/watch?v=NEaUSP4YerM) that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS.

On its own, `VizBin` takes a set of contigs and performs the *t-SNE* projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage `VizBin` only allows a single coverage value per contig, which is not ideal. This is because `VizBin` only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own *t-SNE* projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing `VizBin` with coverage values for sample1 only. 

The only required input file for `VizBin` is a single `.fna` file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement.

*NOTE: When running VizBin, it is often preferable to split long contigs into smaller pieces in order to increase the density of clustering in the **t-SNE**. The data we are working with today are based on our bins output by `DAS_Tool` in the last binning exercise, but have been further processed using the `cut_up_fasta.py` script that comes with the binning tool `CONCOCT` to cut long contigs into 20k fragments. When reviewing our `VizBin` plots and outputs, it is important to remember that here we are looking at the **fragmented sub-contigs**, rather than the full complete contigs (the importance of this will be clear when we are reviewing our `vb_count_table.txt` later in this exercise).*

In the interests of time today, the input files have been generated and are provided in the `6.bin_refinement/` folder: 

* `all_bins.fna` is a concatenation of the bins of *fragmented* sub-contigs (fragmented to 20k)
* `all_bins.sample1.vizbin.ann` is the annotation file containing per-subcontig coverage, label (bin ID), and length values.

For future reference, and for working with your own data, a step-by-step process for generating these files from the curated bins generated by `DAS_Tool` has been provided as an [Appendix](#appendix-generating-input-files-for-vizbin-from-das_tool-curated-bins) at the end of this page.

Let's first have a quick look at the annotation file. 

```bash
head -n5 all_bins.sample1.vizbin.ann
# coverage,label,length
# 17.6361,bin_0.chopped,20000
# 16.2822,bin_0.chopped,20000
# 17.7862,bin_0.chopped,20000
# 16.8073,bin_0.chopped,20000
```

This file is a comma-delimited table (csv file) that presents the information in the way that VizBin expects it. The order of rows in this file corresponds to the order of contigs in the concatentated *fastA* file of our fragmented bins, `all_bins.fna`.

Create a few variations of the *.ann* file with various columns removed, in order to examine the different outputs they can generate.

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

# Make a few different versions of the .ann file with various columns removed
## annotation with bin only (to colour by bin)
cut -f2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.bin_only.ann
## no length, but coverage and bin included
cut -f1,2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.no_length.ann
```

---

### Project a *t-SNE* and examine bin clusters

We can now use these files in `VizBin` to curate the contigs in our bins. We will load and view the data in a few different steps.

*NOTE: Running `VizBin` remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like `VizBin`) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing `VizBin` on your local machine and downloading the relevant input files (e.g. via `scp ...`) to run locally.*

`VizBin` cannot be opened within the NeSI `Jupyter hub`. For today's exercise, open `VizBin` via either:

**A. Install and run `VizBin` locally**

* Download `VizBin-dist.jar` from [here](https://github.com/claczny/VizBin/releases/tag/v1.0.0)
* You may also need to [install Java locally](https://www.java.com/en/download/manual.jsp) (choose the relevant version for your machine).
* Download the required files from NeSI to your machine (via `scp` or right-click > download, within the `Jupyter hub`).
* Double-click `VizBin-dist.jar` to open `VizBin`, and then follow the steps below.

**OR**

**B. Log in via a standard terminal.** 

* Log into NeSI via a standard terminal (*not* the terminal within `Jupyter hub`) and run the following command (*NOTE: this may return an error if X11 fowarding is not set up for your machine. In this case, proceed with the first option above*).

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

java -jar ../tools/vizbin.jar
```

If this fails to open on your PC, or if it runs prohibitively slowly, team up with 2-3 others in the workshop to run through this exercise together on one machine.

#### Loading the input files

Once `VizBin` is open, to get started, simply click the 'Choose...' button then navigate to the *fastA* file.

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_load_fasta.PNG)

Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your **'bin only'** *.ann* file into the 'Annotation file (optional)' field.

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_load_ann.PNG)

#### Executing the *t-SNE*

For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following:

#### Contigs coloured by bin

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_bin_only.PNG)

#### Contigs coloured by bin, sized by length, shaded by coverage

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_all_ann.PNG)

Similar to any other projection technique, we interpret the closeness of points as a proxy for how similar they are, and because of our *.ann* file we can see which contigs belong to the same bin.

---

### Picking refined bins

We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands:

1. Left-click and drag: Highlight an area of the ordination to zoom into
1. Right-click, 'Zoom Out', 'Both Axes': Rest of the view
1. Left-click several points: Create a selection of contigs to extract from the data
1. Right-click, 'Selection', 'Export': Save the selected contigs into a new file
1. Right-click, 'Selection', 'Clear selection': Clear the current selection

How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed.

Today, we will run through an example of selecting potentially problematic (sub)contigs, and then deciding whether or not we want to filter these contigs out of our refined bins. We will use a combination of `VizBin` and `seqmagick` to remove contigs from bins where we do not trust the placement of the contig. We are aiming to reduce each bin to a trusted set of contigs.

### 1. Export *VizBin* clusters

First, for each `VizBin` cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as `cluster_1.fna`. 

Try this for one or two clusters. In practice, we would do this for each `VizBin` cluster, saving each as a new `cluster_n.fna` file.

#### Highlight a cluster to zoom into

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_select_to_zoom.PNG)

#### Select the cluster to export

Left-click several points around the cluster

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_select_cluster.PNG)

#### Export the cluster

Right-click, 'Selection', 'Export'. Save the output as `cluster_1.fna`. 

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_export.PNG)


### 2. Export potentially problematic contigs

#### Select problematic contigs to examine

Zoom in,  make a selection of potentially problematic contigs, and export as above.

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_select_outlier.png)

Try this for one or two problematic contigs (or subsets of contigs). In practice, you could repeat this for all potentially problemtic contigs, saving each export as a new `contigs_n.fna` file.

*NOTE: for the subsequent step using `vizbin_count_table.sh`, all exported cluster files must share a common prefix (e.g. `cluster...fna`), and all files of problematic contigs must also share a common prefix (e.g. `contigs...fna`).*

---

### *Optional:* Refine and filter problematic contigs from bins

#### Create a count table of counts of our problematic contigs across each bin

You'll recall that, prior running `VizBin`, the contigs in our bins were first cut into fragments to improve the density of the clusters in the *t-SNE* projection. As such, the problematic contigs we have exported from `VizBin` are *sub-contig* fragments, rather than full contigs from our bins. It is entirely possible that different fragments of the original contigs have been placed in different clusters during our `VizBin` analysis - including cases where most sub-contigs have clustered with the bin we expect, and a small number have been identified as "problematic" (i.e. clustered with other bins). Based on the information from these extracted problematic sub-contigs, we now have to carefully consider whether or not we want to remove the *full* contig from our bin data.

To do this, we will generate a table containing each exported "problematic" sub-contig, and counts of how many of its sister sub-contigs (each of the other sub-contig fragments derived from the same original parent contig) fall into each `VizBin` cluster.

For this exercise, a folder of the exported files from `VizBin` for all clusters (`cluster_[1-n].fna`) and problematic sub-contigs (`contigs_[1-n].fna`) has been provided at `vizbin_example_exports/`

We will input these files to the shell script `vizbin_count_table.sh` to generate a count table of the exprted subcontigs across each `VizBin` cluster (`vb_count_table.txt`), as well as a working list of contigs to potentially remove from our final bin data (`vb_omit_contigs_tmp.txt`).

For future reference, a copy of this script is available for download [here](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/scripts/).

```bash
./vizbin_count_table.sh -i vizbin_example_exports/
```

The only required input to `vizbin_count_table.sh` is the path to the cluster and contigs files exported from `VizBin`. By default, the script looks for the prefix `cluster...` for the cluster file names, `contig...` for the files of problematic sub-contigs, and the file extension `.fna` for each. The arguments `-s <contig_file_prefix> -c <cluster_file_prefix> -e <fasta_file_extension>` can optionally be provided if your file name formats differ from the default.

View the output count table:

```bash
less vb_count_table.txt
```

Example excerpt:

| Subcontig_ID | Subcontig_vb_cluster | cluster_10_count | cluster_1_count | cluster_2_count | cluster_3_count | cluster_4_count | cluster_5_count | cluster_6_count | cluster_7_count | cluster_8_count | cluster_9_count | Total_count |
| :- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| >bin_5_NODE_826_length_1788_cov_0.154169 |cluster_5   |    0     |  0    |   0    |   0   |    0   |    1   |    0     |  0   |    0     |  0    |   1 |
| >bin_5_NODE_848_length_1686_cov_0.184026   |     cluster_5    |   0    |   0    |   0   |   0    |   0 |     1    |   0    |   0     |  0     |  0    |   1  |
| >bin_9_NODE_4_length_793571_cov_0.517196.33  |   cluster_6   |    0    |   0    |   0    |   0    |   0  |     0   |    1    |   38  |    0   |    0    |   39 |

Note that in the case of the third contig from the excerpt above, the 'problematic' contig is only one of 39 sub-contigs, and all other 38 sub-contigs are in the expected cluster. In this case, we likely do *not* want to remove this contig from the bin.

#### Generate a list of contigs to *exclude* from filtering

Create a list of contigs identified from `vb_count_table.txt` that are *not* to be filtered out by seqmagick in the next step. For example, those contigs that have sub-contigs split across multiple vizbin clusters, and for which it's reasonable to actually keep the contig (such as when a flagged selected sub-contig exported from vizbin is in one unexpected cluster, but all other sub-contigs from that parent contig are in the expected cluster; in this case, you likely *don't* want to filter out the parent contig from the data set moving forward). 

Below is an example. Simply replace the contig IDs between the quotes for as many lines as necessary for your data. 

*NOTES:*

1. *The first line below must always have only one `>` character, while all subsequent lines must have two (i.e. `>>`) to append correctly to the list.*
2. *We want the original contig ID here, *not* the sub-contig, so make sure to remove the `.xx` fragment number at the end if there is one.*

```bash
echo "bin_0_NODE_9_length_392609_cov_1.038712" > vb_keep_contigs.txt
echo "bin_9_NODE_4_length_793571_cov_0.517196" >> vb_keep_contigs.txt
echo "bin_1_NODE_182_length_42779_cov_1.585353" >> vb_keep_contigs.txt
```

#### Create final *vb_omit_contigs_filtered.txt* list of contigs to filter from bins

Using `grep`, filter contigs we wish to keep (after assessing `vb_count_table.txt`) out of the working `vb_omit_contigs_tmp.txt` list. 

This creates `vb_omit_contigs_filtered.txt`, which we will then pass to `seqmagick` to filter these contigs out of our actual bin fasta files.

```bash
grep -v -f vb_keep_contigs.txt vb_omit_contigs_tmp.txt > vb_omit_contigs_filtered.txt
```

#### Filter suspect contigs (based on *VizBin* analysis) from the bin data

Use `seqmagick --exclude-from-file ...` to filter problematic contigs (those contigs listed in `vb_omit_contigs_filtered.txt`) out of the initial *unchopped* bin fasta files, generating final bins for downstream processing.

```bash 
mkdir filtered_bins/

# Load seqmagick
module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3

# filter problematic contigs out of original bin files
for bin_file in example_data_unchopped/*.fna; do
    bin_name=$(basename ${bin_file} .fna)
    seqmagick convert --exclude-from-file vb_omit_contigs_filtered.txt ${bin_file} filtered_bins/${bin_name}.filtered.fna
done
```

Our filtered bins for downstream use are now in `filtered_bins/`

---

### *Optional:* Creating new *VizBin* profiles with different fragment lengths

The data you have been working with was created using the `cut_up_fasta.py` script that comes with the binning tool `CONCOCT`. It was run to cut contigs into 20k fragments, to better add density to the cluster. If you would like to visualise the data using different contig fragment sizes, you can create these using the following commands (replace `YOUR_CONTIG_SIZE` with the size of interest, e.g. `10000`):

```bash
module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16

mkdir custom_chop/

# Fragment contigs within each bin, outputting to custom_chop/
for bin_file in example_data_unchopped/*;
do
    bin_name=$(basename ${bin_file} .fna)
    cut_up_fasta.py -c YOUR_CONTIG_SIZE -o 0 --merge_last ${bin_file} > custom_chop/${bin_name}.chopped.fna
done

# Concatenate the chopped bins into single .fna
cat custom_chop/*.fna > all_bins_custom_chop.fna
```

You can open `all_bins_custom_chop.fna` in VizBin to view the clustering with this new fragmentation threshold. 

If you wish to also provide an annotation file to colour by bin, this can be generated with the following:

```bash
# Set up annotation file headers
echo "label" > custom_chop.vizbin.ann

# loop through custom_chop .fna files
for bin_file in custom_chop/*.fna; do
    # extract bin ID
    binID=$(basename ${bin_file} .fna)
    # loop through each sequence header in bin_file, adding binID to custom_chop.vizbin.ann for each header present
    for header in `grep ">" ${bin_file}`; do
        # Add binID to vizbin.ann for each header present
        echo "${binID}" >> custom_chop.vizbin.ann
    done
done
```

If you wish to generate the full annotation file, including coverage and length values, you will need to go through the process outlined in the [Appendix](#appendix-generating-input-files-for-vizbin-from-das_tool-curated-bins) below.  

---

### *Optional:* Scripts for processing data with *ESOMana*

A suite of tools for creating input files for `ESOMana` can be found on github [here](https://github.com/tetramerFreqs/Binning).

The tool `ESOMana` can be downloaded from [SourceForge](http://databionic-esom.sourceforge.net/).

---

### Appendix: Generating input files for *VizBin* from *DAS_Tool* curated bins

The final bins that we obtained in the previous step (output from `DAS_Tool`) have been copied into `6.bin_refinement/dastool_out/_DASTool_bins/`

**1. Generalise bin naming and add bin IDs to sequence headers**

We will first modify the names of our bins to be simply numbered 1 to n bins. We will use a loop to do this, using the wildcard ( * ) to loop over all files in the _DASTool_bins folder, copying to the new example_data_unchopped/ folder and renaming as bin_[1-n].fna. The sed command then adds the bin ID to the start of sequence headers in each of the new bin files (this will be handy information to have in the sequence headers for downstream processing).

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

# Make a new directory the renamed bins
mkdir example_data_unchopped/

# Copy and rename bins into generic <bin_[0-n].fna> filenames
i=0
for file in dastool_out/_DASTool_bins/*;
do
    # Copy and rename bin file
    cp ${file} example_data_unchopped/bin_${i}.fna
    # extract bin ID
    binID=$(basename example_data_unchopped/bin_${i}.fna .fna)
    # Add bin ID to sequence headers
    sed -i -e "s/>/>${binID}_/g" example_data_unchopped/bin_${i}.fna
    # Increment i
    ((i+=1))
done
```

**2. Fragment contigs**

Using the `cut_up_fasta.py` script that comes with the binning tool `CONCOCT`, cut contigs into 20k fragments to add better density to the cluster.

```bash
# Load CONCONCT module
module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16

# Make directory to add chopped bin data into
mkdir example_data_20k/

# loop over .fna files to generate chopped (fragmented) files using CONCONT's cut_up_fasta.py
for bin_file in example_data_unchopped/*;
do
    bin_name=$(basename ${bin_file} .fna)
    cut_up_fasta.py -c 20000 -o 0 --merge_last ${bin_file} > example_data_20k/${bin_name}.chopped.fna
done
```

**3. Concatenate fragmented bins**

Concatenate chopped bins into a single *fastA* file.

Concatenate chopped bins into a single `all_bins.fna` file to use as input for both subcontig read mapping via `Bowtie2` and visualisation via `VizBin`.

```bash
cat example_data_20k/*.fna > all_bins.fna
```

**4. Read mapping of subcontigs (fragmented contigs based on 20k length)**

**4a. Build mapping index**

Build `Bowtie2` mapping index based on the concatenated chopped bins.

```bash
mkdir -p read_mapping/

module load Bowtie2/2.3.5-GCC-7.4.0

bowtie2-build all_bins.fna read_mapping/bw_bins
```

**4b. Map sample reads to index**

Map quality filtered reads to the index using `Bowtie2`.

Example slurm script:

```bash
#!/bin/bash -e
#SBATCH -A nesi02659
#SBATCH -J 6.bin_refinement_mapping
#SBATCH --time 00:05:00
#SBATCH --mem 1GB
#SBATCH --ntasks 1
#SBATCH --cpus-per-task 10
#SBATCH -e 6.bin_refinement_mapping.err
#SBATCH -o 6.bin_refinement_mapping.out

module purge
module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b

cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

# Step 1
for i in sample1 sample2 sample3 sample4;
do

  # Step 2
  bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \
          -x read_mapping/bw_bins \
          -1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \
          -S read_mapping/${i}.sam

  # Step 3
  samtools sort -@ 10 -o read_mapping/${i}.bam read_mapping/${i}.sam

done
```

*Note: These settings are appropriate for this workshop's mock data. Full data sets will likely require considerably greater memory and time allocations.*

**5. Generate coverage table of subcontigs (contigs fragmented based on 20k length)**

Use `MetaBAT`'s `jgi_summarise_bam_contig_depths` to generate a coverage table.

```bash
# Load module
module load MetaBAT/2.13-GCC-7.4.0

# calculate coverage table
jgi_summarize_bam_contig_depths --outputDepth example_data_20k_cov.txt read_mapping/sample*.bam
```

**6. Generate annotation table for `VizBin`**

Using the chopped bin files (`example_data_20k/`) and the coverage table generated above (`example_data_20k_cov.txt`), we can use the following script to generate an annotation table in the format that `VizBin` is expecting. Note that here we are including columns for per-sub-contig coverage based on *sample1* (see note at the start of this exercise), label (bin ID), and length values, and storing this in `all_bins.sample1.vizbin.ann`.

What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information for sample1 which is associated with that contig in the `example_data_20k_cov.txt` file, and adding that as a new column to the file. (If you wished to instead look based on sample2, you would need to modify the `cut` command in the line `sample1_cov=$(grep -P "${contigID}\t" example_data_20k_cov.txt | cut -f4)` accordingly (e.g. `cut -f6`).

```bash
# Set up annotation file headers
echo "coverage,label,length" > all_bins.sample1.vizbin.ann

# loop through bin .fna files
for bin_file in example_data_20k/*.fna; do
    # extract bin ID
    binID=$(basename ${bin_file} .fna)
    # loop through each sequence header in bin_file
    for header in `grep ">" ${bin_file}`; do
        contigID=$(echo ${header} | sed 's/>//g')
        # identify this line from the coverage table (example_data_20k_cov.txt), and extract contigLen (column 2) and coverage for sample1.bam (column 4)
        contigLen=$(grep -P "${contigID}\t" example_data_20k_cov.txt | cut -f2)
        sample1_cov=$(grep -P "${contigID}\t" example_data_20k_cov.txt | cut -f4)
        # Add to vizbin.ann file
        echo "${sample1_cov},${binID},${contigLen}" >> all_bins.sample1.vizbin.ann
    done
done
```

We now have the `all_bins.fna` and `all_bins.sample1.vizbin.ann` files that were provided at the [start of this exercise](#prepare-input-files-for-vizbin).

---
