# Per-sample coverage and assigning taxonomy

### Objectives

* [Calculate per-sample coverage stats for the filtered prokaryote bins](#calculate-per-sample-coverage-stats-of-the-filtered-prokaryote-bins)
* [Calculate per-sample coverage stats for the viral contigs output by `VIBRANT`](#calculate-per-sample-coverage-stats-of-viral-contigs)
* [*Optional*: Normalise per-sample coverage values by average library size](#optional-normalise-per-sample-coverage-values-by-average-library-size)
* [Assign taxonomy to the refined bins](#assign-taxonomy-to-the-refined-bins)
* [Introduction to `vContact2` for predicting taxonomy of viral contigs](#introduction-to-vcontact2-for-predicting-taxonomy-of-viral-contigs)

---

### Calculate per-sample coverage stats of the filtered prokaryote bins

One of the first questions we often ask when studying the ecology of a system is: What are the pattens of abundance and distribution of taxa across the different samples? With bins of metagenome-assembled genome (MAG) data, we can investigate this by mapping the quality-filtered unassembled reads back to the refined bins to then generate coverage profiles. Genomes in higher abundance in a sample will contribute more genomic sequence to the metagenome, and so the average depth of sequencing coverage for each of the different genomes provides a proxy for abundance in each sample. 

As per the preparation step at the start of the binning process, we can do this using read mapping tools such as `Bowtie`, `Bowtie2`, and `BBMap`. Here we will follow the same steps as before using `Bowtie2`, `samtools`, and `MetaBAT`'s `jgi_summarize_bam_contig_depths`, but this time inputting our refined filtered bins. 

These exercises will take place in the `8.coverage_and_taxonomy/` folder. Our final filtered refined bins from the previous bin refinement exercise have been copied to the `8.coverage_and_taxonomy/filtered_bins/` folder.

First, concatenate the bin data into a single file to then use to generate an index for the read mapper.

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/

cat filtered_bins/*.fna > filtered_bins.fna
```

Now build the index for `Bowtie2` using the concatenated bin data. We will also make a new directory `bin_coverage/` to store the index and read mapping output into.

```bash
mkdir -p bin_coverage/

# Load Bowtie2
module load Bowtie2/2.3.5-GCC-7.4.0

# Build Bowtie2 index
bowtie2-build filtered_bins.fna bin_coverage/bw_bins
```

Map the quality-filtered reads (from `../3.assembly/`) to the index using `Bowtie2`, and sort and convert to `.bam` format via `samtools`.

Create a new script

```bash
nano mapping_bins.sl
```

Paste in the script (replacing `<YOUR FOLDER>`)

```bash 
#!/bin/bash -e
#SBATCH -A nesi02659
#SBATCH --res SummerSchool
#SBATCH -J mapping_filtered_bins
#SBATCH --time 00:05:00
#SBATCH --mem 1GB
#SBATCH --ntasks 1
#SBATCH --cpus-per-task 10
#SBATCH -e mapping_filtered_bins.err
#SBATCH -o mapping_filtered_bins.out
#SBATCH --export NONE

export SLURM_EXPORT_ENV=ALL

module purge
module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b

cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/

# Step 1
for i in sample1 sample2 sample3 sample4;
do

  # Step 2
  bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \
          -x bin_coverage/bw_bins \
          -1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \
          -S bin_coverage/${i}.sam

  # Step 3
  samtools sort -@ 10 -o bin_coverage/${i}.bam bin_coverage/${i}.sam

done
```

Submit the script

```bash
sbatch mapping_bins.sl
```

Finally, generate the per-sample coverage table for each contig in each bin via `MetaBAT`'s `jgi_summarize_bam_contig_depths`.

```bash 
# Load MetaBAT
module load MetaBAT/2.13-GCC-7.4.0

# calculate coverage table
jgi_summarize_bam_contig_depths --outputDepth bins_cov_table.txt bin_coverage/sample*.bam
```

The coverage table will be generated as `bins_cov_table.txt`. As before, the key columns of interest are the `contigName`, and each `sample[1-n].bam` column.

*NOTE: Here we are generating a per-sample table of coverage values for **each contig** within each bin. To get per-sample coverage of **each bin** as a whole, we will need to generate average coverage values based on all contigs contained within each bin. We will do this in `R` during our data visualisation exercises on day 4 of the workshop, leveraging the fact that we added bin IDs to the sequence headers.*

#### Normalising coverage values

Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. 

For an example of one way in which the `cov_table.txt` output generated by `jgi_summarize_bam_contig_depths` above could then be normalised based on average library size, see the section [Normalise per-sample bin coverage values](#optional-normalise-per-sample-coverage-values-by-average-library-size) below.

---

### Calculate per-sample coverage stats of viral contigs

Here we can follow the same steps as outlined above for the bin data, but with a concatenated *fastA* file of viral contigs. 

To quickly recap: 

* In previous exercises, we first used `VIBRANT` to identify viral contigs from the assembled reads, generating a new fasta file of viral contigs: `spades_assembly.m1000.phages_combined.fna` 
* We then processed this file using `CheckV` to generate quality information for each contig, and to further trim any retained (prokaryote) sequence on the ends of prophage contigs. 

The resultant *fasta* files generated by `CheckV` (`proviruses.fna` and `viruses.fna`) have been copied to to the `8.coverage_and_taxonomy/checkv` folder for use in this exercise.

*NOTE: due to the rapid mutation rates of viruses, with full data sets it will likely be preferable to first further reduce viral contigs down based on a percentage-identity threshold using a tool such as `BBMap`'s `dedupe.sh`. This would be a necessary step in cases where you had opted for generating multiple individual assemblies or mini-co-assemblies (and would be comparable to the use of a tool like `dRep` for prokaryote data), but may still be useful even in the case of single co-assemblies incorporating all samples.*

We will first need to concatenate these files together.

```bash
cat checkv/proviruses.fna checkv/viruses.fna > checkv_combined.fna
```

Now build the index for `Bowtie2` using the concatenated viral contig data. We will also make a new directory `viruses_coverage/` to store the index and read mapping output into.

```bash
mkdir -p viruses_coverage/

# Load Bowtie2
module load Bowtie2/2.3.5-GCC-7.4.0

# Build Bowtie2 index
bowtie2-build checkv_combined.fna viruses_coverage/bw_viruses
```

Map the quality-filtered reads (from `../3.assembly/`) to the index using `Bowtie2`, and sort and convert to `.bam` format via `samtools`.

Create a new script

```bash
nano mapping_viruses.sl
```

Paste in the script (replacing `<YOUR FOLDER>`)

```bash 
#!/bin/bash -e
#SBATCH -A nesi02659
#SBATCH --res SummerSchool
#SBATCH -J mapping_filtered_viruses
#SBATCH --time 00:05:00
#SBATCH --mem 1GB
#SBATCH --ntasks 1
#SBATCH --cpus-per-task 10
#SBATCH -e mapping_filtered_viruses.err
#SBATCH -o mapping_filtered_viruses.out
#SBATCH --export NONE

export SLURM_EXPORT_ENV=ALL

module purge
module load Bowtie2/2.3.5-GCC-7.4.0 SAMtools/1.8-gimkl-2018b

cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy/

# Step 1
for i in sample1 sample2 sample3 sample4;
do

  # Step 2
  bowtie2 --minins 200 --maxins 800 --threads 10 --sensitive \
          -x viruses_coverage/bw_viruses \
          -1 ../3.assembly/${i}_R1.fastq.gz -2 ../3.assembly/${i}_R2.fastq.gz \
          -S viruses_coverage/${i}.sam

  # Step 3
  samtools sort -@ 10 -o viruses_coverage/${i}.bam viruses_coverage/${i}.sam

done
```

Run the script

```bash
sbatch mapping_viruses.sl
```

Finally, generate the per-sample coverage table for each viral contig via `MetaBAT`'s `jgi_summarize_bam_contig_depths`.

```bash 
# Load MetaBAT
module load MetaBAT/2.13-GCC-7.4.0

# calculate coverage table
jgi_summarize_bam_contig_depths --outputDepth viruses_cov_table.txt viruses_coverage/sample*.bam
```

The coverage table will be generated as `viruses_cov_table.txt`. As before, the key columns of interest are the `contigName`, and each `sample[1-n].bam` column.

*NOTE: Unlike the prokaryote data, we have not used a binning process on the viral contigs (since many of the binning tools use hallmark characteristics of prokaryotes in the binning process). Here, `viruses_cov_table.txt` is the final coverage table. This can be combined with `CheckV` quality and completeness metrics to, for example, examine the coverage profiles of only those viral contigs considered to be "High-quality" or "Complete".* 

#### Normalising coverage values

Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. 

For an example of one way in which the `cov_table.txt` output generated by `jgi_summarize_bam_contig_depths` above could then be normalised based on average library size, see the section [Normalise per-sample bin coverage values](#optional-normalise-per-sample-coverage-values-by-average-library-size) below.

---

### *Optional*: Normalise per-sample coverage values by average library size

Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by calculating something like [RPKM](https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/) to normalise to both library size (sample sequencing depth) and gene length, or simply by normalising to minimum or average library size alone. 

In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the `cov_table.txt` output generated by `jgi_summarize_bam_contig_depths` above could then be normalised based on average library size. 

#### Normalise to average read depth via the *python* script `norm_jgi_cov_table.py`

The script `norm_jgi_cov_table.py` is available in the folder `8.coverage_and_taxonomy/`, and is also available for download for future reference at [this link](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/scripts/norm_jgi_cov_table.py). 

*NOTE: This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected.*

In brief, this `python` script leverages the fact that the standard error output from `bowtie2` includes read counts for each sample. This has been saved in `mapping_filtered_bins.err`, as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that `bowtie2` processed the samples (based on the loop we provided to `bowtie2`: `for i in sample1 sample2 sample3 sample4`), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the `*.bam` columns) are normalised for each sample based on: `(coverage / sample_read_depth) * average_read_depth`. Finally, this is saved to the new file with the prefix 'normalised_' (e.g. `normalised_bins_cov_table.txt`).

*NOTE: In your own work, if you alternatively chose to use `BBMap` (and `BBMap`s `covstats` output) for the previous coverage calculation step, read counts can similarly be extracted from the `scafstats` output by searching for the line "Reads Used: ...".*

`norm_jgi_cov_table.py` requires two input arguments, and takes an additional optional output path argument.

* `-c`: Coverage table generated by `jgi_summarize_bam_contig_depths`
* `-e`: Standard error file created by read mapping via `bowtie2`
* `-o`: (Optional) Path to output directory (must already exist) (default is the current directory).

Run `norm_jgi_cov_table.py` for microbial bins data and viral contigs, inputting:

* A. the `bins_cov_table.txt` and `mapping_filtered_bins.err` files
* B. the `viruses_cov_table.txt` and `mapping_filtered_viruses.err` files. 

This will generate the outputs `normalised_bins_cov_table.txt` and `normalised_viruses_cov_table.txt`. 

*NOTE: If this `python` script is in the directory you are currently in, you can call it simply by adding `./` in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path.*

```bash
module purge
module load Python/3.8.2-gimkl-2020a

./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins.err
./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses.err
```

---

### Assign taxonomy to the refined bins

It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us:

1. Extract 16S rRNA gene sequences from the MAGs and classify them
1. Annotate each gene in the MAG and take the consensus taxonomy
1. Use a profiling tool like `Kraken`, which matches pieces of DNA to a reference database using *k*-mer searches
1. Identify a core set of genes in the MAG, and use these to compute a species phylogeny

For this exercise, we will use the last option in the list, making use of the `GTDB-TK` software (available on [github](https://github.com/Ecogenomics/GTDBTk)) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, `GTDB-TK` will perform the following steps on a set of bins.

1. Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG.
1. Based on the recovered numbers, identify which domain is a more likely assignment for each MAG
1. Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions
1. Filter the alignment down to approximately 5,000 informative sites
1. Insert each MAG into a reference tree create from type material and published MAGs
1. Scale the branch lengths of the resulting tree, as described in [Parks et al.](https://www.ncbi.nlm.nih.gov/pubmed/30148503), to identify an appropriate rank to each branch event in the tree
1. Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree
1. Report the resulting taxonomic assignment, and gene alignment

This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process.

Create a new script

```bash
nano gtdbtk_test.sl
```

Paste in the script (replacing `<YOUR FOLDER>`)

```bash
#!/bin/bash
#SBATCH -A nesi02659
#SBATCH -J gtdbtk_test
#SBATCH --res SummerSchool
#SBATCH --time 00:30:00
#SBATCH --mem 140GB
#SBATCH --cpus-per-task 10
#SBATCH -e gtdbtk_test.err
#SBATCH -o gtdbtk_test.out
#SBATCH --export NONE

export SLURM_EXPORT_ENV=ALL

module purge
module load GTDB-Tk/0.2.2-gimkl-2018b-Python-2.7.16

cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/8.coverage_and_taxonomy

gtdbtk classify_wf -x fna --cpus 10 --genome_dir filtered_bins/ --out_dir gtdbtk_out/
```

Submit the script

```bash
sbatch gtdbtk_test.sl
```

As usual, lets look at the parameters here

|Parameter|Function|
|:---|:---|
|**classify_wf**|Specifies the sub-workflow from `GTDB-TK` that we wish to use|
|**-x ...**|Specify the file extension for MAGs within our input directory.<br>Default is *.fna*, but it's always good practice to specify it anyway|
|**--cpus ...**|Number of CPUs to use when finding marker genes, and performing tree insertion operations|
|**--genome_dir ...**|Input directory containing MAGs as individual *fastA* files|
|**--out_dir ...**|Output directory to write the final set of files|

Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw `DAS_Tool` outputs in the `../6.bin_refinement/dastool_out/_DASTool_bins/` folder, the renamed set of bins in the `../6.bin_refinement/example_data_unchopped/` folder, the set of curated bins in the `filtered_bins/` folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here.

When the task completes, you will have a number of output files provided. The main ones to look for are `gtdbtk.bac120.summary.tsv` and `gtdbtk.arch122.summary.tsv` which report the taoxnomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the `gtdbtk.arch122.summary.tsv` file.

If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the `gtdbtk.bac120.msa.fasta` and `gtdbtk.arch122.msa.fasta` files.

Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop.

---

### Introduction to *vContact2* for predicting taxonomy of viral contigs

Even more so than prokaryote taxonomy, establishing a coherent system for viral taxonomy is complex and continues to evolve. Just in the last year, the International Committee on Taxonomy of Viruses ([ICTV](https://talk.ictvonline.org/)) overhauled the classification code into [15 hierarchical ranks](https://www.nature.com/articles/s41564-020-0709-x). Furthermore, the knowledge gap in databases of known and taxonomically assigned viruses remains substantial, and so identifying the putative taxonomy of viral contigs from environmental metagenomics data remains challenging.

There are a number of approaches that can be used to attempt to predict the taxonomy of the set of putative viral contigs output by programs such as `VIBRANT`, `VirSorter`, and `VirFinder`. [vContact2](https://www.nature.com/articles/s41587-019-0100-8) is one such method that uses 'guilt-by-contig-association' to predict the potential taxonomy of viral genomic sequence data based on relatedness to known viruses within a reference database (such as viral RefSeq). The principle is that, to the extent that the 'unknown' viral contigs cluster closely with known viral genomes, we can then expect that they are closely related enough to be able to predict a shared taxonomic rank. 

*NOTE: Anecdotally, however, in my own experience with this processes I have unfortunately been unable to predict the taxonomy of the vast majority of the viral contigs ouput by `VIBRANT`, `VirSorter`, or `VirFinder` from an environmental metagenomic data set (due to not clustering closely enough with known viruses in the reference database).*

Running `vContact2` can require a considerable amount of computational resources, and so we won't be running this in the workshop today. The required process is outlined for reference in an [Appendix for this exercise](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/resources/APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2.md), should you wish to experiment with this on your own data in the future. 

For today, we have provided the final two output files from this process when applied to our mock metagenome data. These can be viewed in the folder `8.coverage_and_taxonomy/vConTACT2_Results/` via `head` or `less`.

```bash
less vConTACT2_Results/genome_by_genome_overview.csv
```

```bash
less vConTACT2_Results/tax_predict_table.txt
```

A few notes to consider: 

* You will see that the `genome_by_genome_overview.csv` file contains entries for the full reference database used as well as the input viral contigs (contigs starting with `NODE`). 
* You can use a command such as `grep "NODE" vConTACT2_Results/genome_by_genome_overview.csv | less` to view only the lines for the input contigs of interest. 
  * Note also that these lines however will *not* contain taxonomy information. 
  * See the notes in the [Appendix](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/resources/APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2.md) for further information about why this might be.
* As per the notes in the [Appendix](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/resources/APPENDIX_ex11_viral_taxonomy_prediction_via_vContact2.md), the `tax_predict_table.txt` file contains *predictions* of potential taxonomy (and or taxonom*ies*) of the input viral contigs for order, family, and genus, based on whether they clustered with any viruses in the reference database.
  * Bear in mind that these may be lists of *multiple* potential taxonomies, in the cases where viral contigs clustered with multiple reference viruses representing more than one taxonomy at the given rank.
  * *NOTE: The taxonomies are deliberately enclosed in square brackets (`[ ]`) to highlight the fact that these are **predictions**, rather than definitive taxonomy **assignments**.*

---
