### *APPENDIX (ex11)*: Normalise per-sample coverage values by average library size (example)

Having generated per-sample coverage values, it is usually necessary to also normalise these values across samples of differing sequencing depth. Commonly, this is done by calculating something like [RPKM](https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/) to normalise to both library size (sample sequencing depth) and gene length, or simply by normalising to minimum or average library size alone. 

In this case, the mock metagenome data we have been working with are already of equal depth, and so this is an unnecessary step for the purposes of this workshop. The steps below are provided for future reference as an example of one way in which the `cov_table.txt` output generated by `jgi_summarize_bam_contig_depths` above could then be normalised based on average library size. 

#### Normalise to average read depth via the *python* script `norm_jgi_cov_table.py`

The script `norm_jgi_cov_table.py` is available in the folder `8.coverage_and_taxonomy/`, and is also available for download for future reference at [this link](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/scripts/norm_jgi_cov_table.py). 

*NOTE: This script was developed as a simple example for this workshop. It has not yet been widely tested: it is recommended in early usage to manually check a few values to ensure the conversions in the output file are as expected.*

In brief, this `python` script leverages the fact that the standard error output from `bowtie2` includes read counts for each sample. This has been saved in `mapping_filtered_bins.err`, as per the slurm script that was submitted for the read mapping step. Note that, since we know the order that `bowtie2` processed the samples (based on the loop we provided to `bowtie2`: `for i in sample1 sample2 sample3 sample4`), we know that the read count lines in the output error file will appear in the same order. We can therefore iterate through each of these lines, extracting the individual sample read count each time. These values are then used to calculate the average read depth for all samples. Coverage values (in each of the `*.bam` columns) are normalised for each sample based on: `(coverage / sample_read_depth) * average_read_depth`. Finally, this is saved to the new file with the prefix 'normalised_' (e.g. `normalised_bins_cov_table.txt`).

*NOTE: In your own work, if you alternatively chose to use `BBMap` (and `BBMap`s `covstats` output) for the previous coverage calculation step, read counts can similarly be extracted from the `scafstats` output by searching for the line "Reads Used: ...".*

`norm_jgi_cov_table.py` requires two input arguments, and takes an additional optional output path argument.

* `-c`: Coverage table generated by `jgi_summarize_bam_contig_depths`
* `-e`: Standard error file created by read mapping via `bowtie2`
* `-o`: (Optional) Path to output directory (must already exist) (default is the current directory).

Run `norm_jgi_cov_table.py` for microbial bins data and viral contigs, inputting:

* A. the `bins_cov_table.txt` and `mapping_filtered_bins.err` files
* B. the `viruses_cov_table.txt` and `mapping_filtered_viruses.err` files. 

This will generate the outputs `normalised_bins_cov_table.txt` and `normalised_viruses_cov_table.txt`. 

*NOTE: If this `python` script is in the directory you are currently in, you can call it simply by adding `./` in front of the script name. If you have saved the script elsewhere, you will need to add the absolute path to the script, or add the script to your bin path.*

```bash
module purge
module load Python/3.8.2-gimkl-2020a

./norm_jgi_cov_table.py -c bins_cov_table.txt -e mapping_filtered_bins.err
./norm_jgi_cov_table.py -c viruses_cov_table.txt -e mapping_filtered_viruses.err
```

---
